# Machine Learning with `mlr3` {#sec-mlr3}

The `mlr3` package is an R package that makes it easy for you to implement standard machine learning procedures (e.g., training, prediction, cross-validation) in a unified framework. It is similar to the Python scikit-learn package. In this section, we cover the basic use cases of the `mlr3` package accompanied by an real-world example at the end. The authors of the package is currently writing a [book](https://mlr3book.mlr-org.com/02-basics-learners.html) on how to use it. Materials covered in this section is basically a condensed (and far less comprehensive) version of the book. Yet, they should still be sufficient for the majority of ML tasks you typically implement in practice.

To start working with `mlr3`, it is convenient to load the `mlr3verse` package, which is a collection of packages that provide useful extensions (e.g., quick visualization (` mlr3viz`), machine learning methods (`mlr3leaners`), etc). It is like the `tidyverse` package.

::: {.column-margin}
See [here](https://github.com/mlr-org/mlr3verse) for the list of included packages.
:::

```{r}
library(mlr3verse)
```

:::{.callout-note}
An alternative ML utility package in R is the `tidymodels` package. In this book, we do not cover how to use the package. Interested readers can read an excellent free on-line book [here](https://www.tmwr.org/).
:::

For those who have been using solely R for their programming needs are not likely to be familiar with the way `mlr3` works. It uses R6 classes provided the `R6` package. The package provides an implementation of encapsulated object-oriented programing, which how Python works. So, if you are familiar with Python, then `mlr3` should come quite natural to you. Fortunately, understanding how `R6` works is not too hard (especially for us who are just using it). Reading the introduction of the `R6` provided [here](https://r6.r-lib.org/articles/Introduction.html) should suffice. 

To implement ML tasks, we need two core components at least: task, and learner. Roughly speaking, here are what they are.

+ Task: data
+ Learner: model

We will take a deeper look at these two components first, and then training, prediction, and other ML tasks.

## Tasks

::: {.column-margin}
**Packages to load for replication**

```{r}
#| include: false

library(data.table)
library(DoubleML)
library(mlr3verse)
library(tidyverse)
library(mltools)
library(parallel)
```

```{r}
#| eval: false
library(data.table)
library(DoubleML)
library(mlr3verse)
library(tidyverse)
library(mltools)
library(parallel)
```
:::

A task in the `mlr3` parlance is basically a dataset (called `backend`) with information on which variable is the dependent variable (called `target`) and which variables are explanatory variables (called `features`). 

Here, we will use `mtcars` data. 

```{r}
#=== load mtcars ===#
data("mtcars", package = "datasets")

#=== see the first 6 rows ===#
head(mtcars)
```

When you create a task, you need to recognize what type of analysis you will be doing and use an appropriate class. Here, we will be running regression, so we use the `TaskRegr` class (see [here](https://mlr3book.mlr-org.com/02-basics-tasks.html#tasks-types) for other task types). 

Now, let's create a task using the `TaskRegr` class with 

+ `mtcars` as `backend` (data)
+ `mpg` as `target` (dependent variable)
+ `example` as `id` (this is the id for the task, you can give any name)

You can instantiate a new `TaskRegr` instance using the `new()` methods on the `TaskRegr` class.

```{r}
(
reg_task <-
  TaskRegr$new(
    id = "example",
    backend = mtcars,
    target = "mpg"
  )
)
```

As you can see, `mpg` is the `Target` and the rest was automatically set to `Features`.

You can use the `cole_roles()` method to return the roles of the variables.

```{r}
reg_task$col_roles 
``` 

You can extract information from the task using various methods:

+ $nrow: returns the number of rows
+ $ncol: returns the number of columns
+ $feature_names: returns the name of the feature variables
+ $target_names: returns the name of the target variable(s)
+ $row_ids: return row ids (integers starting from 1 to the number of rows)
+ $data(): returns the backend (data) as a `data.table`

Let's see some of these.

```{r}
#=== row ids ===#
reg_task$row_ids
```

```{r}
#=== data ===#
reg_task$data()
```

It is possible to retrieve only a portion of the data using `rows` and `cols` options inside `data()` as follows:

```{r}
reg_task$data(rows = 1:10, cols = c("mpg", 'wt'))
```

To retrieve the complete data from the task, you can apply `as.data.table()` to the task.

```{r}
(
data_extracted <- as.data.table(reg_task)
)
```

You can mutate tasks using the `select()` and `filter()` methods. It is important to remember here that the instance at which these mutations are implemented is indeed mutated. Let's see what I mean by this. 


```{r}
#=== first select few variables ===#
reg_task$select(c("am", "carb", "cyl"))

#=== see the backend now ===#
reg_task$data()
```

As you can see, `reg_task` now holds only the variables that were selected (plus the target variable `mpg`). This behavior is similar to how `data.table` works when you create a new variable using `data[, := ]` syntax. And, this is different from how `dplyr::select` works. 

```{r}
#=== create a dataset ===#
data_temp <- reg_task$data()

#=== select mpg, carb ===#
dplyr::select(data_temp, mpg, carb)

#=== look at data_temp ===#
data_temp
```

As you can see, `data_temp` is not mutated after `select()`. To save the the result of `dplyr::select()`, you need to explicitly assign it to another R object.


::: {.column-margin}
The target variable cannot be selected in `select()`. It is automatically selected.
:::

If you would like to keep the original task, you can use the `clone()` method to create a distinct instance.

```{r}
#=== create a clone ===#
reg_task_independent <- reg_task$clone()
```

Let's filter the data using the `filter()` method.

```{r}
#=== filter ===#
reg_task$filter(1:10)

#=== see the backend ===#
reg_task$data()
```

However, `reg_task_independent` is not affected.

```{r}
reg_task_independent$data()
```

You can use the `rbind()` and `cbind()` methods to append data vertically and horizontally, respectively. 

Here is an example use of `rbind()`.

```{r}
reg_task$rbind(
  data.table(mpg = 20, am = 1, carb = 3, cyl = 99)
)

#=== see the change ===#
reg_task$data()
```

## Learners

In Python, the majority of ML packages are written to be compatible with `scikit-learn` framework. However, in R, there is no single framework that is equivalent to `scikit-learn` to which all the developers of ML packages conform to. Fortunately, the author of the `mlr3` package picked popular ML packages (e.g., `ranger`, `xgboost`) and made it easier for us to use those packages under the unified framework.

::: {.column-margin}
`tidymodels` have their own collection of packages, which is very similar to what `mlr3` has. 
:::

Here is the list of learners that is available after loading the `mlr3verse` package.

```{r}
mlr_learners
```

As you can see, packages that we have seen earlier (e.g., `glmnet`,  `ranger`, `xgboost`, `gam`) are available. the `mlr3extralearners` package provides you with additional but less-supported learners. You can check the complete list of learners [here](https://mlr3extralearners.mlr-org.com/articles/learners/list_learners.html).

You set up a learner by giving the name of the learner you would like to implement from the list to `lrn()` like below.

```{r}
learner <- lrn("regr.ranger")
```

Note that you need to pick the one with the appropriate prediction type prefix (the prefixes are self-explanatory). Here, since we are interested in regression, we picked `"regr.ranger"`.

Once you set up a learner, you can see the set of the learner's hyper-parameters.

```{r}
learner$param_set
```

Right now, only `num.threads` is set explicitly.

```{r}
learner$param_set$values
```

You can update or assign the value of a parameter like this:

```{r}
#=== set max.depth to 5 ===#
learner$param_set$values$max.depth <- 5

#=== see the values ===#
learner$param_set$values
```

When you would like to set values for multiple hyper-parameters at the same time, you can provide a named list to `$param_set$values`. Here is an example:

```{r}
#=== create a named vector ===#
parameter_values <- list("min.node.size" = 10, "mtry" = 5, "num.trees" = 500)

#=== assign them ===#
learner$param_set$values <- parameter_values

#=== see the values ===#
learner$param_set$values
```

But, notice that the values we set previously for `max.depth` and `num.threads` are gone. 


## Train, predict, assessment

### Train

Training a model can be done using the `$train()` method on a leaner by supplying a task to it. Let's first set up a task and learner.

```{r}
#=== define a task ===#
reg_task <-
  TaskRegr$new(
    id = "example",
    backend = mtcars,
    target = "mpg"
  )

#=== set up a learner ===#
learner <- lrn("regr.ranger")
learner$param_set$values <- 
  list(
    "min.node.size" = 10, 
    "mtry" = 5, 
    "num.trees" = 500
  )
```

Notice that the `model` attribute of the learner is empty at this point.

```{r}
learner$model
```

Now, let's train.

```{r}
learner$train(reg_task)
```

We now how information about the trained model in the `model` attribute.

```{r}
learner$model
```

Notice that this is exactly what you would get if you use `ranger()` to train your model.

The `train()` function has the `row_ids()` option, where you can specify which rows of the data backend in the task are used for training.

Let's split extract the `row_ids` attribute and then split it for train and test purposes.


```{r}
#=== extract row ids ===#
row_ids <- reg_task$row_ids

#=== train ===#
train_ids <- row_ids[1:(length(row_ids) / 2)]

#=== test ===#
test_ids <- row_ids[!(row_ids %in% train_ids)]
```

:::{.callout-note}
He, we are doing the split manually. But, you should use resampling methods, which we will look at later. 
::: 

Now train using the train data.

```{r}
#=== train ===#
learner$train(reg_task, row_ids = train_ids)

#=== seed the trained model ===#
learner$model
```

### Prediction

We can use the `predict()` method to make predictions by supplying a task to it. Just like the `train()` method, we can use the `row_ids` option inside `predict_newdata()` to apply prediction only on a portion of the data. Let's use `test_ids` we created above.

```{r}
prediction <- learner$predict(reg_task, row_ids = test_ids)
```

`prediction` is of a class called `Prediction`. You can make the prediction available as a `data.table` by using `as.data.table()`.

```{r}
as.data.table(prediction)
```

You can predict on a new dataset by supplying a new dataset as a `data.frame`/`data.table` to the `predict_newdata()` method. Here, we just use parts of `mtcars` (just pretend this is a newdataset).

```{r}
prediction <- learner$predict_newdata(mtcars)
```

### Performance assessment

There are many measure of performance we can use under `mlr3`. Here is the list:

```{r}
mlr_measures
```

We can access a measure using the `msr()` function.

```{r}
#=== get a measure ===#
measure <- msr("regr.mse")

#=== check the class ===#
class(measure)
```

We can do performance evaluation using the `$score()` method on a `Prediction` object by supplying a `Measure` object.

```{r}
prediction$score(measure)
```


## Resampling, cross-validation, and cross-fitting

### Resampling

`mlr3` offers the following resampling methods:

```{r}
as.data.table(mlr_resamplings)
```

You can access a resampling method using the `rsmp()` function. You can specify parameters at the same time.

```{r}
(
resampling <- rsmp("repeated_cv", repeats = 2, folds = 3)
)
```

You can check the number of iterations (number of train-test datasets combinations) by accessing the `iters` attribute.

```{r}
resampling$iters
```

You can override parameters just like you did for a leaner.

```{r}
#=== update ===#
resampling$param_set$values = list(repeats = 3, folds = 4)

#=== see the updates ===#
resampling
```

We can use the `instantiate()` method to implement the specified resampling method:

```{r}
resampling$instantiate(reg_task)
``` 

You can access the train and test datasets using the `train_set()` and `test_set()` method. respectively. Since `repeats = 3` and `folds = 4`, we have 12 sets of train and test datasets. You indicate which set you want inside `train_set()` and `test_set()`.

First pair:

```{r}
resampling$train_set(1)
resampling$test_set(1)
```

Last pair:

```{r}
resampling$train_set(12)
resampling$test_set(12)
```

### Cross-validation and cross-fitting

Now that data splits are determined (along with a task and leaner), we can conduct a cross-validation using the `resample()` function like below.

```{r}
cv_results <- resample(reg_task, learner, resampling)
```

This code applies the method specified in `leaner` to each of the 12 train datasets, and evaluate the trained model on each of the 12 test datasets.

You can look at the prediction results using the `predictions()` method.

```{r}
cv_results$predictions()
```

You can get the all predictions combined using the `prediction()` method.

```{r}
#=== all combined ===#
all_predictions <- cv_results$prediction()

#=== check the class ===#
class(all_predictions)
```

Since it is a `Prediciton` object, we can apply the `score()` method like this. 

```{r}
all_predictions$score(msr("regr.mse"))
```

Of course, you are also cross-fitting when you are doing cross-validation. You can just take the prediction results and use them if you are implementing DML for example.

```{r}
(
cross_fitted_yhat <-
  as.data.table(all_predictions) %>%
  .[, .(y_hat_cf = mean(response)), by = row_ids]
)
```

## Hyper-parameter tuning

In conducting hyper-parameter tuning under the `mlr3` framework, you define `TuningInstance*` class, select the tuning method, and then trigger it. 

### TuningInstance

There are two tuning instance classes.

+ TuningInstanceSingleCrit 
+ TuningInstanceMultiCrit

The difference should be clear by looking at the name of the classes. We focus on the `TuningInstanceSingleCrit` class here. 

Tuning instance consists of six elements:

+ `task`
+ `learner`
+ `resampling`
+ `measure`
+ `search_space`
+ `terminator`

We have covered all except `search_space` and `terminator`. Let's look at these two.

Let's quickly create the first four elements.

```{r}
#=== task ===#
reg_task <-
  TaskRegr$new(
    id = "example",
    backend = mtcars,
    target = "mpg"
  )

#=== learner ===#
learner <- lrn("regr.ranger")
learner$param_set$values$max.depth <- 10

#=== resampling ===#
resampling <- rsmp("cv", folds = 3) # k-fold cv

#=== measure ===#
measure <- msr("regr.mse")
```

`search_space` defines which hyper-parameters to tune and their ranges. You can use `ps()` to create a search space. Before doing so, let's look at the parameters of the learner. 

```{r}
learner$param_set
```

Let's tune three parameters here: `mtry.ratio`, `min.node.size`, and `num.trees`. For each parameter to tune, we need to use an appropriate function to define the range. You can see what functions to use from `class` variable.

```{r}
learner$param_set %>% 
  as.data.table() %>% 
  .[id %in% c("mtry.ratio", "min.node.size"), .(id, class, lower, upper)]
``` 

In this case, we use `p_int()` for "min.node.size" as its class is `ParamInt` and use `p_dbl()` for "mtry.ratio" as its class is `ParamDbl`. You cannot specify the range that go beyond the `lower` and `upper` for each parameter.


```{r}
search_space <- 
  ps(
    mtry.ratio = p_dbl(lower = 0.5, upper = 0.9),
    min.node.size = p_int(lower = 1, upper = 20)
  )
```

Let's define a `terminator` now. `mlr3` offers five different options. We will just look at the most common one here, which is `TerminatorEvals`. `TerminatorEvals` terminates tuning after a given number of iterations specified by the user (see [here](https://mlr3book.mlr-org.com/04-optimization-tuning.html#tuning-optimization) for other terminator options).

You can use the `trm()` function to define a Terminator object. 

```{r}
terminator <- trm("evals", n_evals = 100) 
```

Inside `trm()`, "evals" indicates that we would like to use the `TerminatorEvals` option. 

Now that we have all the components specified, we can instantiate (generate) a TuningInstanceSingleCrit class.

```{r}
(
tuning_instance <- 
  TuningInstanceSingleCrit$new(
    task = reg_task,
    learner = learner,
    resampling = resampling,
    measure = measure,
    search_space = search_space,
    terminator = terminator
  )
)

```

### Tuner  

Let's now define the method of tuning. `mlr3` offers four options:

+ Grid Search (`TunerGridSearch`)
+ Random Search (`TunerRandomSearch`)
+ Generalized Simulated Annealing (`TunerGenSA`)
+ Non-Linear Optimization (`TunerNLoptr`)

Here we use `TunerGridSearch`. We can set up a tuner using the `tnr()` function.

```{r}
tuner <- tnr("grid_search", resolution = 4) 
```

`resolution = 4` means that each parameter takes four values where the values are equidistant between the upper and lower bounds specified in `search_space`. So, this tuning will look at $4^2 = 16$ parameter configurations. Notice that we set the number of evaluations to 100 above. So, all $16$ cases will be evaluated. However, if you set `n_evals` lower than $16$, then the tuning will not look at all the cases. 

### Tuning 

You can trigger tuning by supplying a tuning instance to the `optimizer()` method on `tuner`.

```{r}
#| eval: false
tuner$optimize(tuning_instance)
```

```{r}
#| include: false
tuner$optimize(tuning_instance)
```

Since the execution of this tuning prints so many lines of results, it is not presented here. 

One the tuning is done, you can get the optimized parameters by accessing the `result_learner_param_values` attribute of the tuning instance.

```{r}
tuning_instance$result_learner_param_vals
```

You can look at the evaluation results of other parameter configurations by accessing the `archive` attribute of the tuning instance.

```{r}
as.data.table(tuning_instance$archive) %>% head()
```

### AutoTuner

`AutoTuner` sounds like it is a tuner, but it is really learner where tuning is automatically implemented when training is triggered with a task. An `AutoTuner` class can be instantiated using the `new()` method on `AutoTuner` class like below.

```{r}
(
auto_tunning_learner <- 
  AutoTuner$new(
    learner = learner,
    resampling = resampling,
    measure = measure,
    search_space = search_space,
    terminator = terminator,
    tuner = tuner
  )
)
```

Note that unlike TuningInstance, `AutoTuner` does not take a task as its element and takes a `Tuner` instead. Once an `AutoTuner` is instantiated, you can use it like a learner and invoke the `train()` method with a task to train a model. The difference from a regular leaner is that it automatically tune the parameters internally and use the optimized parameter values to train.  

```{r}
#| message: false 
auto_tunning_learner$train(reg_task)
```

You can access the optimized learner by accessing the `model$learner` attribute.

```{r}
auto_tunning_learner$model$learner
```

You can look at the history of tuning process like below:

```{r}
auto_tunning_learner$model$tuning_instance
```

You can of course use the `predict()` method as well. 

```{r}
auto_tunning_learner$predict(reg_task)
```

## `mlr3` in action {#sec-mlr3-in-action}

Here, an example usage of the `mlr3` framework as a part of a research process is presented. Suppose our goal is to estimate the heterogeneous treatment effect using causal forest using the R `grf` package. The `grf::causal_forest()` function implements causal forest as an R-learner and it uses random forest for its first-stage estimations by default. However, the function allows the users to provide their own estimated values of $y$ (dependent variable) and $T$ (treatment). We will code the process of conducting the first stage using `mlr3` and then use `grf::causal_forest()` to estimate heterogeneous treatment effects.


```{r}
#=== load the Treatment dataset ===#
data("Treatment", package = "Ecdat")

#=== convert to a data.table ===#
(
data <- 
  data.table(Treatment)
)
```

The dependent variable is `re78` ($Y$), which is real annual earnings in 1978 (after treatment). The treatment variable of interest is `treat` ($T$), which is `TRUE` if a person had gone through a training, `FALSE` otherwise. The features that are included as potential drivers of the heterogeneity in the impact of `treat` on `re78` is `age`, `educ`, `ethn`, and `married` ($X$). Note that the focus of this section is just showcasing the use of `mlr3` and no attention is paid to potential endogeneity problems. 

### First stage

We will use `ranger()` and `xgboost()` as learners. While `ranger()` accepts factor variables, `xgboost()` does not. So, we will one-hot-encode the data using `mltools::one_hot()` so that we can just create a single `Task` and use it for both. We also turn `treat` to a factor so it is amenable with classification jobs by the two learner functions.

```{r}
(
data_trt <- 
  mltools::one_hot(data) %>% 
  .[, treat := factor(treat)]
)
```

Note that we need separate procedures for estimating $E[Y|X]$ and $E[T|X]$. The former is a regression and the latter is a classification task. 

Let's first work on estimating $E[Y|X]$. First, we set up a task.

```{r}
y_est_task <- 
  TaskRegr$new(
    id = "estimate_y_on_x",
    backend = data_trt[, .(
      re78, age, educ, married, 
      ethn_other, ethn_black, ethn_hispanic 
    )],
    target = "re78"
  )
```

We consider two modeling approaches: random forest by `ranger` and gradient boosted forest by `xgboost`. 

```{r}
y_learner_ranger <- lrn("regr.ranger")
y_learner_xgboost <- lrn("regr.xgboost")
```

We will implement a K-fold cross-validation to select the better model with optimized hyper-parameter values. We do this by applying triggering `Tuner` classed defined separately for the two learners.

Let's define the resampling method, measure, terminator, and tuner that will be shared by the two approaches.

```{r}
resampling_y <- rsmp("cv", folds = 4)
measure_y <- msr("regr.mse")
terminator <- trm("evals", n_evals = 100)
tuner <- tnr("grid_search", resolution = 4)
```

Now, when we compare multiple models, we should use the same CV splits to have a fair comparison their model performance. To ensure this, we need to use an instantiated `Resampling` object. 

::: {.column-margin}
If you provide an un-instantiated `Resampling` object to an `AutoTuner`, it will instantiate the `Resampling` object internally and two separate `AutoTuner`s can result in two distinct splits.  
:::

```{r}
#=== instantiate ===#
resampling_y$instantiate(y_est_task)

#=== confirm it is indeed instantiated ===#
resampling_y
```

Let's define search space for each of the learners.

```{r}
search_space_ranger <-
  ps(
    mtry = p_int(lower = 1, upper = length(y_est_task$feature_names)),
    min.node.size = p_int(lower = 1, upper = 20)
  )

search_space_xgboost <-
  ps(
    nrounds = p_int(lower = 100, upper = 400),
    eta = p_dbl(lower = 0.01, upper = 1)
  )
```

We have all the ingredients to set up `TuningInstance`s for the learners.

```{r}
tuning_instance_ranger <-
  TuningInstanceSingleCrit$new(
    task = y_est_task,
    learner = y_learner_ranger,
    resampling = resampling_y,
    measure = measure_y,
    search_space = search_space_ranger,
    terminator = terminator
  )

tuning_instance_xgboost <-
  TuningInstanceSingleCrit$new(
    task = y_est_task,
    learner = y_learner_xgboost,
    resampling = resampling_y,
    measure = measure_y,
    search_space = search_space_xgboost,
    terminator = terminator
  )
```

Let's tune them now (this can take a while).

::: {.column-margin}
We use using the same tuner here, but you can use different tuning processes for the learners. For example, you can have `resolution = 5` for tuning `regr.xgboost`.
:::


```{r}
#| include: false
#=== tune ranger ===#
tuner$optimize(tuning_instance_ranger)

#=== tune xgboost ===#
tuner$optimize(tuning_instance_xgboost)
```

```{r}
#| eval: false

#=== tune ranger ===#
tuner$optimize(tuning_instance_ranger)

#=== tune xgboost ===#
tuner$optimize(tuning_instance_xgboost)
```

Here are the MSEs from the two individually tuned learners.

```{r}
tuning_instance_ranger$result_y

tuning_instance_xgboost$result_y
```

So, in this example, we go with `regr.ranger` with its optimized hyper-parameter values. Let's update our learner with the optimized hyper-parameter values.

```{r}
(
y_learner_ranger$param_set$values <- tuning_instance_ranger$result_learner_param_vals
)
```

Now that we have decided on the model to use for predicting $E[y|X]$, let's implement cross-fitting.

```{r}
cv_results_y <-
  resample(
    y_est_task, 
    y_learner_ranger, 
    rsmp("repeated_cv", repeats = 4, folds = 3) 
  )

#=== all combined ===#
all_predictions_y <- 
  cv_results_y$prediction() %>% 
  as.data.table() %>% 
  .[, .(y_hat = mean(response)), by = row_ids] %>%
  .[order(row_ids), ]
```

We basically follow the same process for estimating $E[T|X]$. Let's set up tuning processes for the learners.

```{r}
t_est_task <-
  TaskClassif$new(
    id = "estimate_t_on_x",
    backend = 
      data_trt[, .(
        treat, age, educ, married, 
        ethn_other, ethn_black, ethn_hispanic 
      )],
    target = "treat"
  )

t_learner_ranger <- lrn("classif.ranger")
t_learner_xgboost <- lrn("classif.xgboost")

resampling_t <- rsmp("cv", folds = 4)
resampling_y$instantiate(t_est_task)
measure_t <- msr("classif.ce")
terminator <- trm("evals", n_evals = 100)
tuner <- tnr("grid_search", resolution = 4)

tuning_instance_ranger <-
  TuningInstanceSingleCrit$new(
    task = t_est_task,
    learner = t_learner_ranger,
    resampling = resampling_y,
    measure = measure_t,
    search_space = search_space_ranger,
    terminator = terminator
  )

tuning_instance_xgboost <-
  TuningInstanceSingleCrit$new(
    task = t_est_task,
    learner = t_learner_xgboost,
    resampling = resampling_y,
    measure = measure_t,
    search_space = search_space_xgboost,
    terminator = terminator
  )
```

Here are the classification error from the two individually tuned learners.

```{r}
#| include: false 

#=== tune ranger ===#
tuner$optimize(tuning_instance_ranger)
tuning_instance_ranger$result_y
```

```{r}
#| eval: false 

#=== tune ranger ===#
tuner$optimize(tuning_instance_ranger)
tuning_instance_ranger$result_y
```

```{r}
#| include: false 

#=== tune xgboost ===#
tuner$optimize(tuning_instance_xgboost)
tuning_instance_xgboost$result_y
```

```{r}
#| eval: false 

#=== tune xgboost ===#
tuner$optimize(tuning_instance_xgboost)
tuning_instance_xgboost$result_y
```

So, we are picking the `classif.ranger` option here as well as it has a lower classification error.

```{r}
t_learner_ranger$param_set$values <- tuning_instance_ranger$result_learner_param_vals
```

Now that we have decided on the model to use for predicting $E[T|X]$, let's implement cross-fitting. Before cross-fitting, we need to tell `t_learner_ranger` to predict probability instead of classification (either 0 or 1).

```{r}
t_learner_ranger$predict_type <- "prob"

cv_results_t <-
  resample(
    t_est_task, 
    t_learner_ranger, 
    rsmp("repeated_cv", repeats = 4, folds = 3) 
  )

#=== all combined ===#
all_predictions_t <- 
  cv_results_t$prediction() %>% 
  as.data.table() %>% 
  .[, .(t_hat = mean(prob.TRUE)), by = row_ids] %>% 
  .[order(row_ids), ]
```

We now use `all_predictions_y` and `all_predictions_t` for `Y.hat` and `W.hat` in `grf::causal_forest()`.

```{r}
grf::causal_forest(
  X = data_trt[, .(age, educ, married, ethn_other, ethn_black, ethn_hispanic)] %>% as.matrix(),
  Y = data_trt[, re78],
  W = data_trt[, fifelse(treat == TRUE, 1, 0)],
  Y.hat = all_predictions_y[, y_hat],
  W.hat = all_predictions_t[, t_hat]
)
```


