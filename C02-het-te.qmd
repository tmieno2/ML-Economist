# R-learner {#sec-het-dml}

## Heterogeneous treatment effect estimation

In @sec-dml, the basic idea of double machine learning (DML) methods was introduced when the treatment effect is homogeneous. We now turn our focus to the task of estimating heterogeneous treatment effects: the impact of a treatment varies based on observed attributes of the subjects. Heterogeneous treatment effect is also referred to as <span style="color:blue"> conditional </span> average treatment effect (CATE).

::: {.column-margin}
<span style="color:blue"> Conditional </span> on observed attributes.
:::

## Motivation

Understanding how treatment effects vary can be highly valuable in many circumstances. 

<span style="color:blue"> Example 1: </span>
If we come to know a particular drug is effective on elderly people but detrimental to kids, then doctors can make a smart decision of prescribing the drug to elderly people, but not to kids. 

::: {.column-margin}
In this example, the heterogeneity driver is age.
:::

<span style="color:blue"> Example 2: </span>
If we come to know that fertilizer is more effective in increasing corn yield in soil type A than B, then farmers can apply more fertilizer on the parts of the field where soil type is A but less on where soil type is B. 

::: {.column-margin}
In this example, the heterogeneity driver is soil type.
:::

As you can see in these examples, knowledge on the heterogeneity of the treatment effect and its drivers can help decision makers smart-target treatments.


## Modeling Framework

The model of interest in a general form here is as follows:

$$
\begin{aligned}
Y & = \theta(X)\cdot T + g(X, W) + \varepsilon \\
T & = f(X, W) + \eta 
\end{aligned}
$$

+ $Y$: dependent variable
+ $T$: treatment variable (can be either binary dummy or continuous)
+ $X$: collection of variables that affect Y indirectly through the treatment ($\theta(X)\cdot T$) and directly ($g(X, W)$) independent of the treatment
+ $W$: collection of variables that affect directly ($g(X, W)$) independent of the treatment, but not through the treatment

Here are the key assumptions:

+ $E[\varepsilon|X, W] = 0$
+ $E[\eta|X, W] = 0$
+ $E[\eta\cdot\varepsilon|X, W] = 0$

Our objective is to estimate the <span style = "color: red;"> constant </span> marginal CATE $\theta(X)$. (constant in the sense marginal CATE is the same irrespective of the value of the treatment)

## R-learner

### Theoretical background

Under the assumptions,

$$
\begin{aligned}
E[Y|X, W] = \theta(X)\cdot E[T|X,W] + g(X,W)
\end{aligned}
$$

Thus,

$$
\begin{aligned}
Y & = \theta(X)\cdot T + g(X,W) + \varepsilon \\
\Rightarrow Y - E[Y|X, W] & = \theta(X)\cdot T + g(X,W) + \varepsilon - \theta(X)\cdot E[T|X,W] - g(X,W) \\
\Rightarrow Y - E[Y|X, W] & = \theta(X)\cdot (T - E[T|X,W]) + \varepsilon \\
\end{aligned}
$$


$$
\begin{aligned}
Y - E[Y|X, W] & = \theta(X)\cdot (T - E[T|X,W]) + \varepsilon 
\end{aligned}
$$ 

Suppose we know $E[Y|X, W]$ and $E[T|X,W]$, then we can construct the following new variables:

+ $\tilde{Y} = Y - E[Y|X, W]$
+ $\tilde{T} = T - E[T|X, W] = \eta$

Then, the problem of identifying $\theta(X)$ reduces to estimating the following model:

$$
\begin{aligned}
\tilde{Y} = \theta(X)\cdot \tilde{T} + \varepsilon
\end{aligned}
$$


Since $E[\eta\cdot\varepsilon|X] = 0$ by assumption, we can regress $\tilde{Y}$ on $X$ and $\tilde{T}$,

$$
\begin{aligned}
\hat{\theta} = argmin_{\theta} \;\; E[(\tilde{Y} - \theta(X)\cdot \tilde{T})^2]
\end{aligned}
$$ {#eq-est-equation}

### Estimation steps

In practice, we of course do not observe $E[Y|X, W]$ and $E[T|X, W]$. So, we first need to estimate them using the data at hand to construct $\hat{\tilde{Y}}$ and $\hat{\tilde{T}}$. You can use any suitable statistical methods to estimate $E[Y|X, W]$ and $E[T|X, W]$. Some machine learning methods allow you to estimate them without assuming any functional form or structural assumptions. If you believe they are linear functions of $X$ and $W$, may could alternatively use lasso or other linear models. It is important to keep in mind that the estimation of $E[Y|X, W]$ and $E[T|X, W]$ is done by cross-fitting (see @sec-cf) to avoid over-fitting bias. Let, $f(X, W)$ and $g(X,W)$ denote $\tilde{Y}$ and $\tilde{T}$, respectively. Further, let $I_{-i}$ denote all the observations that belong to the folds that $i$ does <span style="color:blue"> not </span> belong to. Finally, let $\hat{f}(X_i, W_i)^{I_{-i}}$ and $\hat{g}(X_i, W_i)^{I_{-i}}$ denote $\tilde{Y}$ and $\tilde{T}$ estimated using $I_{-i}$. 

::: {.column-margin}
Just like the DML approach discussed in @sec-dml, both $Y$ and $T$ are orthogonalized.
:::

Then the quality of fit (explaining the heterogeneity in the impact of treatment) can be expressed as follows, which is the empirical version of @eq-est-equation:

$$
\begin{aligned}
\sum_{i=1}^N [Y_i - \hat{f}(X_i,W_i)^{I_{-i}} - \theta(X)\cdot (T_i - \hat{g}(X_i,W_i)^{I_{-i}})]^2
\end{aligned}
$$

This is called <span style="color:blue"> R-score</span>, and it can be used for causal model selection, which is covered later. 

The final stage of the R-learner is to estimate $\theta(X)$ by minimizing the R-score plus the regularization term (if desirable).

$$
\begin{aligned}
\hat{\theta}(X) = argmin_{\theta(X)}\;\;\sum_{i=1}^N [Y_i - \hat{f}(X_i,W_i)^{I_{-i}} - \theta(X)\cdot (T_i - \hat{g}(X_i,W_i)^{I_{-i}})]^2 + \Lambda(\theta(X))
\end{aligned}
$$

where $\Lambda(\theta(X))$ is the penalty on the complexity of $\theta(X)$. For example, if you choose to use lasso, then $\Lambda(\theta(X))$ is the L1 norm. You have lots of freedom as to what model you use in the final stage. The `econml` package offers several off-the-shelf choices of R-learner (DML) approaches that differ in the model used at the final stage, including causal forest, lasso, etc.

## Implementation Example 

::: {.column-margin}
**Packages to load for replication**

```{r }
#| include: false
library(data.table)
library(magick)
library(fixest)
library(officer)
library(dplyr)
library(ggplot2)
library(reticulate)
library(DoubleML)
library(MASS)
```

```{r}
#| eval: false
library(data.table)
library(magick)
library(fixest)
library(officer)
library(dplyr)
library(ggplot2)
library(reticulate)
library(DoubleML)
library(MASS)
```
:::

We use the python `econml` pacakge (in conjunction with the R `reticulate` package), which offers one of the most comprehensive sets of off-the-shelf R-leaner (DML) methods [@econml]. 

Let's first take a look at the `DML` class, which implements DML where the final stage has to be a linear model. That is,

::: {.column-margin}
`DML` is a child class of `_Rlearner`, which is a private class. The `DML` class has several child classes: `LinearDML`, `SpatseLinearDML`, `NonParamDML`, and `CausalForestDML`. 
:::

$$
\begin{aligned}
\theta(x) = \beta_1 + x_1 + \dots + \beta_k + x_k
\end{aligned}
$$

As we saw above, we need to specify three models.

+ `model_y`: model for estimating $E[Y|X,W]$
+ `model_t`: model for estimating $E[T|X,W]$
+ `model_final`: model for estimating $\theta(X)$

In this example, let's use gradient boosting regression for both `model_y` and `model_t` and use lasso with cross-validation for `model_final`.

First import all the functions we will need.

```{r}
#| eval: false 

library(reticulate)
use_virtualenv("ml-learning")

#=== econml.dml ===#
em_dml <- import("econml.dml")

#=== sklearn ===#
sl <- import("sklearn") 
gbr <- sl$ensemble$GradientBoostingRegressor
lassocv <- sl$linear_model$LassoCV
```

Let's generates data according to the following data generating process:

$$
\begin{aligned}
y_i = exp(x_{i,1}) d_i + x_{i,1} + \frac{1}{4}\cdot\frac{exp(x_{i,3})}{1 + exp(x_{i,3})} + \mu_i \\
d_i = \frac{exp(x_{i,1})}{1 + exp(x_{i,1})} + \frac{1}{4}\cdot x_{i,3}+ \eta_i
\end{aligned}
$$

```{r}
#| eval: false 

#=== sample size ===#
N <- 1000 

#=== generate data ===#
synth_data <-
  gen_data(
    te_formula = formula(~ I(exp(x1)*d)),
    n_obs = N *2
  )
```

We now split the data into training and test datasets. 

```{r}
#| eval: false 

row_indices <- sample(1:(2*N), N, replace = FALSE)

#=== train data ===#
train_data <- synth_data[row_indices,]

X_train <- 
  dplyr::select(train_data, starts_with("x")) %>% 
  as.matrix()
y_train <- train_data[, y]
d_train <- train_data[, d]

#=== test data ===#
test_data <- synth_data[-row_indices,]

X_test <- 
  dplyr::select(test_data, starts_with("x")) %>% 
  as.matrix()
y_test <- test_data[, y]
d_test <- test_data[, d]
```

Before we train a DML model, we first set up a DML estimation framework like below.

```{r}
#| eval: false 
#=== set up DML estimation ===#
est <-
  em_dml$DML(
    model_y = gbr(),
    model_t = gbr(),
    model_final = lassocv(fit_intercept=FALSE) 
  )
```

Here, training has not happened yet. We simply created a recipe. Once we provide ingredients (data), we can cook (train) with the `fit()` method. 

```{r}
#| eval: false 

est$fit(
  y_train, 
  d_train, 
  X = X_train, 
  W = X_train
)
```

Once, the training is done. We can use the `effect()` method to predict $\theta(X)$.

```{r}
#| eval: false 

#=== calculate theta_hat(X) ===#
theta_pred <- est$effect(X_test)

#=== assign the predicted theta_hat to a variable ===#
test_data[, theta_hat := theta_pred]
```

@fig-est-theta-hat presents the estimated and true marginal treatment effect ($\theta(X)$) as a function of `x1`. 

```{r}
#| eval: false 
ggplot(test_data) +
  geom_point(aes(y = theta_hat, x = x1)) +
  geom_line(aes(y = exp(x1), x = x1), color = "blue") +
  theme_bw()

```

```{r}
#| fig-cap: Estimated and true marginal treatment effects
#| label: fig-est-theta-hat

# g_het_te <-
#   ggplot(test_data) +
#     geom_point(aes(y = theta_hat, x = x1)) +
#     geom_line(aes(y = exp(x1), x = x1), color = "blue") +
#     theme_bw()

# saveRDS(g_het_te, "LectureNotes/g_hte_te.rds")

g_het_te <- readRDS("g_hte_te.rds")
g_het_te
```


Since we forced $\theta(X)$ to be linear in `x1`, it is not surprising that the estimated MTE looks linear in `x1` even though the true MTE is an exponential function of `x1`.


:::{.callout-tip}
I recommend going through examples presented [here](https://github.com/microsoft/EconML/blob/main/notebooks/Double%20Machine%20Learning%20Examples.ipynb) for `DML`
:::





