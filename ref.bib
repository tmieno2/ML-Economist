@book{wooldridge2015introductory,
  title     = {Introductory econometrics: A modern approach},
  author    = {Wooldridge, Jeffrey M},
  year      = {2015},
  publisher = {Cengage learning}
}

@book{james2013introduction,
  title     = {An introduction to statistical learning},
  author    = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  volume    = {112},
  publisher = {Springer}
}

@article{friedberg2020local,
  title={Local linear forests},
  author={Friedberg, Rina and Tibshirani, Julie and Athey, Susan and Wager, Stefan},
  journal={Journal of Computational and Graphical Statistics},
  volume={30},
  number={2},
  pages={503--517},
  year={2020},
  publisher={Taylor \& Francis}
}

@book{hastie2009elements,
  title     = {The elements of statistical learning: data mining, inference, and prediction},
  author    = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome H and Friedman, Jerome H},
  volume    = {2},
  year      = {2009},
  publisher = {Springer}
}

@article{athey2016recursive,
  title={Recursive partitioning for heterogeneous causal effects},
  author={Athey, Susan and Imbens, Guido},
  journal={Proceedings of the National Academy of Sciences},
  volume={113},
  number={27},
  pages={7353--7360},
  year={2016},
  publisher={National Acad Sciences}
}

@book{wood2006generalized,
  title     = {Generalized additive models: an introduction with R},
  author    = {Wood, Simon N},
  year      = {2006},
  publisher = {chapman and hall/CRC}
}

@article{Chernozhukov2018,
  author  = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
  title   = {{Double/debiased machine learning for treatment and structural parameters}},
  journal = {The Econometrics Journal},
  volume  = {21},
  number  = {1},
  pages   = {C1-C68},
  year    = {2018},
  month   = {01},
  issn    = {1368-4221},
  doi     = {10.1111/ectj.12097},
  url     = {https://doi.org/10.1111/ectj.12097},
  eprint  = {https://academic.oup.com/ectj/article-pdf/21/1/C1/27684918/ectj00c1.pdf}
}

@misc{econml,
  author       = {Keith Battocchi, Eleanor Dillon, Maggie Hei, Greg Lewis, Paul Oka, Miruna Oprescu, Vasilis Syrgkanis},
  title        = {{EconML}: {A Python Package for ML-Based Heterogeneous Treatment Effects Estimation}},
  howpublished = {https://github.com/microsoft/EconML},
  note         = {Version 0.x},
  year         = {2019}
}


@inproceedings{Chen_2016,
  doi = {10.1145/2939672.2939785},
  url = {https://doi.org/10.1145%2F2939672.2939785},
  year = 2016,
  month = {aug},
  publisher = {{ACM}},
  author = {Tianqi Chen and Carlos Guestrin},
  title = {{XGBoost}},
  booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining}
}

@article{wager_estimation_2018,
  title    = {Estimation and {Inference} of {Heterogeneous} {Treatment} {Effects} using {Random} {Forests}},
  volume   = {113},
  issn     = {1537274X},
  url      = {https://doi.org/10.1080/01621459.2017.1319839},
  doi      = {10.1080/01621459.2017.1319839},
  abstract = {Many scientific and engineering challenges—ranging from personalized medicine to customized marketing recommendations—require an understanding of treatment effect heterogeneity. In this article, we develop a nonparametric causal forest for estimating heterogeneous treatment effects that extends Breiman's widely used random forest algorithm. In the potential outcomes framework with unconfoundedness, we show that causal forests are pointwise consistent for the true treatment effect and have an asymptotically Gaussian and centered sampling distribution. We also discuss a practical method for constructing asymptotic confidence intervals for the true treatment effect that are centered at the causal forest estimates. Our theoretical results rely on a generic Gaussian theory for a large family of random forest algorithms. To our knowledge, this is the first set of results that allows any type of random forest, including classification and regression forests, to be used for provably valid statistical inference. In experiments, we find causal forests to be substantially more powerful than classical methods based on nearest-neighbor matching, especially in the presence of irrelevant covariates.},
  number   = {523},
  journal  = {Journal of the American Statistical Association},
  author   = {Wager, Stefan and Athey, Susan},
  year     = {2018},
  note     = {Publisher: Taylor \& Francis
              \_eprint: 1510.04342},
  keywords = {Adaptive nearest neighbors matching, Asymptotic normality, Potential outcomes, Unconfoundedness},
  pages    = {1228--1242}
}


@article{breiman2001random,
  title={Random forests},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={45},
  number={1},
  pages={5--32},
  year={2001},
  publisher={Springer}
}

@article{kleiner_scalable_2014,
  title    = {A scalable bootstrap for massive data},
  volume   = {76},
  issn     = {13697412},
  url      = {https://onlinelibrary.wiley.com/doi/10.1111/rssb.12050},
  doi      = {10.1111/rssb.12050},
  language = {en},
  number   = {4},
  urldate  = {2022-07-06},
  journal  = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  author   = {Kleiner, Ariel and Talwalkar, Ameet and Sarkar, Purnamrita and Jordan, Michael I.},
  month    = sep,
  year     = {2014},
  pages    = {795--816},
  file     = {Kleiner et al. - 2014 - A scalable bootstrap for massive data.pdf:/Users/tmieno2/Zotero/storage/C2U6WR69/Kleiner et al. - 2014 - A scalable bootstrap for massive data.pdf:application/pdf}
}

@article{athey2019generalized,
  title={Generalized random forests},
  author={Athey, Susan and Tibshirani, Julie and Wager, Stefan},
  journal={The Annals of Statistics},
  volume={47},
  number={2},
  pages={1148--1178},
  year={2019},
  publisher={Institute of Mathematical Statistics}
}

@article{cawley_over-tting_nodate, title = {On {Over}-ﬁtting in {Model} {Selection} and {Subsequent} {Selection} {Bias} in {Performance} {Evaluation}},
  language = {en},
  author = {Cawley, Gavin C and Talbot, Nicola L C},
  pages = {29},
  file = {Cawley and Talbot - On Over-ﬁtting in Model Selection and Subsequent S.pdf:/Users/tmieno2/Zotero/storage/PXYDHQAJ/Cawley and Talbot - On Over-ﬁtting in Model Selection and Subsequent S.pdf:application/pdf},
}

@inproceedings{oprescu2019orthogonal,
  title={Orthogonal random forest for causal inference},
  author={Oprescu, Miruna and Syrgkanis, Vasilis and Wu, Zhiwei Steven},
  booktitle={International Conference on Machine Learning},
  pages={4932--4941},
  year={2019},
  organization={PMLR}
}

@article{DoubleML2022Python,
  title   = {{DoubleML} -- {A}n Object-Oriented Implementation of Double Machine Learning in {P}ython},
  author  = {Philipp Bach and Victor Chernozhukov and Malte S. Kurz and Martin Spindler},
  journal = {Journal of Machine Learning Research},
  year    = {2022},
  volume  = {23},
  number  = {53},
  pages   = {1--6},
  url     = {http://jmlr.org/papers/v23/21-0862.html}
}

@article{lalonde1986evaluating,
  title={Evaluating the econometric evaluations of training programs with experimental data},
  author={LaLonde, Robert J},
  journal={The American economic review},
  pages={604--620},
  year={1986},
  publisher={JSTOR}
}


@misc{DoubleML2021R,
  title={{DoubleML} -- {A}n Object-Oriented Implementation of Double Machine Learning in {R}},
  author={P. Bach and V. Chernozhukov and M. S. Kurz and M. Spindler},
  year={2021},
  eprint={2103.09603},
  archivePrefix={arXiv},
  primaryClass={stat.ML},
  note={arXiv:\href{https://arxiv.org/abs/2103.09603}{2103.09603} [stat.ML]}
}

@inproceedings{chen2016xgboost,
  title={Xgboost: A scalable tree boosting system},
  author={Chen, Tianqi and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining},
  pages={785--794},
  year={2016}
}