# Spatial Cross-validation

K-fold and repeated K-fold cross-validation methods can under-estimate test MSE (that is how good the trained model is in predicting $y$ in a new dataset) when the observations in the train data are not independent with each other. A typical example would be spatial correlation of the error term. In many applications including environmental and agricultural events, spatially correlated error is commonly observed. In order to combat the problem of test MSE underestimation by regular K-fold cross-validation (hereafter, simply KCV), you can use spatial K-fold cross-validation (SKCV) instead. 

::: {.column-margin}

**Packages to load for replication**

```{r}
#| include: false

library(spatialsample)
library(tidyverse)
library(data.table)
library(sf)
library(patchwork)
library(mgcv)
library(parallel)
library(gstat)
```

```{r}
#| eval: false
library(spatialsample)
library(tidyverse)
library(data.table)
library(sf)
library(patchwork)
library(mgcv)
library(parallel)
library(gstat)
```
:::

:::{.callout-note}
**Goals of this section**

+ Examine the consequence of using KCV when the data are spatially correlated
+ Understand how SKCV is conducted and how to implement it in R
+ Examine the difference in the estimate of MSEs from KCV and spatial KCV
+ Check if the optimal hyper-parameters suggested by KCV and SKCV are any different

:::

For this section, we will consider the following data generating process. 

$$
\begin{aligned}
y = \alpha + \beta_1 x + \beta_2 x^2 + u 
\end{aligned}
$$

where $x$ and $u$ are spatially correlated (which makes $y$ spatially correlated).


::: {.column-margin}
**Code to create the dataset**

```{r}
#| message: false 
#| warning: false 

#=== create grids (on top of IL state border) ===#
# any spatial object will do. IL state border is picked
# just because it is easy to get without reading a shapefile

grids_il <-
  tigris::counties(
    state = "IL",
    progress_bar = FALSE
  ) %>% 
  st_as_sf() %>% 
  st_make_grid(n = c(30, 30)) %>% 
  st_as_sf() %>% 
  mutate(id := 1:n()) %>% 
  rename(geometry = x)

#=== set up a model ===#
g_temp <- 
  gstat(
    formula = z ~ 1,
    locations = ~ X + Y,
    beta = 0,
    dummy = T,
    model = vgm(
      psill = 20,
      range = 100,
      nugget = 0,
      model = "Sph" 
    ),
    nmax = 50
  )

#=== get coordinates ===#
xy <- 
  st_coordinates(st_centroid(grids_il)) %>% 
  data.table()

gen_data <- function(seed) {

  set.seed(seed)

  #=== generate error ===#
  error <- 
    predict(g_temp, newdata = xy, nsim = 1, debug.level = 0) %>% 
    data.table() %>% 
    #=== normalize ===#
    .[, sim1 := (sim1 - mean(sim1))/sd(sim1)]

  #=== generate x ===#
  var <- 
    predict(g_temp, newdata = xy, nsim = 1, debug.level = 0) %>% 
    data.table() %>% 
    #=== normalize ===#
    .[, sim1 := pnorm(sim1, mean = mean(sim1), sd = sd(sim1))]

  #=== assign the generated values to the data ===#
  data <- 
    grids_il %>% 
    mutate(
      e = error[, sim1] * 20, # N(0, 100)
      x = var[, sim1] * 10, # ranges from 0 to 10
      y_det = 10 + 48 * x - 4 * x^2,
      y = y_det + e
    )

  return(data)
}

```

:::


Here is the data we are going to work with (see the side note for the code to define `gen_data`, which takes a seed value and generate a spatial dataset):

```{r}
(
data  <- gen_data(seed = 47823)
)
```

We have three main variables, `y` (dependent variable), `x` (explanatory variable), and `e` (error). @fig-viz shows how they are spatially distributed. It also shows that all of them are spatially positively correlated.

```{r}
#| fig-cap: Visualization of the data
#| label: fig-viz 
g_error <-
  ggplot(data = data) +
  geom_sf(aes(fill = e), color = NA) +
  scale_fill_viridis_c() +
  theme_void()

g_x <-
  ggplot(data = data) +
  geom_sf(aes(fill = x), color = NA) +
  scale_fill_viridis_c() +
  theme_void()

g_y <-
  ggplot(data = data) +
  geom_sf(aes(fill = y), color = NA) +
  scale_fill_viridis_c() +
  theme_void()

g_y | g_x | g_error  
```

We are going to use `gam()` with `k` $= 30$ and `sp` $= 0$ as the model in conducting KCV and spatial KCV. Let's first create folds for KCV and SKCV. First, here is KCV folds.

```{r}
set.seed(93043)
(
kcv_folds <- 
  rsample::vfold_cv(data, v = 6) %>% 
  mutate(type := "KCV")
)
```

@fig-kcv is the visualization of the spatial distribution of training and test datasets for each of the five folds for KCV.

```{r}
#| fig-cap: Spatial distribution of train and test data points by fold
#| label: fig-kcv
#| code-fold: true
#| warning: false

plot_kcv_data <-
  kcv_folds %>% 
  rowwise() %>% 
  mutate(folds = list(
    rbind(
      analysis(splits) %>% mutate(type = "training"),
      assessment(splits) %>% mutate(type = "test")
    )
  )) %>% 
  dplyr::select(id, folds) %>% 
  unnest() %>% 
  st_as_sf()

ggplot(plot_kcv_data) +
  geom_sf(aes(fill = type), color = NA) +
  facet_wrap(id ~ .) +
  theme_void()

```

Now, let's create a five spatially clustered folds using the `spatialsample` package for SKCV.

```{r}
set.seed(482943)
(
skcv_folds <- 
  spatial_clustering_cv(data, v = 6) %>% 
  mutate(type := "SKCV")
)
```

@fig-sp-kcv presents the spatial distribution of training and test datasets for each of the five folds for SKCV.

```{r}
#| fig-cap: Spatial distribution of spatially clustered train and test data points by fold 
#| label: fig-sp-kcv
#| code-fold: true
#| warning: false

plot_kcv_data <-
  skcv_folds %>% 
  rowwise() %>% 
  mutate(folds = list(
    rbind(
      analysis(splits) %>% mutate(type = "training"),
      assessment(splits) %>% mutate(type = "test")
    )
  )) %>% 
  dplyr::select(id, folds) %>% 
  unnest() %>% 
  st_as_sf()

ggplot(plot_kcv_data) +
  geom_sf(aes(fill = type), color = NA) +
  facet_wrap(id ~ .) +
  theme_void()

```

Let's now implement KCV and SKCV. Since we observe the true generating process, we can calculate how good the fitted curve is compared to true $E[y|X]$ in addition to observed $y$ for the left-out samples in each fold. 

```{r}
#=== conduct CV ===#
cv_results <-
  rbind(kcv_folds, skcv_folds) %>% 
  #=== make it possible to apply function row by row ===#
  rowwise() %>% 
  mutate(
    trainining_data = list(analysis(splits)),
    test_data = list(assessment(splits))
  ) %>% 
  #=== train the model ===#
  mutate(gam_fit = list(
    gam(y ~ s(x, k = 6), data = trainining_data)
  )) %>% 
  #=== get mse ===#
  mutate(mse_data = list(
    test_data %>% 
    data.table() %>% 
    .[, y_hat := predict(gam_fit, newdata = .)] %>% 
    .[, .(
      mse_obs = mean((y - y_hat)^2), # MSE
      mse_true = mean((y_det - y_hat)^2) # deviation from E[y|x]
    )]
  )) 

#=== MSE summary ===#
(
mse_summary <- 
  cv_results %>% 
  dplyr::select(id, type, mse_data) %>% 
  unnest() %>% 
  data.table()
)
```

You can see that MSE values (`mse_obs`) are mostly greater and also more variable for SKCV. By averaging MSE over folds by CV type,

```{r}
mse_summary[, .(mse_obs = mean(mse_obs)), by = type]
```

So, indeed KCV provides lower estimate of test MSE than SKCV. Now, it is important to recognize the fundamental difference in what is measured by KCV and SKCV. KCV measures the accuracy of the trained model applied to the new data points that are located inside the area where the train data covers geographically. SKCV, on the other hand, measures the accuracy of the trained model applied to the new data points that are <span style="color:blue"> outside </span> the area where the train data covers geographically. In other words, it measures the modeling accuracy when the trained model is applied to a new region. So, KCV does overstate the accuracy of the model if your interest is applying the model to a new region. 

Let's look into the SKCV results a bit more. Looking at MSE values by fold  for KCV and SKCV, you can notice that MSE is much more variable for SKCV. @fig-tp-curv plots data points in the test data and the curve fitted on the training data by cross-validation type and fold (Note that `Fold1` for KCV and SKCV have nothing to do with each other). Since KCV uses randomly split data, all the KCV test datasets are scattered all across the area and they look similar to each other. On the other hand, SKCV test datasets look quite different from each other as they are sampled in a spatially clustered manner. If the original data has no spatial correlation, then SKCV and KCV would have resulted in very simple test datasets. However, since the original data is spatially positively correlated, one packet of a field may look very different from another pocket of the field. By looking at @fig-tp-curv, you can easily see that `FOLD2` and `FOLD5` have very high MSE for SKCV. This observation is confirmed with the MSE value by fold presented above. 


```{r}
#| fig-cap: Test data points and the curve fitted on the training dataset
#| label: fig-tp-curv
#| code-fold: true

pred_data <-
  cv_results %>% 
  mutate(eval_data = list(
    data.table(
      x = seq(min(data$x), max(data$x), length = 100)
    ) %>% 
    .[, y_hat := predict(gam_fit, newdata = .)]
  )) %>% 
  dplyr::select(id, type, eval_data) %>% 
  unnest()

raw_test_data <-
  cv_results %>% 
  dplyr::select(id, type, test_data) %>% 
  unnest()

ggplot() +
  geom_point(
    data = raw_test_data, 
    aes(y = y, x = x),
    color = "darkgrey",
    size = 0.7
  ) +
  geom_line(
    data = pred_data, aes(y = y_hat, x = x), 
    color = "red"
  ) +
  facet_grid(id ~ type) +
  theme_bw()
```

This is primarily because of the data imbalance between the training and test datasets, in particular the error term in this case. @fig-dist-y compares the distribution of the error term for the training and test datasets by fold for SKCV. As you can see, their distributions are very different for `FOLD2` and `FOLD5`. In `FOLD2`, the average of the error term for the training data is much higher than that for the test data. The opposite is true for `FOLD5`. These differences of course result in large differences in the average value of the dependent variable. This, then, further results in consistent under or over-estimation of the level of the dependent variable. Since squared error is defined as $[y_i -\hat{f}(x_i)]^2$, the consistent bias (you can consider this as bias in intercept in the linear-in-parameter modeling setting) will naturally lead to a higher MSE. That is exactly what is observed for `FOLD2` and `FOLD5`. 

```{r}
#| fig-cap: Distribution of the dependent variable for the training and test dataset by fold
#| label: fig-dist-y
#| code-fold: true

plot_data_split <-
  cv_results %>%
  filter(type == "SKCV") %>% 
  dplyr::select(id, trainining_data, test_data) %>% 
  data.table() %>% 
  melt(id.var = "id") %>% 
  unnest() 

ggplot() +
  geom_density(
    data = plot_data_split, 
    aes(x = e, fill = variable),
    alpha = 0.4
  ) +
  facet_wrap(id ~ .) +
  theme_bw()

```

While MSE values for `FOLD2` and `FOLD5` are high in SKCV, notice that their fitted curves are very much similar to the other folds and trace the true $E[y|x]$ quite well. The culprit of their high MSEs is the consistent difference in the average value of $y$ between the training and test datasets. This means that if your interest is in understanding the causal impact of $x$ on $y$, then the fitted curves for `FOLD2` and `FOLD5` are just as good as those for the other folds despite the fact that their MSE values are much higher than the rest. This illustrates well that MSE is not really a good criteria to rely on if you ultimate interest is in estimating the causal impact of a treatment even though it is a good measure if your ultimate interest in prediction (predicting the <span style="color:blue"> level </span> of the dependent variable). 

Finally, whether KCV is over-optimistic in estimating test MSE than SKCV or not is of little importance for those who are interested in causal inference because how well the trained model predicts the <span style="color:blue"> level </span> of the dependent variable (which MSE measures) is irrelevant. Rather, more critical question is whether the use of KCV leads us to choose sub-optimal hyper parameter values compared to SKCV. This could have a real consequence when we select a morel and its hyper-parameter values for the first-stage ML applications in Double Machine Learning methods. 


## Hyper-parameter tuning 

Now, let's see whether KCV and SKCV lead to different tuning results using `gam()`. Here, we fix `k` at $30$ and vary `sp` to find the best `sp` value. The following function takes an `sp` value and return MSE values from both KCV and SKCV.

```{r}
get_mse <- function(sp) {
  cv_results <-
    rbind(kcv_folds, skcv_folds) %>% 
    #=== make it possible to apply function row by row ===#
    rowwise() %>% 
    #=== train the model ===#
    mutate(gam_fit = list(
      gam(y ~ s(x, k = 50, sp = sp), data = analysis(splits))
    )) %>% 
    #=== get mse ===#
    mutate(mse_data = list(
      assessment(splits) %>% 
      data.table() %>% 
      .[, y_hat := predict(gam_fit, newdata = .)] %>% 
      .[, .(
        mse_obs = mean((y - y_hat)^2)
      )]
    )) %>% 
    dplyr::select(id, type, mse_data) %>% 
    unnest() %>% 
    data.table() %>% 
    .[, sp_v := sp]

  return(cv_results)

}
```

Here is an example at `sp` $= 0.5$.
```{r}
get_mse(0.5)[]
```

Let's try `sp` values ranging from 0 to 1.

```{r}
cv_results <-
  mclapply(
    seq(0.05, 5, length = 20),
    function(x) get_mse(x),
    mc.cores = detectCores() - 2
  ) %>% 
  rbindlist()

```

Now, let's see what value of `sp` would have been the best to estimate $E[y|x]$ (this is what we really want to know, but can't in practice because you do not observe the true data generating process unlike the simulation we are running here). For this purpose, we will create a test dataset that follows exactly the same data generating process (but randomness in the process makes a different dataset). 

```{r}
#=== generate the test data ===#
test_data  <- gen_data(seed = 9843)
```

We will fit `gam()` at the same sequence of `sp` values used just above using the entire test dataset and check what `sp` value minimizes the prediction error from true $E[y|x]$.

```{r}
#| code-fold: true
  

ggplot(test_data) +
  geom_point(aes(y = y, x = x))

test_mse <-
  data.table(
    sp_v = seq(0.05, 10, length = 20)
  ) %>% 
  rowwise() %>% 
  mutate(gam_fit = list(
    gam(y ~ s(x, k = 50), sp = sp_v, data = test_data)
  )) %>% 
  mutate(mse = mean((test_data$y_det - gam_fit$fitted.value)^2))  

test_mse$mse




```




Panel (a) of @fig-mse-tune plots MSE against the value of `sp` for KCV and SKCV. As you can see both of the suggested 0.2 as the MSE-minimizing value of `sp`. Panel (b) of @fig-mse-tune shows the squared deviations from $E[y|x]$ as a function of `sp`, which tells us what value of `sp` would result in the curve that is the closest to $E[y|x]$, the quantity of interest. Both KCV and SKCV suggest that `sp` value of 0.2 is the best. This means that both KCV and SKCV based on train data successfully identified the optimal value of `sp`. Remember that we do not really care about prediction accuracy. That is not our ultimate goal. Using KCV instead of SKCV is likely to result in a more optimistic view of how good the model actually is as we saw earlier. However, the purpose of cross-validation is tuning, but not trying to measure the accuracy of our trained model (at least for those who do not care about prediction itself). Moreover, any type of CV tends to heavily underestimate the true test ME anyway.

```{r}
#| fig-cap: MSE and squared deviation from E[y|x] by KCV and SKCV
#| fig-subcap: 
#|   - "MSE"
#|   - "Square Deviation from E[y|x]"
#| label: fig-mse-tune
#| layout-ncol: 1 
#| code-fold: true
#| eval: false

cv_plot_data <- 
  cv_results[
    , 
    .(mse_obs = mean(mse_obs), mse_true = mean(mse_true)), 
    by = .(type, sp_v)
  ]

ggplot(cv_plot_data) +
  geom_line(aes(y = mse_obs, x = sp_v)) +
  geom_point(aes(y = mse_obs, x = sp_v)) +
  facet_grid(type ~ ., scale = "free_y") +
  ylab("MSE") +
  xlab("sp") +
  theme_bw() 

ggplot(cv_plot_data) +
  geom_line(aes(y = mse_true, x = sp_v)) +
  geom_point(aes(y = mse_true, x = sp_v)) +
  facet_grid(type ~ ., scale = "free_y") +
  ylab("Square Deviation from E[y|x]") +
  xlab("sp") +
  theme_bw()
```








 

