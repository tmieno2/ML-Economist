# Spatial Cross-validation

K-fold and repeated K-fold cross-validation methods can under-estimate test MSE (that is how good the trained model is in predicting $y$ in a new dataset) when the observations in the train data are not independent with each other. A typical example would be spatial correlation of the error term. In many applications including environmental and agricultural events, spatially correlated error is commonly observed. In order to combat the problem of test MSE underestimation by regular K-fold cross-validation (hereafter, simply KCV), you can use spatial K-fold cross-validation (SKCV) instead. 

::: {.column-margin}

**Packages to load for replication**

```{r}
#| include: false

library(spatialsample)
library(tidyverse)
library(data.table)
library(sf)
library(patchwork)
library(mgcv)
library(parallel)
library(gstat)
```

```{r}
#| eval: false
library(spatialsample)
library(tidyverse)
library(data.table)
library(sf)
library(patchwork)
library(mgcv)
library(parallel)
library(gstat)
```
:::

:::{.callout-note}
**Goals of this section**

+ Examine the consequence of using KCV when the data are spatially correlated
+ Understand how SKCV is conducted and how to implement it in R
+ Examine the difference in the estimate of MSEs from KCV and spatial KCV
+ Check if the optimal hyper-parameters suggested by KCV and SKCV are any different

:::

For this section, we will consider the following data generating process. 

$$
\begin{aligned}
y = \alpha + \beta_1 x - 0.5 \beta_2 x^2 + u 
\end{aligned}
$$

where $x$ and $u$ are spatially correlated (which makes $y$ spatially correlated).


::: {.column-margin}
**Code to create the dataset**

```{r}
#| message: false 
#| warning: false 

#=== create grids (on top of IL state border) ===#
# any spatial object will do. IL state border is picked
# just because it is easy to get without reading a shapefile

grids_il <-
  tigris::counties(
    state = "IL",
    progress_bar = FALSE
  ) %>% 
  st_as_sf() %>% 
  st_make_grid(n = c(50, 50)) %>% 
  st_as_sf() %>% 
  mutate(id := 1:n()) %>% 
  rename(geometry = x)

#=== set up a model ===#
g_temp <- 
  gstat(
    formula = z ~ 1,
    locations = ~ X + Y,
    beta = 0,
    dummy = T,
    model = vgm(
      psill = 12,
      range = 10,
      nugget = 0,
      model = "Sph" 
    ),
    nmax = 50
  )

#=== get coordinates ===#
xy <- 
  st_coordinates(st_centroid(grids_il)) %>% 
  data.table()

#=== generate error and x ===#
set.seed(24034)
error <- 
  predict(g_temp, newdata = xy, nsim = 1, debug.level = 0) %>% 
  data.table()

var1 <- 
  predict(g_temp, newdata = xy, nsim = 1, debug.level = 0) %>% 
  data.table()

#=== assign the generated values to the data ===#
data <- 
  grids_il %>% 
  mutate(
    e = error[, sim1], 
    x = var1[, sim1],
    y_det = x - x^2,
    y = y_det + e
  )
```

:::


Here is the data we are going to work with (see the side note for the code to generate the data):

```{r}
data
```

We have three main variables, `y` (dependent variable), `x` (explanatory variable), and `e` (error). @fig-viz shows how they are spatially distributed. It also shows that all of them are spatially positively correlated.

```{r}
#| fig-cap: Visualization of the data
#| label: fig-viz 
g_error <-
  ggplot(data = data) +
  geom_sf(aes(fill = e), color = NA) +
  scale_fill_viridis_c() +
  theme_void()

g_x <-
  ggplot(data = data) +
  geom_sf(aes(fill = x), color = NA) +
  scale_fill_viridis_c() +
  theme_void()

g_y <-
  ggplot(data = data) +
  geom_sf(aes(fill = y), color = NA) +
  scale_fill_viridis_c() +
  theme_void()

g_y | g_x | g_error  
```

We are going to use `gam()` with `k` $= 30$ and `sp` $= 0$ as the model in conducting KCV and spatial KCV. Let's first create folds for KCV and SKCV. First, here is KCV folds.

```{r}
#| cache: true 

set.seed(93043)
(
kcv_folds <- 
  rsample::vfold_cv(data, v = 6) %>% 
  mutate(type := "KCV")
)
```

@fig-kcv is the visualization of the spatial distribution of training and test datasets for each of the five folds for KCV.

```{r}
#| fig-cap: Spatial distribution of train and test data points by fold
#| label: fig-kcv
#| code-fold: true
#| warning: false

plot_kcv_data <-
  kcv_folds %>% 
  rowwise() %>% 
  mutate(folds = list(
    rbind(
      analysis(splits) %>% mutate(type = "training"),
      assessment(splits) %>% mutate(type = "test")
    )
  )) %>% 
  dplyr::select(id, folds) %>% 
  unnest() %>% 
  st_as_sf()

ggplot(plot_kcv_data) +
  geom_sf(aes(fill = type), color = NA) +
  facet_wrap(id ~ .) +
  theme_void()

```

Now, let's create a five spatially clustered folds using the `spatialsample` package for SKCV.

```{r}
#| cache: true 

set.seed(2493)
(
skcv_folds <- 
  spatial_clustering_cv(data, v = 6, cluster_function = "hclust") %>% 
  mutate(type := "SKCV")
)
```

@fig-sp-kcv presents the spatial distribution of training and test datasets for each of the five folds for SKCV.

```{r}
#| fig-cap: Spatial distribution of spatially clustered train and test data points by fold 
#| label: fig-sp-kcv
#| code-fold: true
#| warning: false

plot_kcv_data <-
  skcv_folds %>% 
  rowwise() %>% 
  mutate(folds = list(
    rbind(
      analysis(splits) %>% mutate(type = "training"),
      assessment(splits) %>% mutate(type = "test")
    )
  )) %>% 
  dplyr::select(id, folds) %>% 
  unnest() %>% 
  st_as_sf()

ggplot(plot_kcv_data) +
  geom_sf(aes(fill = type), color = NA) +
  facet_wrap(id ~ .) +
  theme_void()

```

Let's now implement KCV and SKCV. Since we observe the true generating process, we can calculate how good the fitted curve is compared to true $E[y|X]$ in addition to observed $y$ for the left-out samples in each fold. 

```{r}
#| cache: true

(
cv_results <-
  rbind(kcv_folds, skcv_folds) %>% 
  #=== make it possible to apply function row by row ===#
  rowwise() %>% 
  #=== train the model ===#
  mutate(gam_fit = list(
    gam(y ~ s(x, k = 6), data = analysis(splits))
  )) %>% 
  #=== get mse ===#
  mutate(mse_data = list(
    assessment(splits) %>% 
    data.table() %>% 
    .[, y_hat := predict(gam_fit, newdata = .)] %>% 
    .[, .(
      mse_obs = mean((y - y_hat)^2), # MSE
      mse_true = mean((y_det - y_hat)^2) # deviation from E[y|x]
    )]
  )) %>% 
  dplyr::select(id, type, mse_data) %>% 
  unnest() %>% 
  data.table()
)
```

You can see that MSE values (`mse_obs`) are mostly greater and also more variable for SKCV. By averaging MSE over folds by CV type,

```{r}
cv_results[, .(mse_obs = mean(mse_obs)), by = type]
```

So, indeed KCV provides lower estimate of test MSE than SKCV. Now, it is important to recognize the fundamental difference in what is measured by KCV and SKCV. KCV measures the accuracy of the trained model applied to the new data points that are located inside the area where the train data covers geographically. SKCV, on the other hand, measures the accuracy of the trained model applied to the new data points that are <span style="color:blue"> outside </span> the area where the train data covers geographically. In other words, it measure the modeling accuracy when the trained model is applied to a new region. So, KCV does overstate the accuracy of the model if your interest is applying the model to a new region. 

```{r}
cv_results[, .(mse_true = mean(mse_true)), by = type]
```


## Hyper-parameter tuning

Now, let's see whether KCV and SKCV lead to different tuning results using `gam()`. Here, we fix `k` at $30$ and vary `sp` to find the best `sp` value. The following function takes an `sp` value and return MSE values from both KCV and SKCV.

```{r}
get_mse <- function(sp) {
  cv_results <-
    rbind(kcv_folds, skcv_folds) %>% 
    #=== make it possible to apply function row by row ===#
    rowwise() %>% 
    #=== train the model ===#
    mutate(gam_fit = list(
      gam(y ~ s(x, k = 30, sp = sp), data = analysis(splits))
    )) %>% 
    #=== get mse ===#
    mutate(mse_data = list(
      assessment(splits) %>% 
      data.table() %>% 
      .[, y_hat := predict(gam_fit, newdata = .)] %>% 
      .[, .(
        mse_obs = mean((y - y_hat)^2), # MSE
        mse_true = mean((y_det - y_hat)^2) # deviation from E[y|x]
      )]
    )) %>% 
    dplyr::select(id, type, mse_data) %>% 
    unnest() %>% 
    data.table() %>% 
    .[, sp_v := sp]

  return(cv_results)

}
```

Here is an example at `sp` $= 0.5$.
```{r}
get_mse(0.5)[]
```

Let's try `sp` = `r seq(0, 2, length = 11)`.

```{r}
cv_results <-
  mclapply(
    seq(0, 2, length = 10),
    function(x) get_mse(x),
    mc.cores = detectCores() - 2
  ) %>% 
  rbindlist()

```

Panel (a) of @fig-mse-tune plots MSE against the value of `sp` for KCV and SKCV. As you can see both of the suggested 0.2 as the MSE-minimizing value of `sp`. Panel (b) of @fig-mse-tune shows the squared deviations from $E[y|x]$ as a function of `sp`, which tells us what value of `sp` would result in the curve that is the closest to $E[y|x]$, the quantity of interest. Both KCV and SKCV suggest that `sp` value of 0.2 is the best. This means that both KCV and SKCV based on train data successfully identified the optimal value of `sp`. Remember that we do not really care about prediction accuracy. That is not our ultimate goal. Using KCV instead of SKCV is likely to result in a more optimistic view of how good the model actually is as we saw earlier. However, the purpose of cross-validation is tuning, but not trying to measure the accuracy of our trained model (at least for those who do not care about prediction itself). Moreover, any type of CV tends to heavily underestimate the true test ME anyway.

```{r}
#| fig-cap: MSE and squared deviation from E[y|x] by KCV and SKCV
#| fig-subcap: 
#|   - "MSE"
#|   - "Square Deviation from E[y|x]"
#| label: fig-mse-tune
#| layout-ncol: 1 
#| code-fold: true

cv_plot_data <- 
  cv_results[
    , 
    .(mse_obs = mean(mse_obs), mse_true = mean(mse_true)), 
    by = .(type, sp_v)
  ]

ggplot(cv_plot_data) +
  geom_line(aes(y = mse_obs, x = sp_v)) +
  geom_point(aes(y = mse_obs, x = sp_v)) +
  facet_grid(type ~ ., scale = "free_y") +
  ylab("MSE") +
  xlab("sp") +
  theme_bw() 

ggplot(cv_plot_data) +
  geom_line(aes(y = mse_true, x = sp_v)) +
  geom_point(aes(y = mse_true, x = sp_v)) +
  facet_grid(type ~ ., scale = "free_y") +
  ylab("Square Deviation from E[y|x]") +
  xlab("sp") +
  theme_bw()
```








 

