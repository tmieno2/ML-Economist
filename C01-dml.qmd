# Double Machine Learning {#sec-dml}

The most important ideas of the recent development of causal machine learning (CML) methods originate from @Chernozhukov2018, which proposed Double/Debiased ML methods. In this section, we go over those key ideas that are the heart of many other important CML methods we will learn later. 

## Problem Setting

We are interested in the estimating the following econometric model

:::{.column-margin}
We follow the notations of @Chernozhukov2018.
:::

$$
\begin{aligned}
y = \theta T + g_0(X) + \mu \\
T = m_0(X) + \eta
\end{aligned}
$$
Your sole interest is in estimating $\theta$: the impact of the treatment ($T$). $g_0(X)$ is the impact of a collection of variables $X$. $m_0(X)$ expresses how $X$ affects the treatment status, $T$. $T$ may be binary or continuous. The key assumptions here are $E[\mu|X]$ and $E[\eta|X]$....... That is...


:::{.column-margin}
Note that the treatment effect is assumed to be constant irrespective of the value of $X$. So, the treatment effect is not heterogeneous. We will cover heterogeneous treatment effect estimation later.
:::

:::{.callout-tip}
$g_0(X)$ and $m_0(X$ are called nuisance functions or parameters because we are not interested in what they look like. We are only interested in **controlling for** them. 
:::


+ What happens if the first stage regressions are tuned to avoid over-fitting? Do you still need cross-fitting?


## A naive approach and regularization bias

::: {.column-margin}
**Packages to load for replication**

```{r}
#| include: false

library(data.table)
library(magick)
library(fixest)
library(DoubleML)
library(tidyverse)
library(mlr3)
library(parallel)
library(mlr3learners)
library(ggbrace)
library(rsample)
library(MASS)
library(ranger)
```

```{r}
#| eval: false
library(data.table)
library(magick)
library(fixest)
library(DoubleML)
library(tidyverse)
library(mlr3)
library(parallel)
library(mlr3learners)
library(ggbrace)
library(rsample)
library(MASS)
library(ranger)
```
:::

One way to estimate $\theta$ follows the following steps:

+ Step 1: Estimate $g_0(X)$ and then subtract the fitted value of $g_0(X)$ from $y$.
  * Step 1.1: Regress $y$ on $X$ to estimate $E[y|X]$ and call it $\hat{l}_0(x)$ 
  * Step 1.2: Get an initial estimate of $\theta$ using
  $$
  \begin{aligned}
    \hat{\theta}_{init} = (\frac{1}{n}\sum_{i=1}^N d_i d_i)^{-1}\frac{1}{n}\sum_{i=1}^N d_i (y_i - \hat{l}_0(X_i))
  \end{aligned} {#eq-partial-out}
  $$
  * Step 1.3: Regress $y_i - \hat{\theta}_{init}$ on $X$ to estimate $g_0(X)$.
+ Step 2: Regress $y - \hat{g}_0(X)$ on $d$. Or equivalently, use the following formula 
  $$
  \begin{aligned}
    \hat{\theta} = (\frac{1}{n}\sum_{i=1}^N d_i d_i)^{-1}\frac{1}{n}\sum_{i=1}^N d_i (y_i - \hat{g}_0(X_i))
  \end{aligned}
  $$

::: {.column-margin}
The fitted value of $g_0(X)$ is denoted as $\hat{g}_0(X)$.
:::

As it will turn out, this procedure suffers from the so-called regularization bias [@Chernozhukov2018].

To demonstrate the bias problem, we work on the following data generating process used in [the user guide for the DoubleML package](https://docs.doubleml.org/stable/guide/basics.html).

$$
\begin{aligned}
y_i = 0.5 d_i + x_{i,1} + \frac{1}{4}\cdot\frac{exp(x_{i,3})}{1 + exp(x_{i,3})} + \mu_i \\
d_i = \frac{exp(x_{i,1})}{1 + exp(x_{i,1})} + \frac{1}{4}\cdot x_{i,3}+ \eta_i
\end{aligned}
$$

where $\mu_i \sim N(0, 1)$ and $\eta_i \sim N(0, 1)$ (So, no endogeneity problem). In this data generating process, $d$ is not binary and its effect on $y$ is assumed to be linear. 

We use the `gen_data()` function (defined on the right), which is a slightly generalized version of the `make_plr_CCDDHNR2018()` function from the `DoubleML` package.

::: {.column-margin}
```{r}
gen_data <- function(
  g_formula = formula(~ I(exp(x1)/(1+exp(x1))) + I(x3/4)), # formula that defines g(x)
  m_formula = formula(~ x1 + I(exp(x3)/(1+exp(x3))/4)), # formula that defines m(x)
  te_formula = formula(~ I(0.5*d)), # formula that defines theta(x) * t
  n_obs = 500, 
  n_vars = 20, 
  mu_x = 0, 
  vcov_x = NULL,
  sigma = 1 # sd of the error term in the y equation
)
{

  if (is.null(vcov_x)) {
    vcov_x <- matrix(rep(0, n_vars^2), nrow = n_vars)
    for (i in seq_len(n_vars)) {
      vcov_x[i, ] <- 0.7^abs(i - seq_len(n_vars)) 
    }
  }

  #=== draw from multivariate normal ===#
  data <- 
    mvrnorm(n_obs, mu = rep(0, n_vars), Sigma = vcov_x) %>% 
    data.table() %>% 
    setnames(names(.), paste0("x", 1:n_vars))  

  #=== generate d ===#
  if (m_formula == "independent") {
    data[, d := rnorm(n_obs)]
  } else {
    data[, d := model.frame(m_formula, data = data) %>% rowSums() + rnorm(n_obs)]
  }

  #=== generate y ===#
  data[, g := model.frame(g_formula, data = data) %>% rowSums()]

  #=== generate treatment effect ===#
  data[, te := model.frame(te_formula, data = data) %>% rowSums()]

  #=== generate y ===#
  data[, y := te + g + rnorm(n_obs, sd = sigma)]

  return(data[])

}
```
:::


```{r}
set.seed(3287423)

training_data <- gen_data()
```

```{r}
set.seed(1234)
n_rep = 1000
n_obs = 500
n_vars = 20
alpha = 0.5

data = list()
for (i_rep in seq_len(n_rep)) {
    data[[i_rep]] = make_plr_CCDDHNR2018(alpha=alpha, n_obs=n_obs, dim_x=n_vars,
                                          return_type="data.frame")
}
```

It has 20 `x` variables (for $X$) along with `d` (treatment) and `y` (dependent variable). Only `x1` and `x3` are the relevant variables and the rest of $X$ are irrelevant. 

```{r}
str(training_data)
```

### Step 1

Let's now work on Step 1. We estimate $g_0(X)$ using random forest (RF). As described above, this is a three-step process.


::: {.column-margin}
It does not have to be RF. Indeed, you can use any statistical methods in this step.
:::

First, we estimate $l_0(X)$ by regressing $y$ on $X$.

```{r}
#--------------------------
# Step 1.1
#--------------------------
rf_fitted_l0 <-
  ranger(
    y ~ .,
    data = dplyr::select(training_data, c("y", starts_with("x"))),
    mtry = 5,
    num.trees = 132,
    max.depth = 5,
    min.node.size = 1
  )

#=== fitted values ===#
l0_hat <- rf_fitted_l0$predictions

#=== create y - l0_hat ===#
training_data[, y_less_l := y - l0_hat]
```

```{r}
rf_fitted_m0 <-
  ranger(
    d ~ .,
    data = dplyr::select(training_data, c("d", starts_with("x"))),
    mtry = 5,
    num.trees = 378,
    max.depth = 3,
    min.node.size = 6
  )

#=== fitted values ===#
m0_hat <- rf_fitted_m0$predictions

#=== create y - m0_hat ===#
training_data[, d_less_m := d - m0_hat]
```

We now get an initial estimate of $\theta$ using @eq-partial-out.

```{r}
#--------------------------
# Step 1.2
#--------------------------
theta_init <- training_data[, sum(d_less_m * y_less_l) / sum(d_less_m * d_less_m) ]
```

Finally, we regress $y - \theta_{init}d$ on $X$ to fit $g_0(X)$.

```{r}
#--------------------------
# Step 1.3
#--------------------------
#=== define y - treatment effect ===#
training_data[, y_less_te := y - theta_init * d]

#=== fit rf ===#
rf_fitted_g0 <-
  ranger(
    y_less_te ~ .,
    data = dplyr::select(training_data, c("y_less_te", starts_with("x"))),
    mtry = 5,
    num.trees = 132,
    max.depth = 5,
    min.node.size = 1
  )

#=== fitted values ===#
g0_hat <- training_data[, rf_fitted_g0$predictions]

#=== create y - g0 ===#
training_data[, y_less_g := y - g0_hat]
```

### Step 2

we regress $y - \hat{g})_0(X)$ on $d$.

```{r}
(
theta_hat <- lm(y_less_g ~ d, data = training_data)$coefficient["d"]
)
```

So, in this instance, we get an estimate of $\theta$ that is a bit lower than the true value of $\theta$. Let's repeat this process many times to see how this procedure performs on average. @fig-naive shows the histogram of $\hat{\theta}$ from 500 simulations. You can see that this procedure has led to consistent underestimation of the treatment effect. There are two sources of bias in this approach: regularization and over-fitting bias. 

```{r}
#| code-fold: true 
#| fig-cap: Simulation results of the naive procedure
#| label: fig-naive

fit_m0 <- function(training_data) {

  rf_fitted_m0 <-
    ranger(
      d ~ .,
      data = dplyr::select(training_data, c("d", starts_with("x"))),
      mtry = 5,
      num.trees = 378,
      max.depth = 3,
      min.node.size = 6
    )

  return(rf_fitted_m0)

}


#===================================
# Define a function that will get you g0_hat
#===================================
# this function will be used later

fit_g0 <- function(training_data, rf_fitted_m0) {
  #--------------------------
  # Step 1.1
  #--------------------------
  rf_fitted_l0 <-
    ranger(
      y ~ .,
      data = dplyr::select(training_data, c("y", starts_with("x"))),
      mtry = 12,
      num.trees = 132,
      max.depth = 5,
      min.node.size = 1
    )

  #=== fitted values ===#
  l0_hat <- rf_fitted_l0$predictions

  #=== create y - l0_hat ===#
  training_data[, y_less_l := y - l0_hat]

  #--------------------------
  # Step 1.2
  #--------------------------
  #=== fitted values ===#
  m0_hat <- rf_fitted_m0$predictions

  #=== create y - m0_hat ===#
  training_data[, d_less_m := d - m0_hat]

  #--------------------------
  # Step 1.2
  #--------------------------
  theta_init <- training_data[, sum(d * y_less_l) / sum(d * d)]

  #--------------------------
  # Step 1.3
  #--------------------------
  #=== define y - treatment effect ===#
  training_data[, y_less_te := y - theta_init * d]

  #=== fit rf ===#
  rf_fitted_g0 <-
    ranger(
      y_less_te ~ .,
      data = dplyr::select(training_data, c("y_less_te", starts_with("x"))),
      mtry = 12,
      num.trees = 132,
      max.depth = 5,
      min.node.size = 1
    )

  return(rf_fitted_g0)

}

#===================================
# Define a function that runs a single simulation and gets you theta_hat
#===================================
run_sim_naive <- function(i){

  training_data <- data[[i]] %>% data.table()

  rf_fitted_m0 <- fit_m0(training_data)
  rf_fitted_g0 <- fit_g0(training_data, rf_fitted_m0)
  g0_hat <- rf_fitted_g0$predictions

  #=== create y - g0 ===#
  training_data[, y_less_g := y - g0_hat]

  theta_hat <- lm(y_less_g ~ d, data = training_data)$coefficient["d"]

  # theta_hat <- training_data[, sum(d * y_less_g) / sum(d * d)]

  return(theta_hat)

}

#===================================
# Repeat MC simulations 500 times
#===================================
theta_hats <-
  mclapply(
    1:500,
    run_sim_naive,
    mc.cores = detectCores() / 4 * 3
  ) %>% 
  unlist() %>% 
  data.table(theta = .)

#===================================
# Plot the results
#===================================
ggplot(theta_hats) +
  geom_histogram(aes(x = theta), color = "white") +
  theme_bw() + 
  geom_vline(xintercept = 0.5, color = "red")
```

## Overcoming the regularization bias

Regularization bias can be overcome by double-debiasing. Specifically, 


+ Step 1: Estimate $g_0(X)$ and then subtract the fitted value of $g_0(X)$ from $y$ 
+ Step 2: Subtract $\hat{m}_0(X)$ from $d$ ($\tilde{d} = d - \hat{m}_0(x)$)
+ Step 3: Calculate $\hat{\theta}$ based on the following formula

$$
\begin{aligned}
\hat{\theta} = (\frac{1}{n}\sum_{i=1}^N \tilde{d}_i d_i)^{-1}\frac{1}{n}\sum_{i=1}^N \tilde{d}_i (y_i - \hat{g}_0(X_i))
\end{aligned}
$$

We have done Step 1 already in the previous approach. Let's work on Steps 2 and 3.

```{r}
#--------------------------
# Step 2
#--------------------------
#=== fitted value ===#
m0_hat <- rf_fitted_m0$predictions

#=== create T - m0_hat ===#
training_data[, d_less_m := d - m0_hat]

#--------------------------
# Step 3
#--------------------------
(
theta_hat <- training_data[, sum(d_less_m * y_less_g) / sum(d_less_m * d)]
)
```

Yes, this is just a single instance, but things are looking better. Now, let's repeat this 500 times. @fig-dd shows the distribution of $\hat{\theta}$, which is almost centered at $0.5$. The current approach still suffers from the so-called over-fitting bias[@Chernozhukov2018]. Let's look at how we can overcome this bias next.

```{r}
#| code-fold: true
#| fig-cap: Simulation results of double-debiased approach
#| label: fig-dd

run_sim_dereg <- function(i)
{
  # training_data <- gen_data()
  training_data <- data[[i]] %>% data.table

  rf_fitted_m0 <- fit_m0(training_data)
  m0_hat <- rf_fitted_m0$predictions

  #=== create d - m0_hat ===#
  training_data[, d_less_m := d - m0_hat]

  #=== get g0_hat ===#
  rf_fitted_g0 <- fit_g0(training_data, rf_fitted_m0)
  g0_hat <- rf_fitted_g0$predictions

  #=== create y - g0 ===#
  training_data[, y_less_g := y - g0_hat]

  theta_hat <- training_data[, sum(d_less_m * y_less_g) / sum(d_less_m * d)]

  return(theta_hat)
}

theta_hats <-
  mclapply(
    1:500,
    function(x) run_sim_dereg(x),
    mc.cores = detectCores() / 4 * 3
  ) %>% 
  unlist() %>% 
  data.table(theta = .)

ggplot(theta_hats) +
  geom_histogram(aes(x = theta), color = "white") +
  theme_bw() + 
  geom_vline(xintercept = 0.5)

```

## Overcoming the over-fitting bias

Over-fitting bias can be overcome by cross-fitting. First, the training data is split into $K$-folds just like K-fold cross-validation. Let's denote them as $I_1, \dots, I_k$. For example, for $I_1$, the following steps are taken (@fig-cross-fitting provides a visual illustration):

+ Step 1: Estimate $\hat{g}_0(x)$ and $\hat{m}_0(x)$ using the data from the other folds ($I_2, \dots, I_K$).
+ Step 2: Estimate $\hat{g}_0(x_i)$ and $\hat{m}_0(x_i)$ for each $i \in I_1$ and calculate $\tilde{y}_i = y_i - \hat{g}_0(x_i)$ and $\tilde{d}_i = d_i - \hat{m}_0(x_i)$.

This process is repeated for all the $K$ folds. 

We then use the following formula to obtain $\hat{\theta}$.

::: {.column-margin}
This is the same as the Step 3 of the previous approach.
:::

$$
\begin{aligned}
\hat{\theta} = (\frac{1}{n}\sum_{i=1}^N \tilde{d}_i d_i)^{-1}\frac{1}{n}\sum_{i=1}^N \tilde{d}_i (y_i - \hat{g}_0(X_i))
\end{aligned}
$$ {#eq-iv-score}


```{r}
#| code-fold: true
#| fig-width: 9
#| fig-cap: Illustration of cross-fitting
#| label: fig-cross-fitting

ggplot() +
  #=== fold 1 ===#
  geom_rect(
    aes(xmin = 0, xmax = 10, ymin = 0, ymax = 2),
    fill = "grey",
    color = "black",
    size = 1.2
  ) +
  geom_rect(
    aes(xmin = 0, xmax = 2.5, ymin = 0, ymax = 2),
    fill = "black"
  ) +
  #=== fold 2 ===#
  geom_rect(
    aes(xmin = 0, xmax = 10, ymin = 2.2, ymax = 4.2),
    fill = "grey",
    color = "black",
    size = 1.2
  ) +
  geom_rect(
    aes(xmin = 2.5, xmax = 5, ymin = 2.2, ymax = 4.2),
    fill = "black"
  ) +
  #=== fold 3 ===#
  geom_rect(
    aes(xmin = 0, xmax = 10, ymin = 4.4, ymax = 6.4),
    fill = "grey",
    color = "black",
    size = 1.2
  ) +
  geom_rect(
    aes(xmin = 5, xmax = 7.5, ymin = 4.4, ymax = 6.4),
    fill = "black"
  ) +
  #=== fold 4 ===#
  geom_rect(
    aes(xmin = 0, xmax = 10, ymin = 6.6, ymax = 8.6),
    fill = "grey",
    color = "black",
    size = 1.2
  ) +
  geom_rect(
    aes(xmin = 7.5, xmax = 10, ymin = 6.6, ymax = 8.6),
    fill = "black"
  ) +
  geom_brace(aes(c(0, 7.4), c(8.7, 9.2)), inherit.data=F) +
  annotate(
    "text", x = 3.5, y = 9.7, parse = TRUE,
    label = "'Find ' * hat(g)[0](x) * ' and ' * hat(m)[0](x) * ' from this data.'",
    size = 6
  ) +
  geom_curve(
    aes(x = 2.2, xend = 8.5, y = 10.3, yend = 8.8),
    arrow = arrow(length = unit(0.03, "npc")),
    curvature = -0.3
  ) +
  geom_curve(
    aes(x = 3.4, xend = 8, y = 10.3, yend = 8.8),
    arrow = arrow(length = unit(0.03, "npc")),
    curvature = -0.3
  ) +
  ylim(NA, 11) +
  # coord_equal() +
  theme_void()
```

Let's code the cross-fitting procedure. We first split the data into 4 folds ($K = 4$).

```{r}
(
data_folds <- rsample::vfold_cv(training_data, v = 4)
)
```

Let's cross-fit for fold 1.

```{r}
split_1 <- data_folds[1, ]

#=== data for estimating g_0  and m_0 ===#
data_train <- analysis(split_1$splits[[1]]) 

#=== data for which g_0(x_i) and m_0(x_i) are calculated ===#
data_target <- assessment(split_1$splits[[1]]) 
```

First, we fit $\hat{g}_0(x)$ and $\hat{m}_0(x)$ using the data from the other folds.

```{r}
#=== g0 ===#
g_rf_fit <- fit_g0(data_train)

#=== m0 ===#
m_rf_fit <- fit_m0(data_train)
```

Next, we estimate $\hat{g}_0(x_i)$ and $\hat{m}_0(x_i)$ for each $i$ and calculate $\tilde{y}_i = y_i - \hat{g}_0(x_i)$ and $\tilde{d}_i = d_i - \hat{m}_0(x_i)$.

```{r}
#| warning: false

data_orth <-
  data_target %>% 
  #=== prediction of g_0(x_i) ===#
  .[, g_0_hat := predict(g_rf_fit, data = .)$predictions] %>% 
  #=== orthogonalize y ===#
  .[, y_tilde := y - g_0_hat] %>% 
  #=== prediction of m_0(x_i) ===#
  .[, m_0_hat := predict(m_rf_fit, data = .)$predictions] %>% 
  #=== orthogonalize d ===#
  .[, d_tilde := d - m_0_hat]
```

We can repeat this for all the folds.

```{r}
#| warning: false

cross_fit <- function(i, data_folds)
{

  #--------------------------
  # Prepare data
  #--------------------------
  #=== ith split ===#
  working_split <- data_folds[i, ]

  #=== data for estimating g_0  and m_0 ===#
  data_train <- analysis(working_split$splits[[1]]) 

  #=== data for which g_0(x_i) and m_0(x_i) are calculated ===#
  data_target <- assessment(working_split$splits[[1]]) 

  #--------------------------
  # Fit g0 and m0
  #--------------------------
  #=== m0 ===#
  m_rf_fit <- fit_m0(data_train)

  #=== g0 ===#
  g_rf_fit <- fit_g0(data_train, m_rf_fit)

  #--------------------------
  # Get y_tilde and d_tilde
  #--------------------------
  data_orth <-
    data_target %>% 
    #=== prediction of g_0(x_i) ===#
    .[, g_0_hat := predict(g_rf_fit, data = .)$predictions] %>% 
    #=== orthogonalize y ===#
    .[, y_tilde := y - g_0_hat] %>% 
    #=== prediction of m_0(x_i) ===#
    .[, m_0_hat := predict(m_rf_fit, data = .)$predictions] %>% 
    #=== orthogonalize d ===#
    .[, d_tilde := d - m_0_hat] %>% 
    .[, .(y_tilde, d_tilde, d)]

  theta_cf <- data_orth[, sum(d_tilde * y_tilde) / sum(d_tilde * d)]

  return(theta_cf)
}

get_cross_fitted_data <- function (data_folds){

  return_data <- 
    lapply(
      seq_len(nrow(data_folds)),
      function(x) cross_fit(x, data_folds)
    ) %>% 
    unlist()

  return(return_data)
}

(
cf_data <-  get_cross_fitted_data(data_folds) %>% mean()
)
```

Now, let's use @eq-iv-score to calculate $\hat{\theta}$.

```{r}
theta_hat <- cf_data[, sum(d_tilde * y_tilde)] / cf_data[, sum(d_tilde * d)] 
```

Okay, now that we understand the steps of this approach, let's repeat this many times.

```{r}
#| cache: true 
#| fig-cap: The distribution of treatment effect estimated by the double-debiased approach with cross-fitting
#| label: fig-cf-debiased

#=== run MC simulations ===#
theta_hats_cf <-
  mclapply(
    1:200,
    function(x) {
      print(x)
      training_data <- gen_data()
      data_folds <- rsample::vfold_cv(training_data, v = 4)
      cf_data <-  get_cross_fitted_data(data_folds)
      theta_hat <- cf_data[, sum(d_tilde * y_tilde)] / cf_data[, sum(d_tilde * d)] 
      return(theta_hat)
    },
    mc.cores = detectCores() * 3 / 4
  ) %>% 
  unlist()

#=== visualize the results ===#
ggplot() +
  geom_histogram(aes(x = theta_hats_cf), color = "white") +
  theme_bw() + 
  geom_vline(xintercept = 0.5)

```












