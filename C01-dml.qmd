# Double Machine Learning {#sec-dml}

One of the most important ideas of the recent development of causal machine learning (CML) methods originate from @Chernozhukov2018, which proposed Double/Debiased ML methods. In this section, we go over those key ideas that are at the heart of many other important CML methods we will learn later. We then learn various models you can estimate using the R and Python `DoubleML` package [@DoubleML2021R; @DoubleML2022Python].

:::{.callout-important}

## What you will learn

+ How double/debiased machine learning works 
  * avoiding regularlization bias through orthogonalization
  * avoiding over-fitting bias through cross-fitting
+ Mechanics of DML
  * method of moment
  * score function
  * cross-fitting
+ How to estimate ATE under conditional unconfoundedness using `DoubleMLPLR()` and under confoundedness using `DoubleMLPLIV()`.

:::

:::{.callout-tip}

## Preferable background knowledge

+ how to use `mlr3` (see @sec-mlr3)
+ idea of method of moment
+ causal diagram 

:::

## DML: the basic idea

### Problem Setting

Throughout this section, we are interested in estimating the following econometric model
, which is called a partially linear regression model (PLR).

:::{.column-margin}
We try to follow the notations of @Chernozhukov2018 as much as possible.
:::

$$
\begin{aligned}
y = \theta d + g_0(X) + \mu \\
d = m_0(X) + \eta
\end{aligned}
$$ 

::: {.column-margin}
Partially linear regression model is a class of models where some of the variables are linear in parameter (here, $\theta d$) and the rest of the variables are modeled non-parametrically (here, $g_0(X)$).
:::

Your sole interest is in estimating $\theta$: the impact of the treatment ($d$). $g_0(X)$ is the impact of a collection of variables $X$. $m_0(X)$ expresses how $X$ affects the treatment status, $d$. $d$ may be binary or continuous. $

:::{.callout-note}

## Assumptions on the error terms

+ $E[\mu|D,X] = 0$
+ $E[\eta|X] = 0$

:::

:::{.callout-note}
The treatment effect is assumed to be constant irrespective of the value of $X$. So, the treatment effect is not heterogeneous. We will cover heterogeneous treatment effect estimation later.
:::

:::{.callout-tip}

## Terminology alert

$g_0(X)$ and $m_0(X)$ are called <span style="color:blue"> nuisance functions </span>because knowing them is not the ultimate goal. We are only interested in **controlling for** them to estimate $\theta$ accurately. 
:::

### An intuitive, yet naive approach {#sec-dml-naive}

::: {.column-margin}
**Packages to load for replication**

```{r}
#| include: false

library(data.table)
library(magick)
library(fixest)
library(DoubleML)
library(tidyverse)
library(mlr3)
library(parallel)
library(mlr3learners)
library(ggbrace)
library(rsample)
library(MASS)
library(ranger)
```

```{r}
#| eval: false
library(data.table)
library(magick)
library(fixest)
library(DoubleML)
library(tidyverse)
library(mlr3)
library(parallel)
library(mlr3learners)
library(ggbrace)
library(rsample)
library(MASS)
library(ranger)
```
:::


Consider the estimating equation of interest below.

$$
\begin{aligned}
y = \theta d + g_0(X) + \mu
\end{aligned}
$$ {#eq-model-naive}

Subtracting $g_0(x)$ from both sides,

$$
\begin{aligned}
y - g_0(x) = \theta d + \mu
\end{aligned}
$$

So, if we know $g_0(X)$, then we can simply regress $y - g_0(x)$ on $d$. Of course, we do not know $g_0(X)$, so we need to estimate $g_0(x)$. Let $\hat{g}_0(x)$ denote $g_0(x)$ estimated by any appropriate machine learning method. Then, we can regress $y - \hat{g}_0(x)$ on $d$ to estimate $\theta$ using OLS. Mathematically, it can be written as follows:

$$
\begin{aligned}
\hat{\theta} = (\frac{1}{n}\sum_{i=1}^N d_i d_i)^{-1}\frac{1}{n}\sum_{i=1}^N d_i (y_i - \hat{g}_0(X_i))
\end{aligned}
$$

Now, subtracting $\hat{g}_0(x)$ from both sides of @eq-model-naive,

$$
\begin{aligned}
y - \hat{g}_0(x) = \theta d + (g_0(x) - \hat{g}_0(x) + \mu)
\end{aligned}
$$

So, as long as $d$ is not correlated with $g_0(x) - \hat{g}_0(x) + \mu$, then the regression of $y - \hat{g}_0(x)$ on $d$ should work. Unfortunately, this approach turns out to be naive and suffer from bias in general [@Chernozhukov2018].

As a way to implement this naive approach, consider the following procedures.

+ Step 1: Estimate $g_0(X)$ and then subtract the fitted value of $g_0(X)$ from $y$.
  * Step 1.1: Regress $y$ on $X$ to estimate $E[y|X]$ and call it $\hat{l}_0(x)$ 
  * Step 1.2: Regress $d$ on $X$ to estimate $E[d|X]$ ($m_0(X)$), call it $\hat{m}_0(x)$, and calculate $\tilde{d} = d - \hat{m}_0(X)$.
  * Step 1.3: Get an initial estimate of $\theta$ using
  $$
  \begin{aligned}
    \hat{\theta}_{init} = (\frac{1}{n}\sum_{i=1}^N \tilde{d}_i \tilde{d}_i)^{-1}\frac{1}{n}\sum_{i=1}^N \tilde{d}_i (y_i - \hat{l}_0(X_i))
  \end{aligned} 
  $$ {#eq-partial-out}
  * Step 1.4: Regress $y_i - \hat{\theta}_{init}d$ on $X$ to estimate $g_0(X)$ and call it $\hat{g}_0(X)$.
+ Step 2: Regress $y - \hat{g}_0(X)$ on $d$ using OLS. Or equivalently, use the following formula 
  $$
  \begin{aligned}
    \hat{\theta} = (\frac{1}{n}\sum_{i=1}^N d_i d_i)^{-1}\frac{1}{n}\sum_{i=1}^N d_i (y_i - \hat{g}_0(X_i))
  \end{aligned}
  $$ {#eq-theta-partial}

To demonstrate the bias problem, we work on the following data generating process used in [the user guide for the DoubleML package](https://docs.doubleml.org/stable/guide/basics.html).

$$
\begin{aligned}
y_i = 0.5 d_i  + \frac{exp(x_{i,1})}{1 + exp(x_{i,1})} + \frac{1}{4}\cdot x_{i,3} + \mu_i \\
d_i = x_{i,1} + \frac{1}{4}\cdot\frac{exp(x_{i,3})}{1 + exp(x_{i,3})} \eta_i
\end{aligned}
$$ {#eq-simple-synthetic}

where $\mu_i \sim N(0, 1)$ and $\eta_i \sim N(0, 1)$. The error terms ($\mu_i$ and $\eta_i$) are independent. In this data generating process, $d$ is continuous (not binary) and its effect on $y$ is assumed to be linear. 

We use the `gen_data()` function (defined on the right), which is a slightly generalized version of the `make_plr_CCDDHNR2018()` function from the `DoubleML` package.

::: {.column-margin}
`gen_data()` allows you to specify $g_0(X)$ and $m_0(X)$ unlike `make_plr_CCDDHNR2018()`.

```{r}

# g(x) = E[y-\theta\cdot d|X]
# m(x) = E[d|X]
# f(x) = E[z|X]

gen_data <- function(
  g_formula = formula(~ I(exp(x1)/(1+exp(x1))) + I(x3/4)), # formula that defines m(x)
  m_formula = formula(~ x1 + I(exp(x3)/(1+exp(x3))/4)), # formula that defines g(x)
  te_formula = formula(~ I(0.5*d)), # formula that defines theta(x) * t
  n_obs = 500, 
  n_vars = 20, 
  mu_x = 0, 
  vcov_x = NULL,
  sigma = 1 # sd of the error term in the y equation
)
{

  if (is.null(vcov_x)) {
    vcov_x <- matrix(rep(0, n_vars^2), nrow = n_vars)
    for (i in seq_len(n_vars)) {
      vcov_x[i, ] <- 0.7^abs(i - seq_len(n_vars)) 
    }
  }

  #=== draw from multivariate normal ===#
  data <- 
    mvrnorm(n_obs, mu = rep(0, n_vars), Sigma = vcov_x) %>% 
    data.table() %>% 
    setnames(names(.), paste0("x", 1:n_vars))  

  #=== generate d ===#
  if (m_formula == "independent") {
    data[, d := rnorm(n_obs)]
  } else {
    data[, d := model.frame(m_formula, data = data) %>% rowSums() + rnorm(n_obs)]
  }

  #=== generate y ===#
  data[, g := model.frame(g_formula, data = data) %>% rowSums()]

  #=== generate treatment effect ===#
  data[, te := model.frame(te_formula, data = data) %>% rowSums()]

  #=== generate y ===#
  data[, y := te + g + rnorm(n_obs, sd = sigma)]

  return(data[])

}
```
:::


```{r}
set.seed(782394)

training_data <- gen_data()
```

```{r}
#| include: false
#| eval: false
set.seed(19343)
n_rep = 1000
n_obs = 500
n_vars = 20
alpha = 0.5

data = list()
for (i_rep in seq_len(n_rep)) {
    data[[i_rep]] = make_plr_CCDDHNR2018(alpha=alpha, n_obs=n_obs, dim_x=n_vars,
                                          return_type="data.frame")
}
```

It has 20 `x` variables (for $X$) along with `d` (treatment) and `y` (dependent variable). Only `x1` and `x3` are the relevant variables. The rest of $X$ do not play any role in explaining either $Y$ or $d$. However, they are correlated with `x1` and `x3` and interfere with estimating the nuisance functions.

```{r}
str(training_data)
```

#### Step 1

Let's now work on Step 1. We estimate $g_0(X)$ using random forest (RF). As described above, this is a four-step process.


::: {.column-margin}
It does not have to be RF. Indeed, you can use any statistical methods in this step.
:::

**Step 1.1**: Estimate $l_0(X)$ by regressing $y$ on $X$.

```{r}
#--------------------------
# Step 1.1
#--------------------------
rf_fitted_l0 <-
  ranger(
    y ~ .,
    data = dplyr::select(training_data, c("y", starts_with("x"))),
    mtry = 5,
    num.trees = 132,
    max.depth = 5,
    min.node.size = 1
  )

#=== fitted values ===#
l0_hat <- predict(rf_fitted_l0, data = training_data)$predictions

#=== create y - l0_hat ===#
training_data[, y_less_l := y - l0_hat]
```

**Step 1.2**: Estimate $m_0(X)$ by regressing $d$ on $X$.

```{r}
#--------------------------
# Step 1.2
#--------------------------
rf_fitted_m0 <-
  ranger(
    d ~ .,
    data = dplyr::select(training_data, c("d", starts_with("x"))),
    mtry = 5,
    num.trees = 378,
    max.depth = 3,
    min.node.size = 6
  )

#=== fitted values ===#
m0_hat <- predict(rf_fitted_m0, data = training_data)$predictions

#=== create y - m0_hat ===#
training_data[, d_less_m := d - m0_hat]
```

::: {.column-margin}
Figure of `d` (treatment variable) plotted against `m0_hat` ($\hat{m}_0(X)$).

```{r}
#| code-fold: true

ggplot(training_data) +
  geom_point(aes(y = d, x = m0_hat)) +
  geom_abline(slope = 1, color = "red") +
  theme_bw()
```
:::

**Step 1.3**: Get an initial estimate of $\theta$ using @eq-partial-out.

```{r}
#--------------------------
# Step 1.2
#--------------------------
theta_init <- training_data[, sum(d_less_m * y_less_l) / sum(d_less_m * d_less_m) ]
```

**Step 1.4**: Regress $y - \theta_{init}d$ on $X$ to fit $g_0(X)$.

```{r}
#--------------------------
# Step 1.3
#--------------------------
#=== define y - treatment effect ===#
training_data[, y_less_te := y - theta_init * d]

#=== fit rf ===#
rf_fitted_g0 <-
  ranger(
    y_less_te ~ .,
    data = dplyr::select(training_data, c("y_less_te", starts_with("x"))),
    mtry = 5,
    num.trees = 132,
    max.depth = 5,
    min.node.size = 1
  )

#=== fitted values ===#
g0_hat <- predict(rf_fitted_g0, data = training_data)$predictions

#=== create y - g0 ===#
training_data[, y_less_g := y - g0_hat]
```

@fig-g0-ghat plots true $g_0(X)$ (`g`) against $\hat{g}_0(X)$ (`g0_hat`). As you can see, $\hat{g}_0(X)$ is a bit biased.

```{r}
#| fig-cap: ""
#| label: fig-g0-ghat
#| code-fold: true
ggplot(training_data) +
  geom_point(aes(y = g, x = g0_hat)) +
  geom_abline(slope = 1, color = "red") +
  theme_bw() +
  coord_equal()
```

#### Step 2

Finally, we regress $y - \hat{g}_0(X)$ on $d$ (or equivalently using @eq-theta-partial).

```{r}
(
theta_hat <- lm(y_less_g ~ d, data = training_data)$coefficient["d"]
)
```

So, in this instance, we get an estimate of $\theta$ that is a bit lower than the true value of $\theta$. Let's repeat this process many times to see how this procedure performs on average. 

```{r}
#| code-fold: true 
#| fig-cap: Simulation results of the naive procedure
#| label: fig-naive
#| warning: false
#| cache: true 

fit_m0 <- function(training_data, mtry = 10) {

  rf_fitted_m0 <-
    ranger(
      d ~ .,
      data = dplyr::select(training_data, c("d", starts_with("x"))),
      # mtry = 5,
      mtry = mtry,
      # num.trees = 378,
      num.trees = 500,
      max.depth = 3,
      # min.node.size = 6
      min.node.size = 10
    )

  return(rf_fitted_m0)

}

fit_l0 <- function(training_data, mtry = 12)
{
  rf_fitted_l0 <-
    ranger(
      y ~ .,
      data = dplyr::select(training_data, c("y", starts_with("x"))),
      mtry = mtry,
      # num.trees = 132,
      num.trees = 500,
      max.depth = 5,
      # min.node.size = 1
      min.node.size = 10
    )

  return(rf_fitted_l0)
}

#===================================
# Define a function that will get you g0_hat
#===================================
# this function will be used later

fit_g0 <- function(training_data, rf_fitted_m0, mtry_l = 12, mtry_g = 12) {
  #--------------------------
  # Step 1.1
  #--------------------------
  rf_fitted_l0 <- fit_l0(training_data, mtry_l)

  #=== fitted values ===#
  l0_hat <- predict(rf_fitted_l0, data = training_data)$predictions

  #=== create y - l0_hat ===#
  training_data[, y_less_l := y - l0_hat]

  #--------------------------
  # Step 1.2
  #--------------------------
  #=== fitted values ===#
  m0_hat <- predict(rf_fitted_m0, data = training_data)$predictions

  #=== create y - m0_hat ===#
  training_data[, d_less_m := d - m0_hat]

  #--------------------------
  # Step 1.2
  #--------------------------
  theta_init <- training_data[, sum(d_less_m * y_less_l) / sum(d_less_m * d_less_m)]

  #--------------------------
  # Step 1.3
  #--------------------------
  #=== define y - treatment effect ===#
  training_data[, y_less_te := y - theta_init * d]

  #=== fit rf ===#
  rf_fitted_g0 <-
    ranger(
      y_less_te ~ .,
      data = dplyr::select(training_data, c("y_less_te", starts_with("x"))),
      mtry = mtry_g,
      # num.trees = 132,
      num.trees = 500,
      max.depth = 5,
      # min.node.size = 1
      min.node.size = 10
    )

  return(rf_fitted_g0)

}

#===================================
# Define a function that runs a single simulation and gets you theta_hat
#===================================
run_sim_naive <- function(i){

  # training_data <- data[[i]] %>% data.table()
  training_data <- gen_data()

  rf_fitted_m0 <- fit_m0(training_data)
  rf_fitted_g0 <- fit_g0(training_data, rf_fitted_m0)
  g0_hat <- predict(rf_fitted_g0, data = training_data)$predictions

  #=== create y - g0 ===#
  training_data[, y_less_g := y - g0_hat]

  theta_hat <- lm(y_less_g ~ d, data = training_data)$coefficient["d"]

  # theta_hat <- training_data[, sum(d * y_less_g) / sum(d * d)]

  return(theta_hat)

}

#===================================
# Repeat MC simulations 500 times
#===================================
theta_hats_apr1 <-
  mclapply(
    1:500,
    run_sim_naive,
    mc.cores = detectCores() / 4 * 3
  ) %>% 
  unlist() %>% 
  data.table(theta = .)

#===================================
# Plot the results
#===================================
ggplot(theta_hats_apr1) +
  geom_histogram(aes(x = theta), color = "grey", fill = "white") +
  theme_bw() + 
  geom_vline(aes(xintercept = 0.5,color = "True Value")) +
  geom_vline(aes(xintercept = theta_hats_apr1[, mean(theta)], color = "Mean of the Estimates")) +
  scale_color_manual(
    values = c("True Value" = "red", "Mean of the Estimates" = "blue"),
    name = ""
  ) +
  guides(color = guide_legend(nrow = 1, byrow = TRUE)) +
  theme(legend.position = "bottom")
```

@fig-naive shows the histogram of $\hat{\theta}$ from 500 simulations. You can see that this procedure has led to consistent underestimation of the treatment effect (mean value of $\hat{\theta}$ is `r theta_hats_apr1[, round(mean(theta), digits = 3)]`). There are two sources of bias in this approach: regularization and over-fitting bias. 

:::{.callout-note}

+ Regularization bias: the bias coming from bias in estimating $g_0(X)$
+ Over-fitting bias: the bias coming from over-fitting $g_0(X)$ and $m_0(X)$ due to the fact that the same sample is used for $g_0(X)$ and $m_0(X)$ estimation and $\theta$ estimation

:::

Regularization bias is termed so because bias in estimating $g_0(X)$ can occur when some form of regularization is implemented (e.g., lasso). 

<!-- However, its name is slightly misleading because $g_0(X)$ cannot be estimated without bias even without any regularization in general. This is because the estimation of initial $\theta$ (in Step 1.3) is biased, which comes from the fact that $m_0(X)$ is correlated with $g_0(X)$ through $X$.  -->

<!-- ::: {.column-margin}
Another (unofficial) implementation of $g_0(X)$ estimation is to just use $\hat{l}_0(X_i)$ as $\hat{g}_0(X_i)$ ([a blog post](https://www.r-bloggers.com/2017/06/cross-fitting-double-machine-learning-estimator/)) and then use the following formula.

$$
\begin{aligned}
  \hat{\theta} = (\frac{1}{n}\sum_{i=1}^N d_i d_i)^{-1}\frac{1}{n}\sum_{i=1}^N d_i (y_i - \hat{l}_0(X_i))
\end{aligned}
$$

$\hat{\theta}$ is biased because $\hat{l}_0(X_i)$ is a biased estimator of $\hat{g}_0(X_i)$ when $m_0(X)$ and $g_0(X)$ are correlated. The point here is that, $g_0(X)$ is hard to estimate without bias irrespective of whether any regularization happens or not.
::: -->

<!-- However, if the treatment is independent, then, $g_0(X)$ can be estimated well and this approach works well except it still suffers from over-fitting bias. @fig-apr1-indep shows that the distribution of $\hat{\theta}$ when $m_0(X)$ is independent of $g_0(X)$ and the RF with the same hyper-parameters are used. While regularization can lead to bias in the estimation of $g_0(X)$, regularization is not the only source of bias.  -->

<!-- ```{r}
#| fig-cap: Performance of approach 1 when the treatment status is independent
#| label: fig-apr1-indep
#| code-fold: true
#| warning: false
#| cache: true 

run_sim_naive <- function(i){

  training_data <- gen_data(m_formula = "independent")

  rf_fitted_m0 <- fit_m0(training_data)
  rf_fitted_g0 <- fit_g0(training_data, rf_fitted_m0)
  # rf_fitted_g0 <- fit_l0(training_data)

  g0_hat <- predict(rf_fitted_g0, data = training_data)$predictions

  #=== create y - g0 ===#
  training_data[, y_less_g := y - g0_hat]

  theta_hat <- lm(y_less_g ~ d, data = training_data)$coefficient["d"]

  # theta_hat <- training_data[, sum(d * y_less_g) / sum(d * d)]

  return(theta_hat)

}

#===================================
# Repeat MC simulations 500 times
#===================================
theta_hats_apr1_indep <-
  mclapply(
    1:500,
    run_sim_naive,
    mc.cores = detectCores() / 4 * 3
  ) %>% 
  unlist() %>% 
  data.table(theta = .)

#===================================
# Plot the results
#===================================
ggplot(theta_hats_apr1_indep) +
  geom_histogram(aes(x = theta), color = "grey", fill = "white") +
  theme_bw() + 
  geom_vline(aes(xintercept = 0.5,color = "True Value")) +
  geom_vline(aes(xintercept = theta_hats_apr1_indep[, mean(theta)], color = "Mean of the Estimates")) +
  scale_color_manual(
    values = c("True Value" = "red", "Mean of the Estimates" = "blue"),
    name = ""
  ) +
  guides(color = guide_legend(nrow = 1, byrow = TRUE)) +
  theme(legend.position = "bottom")
``` -->

### Overcoming the regularization bias

Regularization bias can be overcome by double-debiasing (orthogonalizing both $d$ and $y$). Specifically, 

+ Step 1: Estimate $g_0(X)$ and then subtract the fitted value of $g_0(X)$ from $y$ 
+ Step 2: Subtract $\hat{m}_0(X)$ from $d$ ($\tilde{d} = d - \hat{m}_0(x)$)
+ Step 3: Calculate $\hat{\theta}$ based on the following formula

$$
\begin{aligned}
\hat{\theta} = (\frac{1}{n}\sum_{i=1}^N \tilde{d}_i d_i)^{-1}\frac{1}{n}\sum_{i=1}^N \tilde{d}_i (y_i - \hat{g}_0(X_i))
\end{aligned}
$$

The key difference from the previous approach is that this approach uses **IV-like** formula, where $\tilde{d}$ is acting like an instrument. 

::: {.column-margin}
For $y = X\beta + \mu$ with instruments $Z$, the IV estimator is
$$
\begin{aligned}
\hat{\beta} = (Z'X)^{-1}Z'y
\end{aligned}
$$
::: 

We have done Steps 1 and 2 already in the previous approach. So,

```{r}
#--------------------------
# Step 3
#--------------------------
(
theta_hat <- training_data[, sum(d_less_m * y_less_g) / sum(d_less_m * d)]
)
```

Now, let's repeat this 500 times. 

```{r}
#| code-fold: true
#| fig-cap: Simulation results of double-debiased approach
#| label: fig-dd
#| warning: false
#| cache: true 

run_sim_dereg <- function(i)
{
  training_data <- gen_data()
  # training_data <- data[[i]] %>% data.table

  rf_fitted_m0 <- fit_m0(training_data)
  m0_hat <- predict(rf_fitted_m0, data = training_data)$predictions

  #=== create d - m0_hat ===#
  training_data[, d_less_m := d - m0_hat]

  #=== get g0_hat ===#
  rf_fitted_g0 <- fit_g0(training_data, rf_fitted_m0)
  g0_hat <- predict(rf_fitted_g0, data = training_data)$predictions

  #=== create y - g0 ===#
  training_data[, y_less_g := y - g0_hat]

  theta_hat <- training_data[, sum(d_less_m * y_less_g) / sum(d_less_m * d)]

  return(theta_hat)
}

theta_hats_apr2 <-
  mclapply(
    1:500,
    function(x) run_sim_dereg(x),
    mc.cores = detectCores() / 4 * 3
  ) %>% 
  unlist() %>% 
  data.table(theta = .)

ggplot(theta_hats_apr2) +
  geom_histogram(aes(x = theta), color = "grey", fill = "white") +
  theme_bw() + 
  geom_vline(aes(xintercept = 0.5,color = "True Value")) +
  geom_vline(aes(xintercept = theta_hats_apr2[, mean(theta)], color = "Mean of the Estimates")) +
  scale_color_manual(
    values = c("True Value" = "red", "Mean of the Estimates" = "blue"),
    name = ""
  ) +
  guides(color = guide_legend(nrow = 1, byrow = TRUE)) +
  theme(legend.position = "bottom")

```

@fig-dd shows the distribution of $\hat{\theta}$, which is centered about $`r round(theta_hats_apr2[, mean(theta)], digits = 3)`$. The current approach still suffers from the so-called over-fitting bias[@Chernozhukov2018]. Let's look at how we can overcome this bias next.

### Overcoming the over-fitting bias {#sec-cf}

Over-fitting bias can be overcome by cross-fitting. First, the training data is split into $K$-folds just like K-fold cross-validation. Let's denote them as $I_1, \dots, I_k$. For example, for $I_1$, the following steps are taken (@fig-cross-fitting provides a visual illustration):

+ Step 1: Estimate $\hat{g}_0(x)$ and $\hat{m}_0(x)$ using the data from the other folds ($I_2, \dots, I_K$).
+ Step 2: Estimate $\hat{g}_0(x_i)$ and $\hat{m}_0(x_i)$ for each $i \in I_1$ and calculate $\tilde{y}_i = y_i - \hat{g}_0(x_i)$ and $\tilde{d}_i = d_i - \hat{m}_0(x_i)$.
+ Step 3: Use the following formula to obtain $\hat{\theta}$.

$$
\begin{aligned}
\hat{\theta} = (\frac{1}{n}\sum_{i=1}^N \tilde{d}_i d_i)^{-1}\frac{1}{n}\sum_{i=1}^N \tilde{d}_i (y_i - \hat{g}_0(X_i))
\end{aligned}
$$ {#eq-iv-score}

This process is repeated for all the $K$ folds, and then the the final estimate of $\hat{\theta}$ is obtained as the average of $\hat{\theta}$s.

::: {.column-margin}
You can implement repeated K-fold cross-fitting using the `DoublML` package, which is not demonstrated here as it is very much similar in concept to repeated K-fold CV explained in @sec-cross-validation.
:::

<span style="color:blue"> talk about `algorithm_2` </span>

```{r}
#| code-fold: true
#| fig-width: 9
#| fig-cap: Illustration of cross-fitting
#| label: fig-cross-fitting

ggplot() +
  #=== fold 1 ===#
  geom_rect(
    aes(xmin = 0, xmax = 10, ymin = 0, ymax = 2),
    fill = "grey",
    color = "black",
    size = 1.2
  ) +
  geom_rect(
    aes(xmin = 0, xmax = 2.5, ymin = 0, ymax = 2),
    fill = "black"
  ) +
  #=== fold 2 ===#
  geom_rect(
    aes(xmin = 0, xmax = 10, ymin = 2.2, ymax = 4.2),
    fill = "grey",
    color = "black",
    size = 1.2
  ) +
  geom_rect(
    aes(xmin = 2.5, xmax = 5, ymin = 2.2, ymax = 4.2),
    fill = "black"
  ) +
  #=== fold 3 ===#
  geom_rect(
    aes(xmin = 0, xmax = 10, ymin = 4.4, ymax = 6.4),
    fill = "grey",
    color = "black",
    size = 1.2
  ) +
  geom_rect(
    aes(xmin = 5, xmax = 7.5, ymin = 4.4, ymax = 6.4),
    fill = "black"
  ) +
  #=== fold 4 ===#
  geom_rect(
    aes(xmin = 0, xmax = 10, ymin = 6.6, ymax = 8.6),
    fill = "grey",
    color = "black",
    size = 1.2
  ) +
  geom_rect(
    aes(xmin = 7.5, xmax = 10, ymin = 6.6, ymax = 8.6),
    fill = "black"
  ) +
  geom_brace(aes(c(0, 7.4), c(8.7, 9.2)), inherit.data=F) +
  annotate(
    "text", x = 3.5, y = 9.7, parse = TRUE,
    label = "'Find ' * hat(g)[0](x) * ' and ' * hat(m)[0](x) * ' from this data.'",
    size = 6
  ) +
  geom_curve(
    aes(x = 2.2, xend = 8.5, y = 10.3, yend = 8.8),
    arrow = arrow(length = unit(0.03, "npc")),
    curvature = -0.3
  ) +
  geom_curve(
    aes(x = 3.4, xend = 8, y = 10.3, yend = 8.8),
    arrow = arrow(length = unit(0.03, "npc")),
    curvature = -0.3
  ) +
  ylim(NA, 11) +
  # coord_equal() +
  theme_void()
```

Let's code the cross-fitting procedure. We first split the data into 2 folds ($K = 2$).

::: {.column-margin}
$K$ does not have to be 2.
:::

```{r}
(
data_folds <- rsample::vfold_cv(training_data, v = 2)
)
```

Let's cross-fit for fold 1.

```{r}
split_1 <- data_folds[1, ]

#=== data for estimating g_0  and m_0 ===#
data_train <- analysis(split_1$splits[[1]]) 

#=== data for which g_0(x_i) and m_0(x_i) are calculated ===#
data_target <- assessment(split_1$splits[[1]]) 
```

First, we fit $\hat{g}_0(x)$ and $\hat{m}_0(x)$ using the data from the other folds.

```{r}
#| warning: false
#=== m0 ===#
m_rf_fit <- fit_m0(data_train)

#=== g0 ===#
g_rf_fit <- fit_g0(data_train, m_rf_fit)
```

Next, we predict $\hat{g}_0(x_i)$ and $\hat{m}_0(x_i)$ for each $i$ of fold 1 (the target dataset) and calculate $\tilde{y}_i = y_i - \hat{g}_0(x_i)$ and $\tilde{d}_i = d_i - \hat{m}_0(x_i)$.

```{r}
#| warning: false

data_orth <-
  data_target %>% 
  #=== prediction of g_0(x_i) ===#
  .[, g_0_hat := predict(g_rf_fit, data = .)$predictions] %>% 
  #=== orthogonalize y ===#
  .[, y_tilde := y - g_0_hat] %>% 
  #=== prediction of m_0(x_i) ===#
  .[, m_0_hat := predict(m_rf_fit, data = .)$predictions] %>% 
  #=== orthogonalize d ===#
  .[, d_tilde := d - m_0_hat]
```

Then, $\hat{\theta}$ is obtained for this fold using @eq-iv-score.

```{r}
(
theta_hat <- data_orth[, sum(d_tilde * y_tilde) / sum(d_tilde * d)]
)
```

We can repeat this for all the folds (`cross_fit()` which finds $\hat{\theta}$ for a particular fold is defined on the side).

::: {.column-margin}

```{r}
cross_fit <- function(i, data_folds, mtry_l = 12, mtry_m = 10, mtry_g = 12)
{

  #--------------------------
  # Prepare data
  #--------------------------
  #=== ith split ===#
  working_split <- data_folds[i, ]

  #=== data for estimating g_0  and m_0 ===#
  data_train <- analysis(working_split$splits[[1]]) 

  #=== data for which g_0(x_i) and m_0(x_i) are calculated ===#
  data_target <- assessment(working_split$splits[[1]]) 

  #--------------------------
  # Fit g0 and m0
  #--------------------------
  #=== m0 ===#
  m_rf_fit <- fit_m0(data_train, mtry_m)

  #=== g0 ===#
  g_rf_fit <- fit_g0(data_train, m_rf_fit, mtry_l, mtry_g)

  #--------------------------
  # Get y_tilde and d_tilde
  #--------------------------
  data_orth <-
    data_target %>% 
    #=== prediction of g_0(x_i) ===#
    .[, g_0_hat := predict(g_rf_fit, data = .)$predictions] %>% 
    #=== orthogonalize y ===#
    .[, y_tilde := y - g_0_hat] %>% 
    #=== prediction of m_0(x_i) ===#
    .[, m_0_hat := predict(m_rf_fit, data = .)$predictions] %>% 
    #=== orthogonalize d ===#
    .[, d_tilde := d - m_0_hat] %>% 
    .[, .(y_tilde, d_tilde, d)]

  theta_cf <- data_orth[, sum(d_tilde * y_tilde) / sum(d_tilde * d)]

  return(theta_cf)
}
```
:::


```{r}
#| warning: false

(
theta_hat <- 
  lapply(
    seq_len(nrow(data_folds)), # loop over folds
    function(x) cross_fit(x, data_folds) # get theta_hat
  ) %>% 
  unlist() %>%
  mean() # average them
)
```

Okay, now that we understand the steps of this approach, let's repeat this many times (`get_theta_cf()` that finds $\hat{\theta}$ by cross-fitting is defined on the side). 

::: {.column-margin}
```{r}
get_theta_cf <- function(data_folds, mtry_l = 12, mtry_m = 10, mtry_g = 12){

  theta_hat <- 
    lapply(
      seq_len(nrow(data_folds)),
      function(x) cross_fit(x, data_folds, mtry_l, mtry_m, mtry_g)
    ) %>% 
    unlist() %>% 
    mean()

  return(theta_hat)
}
```
:::


```{r}
#| fig-cap: The distribution of treatment effect estimated by the double-debiased approach with cross-fitting
#| label: fig-cf-debiased
#| code-fold: true
#| warning: false
#| cache: true 

theta_hats_cf <-
  mclapply(
    1:500,
    function(x) {
      print(x)
      training_data <- gen_data()
      data_folds <- rsample::vfold_cv(training_data, v = 2)
      theta_hat <-  get_theta_cf(data_folds)
      return(theta_hat)
    },
    mc.cores = detectCores() * 3 / 4
  ) %>% 
  unlist()

#=== visualize the results ===#
ggplot() +
  geom_histogram(aes(x = theta_hats_cf), color = "grey", fill = "white") +
  theme_bw() + 
  geom_vline(aes(xintercept = 0.5,color = "True Value")) +
  geom_vline(aes(xintercept = mean(theta_hats_cf), color = "Mean of the Estimates")) +
  scale_color_manual(
    values = c("True Value" = "red", "Mean of the Estimates" = "blue"),
    name = ""
  ) +
  guides(color = guide_legend(nrow = 1, byrow = TRUE)) +
  theme(legend.position = "bottom")
```

@fig-cf-debiased shows the distribution of $\hat{\theta}$ with double-debiasing and cross-fitting. It is slightly biased in this instance (mean is `r round(mean(theta_hats_cf), digits = 3)`), but the average $\hat{\theta}$ is very close to the true parameter. 

:::{.callout-important}

+ Double/debiased (double orthogonalization) can help overcome the bias in $\hat{\theta}$ that comes from the bias in estimating $g_0(X)$.

+ Cross-fitting can help overcome the bias from estimating $g_0(X)$, $m_0(X)$, and $\theta$ using the same data.

:::

## Models and implementation by `DoubleML`

In R, `DoubleML` is built on top of `mlr3`, which uses `R6` classes provided by the `R6` package. Since `R6` implements encapsulated object-oriented programming like Python, `DoubleML` in R and Python work in a very similar manner. Here, we will use R for demonstrations. 

### Partially linear model

$$
\begin{aligned}
y = \theta d + g(X) + \mu \\
d = m(X) + \eta
\end{aligned}
$$ {#eq-plr}

:::{.callout-note}

**Assumptions on the error terms**

+ $E[\mu|D,X] = 0$
+ $E[\eta|X] = 0$
+ $E[\eta\cdot \mu|D, X] = 0$

:::

```{r}
#| code-fold: true

DiagrammeR::grViz(
  paste0(
  "
  digraph {
    graph [layout = circo, ranksep = 0.5]

    node [shape = box]
      Y [label = 'Y']
      u [label = '\U03BC']

    node [shape = box]
      X [label = 'X']
      T [label = 'T']
      v [label = '\U03B7']

    edge [minlen = 2]
      {v, X}->T
      {X, T}->Y
      u->Y
  }
  "
  )
)

```


There are two ways to estimate $\theta$ using `DoublMLPLR()`. They differ in how their score functions are defined. You pick either one of the two:

+ `partialling-out`: $[Y - l(X) - \theta(D-m(X))][D-m(X)]$
+ `IV-type`: $[Y - \theta D - g(X)][D-m(X)]$

Since $g(X)$ and $m(X)$ themselves appear in the data generating process (@eq-plr), it is clear what  they are. However, it is not immediately clear what $l(X)$ represents. $l(X)$ refers to $E[Y|X]$.

$$
\begin{aligned}
l(X) = E[Y|X] & = E[\theta d + g(X) + \mu] \\
              & = \theta E[d|X] + E[g(X)] + E[\mu|X] \\
              & = \theta m(X) + g(X) \;\; \mbox{(by the assumptions on the error terms)}\\
\end{aligned}
$$

Given this, the scores functions can be rewritten as:

+ `partialling-out`: $\mu\cdot \eta$
+ `IV-type`: $\mu\cdot \eta$

Therefore, the two score functions are actually identical in meaning, but represented by different terms, which result in different equations to calculate $\hat{\theta}$. Here are how $\theta$ is identified for the two scores functions:

**`partialling-out`**
$$
\begin{aligned}
E\large(\normalsize[Y - l(X) - \theta(D-m(X))][D-m(X)]\large)\normalsize = 0 \\
\theta = \frac{E[(Y - l(X))(D-m(X))]}{E[(D-m(X))(D-m(X))]}
\end{aligned}
$$

The empirical analog of this moment condition is

$$
\begin{aligned}
\hat{\theta} = \frac{\sum_{i=1}^N (Y_i - l(X_i))(D_i-m(X_i))}{\sum_{i=1}^N (D_i-m(X_i))(D_i-m(X_i))}
\end{aligned}
$$

**`IV-type`**

$$
\begin{aligned}
\hat{\theta} = \frac{\sum_{i=1}^N (Y_i - g(X_i))(D_i-m(X_i))}{\sum_{i=1}^N D_i(D_i-m(X_i))}
\end{aligned}
$$

::: {.column-margin}
$D_i-m(X_i)$ is acting like an instrument.
:::

The choice of the score function results in different steps to estimate $\theta$ and what you need to supply to `DoublMLPLR()`. $g_0(X)$ is estimated in several steps (see @sec-dml-naive) as it needs to remove the effect of $\theta D$ (though imperfectly) before estimating only $g_0(X)$. $l_0(X)$, however, is estimated by simply regressing $Y$ on $X$. In running `DoublMLPLR()`, there are three key options:

+ `ml_l`: ML method to use to estimate $l(X)$
+ `ml_m`: ML method to use to estimate $m(X)$
+ `ml_g`: ML method to use to estimate $g(X)$

When `partialling-out` is selected, you need to specify `ml_l`  and `ml_m`. However, you do not need to specify the `ml_g`. When `IV-type` is used you need to specify all three. This is because estimating `g(X)` involves first estimating `l(X)` (see @sec-dml-naive). So, the `partialling-out` option requires a smaller number of steps and easier to specify for the user. 

Let's implement `DoublMLPLR()` using a synthetic dataset that follows the DGP represented by @eq-simple-synthetic. We can use `make_plr_CCDDHNR2018()` to create such a dataset.

```{r}
(
data <- 
  make_plr_CCDDHNR2018(
    alpha = 0.5, 
    n_obs = 500, 
    dim_x = 20, 
    return_type = 'data.table'
  )
)
```

Note that the treatment variable is continuous in this dataset.

We first need to create a `DoubleMLData` object from the train data using `DoubleMLData$new()`. Specify the dependent variable using the `y_col` option and the treatment variable(s) by `d_cols`. The rest of the variables in the dataset provided will be regarded as covariates.

```{r}
(
obj_dml_data <- 
  DoubleMLData$new(
    data, 
    y_col = "y", 
    d_cols = "d"
  )
)
```

Now, let's specify `ml_l`, `ml_m`, and `ml_g`. The `DoubleML` follows `mlr3` (see @sec-mlr3 for how to use `mlr3`).

```{r}
(
ml_l <- 
  lrn(
    "regr.ranger",
    num.trees = 500, 
    mtry = 15, 
    min.node.size = 5
  )
)

```

Let's use the same ML method for `ml_m` and `ml_g` (you can use any appropriate ML methods). We can simply use the `clone()` method to replicate `ml_l`.

```{r}
ml_m <- ml_l$clone()
ml_g <- ml_l$clone()
```

We now set up the `DoubleMLPLR` estimator by providing a `DoubleMLData` and ML methods. By default, `score` is set to `"partialling out"`, so we do not need to provide `ml_g`. The `apply_cross_fitting` option is set to `TRUE` by default.

```{r}
dml_plr_obj <- DoubleMLPLR$new(obj_dml_data, ml_l, ml_m)
```

We can fit the model by invoking `fit()` on `dml_plr_obj`.

```{r}
dml_plr_obj$fit()
```

See the results with `print()`. You can see $\hat{\theta}$ at the bottom.

```{r}
print(dml_plr_obj)
```

If you are using `IV-type`, then you need to provide `ml_g` as well like below.

```{r}
#=== set up ===#
dml_plr_obj_iv <- 
  DoubleMLPLR$new(
    obj_dml_data, 
    ml_l, 
    ml_m, 
    ml_g,
    score = "IV-type"
  )

#=== fit ===#
dml_plr_obj_iv$fit()

#=== print the results ===#
print(dml_plr_obj_iv)
```


### Partially linear IV model

When using observational data, it is rarely the case that conditional unconfoundedness of the treatment is satisfied. In such a case, you may want to consider a DML-IV approach implemented by `DoubleMLPLIV()`. Just like IV for linear models, finding the right instrument is critical for the DML-IV approach to be consistent.

$$
\begin{aligned}
y = \theta d + g(X) + \mu \\
Z = m(X) + \varepsilon \\
d = r(X) + \eta
\end{aligned}
$$ {#eq-plr}

When $\eta$ and $\mu$ are correlated, `DoubleMLPLR()` is inconsistent. However, as long as the following assumptions are satisfied, `DoubleMLPLIV()` is consistent.

:::{.callout-note}

**Assumptions on the error terms**

+ $E[\mu|Z,X] = 0$
+ $E[\varepsilon|X] = 0$
+ $E[\varepsilon\cdot \mu|Z, X] = 0$

:::

Here is the causal diagram for the partially linear model of interest. As you can see, the treatment variable $T$ is confounded as $\mu$ affects both $Y$ and $T$ (through $\eta$). 

```{r}
#| code-fold: true

DiagrammeR::grViz(
  paste0(
  "
  digraph {
    graph [ranksep = 0.6]
    node [shape = box]
      Y [label = 'Y']
      X [label = 'X']
      T [label = 'T']
      Z [label = 'Z']
      varep [label = '\U03B5']
      mu [label = '\U03BC']
      eta [label = '\U03B7']
    edge [minlen = 2]
      X->Y
      T->Y
      Z->T
      X->T
      X->Z
      varep->Z
      mu->{Y, eta}
      eta->{T, mu}
    { rank = same; Y; Z; varep}
    { rank = same; X; T; mu}
  }
  "
  )
)
```

Just like `DoublMLPLR()`, there are two ways to estimate $\theta$ using `DoublMLPLIV()`. The two score functions are:

+ `partialling-out`: $[Y - l(X) - \theta(D-r(X))][Z-m(X)]$
+ `IV-type`: $[Y - \theta D - g(X)][Z-m(X)]$

, where $r(X) = E[D|X]$. 

:::{.callout-warning}

## Confusing notations
$r(X)$ is the same as $m(X)$ in the partially line model case. $m(X)$ represents $E[Z|X]$ in the IV model.
:::

Let $\tilde{Z}$, $\tilde{D}$, $\tilde{Y}_l$, and $\tilde{Y}_g$ denote $Z-m(X)$, $D-r(X)$, $Y-l(X)$, and $Y-g(X)$, respectively. Then,

+ `partialling-out`: $\theta = (\tilde{Z}'\tilde{D})^{-1}\tilde{Z}\tilde{Y}_l$

+ `IV-type`: $\theta = (\tilde{Z}'D)^{-1}\tilde{Z}\tilde{Y}_g$

---

We now implement `DoublMLPLIV()` using a synthetic dataset generated from the following DGP:

$$
\begin{aligned}
y_i & = 0.5 d_i  + \frac{exp(x_{i,1})}{1 + exp(x_{i,1})} + \frac{1}{4}\cdot x_{i,3} + \mu_i \\
z_i & = x_{i,1} + x_{i,2} + \varepsilon_i\\
d_i & = x_{i,1} + \frac{1}{4}\cdot\frac{exp(x_{i,3})}{1 + exp(x_{i,3})} + z_i + \eta_i
\end{aligned}
$$ {#eq-iv-synthetic}

In this DGP, $\mu_i$ and $\eta_i$ are correlated, and using `DoublMLPLR()` would result in biased estimation of the coefficient on $d$.

```{r}
#| code-fold: true
gen_data_iv <- function(
  g_formula = formula(~ I(exp(x1)/(1+exp(x1))) + I(x3/4)), # formula that defines g(x)
  m_formula = formula(~ x1 + I(exp(x3)/(1+exp(x3))/4) + z), # formula that defines m(x)
  f_formula = formula(~ x1 + x2), # formula that defines f(x)
  te_formula = formula(~ I(0.5*d)), # formula that defines theta(x) * t
  n_obs = 500, 
  Kx = 20, 
  vcov_x = NULL,
  sigma = 1 # sd of the error term in the y equation
)
{

  #=== generate X (with correlation between them)  ===#
  if (is.null(vcov_x)) {
    vcov_x <- matrix(rep(0, Kx^2), nrow = Kx)
    for (i in seq_len(Kx)) {
      vcov_x[i, ] <- 0.7^abs(i - seq_len(Kx)) 
    }
  }

  #=== draw from multivariate normal ===#
  data <- 
    mvrnorm(n_obs, mu = rep(0, Kx), Sigma = vcov_x) %>% 
    data.table() %>% 
    setnames(names(.), paste0("x", 1:Kx))  

  #=== generate z ===#
  data[, z := model.frame(f_formula, data = data) %>% rowSums() + rnorm(n_obs)]

  #=== generate d ===#
  if (m_formula == "independent") {
    data[, d := rnorm(n_obs)]
  } else {
    data[, d := model.frame(m_formula, data = data) %>% rowSums() + rnorm(n_obs)]
  }

  mu_eta_shared <- 2 * rnorm(n_obs)
  data[, d := d + mu_eta_shared]

  #=== generate g ===#
  data[, g := model.frame(g_formula, data = data) %>% rowSums()]

  #=== generate treatment effect ===#
  data[, te := model.frame(te_formula, data = data) %>% rowSums()]

  #=== generate y ===#
  data[, mu := rnorm(n_obs, sd = sigma) + mu_eta_shared ] 
  data[, y := te + g + mu]

  return(data[])

}
```

We use the `gen_data_iv()` function (unfold the code block above to see the code of the function).

```{r}
set.seed(54723)

#=== generate data ===#
(
data <- gen_data_iv(n_obs = 1000, Kx = 5) 
)
```

Column named `z` is the excluded instrument in the dataset.

Just like the case with `DoublMLPLR()`, we first need to create a `DoubleMLData` object. Unlike the `DoublMLPLR()` case, we need to specify which column is the excluded instrument(s) with the `z_cols` option.

```{r}
(
obj_dml_data <- 
  DoubleMLData$new(
    dplyr::select(data, - g, -te, -mu),
    y_col="y",
    d_col = "d",
    z_cols= "z"
  )
)
```

We set up learners for $l(X)$, $m(X)$, and $r(X)$ as we are using the default `score`, which is `"partialling out"` here. We just use the same learner for all three. 

```{r}
ml_l <- 
  lrn(
    "regr.ranger",
    num.trees = 500, 
    mtry = 5, 
    min.node.size = 5
  )
ml_r <- ml_l$clone()
ml_m <- ml_l$clone()
```

We now set up DML IV estimation by invoking the `new()` method on `DoubleMLPLIV`.

```{r}
dml_pliv_obj <- 
  DoubleMLPLIV$new(
    obj_dml_data, 
    ml_l,
    ml_m,
    ml_r
  )
```

Then, fit the model.

```{r}
dml_pliv_obj$fit()

print(dml_pliv_obj)
```

The estimate is close to the true value. 

::: {.column-margin}
Run MC simulations to check if `DoubleMLPLIV()` is indeed consistent under the DGP.
:::

Now, let's use `DoublMLPLR()` to see what we get.  

```{r}
obj_dml_data <- 
  DoubleMLData$new(
    dplyr::select(data, - g, -te, -mu),
    y_col="y",
    d_col = "d"
  )

#=== set up MLPLR ===#
dml_pl_obj <- 
  DoubleMLPLR$new(
    obj_dml_data, 
    ml_l,
    ml_m
  )

#=== fit ===#
dml_pl_obj$fit()

print(dml_pl_obj)
```

As you can see, $\hat{\theta}$ is quite large. This is the expected direction of bias as $d$ and $\mu$ are positively correlated.

## Suggested Exercises



## References {.unnumbered}

<div id="refs"></div>




