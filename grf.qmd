---
title: "Generalized Random Forest"
---

## Generalized Method of Moment (GMM)


## Generalized Random Forest (GRF) 


$$
\begin{aligned}
E[\psi_{\theta(x),\nu(x)}|X_i = x] = 0
\end{aligned}
$$

$\psi_{\theta(x),\nu(x)}$ is a scoring (moment) function. 

<span style="color:blue"> Example 1: OLS on linear model </span>

Suppose we assume that $E[u|x] = 0$.  Then, the sample version of this moment condition (score function) is

$$
\begin{aligned}
\frac{1}{N}\sum_{i=1}^N (y_i - X_i\beta) = 0
\end{aligned}
$$

or equivalently,

$$
\begin{aligned}
\sum_{i=1}^N (y_i - X_i\beta) = 0
\end{aligned}
$$

Here, $\theta(X) = X_i\beta$ and $\psi_{\theta(x),\nu(x)} = y_i - \theta(x)$. $O_i = y_i$

::: {.column-margin}
No nuisance function ($\nu(X)$) here.
:::


We can find $\beta$ so that the above condition is satisfied: that is, sum of the residuals is zero. This is called a method of moment. 

::: {.column-margin}
Note that this condition is actually exactly the same as the F.O.C of OLS. OLS

$$
\begin{aligned}
Min_{\beta} \sum_{i=1}^N (y_i - X_i\beta)^2
\end{aligned}
$$

The F.O.C is 

$$
\begin{aligned}
-2 \sum_{i=1}^N (y_i - X_i\beta) = 0
\end{aligned}
$$

or equivalently,

$$
\begin{aligned}
\sum_{i=1}^N (y_i - X_i\beta) = 0
\end{aligned}
$$
:::


Data: $(X_i, O_i)$ 


For the target value $x$ (a particular value of $X$ at which you want to $\theta(x)$).

GRF solves 
$$
\begin{aligned}
\sum_{i=1}^N \alpha_i(x) \cdot \psi_{\theta(x),\nu(x)}(O_i) = 0
\end{aligned}
$$

w.r.t $\theta(x)$ and $\nu(x)$, where $\alpha_i(x)$ is a weight for $i$ given $x$. 

The weights $\alpha_i(x)$ used to specify the above solution were obtained via a deterministic kernel function [Hastie, Tibshirani and Friedman (2009)], which can be sensitive to the curse of dimensionality."

GRF proposes to use forest-based algorithms to learn problem-specific weights $\alpha_i(x)$ adaptively.

$B$ trees ($b = 1, \dots, B$). For each tree, let $L_b(x)$ denote the set of training samples that belong to the same leaf as $x$.

$$
\begin{aligned}
\alpha_{b,i}(x) = \frac{I[X_i \in L_b(x)]}{|L_b(x)|}
\end{aligned}
$$

$I[]$ is the index function that take 1 if the condition inside is true and 0 if not. So, $I[X_i \in L_b(x)] = 1$ if observation $i$'s attributes $X_i$ belongs to the leaf $x$ is in. $|L_b(x)|$ is the number of samples that are in the leaf that $x$ belongs to. If you sum $\alpha_{b,i}(x)$ over $i$, they sum to 1. 

$$
\begin{aligned}
\alpha_{i}(x) = \frac{1}{B}\sum_{i=1}^B \alpha_{b,i}(x)
\end{aligned}
$$

We then average $\alpha_{b,i}(x)$ over B trees. 

The questions is of course how to construct trees.

<span style="color:blue"> Special Case: Random Forest </span>

$\theta(x) = E[Y_i|X_i]$. That is the statistics of interest is the expected value of $Y$ conditional on $X_i = x$. $\psi_{\theta(x)} = Y_i - \theta(x)$ (residual).

We solve

$$
\begin{aligned}
\sum_{i=1}^B\frac{1}{B}\sum_{b=1}^B \alpha_{b,i}(x)(Y_i - \hat{\theta}(x)) = 0
\end{aligned}
$$

We can show that $\hat{\theta}(x) = \frac{1}{B}\sum_{b=1}^B\hat{\theta}_b(x)$, where 

$$
\begin{aligned}
\hat{\theta}_b(x) = \frac{\sum_{\{i: X_i\in L_b(x)\}}Y_i}{|L_b(x)|}
\end{aligned}
$$

satisfies the above condition.

$$
\begin{aligned}
\sum_{i=1}^N\frac{1}{B}\sum_{b=1}^B \alpha_{b,i}(x)(Y_i - \hat{\theta}(x)) \\
\sum_{i=1}^N\frac{1}{B}\sum_{b=1}^B \frac{I[X_i \in L_b(x)]}{|L_b(x)|} \huge(\normalsize Y_i - \frac{\sum_{\{i: X_i\in L_b(x)\}}Y_i}{|L_b(x)|}\huge) 
\end{aligned}
$$


::: {.column-margin}
$\{i: X_i\in L_b(x)\}$ is the set of $i$ that belongs to leaf $L_b(x)$
:::







