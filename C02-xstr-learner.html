<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-0.9.568">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>c02-xstr-learner</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="C02-xstr-learner_files/libs/clipboard/clipboard.min.js"></script>
<script src="C02-xstr-learner_files/libs/quarto-html/quarto.js"></script>
<script src="C02-xstr-learner_files/libs/quarto-html/popper.min.js"></script>
<script src="C02-xstr-learner_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="C02-xstr-learner_files/libs/quarto-html/anchor.min.js"></script>
<link href="C02-xstr-learner_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="C02-xstr-learner_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="C02-xstr-learner_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="C02-xstr-learner_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="C02-xstr-learner_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
</div>
<main class="content page-columns page-full" id="quarto-document-content">



<section id="sec-het-dml" class="level1 page-columns page-full">
<h1>S-, X-, T-, and R-learner</h1>
<p>In this section, we look at the S-, X-, T-, and R-learner, which are method that estimate heterogeneous treatment effects when the treatment is binary. While X-leaner and T-learner cannot be extended to continuous treatment cases, S-learner and R-learner can be. Mathematical notations used in this chapter closely follow those of <span class="citation" data-cites="kunzel_metalearners_2019">[@kunzel_metalearners_2019]</span> and <span class="citation" data-cites="nie_quasi-oracle_2021">[@nie_quasi-oracle_2021]</span>.</p>
<section id="motivation" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>In <strong>?@sec-dml</strong>, the basic idea of double machine learning (DML) methods was introduced when the treatment effect is homogeneous. We now turn our focus to the task of estimating heterogeneous treatment effects: the impact of a treatment varies based on observed attributes of the subjects. Heterogeneous treatment effect is also referred to as <span style="color:blue"> conditional </span> average treatment effect (CATE).</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><span style="color:blue"> Conditional </span> on observed attributes.</p>
</div></div><p>Understanding how treatment effects vary can be highly valuable in many circumstances.</p>
<p><span style="color:blue"> Example 1: </span> If we come to know a particular drug is effective on elderly people but detrimental to kids, then doctors can make a smart decision of prescribing the drug to elderly people, but not to kids.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>In this example, the heterogeneity driver is age.</p>
</div></div><p><span style="color:blue"> Example 2: </span> If we come to know that fertilizer is more effective in increasing corn yield in soil type A than B, then farmers can apply more fertilizer on the parts of the field where soil type is A but less on where soil type is B.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>In this example, the heterogeneity driver is soil type.</p>
</div></div><p>As you can see in these examples, knowledge on the heterogeneity of the treatment effect and its drivers can help decision makers smart-target treatments and policies.</p>
</section>
<section id="modeling-framework" class="level2">
<h2 class="anchored" data-anchor-id="modeling-framework">Modeling Framework</h2>
<p>The model of interest in general form is as follows:</p>
<p><span id="eq-model-framework"><span class="math display">\[
\begin{aligned}
Y_i &amp; = \theta(X_i)\cdot T_i + g(X_i, W_i) + \varepsilon_i \\
T_i &amp; = f(X_i, W_i) + \eta_i
\end{aligned}
\tag{1}\]</span></span></p>
<ul>
<li><span class="math inline">\(Y\)</span>: dependent variable</li>
<li><span class="math inline">\(T\)</span>: treatment variable</li>
<li><span class="math inline">\(X\)</span>: collection of variables that affect Y indirectly through the treatment (<span class="math inline">\(\theta(X)\cdot T\)</span>) and directly (<span class="math inline">\(g(X, W)\)</span>) independent of the treatment</li>
<li><span class="math inline">\(W\)</span>: collection of variables that affect directly (<span class="math inline">\(g(X, W)\)</span>) independent of the treatment, but not through the treatment</li>
</ul>
<p>Here are the assumptions:</p>
<ul>
<li><span class="math inline">\(E[\varepsilon|X, W] = 0\)</span></li>
<li><span class="math inline">\(E[\eta|X, W] = 0\)</span></li>
<li><span class="math inline">\(E[\eta\cdot\varepsilon|X, W] = 0\)</span></li>
</ul>
<p>For the notational convenicence, let <span class="math inline">\(\mu_1(X)\)</span> and <span class="math inline">\(\mu_0(X)\)</span> denote the expected value of the potential conditional outcomes:</p>
<p><span class="math display">\[
\begin{align}
\mu_1(X) &amp; = E[Y|W=1, X] = g(X, W)\\
\mu_0(X) &amp; = E[Y|W=0, X] =  \theta(X) + g(X, W)
\end{align}
\]</span></p>
</section>
<section id="s--t--and-x-learner" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="s--t--and-x-learner">S-, T-, and X-Learner</h2>
<section id="s-leaner" class="level3">
<h3 class="anchored" data-anchor-id="s-leaner">S-leaner</h3>
<p>S-leaner estimates CATE by taking the following steps:</p>
<ol type="1">
<li>Regress <span class="math inline">\(Y\)</span> on <span class="math inline">\(W\)</span> and <span class="math inline">\(X\)</span> to estimate <span class="math inline">\(E[Y|W,X]\)</span> using any appropriate ML regression methods and call it <span class="math inline">\(\hat{\mu}(W,X)\)</span>.</li>
<li>Estimate <span class="math inline">\(\hat{\theta}(X)\)</span> as <span class="math inline">\(\hat{\mu}(W=1,X)-\hat{\mu}(W=0,X)\)</span></li>
</ol>
<p>In this approach, no special treatment is given to <span class="math inline">\(W\)</span>. It is just a covariate along with others (<span class="math inline">\(X\)</span>). This approach is named S-leaner by <span class="citation" data-cites="kunzel_metalearners_2019">@kunzel_metalearners_2019</span> because it involves estimating a <span style="color:blue">s</span>ingle response function.</p>
</section>
<section id="t-learner" class="level3">
<h3 class="anchored" data-anchor-id="t-learner">T-learner</h3>
<ol type="1">
<li>Regress <span class="math inline">\(Y\)</span> on <span class="math inline">\(W\)</span> and <span class="math inline">\(X\)</span> using the treated observations to estimate <span class="math inline">\(\mu_1(X)\)</span> using any appropriate ML regression methods.</li>
<li>Regress <span class="math inline">\(Y\)</span> on <span class="math inline">\(W\)</span> and <span class="math inline">\(X\)</span> using the control observations to estimate <span class="math inline">\(\mu_0(X)\)</span> using any appropriate ML regression methods.</li>
<li>Estimate <span class="math inline">\(\hat{\theta}(X)\)</span> as <span class="math inline">\(\hat{\mu}_1(X)-\hat{\mu}(X)\)</span></li>
</ol>
<p>This approach is named T-leaner by <span class="citation" data-cites="kunzel_metalearners_2019">@kunzel_metalearners_2019</span> because it involves estimating <span style="color:blue">t</span>wo functions.</p>
</section>
<section id="x-learner" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="x-learner">X-learner</h3>
<ol type="1">
<li>Estimate <span class="math inline">\(\mu_1(X)\)</span> and <span class="math inline">\(\mu_0(X)\)</span> using any appropriate ML regression methods. (Steps 1 and 2 of the T-learner)</li>
<li>Impute individual treatment effect for the treated and control groups as follows</li>
</ol>
<p><span class="math display">\[
\begin{align}
\tilde{D}_i^1(X_i) = Y^1_i - \hat{\mu}_0(X_i)\\
\tilde{D}_i^0(X_i) =  \hat{\mu}_1(X_i) - Y^0_i
\end{align}
\]</span></p>

<div class="no-row-height column-margin column-container"><div class="">
<p>This is similar to cross-fitting we saw in <strong>?@sec-dml</strong>, where the folds are the treated and control groups.</p>
</div></div><ol start="3" type="1">
<li></li>
</ol>
<ul>
<li><p>Regress <span class="math inline">\(\tilde{D}_i^1(X_i)\)</span> on <span class="math inline">\(X\)</span> using the observations in the treated group and denote the predicted value as <span class="math inline">\(\hat{\theta}_1(X)\)</span></p></li>
<li><p>Regress <span class="math inline">\(\tilde{D}_i^0(X_i)\)</span> on <span class="math inline">\(X\)</span> using the observations in the control group and denote the predicted value as <span class="math inline">\(\hat{\theta}_0(X)\)</span></p></li>
</ul>
<ol start="4" type="1">
<li>Calculate <span class="math inline">\(\hat{\theta}(X)\)</span> as their weighted average</li>
</ol>
<p><span id="eq-final-X"><span class="math display">\[
\begin{align}
\hat{\theta}(X) = g(X)\cdot\hat{\theta}_0(X) + [1-g(X)]\cdot\hat{\theta}_1(X)
\end{align}
\tag{2}\]</span></span></p>
<p>Any value of <span class="math inline">\(g(X)\)</span> is acceptable. One option of <span class="math inline">\(g(X)\)</span> may be the estimated propensity score <span class="math inline">\(E[W|X]\)</span>.</p>
</section>
</section>
<section id="t-learner-v.s.-x-learner" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="t-learner-v.s.-x-learner">T-learner v.s. X-learner</h2>
<p>Here, an advantage of X-learner over T-learner is demonstrated (This example also serves as an illustration of how these learners are implemented). Specifically, X-learner can be particularly useful when the control-treatment assignments in the sample are unbalanced. For example, it is often the case that there are plenty of observations in the control group, while there are not many treated observations. For the purpose of illustration, consider a rather extreme case where there are only 10 observations in the treated group, while there are 300 observations in the control group. We use the following toy data generating process:</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><strong>Packages to load for replication</strong></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(data.table)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ranger)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rlearner)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mgcv)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div></div><p><span class="math display">\[
\begin{align}
y = \tau W + |x| + v
\end{align}
\]</span></p>
<p>where <span class="math inline">\(\tau = 1\)</span>. So, the treatment effect is not heterogeneous. For the purpose of illustrating the advantage of X-learner over T-learner, it is convenient if the underlying model is simpler.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">4345</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>N_trt <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>N_ctrl <span class="ot">&lt;-</span> <span class="dv">300</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> N_trt <span class="sc">+</span> N_ctrl</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> </span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data.table</span>(</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">W =</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">1</span>,N_trt), <span class="fu">rep</span>(<span class="dv">0</span>, N_ctrl)),</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">type =</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="st">"Treated"</span>, N_trt), <span class="fu">rep</span>(<span class="st">"Control"</span>, N_ctrl)),</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="dv">2</span> <span class="sc">*</span> <span class="fu">runif</span>(N)<span class="sc">-</span><span class="dv">1</span>,</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">v =</span> <span class="fu">rnorm</span>(N) <span class="sc">/</span> <span class="dv">2</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>  .[, y <span class="sc">:</span><span class="er">=</span> W <span class="sc">+</span> <span class="fu">abs</span>(x) <span class="sc">+</span> v]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> data) <span class="sc">+</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">y =</span> y, <span class="at">x =</span> x, <span class="at">color =</span> type))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="C02-xstr-learner_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Let’s first estimate <span class="math inline">\(\mu_1(X)\)</span> and <span class="math inline">\(\mu_0(X)\)</span> (Step 1). Since we have only <span class="math inline">\(20\)</span> observations in the treated group, we will use a linear regression to avoid over-fitting (following the example in <span class="citation" data-cites="kunzel_metalearners_2019">@kunzel_metalearners_2019</span>).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>mu_1_trained <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x, <span class="at">data =</span> data[type <span class="sc">==</span> <span class="st">"Treated"</span>, ])</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>mu_0_trained <span class="ot">&lt;-</span> <span class="fu">gam</span>(y <span class="sc">~</span> <span class="fu">s</span>(x, <span class="at">k =</span> <span class="dv">4</span>), <span class="at">data =</span> data[type <span class="sc">==</span> <span class="st">"Control"</span>, ])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now that <span class="math inline">\(\mu_1(X)\)</span> and <span class="math inline">\(\mu_0(X)\)</span> are estimated, we can estimate <span class="math inline">\(\hat{\theta}(X)\)</span> by T-learner.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>x_seq <span class="ot">&lt;-</span> <span class="fu">data.table</span>(<span class="at">x =</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="at">length =</span> <span class="dv">100</span>))</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co">#=== T-learner ===#</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>tau_hat_data <span class="ot">&lt;-</span> </span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>  x_seq <span class="sc">%&gt;%</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>  .[, mu_1_hat <span class="sc">:</span><span class="er">=</span> <span class="fu">predict</span>(mu_1_trained, <span class="at">newdata =</span> x_seq)] <span class="sc">%&gt;%</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>  .[, mu_0_hat <span class="sc">:</span><span class="er">=</span> <span class="fu">predict</span>(mu_0_trained, <span class="at">newdata =</span> x_seq)] <span class="sc">%&gt;%</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>  .[, tau_hat_T <span class="sc">:</span><span class="er">=</span> mu_1_hat <span class="sc">-</span> mu_0_hat]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As you can see, T-leaner is heavily biased. This is because of the unreliable estimation of <span class="math inline">\(\mu_1(X)\)</span> due to lack of observations in the treated group.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> tau_hat_data) <span class="sc">+</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> tau_hat_T, <span class="at">x =</span> x))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="C02-xstr-learner_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Now, let’s move on to X-learner. We impute individual treatment effects (Step 2).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co">#=== mu (treated) ===#</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>mu_hat_1 <span class="ot">&lt;-</span> <span class="fu">predict</span>(mu_0_trained, <span class="at">newdata =</span> data[type <span class="sc">==</span> <span class="st">"Treated"</span>, ])</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co">#=== mu (control) ===#</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>mu_hat_0 <span class="ot">&lt;-</span> <span class="fu">predict</span>(mu_1_trained, <span class="at">newdata =</span> data[type <span class="sc">==</span> <span class="st">"Control"</span>, ])</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co">#=== assign the values ===#</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>data[type <span class="sc">==</span> <span class="st">"Treated"</span>, mu_hat <span class="sc">:</span><span class="er">=</span> mu_hat_1]</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>data[type <span class="sc">==</span> <span class="st">"Control"</span>, mu_hat <span class="sc">:</span><span class="er">=</span> mu_hat_0]</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="co">#=== find individual TE ===#</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>data[, D <span class="sc">:</span><span class="er">=</span> <span class="fu">ifelse</span>(type <span class="sc">==</span> <span class="st">"Treated"</span>, y <span class="sc">-</span> mu_hat, mu_hat <span class="sc">-</span> y)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can now regress <span class="math inline">\(D\)</span> on <span class="math inline">\(X\)</span> (Step 3),</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co">#--------------------------</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co"># tau (treated)</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co">#--------------------------</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>tau_1_trained <span class="ot">&lt;-</span> <span class="fu">lm</span>(D <span class="sc">~</span> x, <span class="at">data =</span> data[type <span class="sc">==</span> <span class="st">"Treated"</span>, ])</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co">#=== estimate tau_1 ===#</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>tau_hat_data[, tau_hat_1 <span class="sc">:</span><span class="er">=</span> <span class="fu">predict</span>(tau_1_trained, <span class="at">newdata =</span> tau_hat_data)]</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="co">#--------------------------</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="co"># tau (control)</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="co">#--------------------------</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>tau_0_trained <span class="ot">&lt;-</span> <span class="fu">gam</span>(D <span class="sc">~</span> <span class="fu">s</span>(x, <span class="at">k =</span> <span class="dv">4</span>), <span class="at">data =</span> data[type <span class="sc">==</span> <span class="st">"Control"</span>, ])</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="co">#=== estimate tau_1 ===#</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>tau_hat_data[, tau_hat_0 <span class="sc">:</span><span class="er">=</span> <span class="fu">predict</span>(tau_0_trained, <span class="at">newdata =</span> tau_hat_data)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> tau_hat_data) <span class="sc">+</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> tau_hat_1, <span class="at">x =</span> x, <span class="at">color =</span> <span class="st">"Treated"</span>)) <span class="sc">+</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> tau_hat_0, <span class="at">x =</span> x, <span class="at">color =</span> <span class="st">"Control"</span>)) <span class="sc">+</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"Treated"</span> <span class="ot">=</span> <span class="st">"blue"</span>, <span class="st">"Control"</span> <span class="ot">=</span> <span class="st">"red"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="C02-xstr-learner_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Let’s use propensity score as <span class="math inline">\(g(X)\)</span> in Step 4.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>w_gam_trained <span class="ot">&lt;-</span> </span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gam</span>(</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    W <span class="sc">~</span> <span class="fu">s</span>(x, <span class="at">k =</span> <span class="dv">4</span>), </span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> data, </span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> <span class="st">"probit"</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s predict <span class="math inline">\(E[W|X]\)</span> at each value of <span class="math inline">\(X\)</span> at which we are estiamting <span class="math inline">\(\tau\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>tau_hat_data[, g_x <span class="sc">:</span><span class="er">=</span> <span class="fu">predict</span>(w_gam_trained, <span class="at">newdata =</span> tau_hat_data, <span class="at">type =</span> <span class="st">"response"</span>)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As you can see below, the mean value of <span class="math inline">\(g(x)\)</span> is small because the treatment probability is very low (it is only <span class="math inline">\(20\)</span> out of <span class="math inline">\(320\)</span>).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(tau_hat_data[, g_x])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.06451538</code></pre>
</div>
</div>
<p>This number is basically <span class="math inline">\(20/320\)</span>. So, in this example, we could have just used the proportion of the treated observations. Notice that <span class="math inline">\(g(X)\)</span> is multiplied to <span class="math inline">\(\hat{\theta}_0(X)\)</span> in <a href="#eq-final-X">Equation&nbsp;2</a>. So, we are giving a lower weight to <span class="math inline">\(\hat{\theta}_0(X)\)</span>. This is because <span class="math inline">\(\hat{\theta}_0(X)\)</span> is less reliable because <span class="math inline">\(\hat{\mu}_1(X)\)</span> is less reliable due to the lack of samples in the treated group.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>tau_hat_data[, tau_hat_X <span class="sc">:</span><span class="er">=</span> g_x <span class="sc">*</span> tau_hat_0 <span class="sc">+</span> (<span class="dv">1</span><span class="sc">-</span>g_x) <span class="sc">*</span> tau_hat_1]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As you can see, X-learner outperforms T-learner in this particualr instance at least in terms of point estimates of <span class="math inline">\(\tau(X)\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> tau_hat_data) <span class="sc">+</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> tau_hat_T, <span class="at">x =</span> x, <span class="at">color =</span> <span class="st">"T-learner"</span>)) <span class="sc">+</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> tau_hat_X, <span class="at">x =</span> x, <span class="at">color =</span> <span class="st">"X-learner"</span>)) <span class="sc">+</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">1</span>, <span class="fu">aes</span>(<span class="at">color =</span> <span class="st">"True Treatment Effect"</span>)) <span class="sc">+</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">values =</span> <span class="fu">c</span>(</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>        <span class="st">"T-learner"</span> <span class="ot">=</span> <span class="st">"red"</span>, </span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>        <span class="st">"X-learner"</span> <span class="ot">=</span> <span class="st">"blue"</span>, </span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>        <span class="st">"True Treatment Effect"</span> <span class="ot">=</span> <span class="st">"black"</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>      ),</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>    <span class="at">name =</span> <span class="st">""</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>    ) <span class="sc">+</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">"Treatment Effect"</span>) <span class="sc">+</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: geom_hline(): Ignoring `mapping` because `yintercept` was provided.</code></pre>
</div>
<div class="cell-output-display">
<p><img src="C02-xstr-learner_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<!-- Let $e(X)$ denote the propensity score $pr(W=1|X) = E[W|X]$.　

Under the unconfoundedness assumption,

$E[\varepsilon(W_i)|X_i, W_i] = 0$, where $\varepsilon_i(w) = Y_i(w) - {\mu_0(X_i)} + w\tau(X_i)$ 

+ $Y_i(0) = {\mu_0(X_i)} + 0\cdot \tau(X_i) = \mu_0(X_i) + \varepsilon_i$
+ $Y_i(1) = {\mu_0(X_i)} + 1\cdot \tau(X_i) = \mu_0(X_i) + \tau(X_i)  + \varepsilon_i$

Conditional mean outcome (averaged across both treated and untreated) denoted by $m(x)$ is



$$
\begin{align}
m(x) = E[Y|X=x] = \mu_0(x) + e(x)\cdot \tau(x)
\end{align}
$$



Note that that observed outcome can be written as follows:



$$
\begin{align}
Y_i =  \mu_0(X_i) + W_i \tau(X_i)  + \varepsilon_i 
\end{align}
$$



Subtracting $m(X_i)$ from both sides,

$Y_i - m(X_i) = [W_i - e(X_i)]\cdot \tau(X_i) + \varepsilon_i$

This is termed **Robinson transformation**, which is originally proposed by @robinson1998.

According to Robins (2004), 

$\tau(X_i) = argmin_{\tau}\large\{\normalsize E\large(\normalsize[\{Y_i-m(X_i)\}-{W_i - e(X_i)}\tau]^2\large)\large\}$

So, if we were to know $m(X_i)$ and $e(X_i)$ for some reason, we can estimate $\tau(X_i)$ by solving the following sample analog of the loss minimization problem:

$\tilde{\tau}(X_i)= argmin_{\tau}\large\{\normalsize \frac{1}{n}\sum_{i=1}^{n}\normalsize[\{Y_i-m(X_i)\}-\{W_i - e(X_i)\}\tau]^2+\Lambda_n(\tau)\large\}$

where $\Lambda_n(\tau)$ is interpreted as a regularizer on the complexity of the $\tau$ function.

Of course the problem is that we do not know $m(X_i)$ and $e^*(X_i)$, so the above solution is not feasible.
 -->
</section>
<section id="r-learner" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="r-learner">R-learner</h2>
<section id="theoretical-background" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="theoretical-background">Theoretical background</h3>
<p>Under the assumptions,</p>
<p><span id="eq-yxw"><span class="math display">\[
\begin{aligned}
E[Y|X, W] = \theta(X)\cdot f(X,W) + g(X,W)
\end{aligned}
\tag{3}\]</span></span></p>

<div class="no-row-height column-margin column-container"><div class="">
<p><span class="math inline">\(f(X,W) = E[T|X,W]\)</span></p>
</div></div><p>Let, <span class="math inline">\(l(X,W)\)</span> denote <span class="math inline">\(E[Y|X, W]\)</span>. Taking the difference of <a href="#eq-model-framework">Equation&nbsp;1</a> and <a href="#eq-yxw">Equation&nbsp;3</a> on both sides,</p>
<p><span class="math display">\[
\begin{aligned}
Y_i - l(X_i,Y_i) &amp; = \theta(X_i)\cdot T_i + g(X_i,W_i) + \varepsilon_i - [\theta(X_i)\cdot f(X_i,W_i) + g(X_i,W_i)] \\
\Rightarrow Y_i - l(X_i,Y_i) &amp; = \theta(X_i)\cdot (T_i -f(X_i,W_i)) + \varepsilon_i \\
\end{aligned}
\]</span></p>

<div class="no-row-height column-margin column-container"><div class="">
<p>This is akin to residualization/orthogonalization seen in the DML approach in <strong>?@sec-dml</strong>.</p>
</div></div><p>So, the problem of identifying <span class="math inline">\(\theta(X)\)</span> reduces to estimating the following model:</p>
<p><span class="math display">\[
\begin{aligned}
Y_i - l(X_i,Y_i) &amp; = \theta(X_i)\cdot (T_i -f(X_i,W_i)) + \varepsilon_i
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\(E[(T_i -f(X_i,W_i))\cdot\varepsilon_i|X] = E[\eta_i\cdot\varepsilon_i|X] = 0\)</span> by assumption, we can regress <span class="math inline">\(\tilde{Y}_i\)</span> on <span class="math inline">\(X_i\)</span> and <span class="math inline">\(\tilde{T}_i\)</span> to estimate <span class="math inline">\(\theta(X)\)</span>. Specifically, we can minimize the following objective function:</p>
<p><span id="eq-est-equation"><span class="math display">\[
\begin{aligned}
Min_{\theta(X)}\sum_{i=1}^N \large(\normalsize[Y_i - l(X_i,Y_i)] - [\theta(X_i)\cdot (T_i -f(X_i,W_i))]\large)^2
\end{aligned}
\tag{4}\]</span></span></p>
</section>
<section id="sec-est-steps" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-est-steps">Estimation steps</h3>
<p>In practice, we of course do not observe <span class="math inline">\(l(X,W)\)</span> <span class="math inline">\(( \equiv E[Y|X, W])\)</span> and <span class="math inline">\(f(X,W)\)</span> <span class="math inline">\((\equiv E[T|X, W])\)</span>. So, we first need to estimate them using the data at hand to construct <span class="math inline">\(\hat{\tilde{Y}}\)</span> and <span class="math inline">\(\hat{\tilde{T}}\)</span>. You can use any suitable statistical methods to estimate <span class="math inline">\(E[Y|X, W]\)</span> and <span class="math inline">\(f(X,W)\)</span>. Some machine learning methods allow you to estimate them without assuming any functional form or structural assumptions. If you believe they are linear functions of <span class="math inline">\(X\)</span> and <span class="math inline">\(W\)</span>, you could alternatively use lasso or other linear models. <span class="citation" data-cites="nie_quasi-oracle_2021">@nie_quasi-oracle_2021</span> proposes that the estimation of <span class="math inline">\(l(X,W)\)</span> and <span class="math inline">\(f(X,W)\)</span> is done by cross-fitting (see <strong>?@sec-cf</strong>) to avoid over-fitting bias. Let <span class="math inline">\(I_{-i}\)</span> denote all the observations that belong to the folds that <span class="math inline">\(i\)</span> does <span style="color:blue"> not </span> belong to. Further, let <span class="math inline">\(\hat{f}(X_i, W_i)^{I_{-i}}\)</span> and <span class="math inline">\(\hat{g}(X_i, W_i)^{I_{-i}}\)</span> denote <span class="math inline">\(f(X_i, W_i)\)</span> and <span class="math inline">\(g(X_i, W_i)\)</span> estimated using <span class="math inline">\(I_{-i}\)</span>.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>Just like the DML approach discussed in <strong>?@sec-dml</strong>, both <span class="math inline">\(Y\)</span> and <span class="math inline">\(T\)</span> are orthogonalized.</p>
</div></div><p>Then the quality of fit (explaining the heterogeneity in the impact of treatment) can be expressed as follows, which is the empirical version of <a href="#eq-est-equation">Equation&nbsp;4</a>:</p>
<p><span class="math display">\[
\begin{aligned}
\sum_{i=1}^N [Y_i - \hat{f}(X_i,W_i)^{I_{-i}} - \theta(X)\cdot (T_i - \hat{g}(X_i,W_i)^{I_{-i}})]^2
\end{aligned}
\]</span></p>
<p>This is called <span style="color:blue"> R-score</span>, and it can be used for causal model selection, which is covered later.</p>
<p>The final stage of the R-learner is to estimate <span class="math inline">\(\theta(X)\)</span> by minimizing the R-score plus the regularization term (if desirable).</p>
<p><span class="math display">\[
\begin{aligned}
\hat{\theta}(X) = argmin_{\theta(X)}\;\;\sum_{i=1}^N [Y_i - \hat{f}(X_i,W_i)^{I_{-i}} - \theta(X)\cdot (T_i - \hat{g}(X_i,W_i)^{I_{-i}})]^2 + \Lambda(\theta(X))
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\Lambda(\theta(X))\)</span> is the penalty on the complexity of <span class="math inline">\(\theta(X)\)</span>. For example, if you choose to use lasso, then <span class="math inline">\(\Lambda(\theta(X))\)</span> is the L1 norm. You have lots of freedom as to what model you use in the final stage. The <code>econml</code> package offers several off-the-shelf choices of R-learner (DML) approaches that differ in the model used at the final stage, including causal forest, lasso, etc.</p>
</section>
<section id="r-learner-by-hand" class="level3">
<h3 class="anchored" data-anchor-id="r-learner-by-hand">R-learner by hand</h3>
<p>This section goes through the estimation steps provided above to further the understanding of how R-learner works.</p>
</section>
</section>
<section id="comparing-the-learners" class="level2">
<h2 class="anchored" data-anchor-id="comparing-the-learners">Comparing the learners</h2>
<p><span class="math display">\[
\begin{aligned}
Y_i =\theta(X_i)\cdot T + \alpha g(X_i) + \mu_i
\end{aligned}
\]</span></p>
<ul>
<li><span class="math inline">\(X_i = \{X_{i,1}, X_{i,2}, X_{i,3}, X_{i,4}, X_{i,5}\}\)</span></li>
<li><span class="math inline">\(T_i|X_i \sim Bernouli(f(X_i))\)</span></li>
<li><span class="math inline">\(\mu_i|X_i \sim N(0,1)\)</span></li>
</ul>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Case A
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math display">\[
\begin{aligned}
g(X_i) &amp; = sin(\pi X_{i,1}X_{i,2}) + 2(X_{i,3}-0.5)^2 + X_{i,4} + 0.5 X_{i,5}\\
e(X_i) &amp; = max(0.1, min(sin(\pi X_{i,1}X_{i,2}), 0.9)) \\
\theta(X_i) &amp; = (X_{i,1}, X_{i,2}) / 2 \\
X_i &amp; \sim Uni(0,1)^5
\end{aligned}
\]</span></p>
</div>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Case B (randomized trial)
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math display">\[
\begin{aligned}
g(X_i) &amp; = max(X_{i,1} + X_{i,2}, X_{i,3}, 0) + max(X_{i,4}+ X_{i,5},0)\\
e(X_i) &amp; = 1/2 \\
\theta(X_i) &amp; = X_{i,1} + log(1+exp(X_{i,2})) \\
X_i &amp; \sim N(0,I_5)
\end{aligned}
\]</span></p>
</div>
</div>
</section>
<section id="x--s--t--r-learner-in-python" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="x--s--t--r-learner-in-python">X-, S-, T-, R-learner in Python</h2>
<p>We saw a general R-learner framework for CATE estimation. We now look at an example of Linear DML, which uses a linear model at the final stage. So, we are assuming that <span class="math inline">\(\theta(X)\)</span> can be written as follows in <a href="#eq-model-framework">Equation&nbsp;1</a>:</p>
<p><span class="math display">\[
\begin{aligned}
\theta(X) = \alpha + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_k x_k
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(x_1\)</span> through <span class="math inline">\(x_k\)</span> are the drivers of heterogeneity in treatment effects and <span class="math inline">\(\beta_1\)</span> through <span class="math inline">\(\beta_k\)</span> are their coefficients.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><strong>Packages to load for replication</strong></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(data.table)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(magick)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(fixest)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(officer)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(reticulate)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(DoubleML)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div></div><p>We use both Python and R for this demonstration. So, let’s set things up for that.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(reticulate)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="fu">use_virtualenv</span>(<span class="st">"ml-learning"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For this demonstration, we use synthetic data according to the following data generating process:</p>
<p><span class="math display">\[
\begin{aligned}
y_i = exp(x_{i,1}) d_i + x_{i,1} + \frac{1}{4}\cdot\frac{exp(x_{i,3})}{1 + exp(x_{i,3})} + \mu_i \\
d_i = \frac{exp(x_{i,1})}{1 + exp(x_{i,1})} + \frac{1}{4}\cdot x_{i,3}+ \eta_i
\end{aligned}
\]</span></p>
<p>Note that this is the same data generating process used in <strong>?@sec-dml</strong> except that the impact of the treatment (<span class="math inline">\(d\)</span>) now depends on <span class="math inline">\(x_1\)</span>. We can use <code>gen_data()</code> function that is defined in <strong>?@sec-dml-naive</strong>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co">#=== sample size ===#</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">1000</span> </span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co">#=== generate data ===#</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>synth_data <span class="ot">&lt;-</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">gen_data</span>(</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">te_formula =</span> <span class="fu">formula</span>(<span class="sc">~</span> <span class="fu">I</span>(<span class="fu">exp</span>(x1)<span class="sc">*</span>d)),</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">n_obs =</span> N <span class="sc">*</span><span class="dv">2</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> dplyr<span class="sc">::</span><span class="fu">select</span>(synth_data, <span class="fu">starts_with</span>(<span class="st">"x"</span>)) <span class="sc">%&gt;%</span> <span class="fu">as.matrix</span>()</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> synth_data[, y]</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> synth_data[, d]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We now split the data into training and test datasets.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test, d_train, d_test<span class="op">=</span> train_test_split(r.X, r.y, r.d,  test_size <span class="op">=</span> <span class="fl">0.5</span>, random_state <span class="op">=</span> <span class="dv">8923</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here, to train a linear DML model, we use the Python <code>econml</code> package, which offers one of the most comprehensive sets of off-the-shelf R-leaner (DML) methods <span class="citation" data-cites="econml">[@econml]</span>. We can use the <code>DML</code> class to implement linear DML.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> econml.dml <span class="im">import</span> DML</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<p><code>DML</code> is a child class of <code>_Rlearner</code>, which is a private class. The <code>DML</code> class has several child classes: <code>LinearDML</code>, <code>SpatseLinearDML</code>, <code>NonParamDML</code>, and <code>CausalForestDML</code>.</p>
</div></div><p>As we saw above in <a href="#sec-est-steps">Section&nbsp;1.5.2</a>, we need to specify three models:</p>
<ul>
<li><code>model_y</code>: model for estimating <span class="math inline">\(E[Y|X,W]\)</span></li>
<li><code>model_t</code>: model for estimating <span class="math inline">\(E[T|X,W]\)</span></li>
<li><code>model_final</code>: model for estimating <span class="math inline">\(\theta(X)\)</span></li>
</ul>
<p>In this example, let’s use gradient boosting regression for both <code>model_y</code> and <code>model_t</code> and use lasso with cross-validation for <code>model_final</code>. Let’s import <code>GradientBoostingRegressor()</code> and <code>LassoCV()</code> from the <code>scikitlearn</code> package.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> GradientBoostingRegressor</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LassoCV</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can now set up our DML framework like below:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>est <span class="op">=</span> DML(</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    model_y <span class="op">=</span> GradientBoostingRegressor(),</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    model_t <span class="op">=</span> GradientBoostingRegressor(),</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    model_final <span class="op">=</span> LassoCV(fit_intercept <span class="op">=</span> <span class="va">False</span>) </span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note that no training has happened yet at this point. We simply created a recipe. Once we provide ingredients (data), we can cook (train) with the <code>fit()</code> method.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>est.fit(y_train, d_train, X <span class="op">=</span> X_train, W <span class="op">=</span> X_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>first argument: dependent variable</li>
<li>second argument: treatment variable</li>
<li><code>X</code>: variables that drive treatment effect heterogeneity</li>
<li><code>W</code>: variables that affect the dependent variable directly</li>
</ul>

<div class="no-row-height column-margin column-container"><div class="">
<p>Here, we set <code>X = W</code>.</p>
</div></div><p>Once, the training is done. We can use the <code>effect()</code> method to predict <span class="math inline">\(\theta(X)\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>te_test <span class="op">=</span> est.effect(X_test)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><a href="#fig-est-theta-hat">Figure&nbsp;1</a> presents the estimated and true marginal treatment effect (<span class="math inline">\(\theta(X)\)</span>) as a function of <code>x1</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>plot_data <span class="ot">&lt;-</span> </span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data.table</span>(</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">x1 =</span> py<span class="sc">$</span>X_test[, <span class="dv">1</span>],</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">te =</span> py<span class="sc">$</span>te_test</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(plot_data) <span class="sc">+</span></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">y =</span> te, <span class="at">x =</span> x1)) <span class="sc">+</span></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> <span class="fu">exp</span>(x1), <span class="at">x =</span> x1), <span class="at">color =</span> <span class="st">"blue"</span>) <span class="sc">+</span></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">
<div id="fig-est-theta-hat" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="C02-xstr-learner_files/figure-html/fig-est-theta-hat-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Figure 1: Estimated and true marginal treatment effects</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Since we forced <span class="math inline">\(\theta(X)\)</span> to be linear in <code>x1</code>, it is not surprising that the estimated MTE looks linear in <code>x1</code> even though the true MTE is an exponential function of <code>x1</code>. In the next chapter (<strong>?@sec-forest-cate</strong>), we discuss CATE estimators based on forest, which estimates <span class="math inline">\(\theta(X)\)</span> non-parametrically, relaxing the assumption of <span class="math inline">\(\theta(X)\)</span> being linear-in-parameter.</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>There are many more variations in DML than the one presented here. For those who are interested, I recommend going through examples presented <a href="https://github.com/microsoft/EconML/blob/main/notebooks/Double%20Machine%20Learning%20Examples.ipynb">here</a> for <code>DML</code></p>
</div>
</div>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      let href = ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>