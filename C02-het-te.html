<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-0.9.629">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>c02-het-te</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="C02-het-te_files/libs/clipboard/clipboard.min.js"></script>
<script src="C02-het-te_files/libs/quarto-html/quarto.js"></script>
<script src="C02-het-te_files/libs/quarto-html/popper.min.js"></script>
<script src="C02-het-te_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="C02-het-te_files/libs/quarto-html/anchor.min.js"></script>
<link href="C02-het-te_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="C02-het-te_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="C02-het-te_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="C02-het-te_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="C02-het-te_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
</div>
<main class="content page-columns page-full" id="quarto-document-content">



<section id="sec-het-dml" class="level1 page-columns page-full">
<h1>Heterogeneous Treatment Effect Estimation</h1>
<p>In <strong>?@sec-dml</strong>, the basic idea of double machine learning (DML) methods was introduced when the treatment effect is homogeneous. We now turn our focus to the task of estimating heterogeneous treatment effects: the impact of a treatment varies based on observed attributes of the subjects. Heterogeneous treatment effect is also referred to as <span style="color:blue"> conditional </span> average treatment effect (CATE).</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><span style="color:blue"> Conditional </span> on observed attributes.</p>
</div></div><section id="motivation" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="motivation">Motivation</h2>
<p>Understanding the how treatment effects vary can be highly valuable in many circumstances.</p>
<p><span style="color:blue"> Example 1: </span> If we come to know a particular drug is effective on elderly people but detrimental to kids, then doctors can make a smart decision of prescribing the drug to elderly people, but not to kids.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>In this example, the heterogeneity driver is age.</p>
</div></div><p><span style="color:blue"> Example 2: </span> If we come to know that fertilizer is more effective in increasing corn yield in soil type A than B, then farmers can apply more fertilizer on the parts of the field where soil type is A but less on where soil type is B.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>In this example, the heterogeneity driver is soil type.</p>
</div></div><p>As you can see in these examples, knowledge on the heterogeneity of the treatment effects and its causes can help decision makers smart-target the treatment.</p>
</section>
<section id="modeling-framework" class="level2">
<h2 class="anchored" data-anchor-id="modeling-framework">Modeling Framework</h2>
<p>The model of interest in a general form here is as follows:</p>
<p><span class="math display">\[
\begin{aligned}
Y &amp; = \theta(X)\cdot T + g(X, W) + \varepsilon \\
T &amp; = f(X, W) + \eta
\end{aligned}
\]</span></p>
<ul>
<li><span class="math inline">\(Y\)</span>: dependent variable</li>
<li><span class="math inline">\(T\)</span>: treatment variable (can be either binary dummy or continuous)</li>
<li><span class="math inline">\(X\)</span>: collection of variables that affect Y indirectly through the treatment (<span class="math inline">\(\theta(X)\cdot T\)</span>) and directly (<span class="math inline">\(g(X, W)\)</span>) independent of the treatment</li>
<li><span class="math inline">\(W\)</span>: collection of variables that affect directly (<span class="math inline">\(g(X, W)\)</span>) independent of the treatment, but not through the treatment</li>
</ul>
<p>Here are the key assumptions:</p>
<ul>
<li><span class="math inline">\(E[\varepsilon|X, W] = 0\)</span></li>
<li><span class="math inline">\(E[\eta|X, W] = 0\)</span></li>
<li><span class="math inline">\(E[\eta\cdot\varepsilon|X, W] = 0\)</span></li>
</ul>
<p>Our objective is to estimate the <span style="color: red;"> constant </span> marginal CATE <span class="math inline">\(\theta(X)\)</span>. (constant in the sense marginal CATE is the same irrespective of the value of the treatment)</p>
</section>
<section id="r-learner" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="r-learner">R-learner</h2>
<section id="theoretical-background" class="level3">
<h3 class="anchored" data-anchor-id="theoretical-background">Theoretical background</h3>
<p>Under the assumptions,</p>
<p><span class="math display">\[
\begin{aligned}
E[Y|X, W] = \theta(X)\cdot E[T|X,W] + g(X,W)
\end{aligned}
\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[
\begin{aligned}
Y &amp; = \theta(X)\cdot T + g(X,W) + \varepsilon \\
\Rightarrow Y - E[Y|X, W] &amp; = \theta(X)\cdot T + g(X,W) + \varepsilon - \theta(X)\cdot E[T|X,W] - g(X,W) \\
\Rightarrow Y - E[Y|X, W] &amp; = \theta(X)\cdot (T - E[T|X,W]) + \varepsilon \\
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
Y - E[Y|X, W] &amp; = \theta(X)\cdot (T - E[T|X,W]) + \varepsilon
\end{aligned}
\]</span></p>
<p>Suppose we know <span class="math inline">\(E[Y|X, W]\)</span> and <span class="math inline">\(E[T|X,W]\)</span>, then we can construct the following new variables:</p>
<ul>
<li><span class="math inline">\(\tilde{Y} = Y - E[Y|X, W]\)</span></li>
<li><span class="math inline">\(\tilde{T} = T - E[T|X, W] = \eta\)</span></li>
</ul>
<p>Then, the problem of identifying <span class="math inline">\(\theta(X)\)</span> reduces to estimating the following model:</p>
<p><span class="math display">\[
\begin{aligned}
\tilde{Y} = \theta(X)\cdot \tilde{T} + \varepsilon
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\(E[\eta\cdot\varepsilon|X] = 0\)</span> by assumption, we can regress <span class="math inline">\(\tilde{Y}\)</span> on <span class="math inline">\(X\)</span> and <span class="math inline">\(\tilde{T}\)</span>,</p>
<p><span id="eq-est-equation"><span class="math display">\[
\begin{aligned}
\hat{\theta} = argmin_{\theta} \;\; E[(\tilde{Y} - \theta(X)\cdot \tilde{T})^2]
\end{aligned}
\tag{1}\]</span></span></p>
</section>
<section id="estimation-steps" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="estimation-steps">Estimation steps</h3>
<p>In practice, we of course do not observe <span class="math inline">\(E[Y|X, W]\)</span> and <span class="math inline">\(E[T|X, W]\)</span>. So, we first need to estimate them using the data at hand to construct <span class="math inline">\(\hat{\tilde{Y}}\)</span> and <span class="math inline">\(\hat{\tilde{T}}\)</span>. You can use any suitable statistical methods to estimate <span class="math inline">\(E[Y|X, W]\)</span> and <span class="math inline">\(E[T|X, W]\)</span>. Some machine learning methods allow you to estimate them without assuming any functional form or structural assumptions. If you believe they are linear functions of <span class="math inline">\(X\)</span> and <span class="math inline">\(W\)</span>, may could alternatively use lasso or other linear models. It is important to keep in mind that the estimation of <span class="math inline">\(E[Y|X, W]\)</span> and <span class="math inline">\(E[T|X, W]\)</span> is done by cross-fitting (see <strong>?@sec-cf</strong>) to avoid over-fitting bias. Let, <span class="math inline">\(f(X, W)\)</span> and <span class="math inline">\(g(X,W)\)</span> denote <span class="math inline">\(\tilde{Y}\)</span> and <span class="math inline">\(\tilde{T}\)</span>, respectively. Further, let <span class="math inline">\(I_{-i}\)</span> denote all the observations that belong to the folds that <span class="math inline">\(i\)</span> does <span style="color:blue"> not </span> belong to. Finally, let <span class="math inline">\(\hat{f}(X_i, W_i)^{I_{-i}}\)</span> and <span class="math inline">\(\hat{g}(X_i, W_i)^{I_{-i}}\)</span> denote <span class="math inline">\(\tilde{Y}\)</span> and <span class="math inline">\(\tilde{T}\)</span> estimated using <span class="math inline">\(I_{-i}\)</span>.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>Just like the DML approach discussed in <strong>?@sec-dml</strong>, both <span class="math inline">\(Y\)</span> and <span class="math inline">\(T\)</span> are orthogonalized.</p>
</div></div><p>Then the quality of fit (explaining the heterogeneity in the impact of treatment) can be expressed as follows, which is the empirical version of <a href="#eq-est-equation">Equation&nbsp;1</a>:</p>
<p><span class="math display">\[
\begin{aligned}
\sum_{i=1}^N [\hat{f}(X_i,W_i)^{I_{-i}} - \theta(X)\cdot \hat{g}(X_i,W_i)^{I_{-i}}]^2
\end{aligned}
\]</span></p>
<p>This is called <span style="color:blue"> R-score</span>, and it can be used for causal model selection, which is covered later.</p>
<p>The final stage of the R-learner is to estimate <span class="math inline">\(\theta(X)\)</span> by minimizing the R-score plus the regularization term (if desirable).</p>
<p><span class="math display">\[
\begin{aligned}
\hat{\theta}(X) = argmin_{\theta(X)}\;\;\sum_{i=1}^N [\hat{f}(X_i,W_i)^{I_{-i}} - \theta(X)\cdot \hat{g}(X_i,W_i)^{I_{-i}}]^2 + \Lambda(\theta(X))
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\Lambda(\theta(X))\)</span> is the penalty on the complexity of <span class="math inline">\(\theta(X)\)</span>. For example, if you choose to use lasso, then <span class="math inline">\(\Lambda(\theta(X))\)</span> is the L1 norm. You have lots of freedom as to what model you use in the final stage. The <code>econml</code> package offers several off-the-shelf choices of R-learner (DML) approaches that differ in the model used at the final stage, including causal forest, lasso, etc.</p>
</section>
</section>
<section id="implementation" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="implementation">Implementation</h2>
<p>We use the python <code>econml</code> pacakge, which offers one of the most comprehensive sets of off-the-shelf R-leaner (DML) methods <span class="citation" data-cites="econml">[@econml]</span>.</p>
<p>Let’s first take a look at the <code>DML</code> class, which implements DML where the final stage has to be a linear model. That is,</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><code>DML</code> is a child class of <code>_Rlearner</code>, which is a private class.</p>
</div></div><p><span class="math display">\[
\begin{aligned}
\theta(x) = \beta_1 + x_1 + \dots + \beta_k + x_k
\end{aligned}
\]</span></p>
<p>As we saw above, we need to specify three models.</p>
<ul>
<li><code>model_y</code>: model for estimating <span class="math inline">\(E[Y|X,W]\)</span></li>
<li><code>model_t</code>: model for estimating <span class="math inline">\(E[T|X,W]\)</span></li>
<li><code>model_final</code>: model for estimating <span class="math inline">\(\theta(X)\)</span></li>
</ul>
<p>In this example, let’s use gradient boosting regression for both <code>model_y</code> and <code>model_t</code> and use lasso with cross-validation for <code>model_final</code>.</p>
<p>First import all the functions we will need.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> econml.dml <span class="im">import</span> DML</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>IPython could not be loaded!</code></pre>
</div>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LassoCV</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> GradientBoostingRegressor</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> itertools <span class="im">import</span> product</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We now set up a DML estimation procedure.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>est <span class="op">=</span> DML(</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>  model_y<span class="op">=</span>GradientBoostingRegressor(),</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>  model_t<span class="op">=</span>GradientBoostingRegressor(),</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>  model_final<span class="op">=</span>LassoCV(fit_intercept<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Note that no fitting has occurred yet. We just have provided python with the recipe. We use the following data generating process from <a href="https://arxiv.org/abs/1806.03467">here</a>:</p>
<p><span class="math display">\[
\begin{align}
T = &amp; W\beta + \eta, &amp; \;\eta \sim \text{Uniform}(-1, 1)\\
Y = &amp; T\cdot \exp(2\cdot x_1) + W\gamma + \epsilon, &amp;\; \epsilon \sim \text{Uniform}(-1, 1)\\
W \sim&amp; \text{Normal}(0,\, I_{n_w})\\
X \sim&amp; \text{Uniform}(0,1)^{n_x}
\end{align}
\]</span></p>
<p>where <span class="math inline">\(n_w\)</span> and <span class="math inline">\(n_x\)</span> are the number of variables in <span class="math inline">\(W\)</span> and <span class="math inline">\(X\)</span>, respectively. In this example, the heterogeneity of treatment effect is driven by <span class="math inline">\(x_1\)</span> in the following form:</p>
<p><span class="math display">\[
\begin{align}
\theta(x) = \exp(2\cdot x_1).
\end{align}
\]</span></p>
<p>Let’s now generate a dataset according to the data generating process.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> exp_te(x):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.exp(<span class="dv">2</span><span class="op">*</span>x[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">123</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>n_w <span class="op">=</span> <span class="dv">30</span> </span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>support_size <span class="op">=</span> <span class="dv">5</span> <span class="co"># number of W variables to be part of the DGP</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>n_x <span class="op">=</span> <span class="dv">1</span> <span class="co"># number of te heterogeneity drivers</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co">#-------------------------</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Y equation</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="co">#-------------------------</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="co"># determine which of the 30 variables to be part of the DGP</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>support_Y <span class="op">=</span> np.random.choice(np.arange(n_w), size<span class="op">=</span>support_size, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="co"># generate coefficients (\beta) on W in Y equation</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>coefs_Y <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="dv">1</span>, size<span class="op">=</span>support_size) </span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="co"># error term in Y equation</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>epsilon_sample <span class="op">=</span> <span class="kw">lambda</span> n: np.random.uniform(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, size<span class="op">=</span>n)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="co">#-------------------------</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a><span class="co"># T equation</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="co">#-------------------------</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>support_T <span class="op">=</span> support_Y</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a><span class="co"># generate coefficients (\gamma) on W in T equation</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>coefs_T <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="dv">1</span>, size<span class="op">=</span>support_size)</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a><span class="co"># error term in T equation</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>eta_sample <span class="op">=</span> <span class="kw">lambda</span> n: np.random.uniform(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, size<span class="op">=</span>n)</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a><span class="co">#-------------------------</span></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate controls, covariates, treatments and outcomes</span></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a><span class="co">#-------------------------</span></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, size<span class="op">=</span>(n, n_w))</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="dv">1</span>, size<span class="op">=</span>(n, n_x))</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Heterogeneous treatment effects</span></span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>TE <span class="op">=</span> np.array([exp_te(x_i) <span class="cf">for</span> x_i <span class="kw">in</span> X])</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> np.dot(W[:, support_T], coefs_T) <span class="op">+</span> eta_sample(n)</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> TE <span class="op">*</span> T <span class="op">+</span> np.dot(W[:, support_Y], coefs_Y) <span class="op">+</span> epsilon_sample(n)</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a><span class="co">#-------------------------</span></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample splitting</span></span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a><span class="co">#-------------------------</span></span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>Y_train, Y_val, T_train, T_val, X_train, X_val, W_train, W_val <span class="op">=</span> train_test_split(Y, T, X, W, test_size<span class="op">=</span><span class="fl">.2</span>)</span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate test data </span></span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> np.array(<span class="bu">list</span>(product(np.arange(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.01</span>), repeat<span class="op">=</span>n_x)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can use the <code>fit()</code> method to train the <code>DML</code> model we set up above.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>est.fit(Y_train, T_train, X<span class="op">=</span>X_train, W<span class="op">=</span>W_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;econml.dml.dml.DML object at 0x29a12ab80&gt;</code></pre>
</div>
</div>
<p>Once the model is trained, you can use the trained model to predict <span class="math inline">\(\theta(X)\)</span> with the <code>effect()</code> method to which you supply the value of <span class="math inline">\(X\)</span> at which <span class="math inline">\(\hat{\theta}(X)\)</span> is calculated.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co">#=== calculate theta_hat(X) ===#</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>te_pred <span class="op">=</span> est.effect(X_test)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co">#=== take a look inside ===#</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>te_pred</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0.         0.06408296 0.12816593 0.19224889 0.25633185 0.32041481
 0.38449778 0.44858074 0.5126637  0.57674667 0.64082963 0.70491259
 0.76899556 0.83307852 0.89716148 0.96124444 1.02532741 1.08941037
 1.15349333 1.2175763  1.28165926 1.34574222 1.40982519 1.47390815
 1.53799111 1.60207407 1.66615704 1.73024    1.79432296 1.85840593
 1.92248889 1.98657185 2.05065482 2.11473778 2.17882074 2.2429037 
 2.30698667 2.37106963 2.43515259 2.49923556 2.56331852 2.62740148
 2.69148445 2.75556741 2.81965037 2.88373333 2.9478163  3.01189926
 3.07598222 3.14006519 3.20414815 3.26823111 3.33231407 3.39639704
 3.46048    3.52456296 3.58864593 3.65272889 3.71681185 3.78089482
 3.84497778 3.90906074 3.9731437  4.03722667 4.10130963 4.16539259
 4.22947556 4.29355852 4.35764148 4.42172445 4.48580741 4.54989037
 4.61397333 4.6780563  4.74213926 4.80622222 4.87030519 4.93438815
 4.99847111 5.06255408 5.12663704 5.19072    5.25480296 5.31888593
 5.38296889 5.44705185 5.51113482 5.57521778 5.63930074 5.70338371
 5.76746667 5.83154963 5.89563259 5.95971556 6.02379852 6.08788148
           6.15196445 6.21604741 6.28013037 6.34421334]           </code></pre>
</div>
</div>
<p><a href="#fig-mar-effect-linear">Figure&nbsp;1</a> plots <span class="math inline">\(\hat{\theta}(X)\)</span> and true <span class="math inline">\(\theta(X)\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">6</span>))</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>plt.plot(X_test, te_pred, label<span class="op">=</span><span class="st">'DML default'</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>expected_te <span class="op">=</span> np.array([exp_te(x_i) <span class="cf">for</span> x_i <span class="kw">in</span> X_test])</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>plt.plot(X_test, expected_te, <span class="st">'b--'</span>, label<span class="op">=</span><span class="st">'True effect'</span>)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Treatment Effect'</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x'</span>)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-mar-effect-linear" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="C02-het-te_files/figure-html/fig-mar-effect-linear-1.png" class="img-fluid figure-img" width="960"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Figure 1: Visualization of the marginal effect of the treatment</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      let href = ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>