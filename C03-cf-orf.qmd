# Forest-based CATE Estimators {#sec-cf-orf}

In @sec-het-dml, we saw some special cases of R-learner/DML where the final model is estimate by parametrically using a linear-in-parameter model and non-parametrically using extreme gradient boosting. Here, we learn two methods that estimate CATE non-parametrically: causal forest [@athey2019generalized] and orthogonal forest [@oprescu2019orthogonal].

:::{.callout-important}

## What you will learn

+ Causal forest 
  * Mechanics
  * Hyper-parameter
  * Predict heterogeneous treatment effects (CATE)
  * Predict average treatment effects (ATE)
  * Interpretation
+ Orthogonal forest 
  * Differences from causal forest

:::

:::{.callout-tip}
## Background knowledge

+ (preferable) DML/R-learner (@sec-het-dml)
+ (preferable) GRF (@sec-grf)

:::

:::{.callout-note}

## Packages to load for replication

```{r}
#| include: false
library(data.table)
library(tidyverse)
library(grf)
```

```{r}
#| eval: false
library(data.table)
library(tidyverse)
library(grf)
```
:::


**Left to be added**
+ how variance is estimated
+ balance
+ ATE
+ linear correction with locally weighted linear regression

## Model

The heterogeneous treatment effect model of interest in this chapter is the same as the one in @sec-het-dml.

$$
\begin{aligned}
Y & = \theta(X)\cdot T + g(X) + \varepsilon \\
T & = f(X) + \eta 
\end{aligned}
$$ {#eq-model-het-te}

+ $Y$: dependent variable
+ $T$: treatment variable (can be either binary dummy or continuous)
+ $X$: features

Causal forest and orthogonal random forest is consistent only if the following conditions fold.

+ $E[\varepsilon|X] = 0$
+ $E[\eta|X] = 0$
+ $E[\eta\cdot\varepsilon|X] = 0$

## Causal Forest

### Brief description of how CF works

This section provides a brief and cursory description of how CF works. A much more detailed treatment of how CF (and also GRF) works along with the explanations of hyper-parameters is provided in @sec-understand-grf-example.

Causal Forest (CF) (as implemented by the R `grf` package or python `econml` package) is a special type of R-learner (also a DML) and also a special case of generalized random forest (GRF). 

::: {.column-margin}
CF can be implemented using the R `grf` package or python `econml` package. Both of them implements CF as an R-learner. However, the original causal forest proposed in @wager_estimation_2018 does not follow an R-learner procedure.
:::

Let $\hat{f}(X_i)$ and $\hat{g}(X_i)$ denote the estimation of $E[Y|X]$ and $E[T|X]$, respectively. Further, let $\hat{\tilde{Y_i}}$ and $\hat{\tilde{T_i}}$ denote $Y_i - \hat{f}(X_i)$ and $T_i - \hat{g}(X_i)$, respectively.

Then, CF estimates $\theta(X)$ at $X = X_0$ by solving the following equation:

$$
\begin{aligned}
\hat{\theta}(X_0) = argmin_{\theta}\;\;\sum_{i=1}^N \alpha_i(X_i, X_0)[\hat{\tilde{Y_i}} - \theta\cdot \hat{\tilde{T_i}}]^2
\end{aligned}
$$ {#eq-solve-cf}

where $\alpha_i(X_i, X_0)$ is the weight given to each $i$. The F.O.C is 

$$
\begin{aligned}
2 \cdot \sum_{i=1}^N \alpha_i(X_i, X_0)[\hat{\tilde{Y_i}} - \theta\cdot \hat{\tilde{T_i}}]\hat{\tilde{T_i}} = 0 
\end{aligned}
$$

$$
\begin{aligned}
\hat{\theta}(X_0) = \frac{\sum_{i=1}^N\alpha_i(X_i, X_0)\cdot\hat{\tilde{Y}}_i\cdot \hat{\tilde{T}}_i}{\sum_{i=1}^N\alpha_i(X_i, X_0)\cdot\hat{\tilde{T}}_i\cdot \hat{\tilde{T}}_i}
\end{aligned}
$$ {#eq-theta-solution}

Unlike the DML approaches we saw in @sec-het-dml that uses a linear model as the final model, CF does not assume any functional form for how $X$ affects $\theta$ as you can see from the above minimization problem. As mentioned earlier, CF is a special case of GRF (discussed in @sec-grf), so CF is also a local constant regression (see @sec-local-reg for a brief discussion on local regression).

$\alpha_i(X_i, X_0)$ is determined based on the trees trained based on the pseudo outcomes that are defined specifically for causal forest estimation. Suppose $T$ trees have been built and let $\eta_{i,t}(X_i, X_0)$ be 1 if observation $i$ belongs to the same leaf as $X_0$ in tree $t$. Then, 

$$
\begin{aligned}
\alpha_i(X_i, X_0) = \frac{1}{T}\sum_{t=1}^T\frac{\eta_{i,t}(X_i, X_0)}{\sum_{i=1}^N\eta_{i,t}(X_i, X_0)}
\end{aligned}
$$ {#eq-weight-cf}

So, the weight given to observation $i$ is higher if observation $i$ belongs to the same leaf as the evaluation point $X_0$ in more trees. 


### Training a causal forest

We can use the `causal_forest()` function from the `grf` package to train a CF model in R. In Python, you can use `CausalForestDML()` from the `econml` package or `GRFForestRegressor` from the `skgrf` package.


As of now, there are some notable differences between `grf` and `econml`. 

:::{.callout-tip}

## Some differences between the `grf` and `econml` implementation of causal forest

+ While `grf` support clustering, `econml` does not.

+ It is easy to modify the first stage estimations in `econml`. `grf` uses random forest by default for them. If you would like to try other ML methods, then you need to write a code to predict $Y - E[Y|X, W]$ and $T - E[T|X, W]$ with your choice of ML methods yourself (see @sec-mlr3-in-action for an example code to do this).

+ `grf` does not cross-fit in the first-stage estimation. Rather, it uses out-of-bag prediction from the trained random forest models, which avoids over-fitting (Note that the motivation behind cross-fitting in DML is to avoid over-fitting bias in the second stage estimation. See @sec-dml).

+ `grf` does not make distinctions between $X$ and $W$.

:::


```{r}
#=== load the Treatment dataset ===#
data("Treatment", package = "Ecdat")

#=== convert to a data.table ===#
(
data <- 
  data.table(Treatment) %>% 
  #=== create an id variable ===#
  .[, id := 1:.N]
)
```

Here are the variables in this dataset that we use.

+ `re78` ($Y$): real annual earnings in 1978 (after the treatment)
+ `treat` ($T$): `TRUE` if a person had gone through a training, `FALSE` otherwise. 

$X$ includes

+ `re74`: real annual earnings in 1978 (after the treatment)
+ `age`: age
+ `educ`: education in years
+ `ethn`: one of "other", "black", "hispanic"
+ `married`: married or not

`grf::causal_forest()` takes only numeric values for $X$. So, we will one-hot encode `ethn`, which is a factor variable at the moment.

```{r}
(
data_trt <- mltools::one_hot(data)
)
```

We now have `ethn_black`, `ethn_hispanic`, and `ethn_other` from `ethn`. The model we are estimating is as follows:

$$
\begin{aligned}
re78 & = \theta(age, re74, educ, ethn\_hipanic, ethn\_black, married)\cdot treat + g(age, re74, educ, ethn\_hipanic, ethn\_black, married) + \varepsilon \\
treat & = f(age, re74, educ, ethn\_hipanic, ethn\_black, married) + \eta
\end{aligned}
$$

:::{.callout-warning}
Note that we are paying no attention to the potential endogeneity problem here. This is just a demonstration and the results are likely to be biased. We will look at instrumental forest later in @sec-cf-extension, which is consistent under confoundedness as long as you can find appropriate external instruments.
:::

In running `causal_forest()`, there are many hyper-parameters and options that we need to be aware of. 

Since CF is a GRF and GRF uses random forest algorithm on appropriate pseudo outcome, it is natural that some of the CF hyper-parameters are the same as the ones for RF (by `ranger()`).

+ `num.trees`: number of trees
+ `mtry`: number of variables tried in each split (default is $\sqrt{K}$)
+ `min.node.size`: minimum number of observations in each leaf (default is 5)

A hyper-parameter that certainly affects tree building process is `sample.fraction`, which we saw earlier. 

+ `sample.fraction`: fraction of the data used to build each tree (default is 0.5)

A higher value of `sample.fraction` means that the trees are more correlated as they share more of the same observations. 

There are three honesty-related options. 

+ `honesty`: `TRUE` if using honest-sampling, 0 otherwise (default is `TRUE`)
+ `honesty.fraction`: fraction of the data (after `sample.fraction` is applied) that is grouped into $J_1$, which is used to determine splitting rules
+ `honesty.prune.leaves`: `TRUE` if the leaves with no samples are pruned (default is `TRUE`)

Let's now train CF using `data_trt`. 

```{r}
cf_trained <-
  grf::causal_forest(
    X = data_trt[, .(age, re74, educ, ethn_hispanic, ethn_black, married)] %>% as.matrix(),
    Y = data_trt[, re78],
    W = data_trt[, treat]
  )
```

Here is what `cf_trained` has as its attributes.

```{r}
names(cf_trained)
```

For example, you can get $\theta(X_i)$ by accessing the `predictions` attribute.

```{r}
cf_trained$predictions %>% head()
```

### Predict and interpret CATE

Before looking at how to predict $\theta(X)$, let's look at which variables are used to split tree nodes. You can get such information using `split_frequencies()` on a trained causal forest.

```{r}
split_frequencies(cf_trained)
```

```{r}
#| include: false
split_data <- split_frequencies(cf_trained)
```

In this table, rows represent the depth of the nodes and columns represent covariates. For example, the second variable (`educ`) was used to do the first split `r split_data[1, 2]` times and split a node at the second depth `r split_data[2, 2]` times. A variable with higher numbers of splits at earlier stages is more influential in driving treatment effect heterogeneity. `variable_importance()` returns a measure of how important each variable is in explaining treatment effect heterogeneity based on the split information.

```{r}
variable_importance(cf_trained)
```

So, according to this measure, the second variable (`educ`) is the most important variable. While variable of importance measure is informative, it does not tell us <span style="color:blue">how </span> the variables are affecting treatment effects. For that, we need to look at $\theta(X)$ at different values of $X$.

You can use `predict()` to predict the treatment effect at $X$. For example, consider the following evaluation point.

```{r}
X_0 <- 
  data.table(
    age = 30,
    re74 = 40000, 
    educ = 10,
    ethn_hispanic = 0,
    ethn_black = 1,
    married = TRUE
  )
```

Note that the order of columns of the evaluation data must be the same as that of the `X` in `causal_forest()` when you trained a CF model. You can set `estimate.variance` to `TRUE` to get $var(\hat{\theta}(X))$ along with the point estimate.

```{r}
predict(
  cf_trained, 
  newdata = X_0, 
  estimate.variance = TRUE
)
```

Unlike linear-in-parameter model, there are no coefficients that can immediately tell us how influential each of $X$ is in driving the treatment effect heterogeneity. One way to see the impact of a variable is to change its value while the value of the rest of $X$ is fixed. For example, for the given observed value of $X$ except `educ`, you can vary the value of `educ` to see how `educ` affects the treatment effect. We can do this for all the observations and then can get a good picture of how the treatment effect varies across individuals at different values of `educ`. 

Let's first create a sequence of `educ` values at which $\hat{\theta}$ is predicted. 

```{r}
age_seq <-
  data.table(
    age = data_trt[, seq(min(age), max(age), length = 30)]
  )
```

We then create a dataset where every single individual (observation) in the original data `data_trt` to have all the age values in `age_seq` while the value of the rest of $X$ fixed at their own values.

::: {.column-margin}
Confirm what `reshape::expand.grid.df` does with this simple example.

```{r}
reshape::expand.grid.df(
  data.table(a = c(1, 2, 3)), # first data set
  data.table(b = c(1, 2), c = c("a", "b")) # second data set
)
```
:::

```{r}
data_te <- 
  reshape::expand.grid.df(
    age_seq, 
    data_trt[, .(re74, educ, ethn_hispanic, ethn_black, married, id)]
  ) %>% 
  data.table()
```

Let's now predict $\hat{\theta}$ with their standard error estimates.

```{r}
(
theta_hat_with_se <- 
  predict(cf_trained, newdata = dplyr::select(data_te, -id), estimate.variance = TRUE) %>% 
  data.table() %>% 
  .[, se := sqrt(variance.estimates)] %>% 
  setnames("predictions", "theta_hat") %>% 
  .[, .(theta_hat, se)]
)

```

@fig-ind-3 shows the impact of `age` on treatment effect for the first three individuals of `data_trt`. For example, if an individual that has the identical values for $X$ except `age` and also this person is 40 years old, then the treatment effect of the training program would be about $1,000. Standard errors are fairly large and treatment effects are not statistically significantly different from 0 at any value of `age` for all three individuals. The impact of `age` seems to be very similar for all the individuals. However, you can see shifts in $\hat{\theta}$ among them. Those shifts are due to the differences in other covariates. 

```{r}
#| fig-cap: Estimated treatment effect for the first three individuals at different values of the age variable
#| label: fig-ind-3
plot_data <- cbind(data_te, theta_hat_with_se)

ggplot(plot_data[id %in% 1:3, ]) +
  geom_line(aes(y = theta_hat, x = age)) +
  geom_ribbon(
    aes(
      ymin = theta_hat - 1.96 * se, 
      ymax = theta_hat + 1.96 * se, 
      x = age
    ),
    fill = "blue",
    alpha = 0.4
  ) +
  facet_grid(. ~ id) +
  theme_bw()

```

@fig-all-box shows the box-plot of treatment effects for all the individuals. Note that variations observed at each `age` value is due to heterogeneity in treatment effect driven by covariates other than `age`. It looks like the three individuals looked at are exceptions. For the majority of individuals, the estimated treatment effects are negative at any value of `age`.

```{r}
#| fig-cap: Box-plot of the estimated treatment effects from all the individuals at different values of the age variable 
#| label: fig-all-box

ggplot(plot_data) +
  geom_boxplot(aes(y = theta_hat, x = factor(round(age, digits = 2)))) +
  theme_bw() +
  xlab("Age") +
  ylab("Estimated treatment effect") +
  theme_bw() +
  theme(
    axis.text.x = element_text(angle = 90)
  )

```

You can easily repeat this analysis for other covariates to see their impacts as well.

### Hyper-parameter tuning {#sec-cf-tuning}


## Orthogonal Random Forest



## References {.unnumbered}

<div id="refs"></div>












