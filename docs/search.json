[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Machine Learning for Economists",
    "section": "",
    "text": "This book will provide an introduction to machine learning methods. The main target audience is economists (and possibly other scientific fields that value causal identification). This was originally written for my students to enhance their productivity and their collaborative work with me. So, this book will by no means cover all the things that you would like to know. Rather, it covers a small subset of the vast world of machine learning methods that benefits my students and me. If you are still interested in reading the book. Suit yourself."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Introduction to Machine Learning for Economists",
    "section": "License",
    "text": "License\nThis website is (and will always be) free to use, and is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 4.0 License."
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "This book will teach you how to program in R. You’ll go from loading data to writing your own functions (which will outperform the functions of other R users). But this is not a typical introduction to R. I want to help you become a data scientist, as well as a computer scientist, so this book will focus on the programming skills that are most related to data science.\nThe chapters in the book are arranged according to three practical projects–given that they’re fairly substantial projects, they span multiple chapters. I chose these projects for two reasons. First, they cover the breadth of the R language. You will learn how to load data, assemble and disassemble data objects, navigate R’s environment system, write your own functions, and use all of R’s programming tools, such as if else statements, for loops, S3 classes, R’s package system, and R’s debugging tools. The projects will also teach you how to write vectorized R code, a style of lightning-fast code that takes advantage of all of the things R does best.\nBut, more importantly, the projects will teach you how to solve the logistical problems of data science—and there are many logistical problems. When you work with data, you will need to store, retrieve, and manipulate large sets of values without introducing errors. As you work through the book, I will teach you not just how to program with R, but how to use the programming skills to support your work as a data scientist.\nNot every programmer needs to be a data scientist, so not every programmer will find this book useful. You will find this book helpful if you’re in one of the following categories:\nOne of the biggest surprises in this book is that I do not cover traditional applications of R, such as models and graphs; instead, I treat R purely as a programming language. Why this narrow focus? R is designed to be a tool that helps scientists analyze data. It has many excellent functions that make plots and fit models to data. As a result, many statisticians learn to use R as if it were a piece of software—they learn which functions do what they want, and they ignore the rest.\nThis is an understandable approach to learning R. Visualizing and modeling data are complicated skills that require a scientist’s full attention. It takes expertise, judgement, and focus to extract reliable insights from a data set. I would not recommend that any data scientist distract herself with computer programming until she feels comfortable with the basic theory and practice of her craft. If you would like to learn the craft of data science, I recommend the book R for Data Science, my companion volume to this book, co-written with Hadley Wickham.\nHowever, learning to program should be on every data scientist’s to-do list. Knowing how to program will make you a more flexible analyst and augment your mastery of data science in every way. My favorite metaphor for describing this was introduced by Greg Snow on the R help mailing list in May 2006. Using functions in R is like riding a bus. Writing functions in R is like driving a car.\nGreg compares R to SPSS, but he assumes that you use the full powers of R; in other words, that you learn how to program in R. If you only use functions that preexist in R, you are using R like SPSS: it is a bus that can only take you to certain places.\nThis flexibility matters to data scientists. The exact details of a method or simulation will change from problem to problem. If you cannot build a method tailored to your situation, you may find yourself tempted to make unrealistic assumptions just so you can use an ill-suited method that already exists.\nThis book will help you make the leap from bus to car. I have written it for beginning programmers. I do not talk about the theory of computer science—there are no discussions of big O() and little o() in these pages. Nor do I get into advanced details such as the workings of lazy evaluation. These things are interesting if you think of computer science at the theoretical level, but they are a distraction when you first learn to program.\nInstead, I teach you how to program in R with three concrete examples. These examples are short, easy to understand, and cover everything you need to know.\nI have taught this material many times in my job as Master Instructor at RStudio. As a teacher, I have found that students learn abstract concepts much faster when they are illustrated by concrete examples. The examples have a second advantage, as well: they provide immediate practice. Learning to program is like learning to speak another language—you progress faster when you practice. In fact, learning to program is learning to speak another language. You will get the best results if you follow along with the examples in the book and experiment whenever an idea strikes you.\nThe book is a companion to R for Data Science. In that book, Hadley Wickham and I explain how to use R to make plots, model data, and write reports. That book teaches these tasks as data-science skills, which require judgement and expertise—not as programming exercises, which they also are. This book will teach you how to program in R. It does not assume that you have mastered the data-science skills taught in R for Data Science (nor that you ever intend to). However, this skill set amplifies that one. And if you master both, you will be a powerful, computer-augmented data scientist, fit to command a high salary and influence scientific dialogue."
  },
  {
    "objectID": "preface.html#conventions-used-in-this-book",
    "href": "preface.html#conventions-used-in-this-book",
    "title": "Preface",
    "section": "Conventions Used in This Book",
    "text": "Conventions Used in This Book\nThe following typographical conventions are used in this book:\nItalic:: Indicates new terms, URLs, email addresses, filenames, and file extensions.\nConstant width:: Used for program listings, as well as within paragraphs to refer to program elements such as variable or function names, databases, data types, environment variables, statements, and keywords.\nConstant width bold:: Shows commands or other text that should be typed literally by the user.\nConstant width italic:: Shows text that should be replaced with user-supplied values or by values determined by context.\nTo comment or ask technical questions about this book, please file an issue at github.com/rstudio-education/hopr."
  },
  {
    "objectID": "preface.html#acknowledgments",
    "href": "preface.html#acknowledgments",
    "title": "Preface",
    "section": "Acknowledgments",
    "text": "Acknowledgments"
  },
  {
    "objectID": "cross-validation.html",
    "href": "cross-validation.html",
    "title": "\n2  Cross-validation based on mean squared error (MSE)\n",
    "section": "",
    "text": "Codelibrary(data.table)\nlibrary(tidyverse)\nlibrary(mgcv)\nlibrary(caret)\nlibrary(parallel)"
  },
  {
    "objectID": "cross-validation.html#r-demonstration-using-mgcvgam",
    "href": "cross-validation.html#r-demonstration-using-mgcvgam",
    "title": "\n1  Cross-validation based on mean squared error (MSE)\n",
    "section": "\n3.1 R demonstration using mgcv::gam()\n",
    "text": "3.1 R demonstration using mgcv::gam()\n\nLet’s demonstrate this using R. Here is the dataset we use.\n\nCodeset.seed(943843)\n\n# define the data generating process\n# x fixed\ngen_data <- function(x) {\n  \n  ey <- (x - 2.5)^3 # E[y|x]\n  y <- ey + 3 * rnorm(length(x)) # y = E[y|x] + u\n  return_data <- data.table(y = y, x = x, ey = ey)\n\n  return(return_data)\n}\n\n## generate train data\ndata <- gen_data(x = runif(100) * 5)\n\n\nVisually, here is the relationship between \\(E[y]\\) and \\(x\\):\n\nCodeggplot(data = data) +\n  geom_line(aes(y = ey, x = x))\n\n\n\n\nFor example, for the case where the first observation is left out for validation,\n\nCode# leave out the first observation\nleft_out_observation <- data[1, ]\n\n# all the rest\ntrain_data <- data[-1, ]\n\n\nNow we train a gam model using the train_data, predict \\(y\\) for the first observation, and find the MSE.\n\nCode#=== train the model ===#\nfitted <- gam(y ~ s(x, k = 10), sp = 0, data = train_data)\n\n#=== predict y for the first observation ===#\ny_fitted <- predict(fitted, newdata = left_out_observation)\n\n#=== get MSE ===#\nMSE <- (left_out_observation[, y] - y_fitted) ^ 2\n\n\nAs described above, LOOCV repeats this process for every single observation of the data. Now, let’s write a function that does the above process for any \\(i\\) you specify.\n\nCode#=== define the modeling approach ===#\ngam_k_10 <- function(train_data) \n{\n  gam(y ~ s(x, k = 30), sp = 0, data = train_data)\n}\n\n#=== define the process of getting MSE for ith observation ===#\nget_mse <- function(i, model)\n{\n  left_out_observation <- data[i, ]\n\n  # all the rest\n  train_data <- data[-i, ]\n\n  #=== train the model ===#\n  fitted <- model(train_data)\n\n  #=== predict y for the first observation ===#\n  y_fitted <- predict(fitted, newdata = left_out_observation)\n\n  #=== get MSE ===#\n  MSE <- (left_out_observation[, y] - y_fitted) ^ 2 \n\n  return(MSE)\n} \n\n\nFor example, this gets MSE for the 10th observation.\n\nCodeget_mse(10, gam_k_10)\n\n       1 \n1.523446 \n\n\nLet’s now loop over \\(i = 1:100\\).\n\nCodemse_indiv <-\n  lapply(\n    1:100,\n    function(x) get_mse(x, gam_k_10)\n  ) %>% \n  #=== list to a vector ===#\n  unlist() \n\n\nHere is the distribution of MSEs.\n\nCodehist(mse_indiv)\n\n\n\n\nWe now get the average MSE.\n\nCodemse_average <- mean(mse_indiv)"
  },
  {
    "objectID": "cross-validation.html#selecting-the-best-gam-specification-illustration",
    "href": "cross-validation.html#selecting-the-best-gam-specification-illustration",
    "title": "\n1  Cross-validation based on mean squared error (MSE)\n",
    "section": "\n3.2 Selecting the best GAM specification: Illustration",
    "text": "3.2 Selecting the best GAM specification: Illustration\nNow, let’s try to find the best (among the ones we try) GAM specification using LOOCV. We will try ten different GAM specifications which vary in penalization parameter. Penalization parameter can be set using the sp option for mgcv::gam(). A greater value of sp leads to a more smooth fitted curve.\n\nCodespecify_gam <- function(sp) {\n  function(train_data) {\n    gam(y ~ s(x, k = 30), sp = sp, data = train_data)\n  }\n}\n\nget_mse_by_sp <- function(sp)\n{\n\n  temp_gam <- specify_gam(sp)\n\n  mse_indiv <-\n    lapply(\n      1:100,\n      function(x) get_mse(x, temp_gam)\n    ) %>% \n    #=== list to a vector ===#\n    unlist() %>% \n    mean()\n\n  return_data <-\n    data.table(\n      mse = mse_indiv,\n      sp = sp\n    )\n\n  return(return_data)\n}\n\n\nFor example, the following code gets you the average MSE for sp \\(= 3\\).\n\nCodeget_mse_by_sp(3)\n\n        mse sp\n1: 11.56747  3\n\n\nNow, let’s loop over ten values of sp: 0, 0.2, 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2.\n\nCode(\nmse_data <- \n  lapply(\n    seq(0, 2, by = 0.2),\n    function(x) get_mse_by_sp(x)\n  ) %>% \n  rbindlist()\n)\n\n          mse  sp\n 1: 12.460156 0.0\n 2:  9.909992 0.2\n 3:  9.957858 0.4\n 4: 10.049327 0.6\n 5: 10.164749 0.8\n 6: 10.293142 1.0\n 7: 10.427948 1.2\n 8: 10.565081 1.4\n 9: 10.701933 1.6\n10: 10.836829 1.8\n11: 10.968701 2.0\n\n\n\n\n\nSo, according to the LOOCV, we should pick sp \\(= 0.2\\) as the penalty parameter.\nNow, that we know sp \\(= 0.2\\) produces the lowest LOOCV MSE, we rerun gam() using the entire dataset (not leaving out any of the observations) and make it our final trained model.\n\nCodefinal_gam_spec <- specify_gam(sp = 1)\n\nfit_gam <- final_gam_spec(train_data)\n\n\nHere is what the fitted curve looks like:\n\nCodeplot(fit_gam)\n\n\n\n\nLooks good. By the way, here are the fitted curves for some other sp values.\n\nCodefitted_curves <- \n  lapply(\n    c(0, 0.6, 1, 2),\n    function(x) {\n      temp_gam <- specify_gam(sp = x)\n      fit_gam <- temp_gam(train_data)   \n    }\n  )  \n\nfor (plot in fitted_curves) {\n  plot(plot)\n}\n\n\n\n\n\n(a) k = 0\n\n\n\n\n\n\n(b) k = 0.6\n\n\n\n\n\n\n\n\n(c) k = 1\n\n\n\n\n\n\n(d) k = 2\n\n\n\n\nFigure 3.1: Fitted curves at various penalization parameters\n\n\n\nLOOCV is perfectly general and can be applied to any statistical methods. However, it can be extremely computationally burdensome because you need to fit the same model for as many as the number of observations. So, if you have 10,000 observations, then you need to fit the model 10,000 times, which can take a long long time."
  },
  {
    "objectID": "cross-validation.html#summary",
    "href": "cross-validation.html#summary",
    "title": "\n1  Cross-validation based on mean squared error (MSE)\n",
    "section": "\n3.3 Summary",
    "text": "3.3 Summary\n\n\n\n\n\n\nNote\n\n\n\nLOOCV is perfectly general and can be applied to any statistical methods.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nLOOCV can be highly computation-intensive when the dataset is large"
  },
  {
    "objectID": "cross-validation.html#selecting-the-best-gam-specification-illustration-1",
    "href": "cross-validation.html#selecting-the-best-gam-specification-illustration-1",
    "title": "\n1  Cross-validation based on mean squared error (MSE)\n",
    "section": "\n4.1 Selecting the best GAM specification: Illustration",
    "text": "4.1 Selecting the best GAM specification: Illustration\nJust like we found the best gam specification (choice of penalization parameter), we do the same now using KCV.\n\nCodeget_mse_by_sp_kcv <- function(sp)\n{\n\n  temp_gam <- specify_gam(sp)\n\n  mse_by_k <-\n    lapply(\n      seq_len(length(folds)),\n      function(x) get_mse_by_fold(train_data, x, temp_gam)\n    ) %>% \n    rbindlist()\n\n  return_data <-\n    mse_by_k %>% \n    .[, sp := sp]\n\n  return(return_data[])\n}\n\n\nFor example, the following code gets you the MSE for all the folds for sp \\(= 3\\).\n\nCodeget_mse_by_sp_kcv(3)\n\nWarning in `[.data.table`(data, -folds[[fold]], ): Item 75 of i is -402 but\nthere are only 400 rows. Ignoring this and 25 more like it out of 100.\n\n\nWarning in `[.data.table`(data, -folds[[fold]], ): Item 87 of i is -404 but\nthere are only 400 rows. Ignoring this and 13 more like it out of 100.\n\n\nWarning in `[.data.table`(data, -folds[[fold]], ): Item 77 of i is -406 but\nthere are only 400 rows. Ignoring this and 23 more like it out of 100.\n\n\nWarning in `[.data.table`(data, -folds[[fold]], ): Item 82 of i is -401 but\nthere are only 400 rows. Ignoring this and 18 more like it out of 100.\n\n\nWarning in `[.data.table`(data, -folds[[fold]], ): Item 84 of i is -407 but\nthere are only 400 rows. Ignoring this and 16 more like it out of 100.\n\n\n   k mse sp\n1: 1  NA  3\n2: 2  NA  3\n3: 3  NA  3\n4: 4  NA  3\n5: 5  NA  3\n\n\nNow, let’s loop over ten values of sp: 0, 0.2, 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2.\n\nCode(\nmse_results <- \n  lapply(\n    seq(0, 2, by = 0.2),\n    function(x) get_mse_by_sp_kcv(x)\n  ) %>% \n  rbindlist()\n)\n\n    k       mse  sp\n 1: 1 10.842343 0.0\n 2: 2 10.276725 0.0\n 3: 3 10.351780 0.0\n 4: 4 10.468232 0.0\n 5: 5 11.259309 0.0\n 6: 1 10.640723 0.2\n 7: 2  9.474846 0.2\n 8: 3 10.358743 0.2\n 9: 4  9.391190 0.2\n10: 5 10.858930 0.2\n11: 1 10.777523 0.4\n12: 2  9.617963 0.4\n13: 3 10.283409 0.4\n14: 4  9.523628 0.4\n15: 5 10.908875 0.4\n16: 1 10.913666 0.6\n17: 2  9.765395 0.6\n18: 3 10.277063 0.6\n19: 4  9.659668 0.6\n20: 5 10.966567 0.6\n21: 1 11.051282 0.8\n22: 2  9.913271 0.8\n23: 3 10.308488 0.8\n24: 4  9.797650 0.8\n25: 5 11.033462 0.8\n26: 1 11.189673 1.0\n27: 2 10.060950 1.0\n28: 3 10.363166 1.0\n29: 4  9.936116 1.0\n30: 5 11.108143 1.0\n31: 1 11.327745 1.2\n32: 2 10.207548 1.2\n33: 3 10.432741 1.2\n34: 4 10.073552 1.2\n35: 5 11.188659 1.2\n36: 1 11.464489 1.4\n37: 2 10.352164 1.4\n38: 3 10.511860 1.4\n39: 4 10.208725 1.4\n40: 5 11.273202 1.4\n41: 1 11.599096 1.6\n42: 2 10.494040 1.6\n43: 3 10.596908 1.6\n44: 4 10.340731 1.6\n45: 5 11.360265 1.6\n46: 1 11.730962 1.8\n47: 2 10.632602 1.8\n48: 3 10.685372 1.8\n49: 4 10.468961 1.8\n50: 5 11.448649 1.8\n51: 1 11.859656 2.0\n52: 2 10.767445 2.0\n53: 3 10.775473 2.0\n54: 4 10.593034 2.0\n55: 5 11.537423 2.0\n    k       mse  sp\n\n\nLet’s now get the average MSE by sp:\n\nCode(\nmean_mse_data <- mse_results[, .(mean_mse = mean(mse)), by = sp]\n)\n\n     sp mean_mse\n 1: 0.0 10.63968\n 2: 0.2 10.14489\n 3: 0.4 10.22228\n 4: 0.6 10.31647\n 5: 0.8 10.42083\n 6: 1.0 10.53161\n 7: 1.2 10.64605\n 8: 1.4 10.76209\n 9: 1.6 10.87821\n10: 1.8 10.99331\n11: 2.0 11.10661\n\n\n\n\n\nSo, according to the KCV, we should pick sp \\(= 0.2\\) as the penalty parameter.\nBy the way, here is what MSE values look like for each fold based on the value of sp.\n\nCodeggplot(data = mse_results) +\n  geom_line(aes(y = mse, x = sp, color = factor(k))) +\n  scale_color_discrete(name = \"\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nEven though we compared different specification of the same approach (GAM), we can compare across different models as well. For example, you can find KCV for an RF model with a particular specifications of its hyper-parameters and compare the KCV with those of the GAM model specifications and see what comes at the top."
  },
  {
    "objectID": "lasso.html",
    "href": "lasso.html",
    "title": "\n3  Regression Shrinkage Methods: LASSO (Ridge and Elastic Net)\n",
    "section": "",
    "text": "We have talked about variance-bias trade-off. When you “shrink” coefficients towards zero, you may be able to achieve lower the variance of \\(\\hat{f}(x)\\) while increasing bias, which can result in a lower MSE.\nConsider the following generic linear model:\n\\[\ny = X\\beta + \\mu\n\\]\n\n\n\\(y\\): dependent variable\n\n\\(X\\): a collection of explanatory variables (\\(K\\) variables)\n\n\\(\\beta\\): a collection of coefficients on the explanatory variables \\(X\\)\n\n\n\\(\\mu\\): error term\n\nBorrowing from the documentation of the glmnet package(), the minimization problem shrinkage methods solve to estimate coefficients for a linear model can be written as follows:\n\\[\nMin_{\\beta} \\sum_{i=1}^N (y_i - X_i\\beta)^2 + \\lambda \\huge[\\normalsize(1-\\alpha)||\\beta||^2_2/2 + \\alpha ||\\beta||_1\\huge]\n\\tag{3.1}\\]\n\n\\(||\\beta||_1 = |\\beta_1| + |\\beta_2| + \\dots+ |\\beta_K|\\) (called L1 norm)\n\\(||\\beta||_2 = (|\\beta_1|^2 + |\\beta_2|^2 + \\dots+ |\\beta_K|^2)^{\\frac{1}{2}}\\) (called L2 norm)\n\n\\(\\lambda (> 0)\\) is the penalization parameter that governs how much coefficients shrinkage happens (more details later).\nThe shrinkage method is called LASSO when \\(\\alpha = 0\\), Ridge regression when \\(\\alpha = 1\\), and elastic net when \\(\\alpha \\in (0, 1)\\).\nRidge regression and elastic net are rarely used. So, we are going to cover only LASSO here."
  },
  {
    "objectID": "bootstrap.html",
    "href": "bootstrap.html",
    "title": "\n4  Bootstrap\n",
    "section": "",
    "text": "Bootstrap can be used to quantify the uncertainty associated with an estimator. For example, you can use it to estimate the standard error (SE) of a coefficient of a linear model. Since there are closed-form solutions for that, bootstrap is not really bringing any benefits to this case. However, the power of bootstrap comes in handy when you do NOT have a closed form solution. We will first demonstrate how bootstrap works using a linear model, and then apply it to a case where no-closed form solution is available."
  },
  {
    "objectID": "lasso.html#lasso",
    "href": "lasso.html#lasso",
    "title": "\n3  Regression Shrinkage Methods: LASSO (Ridge and Elastic Net)\n",
    "section": "\n3.2 LASSO",
    "text": "3.2 LASSO\nWhen there are many potential variables to include, it is hard to know which ones to include. LASSO can be used to select variables to build a more parsimonious model, which may help reducing MSE.\nAs mentioned above, LASSO is a special case of shrinkage methods where \\(\\alpha = 1\\) in Equation 3.1. So, the optimization problem of LASSO is\n\\[\nMin_{\\beta} \\sum_{i=1}^N (y_i - X_i\\beta)^2 + \\lambda \\sum_{k=1}^K |\\beta_k|\n\\tag{3.2}\\]\n, where \\(\\lambda\\) is the penalization parameter.\nAlternatively, we can also write the optimization problem as the constrained minimization problem as follows1:\n\\[\nMin_{\\beta} \\sum_{i=1}^N (y_i - X_i\\beta)^2 \\\\\n\\mbox{subject to } \\sum_{k=1}^K |\\beta_k| < t\n\\tag{3.3}\\]\nA graphical representation of the minimization problem is highly illustrative on what LASSO does. Consider the following data generating process:\n\\[\ny  = 0.2 x_1 + 2 * x_2 + \\mu\n\\]\nUsing LASSO, we are trying to estimate the coefficient on \\(x_1\\) and \\(x_2\\) by solving the following problem where \\(t\\) is set to 1 in Equation 3.3:\n\\[\nMin_{\\beta} \\sum_{i=1}^N (y_i - \\beta_1 x_1 - \\beta_2 x_2)^2 \\\\\n\\mbox{subject to } \\sum_{k=1}^K |\\beta_k| < \\textcolor{red}{1}\n\\]\nThis means that, we need to look for the combinations of \\(\\beta_1\\) and \\(\\beta_2\\) such that the sum of their absolute values is less than 1. Graphically, here is what the constraint looks like:\n\nCodeggplot() +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  geom_segment(aes(x = 1, y = 0, xend = 0, yend = 1), color = \"red\", size = 1.2) +\n  geom_segment(aes(x = 1, y = 0, xend = 0, yend = -1), color = \"red\", size = 1.2) +\n  geom_segment(aes(x = -1, y = 0, xend = 0, yend = 1), color = \"red\", size = 1.2) +\n  geom_segment(aes(x = -1, y = 0, xend = 0, yend = -1), color = \"red\", size = 1.2) +\n  coord_equal() +\n  xlab(\"beta_1\") +\n  ylab(\"beta_2\")\n\n\n\n\nNow, let’s calculate what value the objective function takes at different values of \\(\\beta_1\\) and \\(\\beta_2\\).\nWe first generate data.\n\nN <- 1000 # number of observations\nx_1 <- rnorm(N)\nx_2 <- rnorm(N)\nmu <- rnorm(N) # error term\ny <- 2 * x_1 + 0.2 * x_2 + mu\n\ndata <-\n  data.table(\n    y = y,\n    x_1 = x_1,\n    x_2 = x_2\n  )\n\nWithout the constraint, here is the combination of \\(\\beta_1\\) and \\(\\beta_2\\) that minimizes the objective function of Equation 3.3, which is the same as OLS estimates.\n\n(\nols_coefs <- lm(y ~ x_1 + x_2, data = data)$coefficient\n)\n\n(Intercept)         x_1         x_2 \n 0.01489275  1.97818829  0.20761196 \n\n\nWe now calculate the value of the objective functions at different values of \\(\\beta_1\\) and \\(\\beta_2\\). Here is the set of \\(\\{\\beta_1, \\beta_2\\}\\) combinations we look at.\n\n(\nbeta_table <- \n  data.table::CJ(\n    beta_1 = seq(-2, 2, length = 50),\n    beta_2 = seq(-1, 1, length = 50) \n  )\n)\n\n      beta_1     beta_2\n   1:     -2 -1.0000000\n   2:     -2 -0.9591837\n   3:     -2 -0.9183673\n   4:     -2 -0.8775510\n   5:     -2 -0.8367347\n  ---                  \n2496:      2  0.8367347\n2497:      2  0.8775510\n2498:      2  0.9183673\n2499:      2  0.9591837\n2500:      2  1.0000000\n\n\n\n\n\n\n\n\nNote\n\n\n\ndata.table::CJ() takes more than one set of vectors and find the complete combinations the values of the vectors. Trying\n\ndata.table::CJ(x1 = c(1, 2, 3), x2 = c(4, 5, 6))\n\nwill help you understand exactly what it does.\n\n\nLoop over the row numbers of beta_table to find SSE for all the rows (all the combinations of \\(\\beta_1\\) and \\(\\beta_2\\)).\n\n#=== define the function to get SSE ===#\nget_sse <- function(i, data)\n{\n  #=== extract beta_1 and beta_2 for ith observation  ===#\n  betas <- beta_table[i, ]\n\n  #=== calculate SSE ===#\n  sse <-\n    copy(data) %>% \n    .[, y_hat := x_1 * betas[, beta_1] + x_2 * betas[, beta_2]] %>% \n    .[, se := (y - y_hat)^2] %>% \n    .[, sum(se)]\n\n  return(sse)\n}\n\n#=== calculate SSE for each row of beta_table ===#\nsse_all <-\n  lapply(\n    1:nrow(beta_table),\n    function(x) get_sse(x, data)\n  ) %>% \n  unlist()\n\n#=== assign the calculated sse values as a variable ===#\n(\nbeta_table[, sse := sse_all]\n)\n\nHere is the contour map of SSE as a function of \\(\\beta_1\\) and \\(\\beta_2\\). The solution to the unconstrained problem (OLS estimates) is represented by the red point. Since LASSO needs to find a point within the red square, the solution would be \\(\\beta_1 = 1\\) and \\(\\beta_2 = 0\\) (yellow point). LASSO did not give anything to \\(\\beta_2\\) as \\(x_1\\) is a much bigger contributor of the two included variables. LASSO tends to give the coefficient of \\(0\\) to some of the variables when the constraint is harsh, effectively eliminating them from the model. For this reason, LASSO is often used as a variable selection method.\n\nCodeggplot() +\n  stat_contour(\n    data = beta_table, \n    aes(x = beta_1, y = beta_2, z = sse, color = ..level..),\n    size = 1.2,\n    breaks = \n      round(\n        quantile(beta_table$sse, seq(0, 1, 0.05)),\n        0\n      )\n    ) +\n  scale_color_viridis_c(name = \"SSE\") +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  geom_segment(aes(x = 1, y = 0, xend = 0, yend = 1), color = \"red\", size = 1.2) +\n  geom_segment(aes(x = 1, y = 0, xend = 0, yend = -1), color = \"red\", size = 1.2) +\n  geom_segment(aes(x = -1, y = 0, xend = 0, yend = 1), color = \"red\", size = 1.2) +\n  geom_segment(aes(x = -1, y = 0, xend = 0, yend = -1), color = \"red\", size = 1.2) +\n  #=== OLS point estimates (solutions without the constraint) ===#\n  geom_point(\n    aes(x = ols_coefs[\"x_1\"], y = ols_coefs[\"x_2\"]),\n    color = \"red\",\n    size = 3\n  ) +\n  geom_point(\n    aes(x = 1, y = 0),\n    color = \"yellow\",\n    size = 3\n  ) +\n  coord_equal()\n\n\n\n\nLet’s consider a different data generating process: \\(y = x_1 + x_2 + \\mu\\). Here, \\(x_1\\) and \\(x_2\\) are equally important unlike the previous case. Here is what happens:\n\nCodeN <- 1000 # number of observations\nx_1 <- rnorm(N)\nx_2 <- rnorm(N)\nmu <- rnorm(N) # error term\ny <- x_1 + x_2 + mu\n\ndata <-\n  data.table(\n    y = y,\n    x_1 = x_1,\n    x_2 = x_2\n  )\n\nols_coefs <- lm(y ~ x_1 + x_2, data = data)$coefficient\n\n#=== calculate sse for each row of beta_table ===#\nsse_all <-\n  lapply(\n    1:nrow(beta_table),\n    function(x) {\n      betas <- beta_table[x, ]\n      sse <-\n        copy(data) %>% \n        .[, y_hat := x_1 * betas[, beta_1] + x_2 * betas[, beta_2]] %>% \n        .[, se := (y - y_hat)^2] %>% \n        .[, sum(se)]\n      return(sse)\n    }\n  ) %>% \n  unlist()\n\n#=== assign the calculated sse values as a variable ===#\nbeta_table[, sse := sse_all]\n\n#=== visualize ===#\nggplot() +\n  stat_contour(\n    data = beta_table, \n    aes(x = beta_1, y = beta_2, z = sse, color = ..level..),\n    size = 1.2,\n    breaks = \n      round(\n        quantile(beta_table$sse, seq(0, 1, 0.05)),\n        0\n      )\n    ) +\n  scale_color_viridis_c(name = \"SSE\") +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  geom_segment(aes(x = 1, y = 0, xend = 0, yend = 1), color = \"red\", size = 1.2) +\n  geom_segment(aes(x = 1, y = 0, xend = 0, yend = -1), color = \"red\", size = 1.2) +\n  geom_segment(aes(x = -1, y = 0, xend = 0, yend = 1), color = \"red\", size = 1.2) +\n  geom_segment(aes(x = -1, y = 0, xend = 0, yend = -1), color = \"red\", size = 1.2) +\n  #=== OLS point estimates (solutions without the constraint) ===#\n  geom_point(\n    aes(x = ols_coefs[\"x_1\"], y = ols_coefs[\"x_2\"]),\n    color = \"red\",\n    size = 3\n  ) +\n  geom_point(\n    aes(x = 0.5, y = 0.5),\n    color = \"yellow\",\n    size = 3\n  ) +\n  coord_equal()\n\n\n\n\nIn this case, the solution would be (very close to) \\(\\{\\beta_1 = 0.5, \\beta_2 = 0.5\\}\\), with neither of them sent to zero. This is because \\(x_1\\) and \\(x_2\\) are equally important in explaining \\(y\\)."
  },
  {
    "objectID": "lasso.html#lasso-implementation-r-and-python",
    "href": "lasso.html#lasso-implementation-r-and-python",
    "title": "\n2  Regression Shrinkage Methods: Ridge, LASSO, Elastic Net\n",
    "section": "\n2.3 LASSO implementation (R and Python)",
    "text": "2.3 LASSO implementation (R and Python)\nYou can use the glmnet() from the glmnet package run LASSO. For demonstration, we use the QuickStartExample data.\n\n#=== get the data ===#\ndata(QuickStartExample)\n\n#=== see the structure ===#\nstr(QuickStartExample)\n\nList of 2\n $ x: num [1:100, 1:20] 0.274 2.245 -0.125 -0.544 -1.459 ...\n $ y: num [1:100, 1] -1.275 1.843 0.459 0.564 1.873 ...\n\n\nAs you can see, QuickStartExample is a list of two elements. First one (x) is a matrix of dimension 100 by 20, which is the data of explanatory variables. Second one (y) is a matrix of dimension 100 by 1, which is the data for the dependent variable.\n\n\n\n\n\n\nNote\n\n\n\nIf you are used to running regressions in R, you should have specified a model using formula (e.g., y ~ x). However, most of the machine learning functions in R accept the dependent variable and explanatory variables in a matrix form (or data.frame). This is almost always the case for ML methods in Python as well.\n\n\nBy default, alpha parameter for glmnet() (\\(\\alpha\\) in Equation 2.1) is set to 1. So, to run LASSO, you can simply do the following:\n\n#=== extract X and y ===#\nX <- QuickStartExample$x\ny <- QuickStartExample$y\n\n#=== run LASSO ===#\nlasso <- glmnet(X, y)\n\nBy looking at the output below, you can see that glmnet() tried many different values of \\(\\lambda\\).\n\nlasso\n\n\nCall:  glmnet(x = X, y = y) \n\n   Df  %Dev  Lambda\n1   0  0.00 1.63100\n2   2  5.53 1.48600\n3   2 14.59 1.35400\n4   2 22.11 1.23400\n5   2 28.36 1.12400\n6   2 33.54 1.02400\n7   4 39.04 0.93320\n8   5 45.60 0.85030\n9   5 51.54 0.77470\n10  6 57.35 0.70590\n11  6 62.55 0.64320\n12  6 66.87 0.58610\n13  6 70.46 0.53400\n14  6 73.44 0.48660\n15  7 76.21 0.44330\n16  7 78.57 0.40400\n17  7 80.53 0.36810\n18  7 82.15 0.33540\n19  7 83.50 0.30560\n20  7 84.62 0.27840\n21  7 85.55 0.25370\n22  7 86.33 0.23120\n23  8 87.06 0.21060\n24  8 87.69 0.19190\n25  8 88.21 0.17490\n26  8 88.65 0.15930\n27  8 89.01 0.14520\n28  8 89.31 0.13230\n29  8 89.56 0.12050\n30  8 89.76 0.10980\n31  9 89.94 0.10010\n32  9 90.10 0.09117\n33  9 90.23 0.08307\n34  9 90.34 0.07569\n35 10 90.43 0.06897\n36 11 90.53 0.06284\n37 11 90.62 0.05726\n38 12 90.70 0.05217\n39 15 90.78 0.04754\n40 16 90.86 0.04331\n41 16 90.93 0.03947\n42 16 90.98 0.03596\n43 17 91.03 0.03277\n44 17 91.07 0.02985\n45 18 91.11 0.02720\n46 18 91.14 0.02479\n47 19 91.17 0.02258\n48 19 91.20 0.02058\n49 19 91.22 0.01875\n50 19 91.24 0.01708\n51 19 91.25 0.01557\n52 19 91.26 0.01418\n53 19 91.27 0.01292\n54 19 91.28 0.01178\n55 19 91.29 0.01073\n56 19 91.29 0.00978\n57 19 91.30 0.00891\n58 19 91.30 0.00812\n59 19 91.31 0.00739\n60 19 91.31 0.00674\n61 19 91.31 0.00614\n62 20 91.31 0.00559\n63 20 91.31 0.00510\n64 20 91.31 0.00464\n65 20 91.32 0.00423\n66 20 91.32 0.00386\n67 20 91.32 0.00351\n\n\nYou can access the coefficients for each value of \\(lambdata\\) by applying coef() method to lasso.\n\n#=== get coefficient estimates ===#\ncoef_lasso <- coef(lasso)\n\n#=== check the dimension ===#\ndim(coef_lasso)\n\n[1] 21 67\n\n#=== take a look at the first and last three ===#\ncoef_lasso[, c(1:3, 65:67)]\n\n21 x 6 sparse Matrix of class \"dgCMatrix\"\n                   s0           s1         s2          s64          s65\n(Intercept) 0.6607581  0.631235043  0.5874616  0.111208836  0.111018972\nV1          .          0.139264992  0.2698292  1.378068980  1.378335220\nV2          .          .            .          0.023067319  0.023240539\nV3          .          .            .          0.762792114  0.763209604\nV4          .          .            .          0.059619334  0.060253956\nV5          .          .            .         -0.901460720 -0.901862151\nV6          .          .            .          0.613661389  0.614081490\nV7          .          .            .          0.117323876  0.117960550\nV8          .          .            .          0.396890604  0.397260052\nV9          .          .            .         -0.030538991 -0.031073136\nV10         .          .            .          0.127412702  0.128222375\nV11         .          .            .          0.246801359  0.247227761\nV12         .          .            .         -0.063941712 -0.064471794\nV13         .          .            .         -0.045935249 -0.046242852\nV14         .         -0.005878595 -0.1299063 -1.158552963 -1.159038292\nV15         .          .            .         -0.137103471 -0.138012175\nV16         .          .            .         -0.045085698 -0.045661882\nV17         .          .            .         -0.047272446 -0.048039238\nV18         .          .            .          0.051702567  0.052180547\nV19         .          .            .         -0.001791685 -0.002203174\nV20         .          .            .         -1.144262012 -1.144641845\n                     s66\n(Intercept)  0.110845721\nV1           1.378578220\nV2           0.023398270\nV3           0.763589908\nV4           0.060832496\nV5          -0.902227796\nV6           0.614464085\nV7           0.118540773\nV8           0.397596878\nV9          -0.031560145\nV10          0.128960349\nV11          0.247615990\nV12         -0.064955124\nV13         -0.046522983\nV14         -1.159480668\nV15         -0.138840304\nV16         -0.046186890\nV17         -0.048737920\nV18          0.052615915\nV19         -0.002578088\nV20         -1.144987654\n\n\nApplying plot() method gets you how the coefficient estimates change as the value of \\(\\lambda\\) changes:\n\nplot(lasso)\n\n\n\n\nA high L1 Norm is associated with a “lower” value of \\(\\lambda\\) (weaker shrinkage). You can see that as \\(\\lamda\\) increases (L1 Norm decreases), coefficients on more and more variables are set to 0.\nNow, the obvious question is which \\(\\lambda\\) should we pick? One way to select a \\(\\lambda\\) is K-fold cross-validation (KCV), which we covered in section. We can implement KCV using the cv.glmnet() function. You can set the number of folds using the nfolds option (the default is 10). Here, let’s 5-fold CV.\n\ncv_lasso <- cv.glmnet(X, y, nfolds = 5)\n\nThe results of KCV can be readily visualized by applying the plot() method:\n\nplot(cv_lasso)\n\n\n\n\nThere are two vertical dotted lines. The left one indicates the value of \\(\\lambda\\) where CV MSE is minimized (called lambda.min). The right one indicates the  highest  value of \\(\\lambda\\) such that the CV error is within one standard error of the minimum (called lambda.1se).\nYou can access the MSE-minimizing \\(\\lambda\\) as follows:\n\ncv_lasso$lambda.min\n\n[1] 0.07569327\n\n\nYou can access the coefficient estimates when \\(\\lambda\\) is lambda.min as follows\n\ncoef(cv_lasso, s = \"lambda.min\")\n\n21 x 1 sparse Matrix of class \"dgCMatrix\"\n                     s1\n(Intercept)  0.14867414\nV1           1.33377821\nV2           .         \nV3           0.69787701\nV4           .         \nV5          -0.83726751\nV6           0.54334327\nV7           0.02668633\nV8           0.33741131\nV9           .         \nV10          .         \nV11          0.17105029\nV12          .         \nV13          .         \nV14         -1.07552680\nV15          .         \nV16          .         \nV17          .         \nV18          .         \nV19          .         \nV20         -1.05278699\n\n\nThe following code gives you the coefficient estimates when \\(\\lambda\\) is lambda.1se\n\ncoef(cv_lasso, s = \"lambda.1se\")\n\n21 x 1 sparse Matrix of class \"dgCMatrix\"\n                     s1\n(Intercept)  0.15721186\nV1           1.27705027\nV2           .         \nV3           0.59821036\nV4           .         \nV5          -0.75164619\nV6           0.45947857\nV7           .         \nV8           0.26088201\nV9           .         \nV10          .         \nV11          0.05783249\nV12          .         \nV13          .         \nV14         -1.01440902\nV15          .         \nV16          .         \nV17          .         \nV18          .         \nV19          .         \nV20         -0.92227009\n\n\n\n\n\n\n\n\nNote\n\n\n\nglmnet() can be used to much broader class of models (e.g., Logistic regression, Poisson regression, Cox regression, etc). As the name suggests it’s elastic  net  methods for  generalized  linear  model."
  },
  {
    "objectID": "lasso.html#scaling",
    "href": "lasso.html#scaling",
    "title": "\n3  Regression Shrinkage Methods: LASSO (Ridge and Elastic Net)\n",
    "section": "\n3.5 Scaling",
    "text": "3.5 Scaling\nUnlike linear model estimation without shrinkage (regularization), shrinkage method is sensitive to the scaling of independent variables. Scaling of a variable has basically no consequence in linear model without regularization. It simply changes the interpretation of the scaled variable and the coefficient estimates on all the other variables remain unaffected. However, scaling of a single variable has a ripple effect to the other variables in shrinkage methods. This is because the penalization term: \\(\\lambda \\huge[\\normalsize(1-\\alpha)||\\beta||^2_2/2 + \\alpha ||\\beta||_1\\huge]\\). As you can see, \\(\\lambda\\) is applied universally to all the coefficients without any consideration of the scale of the variables.\nLet’s scale the first variable in X (this variable is influential as it survived even when \\(\\lambda\\) is very low) by 1/1000 and see what happens. Now, by default, the standardize option is set to TRUE. So, we need to set it to FALSE explicitly to see the effect.\nHere is before scaling:\n\ncv.glmnet(X, y, nfolds = 5, standardize = FALSE) %>% \n  coef(s = \"lambda.min\")\n\n21 x 1 sparse Matrix of class \"dgCMatrix\"\n                     s1\n(Intercept)  0.14894850\nV1           1.33450311\nV2           .         \nV3           0.68637683\nV4           .         \nV5          -0.81775106\nV6           0.53834467\nV7           0.01605235\nV8           0.33462300\nV9           .         \nV10          .         \nV11          0.15558443\nV12          .         \nV13          .         \nV14         -1.07590955\nV15          .         \nV16          .         \nV17          .         \nV18          .         \nV19          .         \nV20         -1.02266841\n\n\nHere is after scaling:\n\n#=== scale the first variable ===#\nX_scaled <- X\nX_scaled[, 1] <- X_scaled[, 1] / 1000\n\ncv.glmnet(X_scaled, y, nfolds = 5, standardize = FALSE) %>% \n  coef(s = \"lambda.min\")\n\n21 x 1 sparse Matrix of class \"dgCMatrix\"\n                      s1\n(Intercept)  0.490904543\nV1           .          \nV2           .          \nV3           0.598387687\nV4          -0.007149295\nV5          -0.784860502\nV6           0.735536678\nV7           0.047407166\nV8           0.093717145\nV9           .          \nV10          .          \nV11          0.481905661\nV12          .          \nV13          .          \nV14         -1.027583967\nV15          .          \nV16          .          \nV17          .          \nV18          0.109041875\nV19          .          \nV20         -1.302806861\n\n\nAs you can see, the coefficient on the first variable is 0 after scaling. Setting standardize = TRUE (or not doing anything with this option) gives you very similar results whether the data is scaled or not.\n\n#=== not scaled ===#\ncv.glmnet(X, y, nfolds = 5, standardize = TRUE) %>% \n  coef(s = \"lambda.min\")\n\n21 x 1 sparse Matrix of class \"dgCMatrix\"\n                     s1\n(Intercept)  0.14867414\nV1           1.33377821\nV2           .         \nV3           0.69787701\nV4           .         \nV5          -0.83726751\nV6           0.54334327\nV7           0.02668633\nV8           0.33741131\nV9           .         \nV10          .         \nV11          0.17105029\nV12          .         \nV13          .         \nV14         -1.07552680\nV15          .         \nV16          .         \nV17          .         \nV18          .         \nV19          .         \nV20         -1.05278699\n\n#=== scaled ===#\ncv.glmnet(X_scaled, y, nfolds = 5, standardize = TRUE) %>% \n  coef(s = \"lambda.min\")\n\n21 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s1\n(Intercept)    0.14291659\nV1          1344.31994962\nV2             .         \nV3             0.71047211\nV4             .         \nV5            -0.85182173\nV6             0.55960366\nV7             0.04475230\nV8             0.35117414\nV9             .         \nV10            0.01985732\nV11            0.19335515\nV12            .         \nV13            .         \nV14           -1.09350480\nV15           -0.01564233\nV16            .         \nV17            .         \nV18            .         \nV19            .         \nV20           -1.07874251\n\n\nWhile you do not have to worry about scaling issues as long as uinsg glmnet(), this is something worth remembering."
  },
  {
    "objectID": "lasso.html#lasso-implementation-r",
    "href": "lasso.html#lasso-implementation-r",
    "title": "\n3  Regression Shrinkage Methods: LASSO (Ridge and Elastic Net)\n",
    "section": "\n3.3 LASSO implementation: R",
    "text": "3.3 LASSO implementation: R\nYou can use the glmnet() from the glmnet package run LASSO. For demonstration, we use the QuickStartExample data.\n\n#=== get the data ===#\ndata(QuickStartExample)\n\n#=== see the structure ===#\nstr(QuickStartExample)\n\nList of 2\n $ x: num [1:100, 1:20] 0.274 2.245 -0.125 -0.544 -1.459 ...\n $ y: num [1:100, 1] -1.275 1.843 0.459 0.564 1.873 ...\n\n\nAs you can see, QuickStartExample is a list of two elements. First one (x) is a matrix of dimension 100 by 20, which is the data of explanatory variables. Second one (y) is a matrix of dimension 100 by 1, which is the data for the dependent variable.\n\n\n\n\n\n\nNote\n\n\n\nIf you are used to running regressions in R, you should have specified a model using formula (e.g., y ~ x). However, most of the machine learning functions in R accept the dependent variable and explanatory variables in a matrix form (or data.frame). This is almost always the case for ML methods in Python as well.\n\n\nBy default, alpha parameter for glmnet() (\\(\\alpha\\) in Equation 3.1) is set to 1. So, to run LASSO, you can simply do the following:\n\n#=== extract X and y ===#\nX <- QuickStartExample$x\ny <- QuickStartExample$y\n\n#=== run LASSO ===#\nlasso <- glmnet(X, y)\n\nBy looking at the output below, you can see that glmnet() tried many different values of \\(\\lambda\\).\n\nlasso\n\n\nCall:  glmnet(x = X, y = y) \n\n   Df  %Dev  Lambda\n1   0  0.00 1.63100\n2   2  5.53 1.48600\n3   2 14.59 1.35400\n4   2 22.11 1.23400\n5   2 28.36 1.12400\n6   2 33.54 1.02400\n7   4 39.04 0.93320\n8   5 45.60 0.85030\n9   5 51.54 0.77470\n10  6 57.35 0.70590\n11  6 62.55 0.64320\n12  6 66.87 0.58610\n13  6 70.46 0.53400\n14  6 73.44 0.48660\n15  7 76.21 0.44330\n16  7 78.57 0.40400\n17  7 80.53 0.36810\n18  7 82.15 0.33540\n19  7 83.50 0.30560\n20  7 84.62 0.27840\n21  7 85.55 0.25370\n22  7 86.33 0.23120\n23  8 87.06 0.21060\n24  8 87.69 0.19190\n25  8 88.21 0.17490\n26  8 88.65 0.15930\n27  8 89.01 0.14520\n28  8 89.31 0.13230\n29  8 89.56 0.12050\n30  8 89.76 0.10980\n31  9 89.94 0.10010\n32  9 90.10 0.09117\n33  9 90.23 0.08307\n34  9 90.34 0.07569\n35 10 90.43 0.06897\n36 11 90.53 0.06284\n37 11 90.62 0.05726\n38 12 90.70 0.05217\n39 15 90.78 0.04754\n40 16 90.86 0.04331\n41 16 90.93 0.03947\n42 16 90.98 0.03596\n43 17 91.03 0.03277\n44 17 91.07 0.02985\n45 18 91.11 0.02720\n46 18 91.14 0.02479\n47 19 91.17 0.02258\n48 19 91.20 0.02058\n49 19 91.22 0.01875\n50 19 91.24 0.01708\n51 19 91.25 0.01557\n52 19 91.26 0.01418\n53 19 91.27 0.01292\n54 19 91.28 0.01178\n55 19 91.29 0.01073\n56 19 91.29 0.00978\n57 19 91.30 0.00891\n58 19 91.30 0.00812\n59 19 91.31 0.00739\n60 19 91.31 0.00674\n61 19 91.31 0.00614\n62 20 91.31 0.00559\n63 20 91.31 0.00510\n64 20 91.31 0.00464\n65 20 91.32 0.00423\n66 20 91.32 0.00386\n67 20 91.32 0.00351\n\n\nYou can access the coefficients for each value of \\(lambdata\\) by applying coef() method to lasso.\n\n#=== get coefficient estimates ===#\ncoef_lasso <- coef(lasso)\n\n#=== check the dimension ===#\ndim(coef_lasso)\n\n[1] 21 67\n\n#=== take a look at the first and last three ===#\ncoef_lasso[, c(1:3, 65:67)]\n\n21 x 6 sparse Matrix of class \"dgCMatrix\"\n                   s0           s1         s2          s64          s65\n(Intercept) 0.6607581  0.631235043  0.5874616  0.111208836  0.111018972\nV1          .          0.139264992  0.2698292  1.378068980  1.378335220\nV2          .          .            .          0.023067319  0.023240539\nV3          .          .            .          0.762792114  0.763209604\nV4          .          .            .          0.059619334  0.060253956\nV5          .          .            .         -0.901460720 -0.901862151\nV6          .          .            .          0.613661389  0.614081490\nV7          .          .            .          0.117323876  0.117960550\nV8          .          .            .          0.396890604  0.397260052\nV9          .          .            .         -0.030538991 -0.031073136\nV10         .          .            .          0.127412702  0.128222375\nV11         .          .            .          0.246801359  0.247227761\nV12         .          .            .         -0.063941712 -0.064471794\nV13         .          .            .         -0.045935249 -0.046242852\nV14         .         -0.005878595 -0.1299063 -1.158552963 -1.159038292\nV15         .          .            .         -0.137103471 -0.138012175\nV16         .          .            .         -0.045085698 -0.045661882\nV17         .          .            .         -0.047272446 -0.048039238\nV18         .          .            .          0.051702567  0.052180547\nV19         .          .            .         -0.001791685 -0.002203174\nV20         .          .            .         -1.144262012 -1.144641845\n                     s66\n(Intercept)  0.110845721\nV1           1.378578220\nV2           0.023398270\nV3           0.763589908\nV4           0.060832496\nV5          -0.902227796\nV6           0.614464085\nV7           0.118540773\nV8           0.397596878\nV9          -0.031560145\nV10          0.128960349\nV11          0.247615990\nV12         -0.064955124\nV13         -0.046522983\nV14         -1.159480668\nV15         -0.138840304\nV16         -0.046186890\nV17         -0.048737920\nV18          0.052615915\nV19         -0.002578088\nV20         -1.144987654\n\n\nApplying plot() method gets you how the coefficient estimates change as the value of \\(\\lambda\\) changes:\n\nplot(lasso)\n\n\n\n\nA high L1 Norm is associated with a “lower” value of \\(\\lambda\\) (weaker shrinkage). You can see that as \\(\\lambda\\) increases (L1 Norm decreases), coefficients on more and more variables are set to 0.\nNow, the obvious question is which \\(\\lambda\\) should we pick? One way to select a \\(\\lambda\\) is K-fold cross-validation (KCV), which we covered in section. We can implement KCV using the cv.glmnet() function. You can set the number of folds using the nfolds option (the default is 10). Here, let’s 5-fold CV.\n\ncv_lasso <- cv.glmnet(X, y, nfolds = 5)\n\nThe results of KCV can be readily visualized by applying the plot() method:\n\nplot(cv_lasso)\n\n\n\n\nThere are two vertical dotted lines. The left one indicates the value of \\(\\lambda\\) where CV MSE is minimized (called lambda.min). The right one indicates the  highest  (most regularized) value of \\(\\lambda\\) such that the CV error is within one standard error of the minimum (called lambda.1se).\nYou can access the MSE-minimizing \\(\\lambda\\) as follows:\n\ncv_lasso$lambda.min\n\n[1] 0.05725918\n\n\nYou can access the coefficient estimates when \\(\\lambda\\) is lambda.min as follows\n\ncoef(cv_lasso, s = \"lambda.min\")\n\n21 x 1 sparse Matrix of class \"dgCMatrix\"\n                     s1\n(Intercept)  0.14291659\nV1           1.34431995\nV2           .         \nV3           0.71047211\nV4           .         \nV5          -0.85182173\nV6           0.55960366\nV7           0.04475230\nV8           0.35117414\nV9           .         \nV10          0.01985732\nV11          0.19335515\nV12          .         \nV13          .         \nV14         -1.09350480\nV15         -0.01564233\nV16          .         \nV17          .         \nV18          .         \nV19          .         \nV20         -1.07874251\n\n\nThe following code gives you the coefficient estimates when \\(\\lambda\\) is lambda.1se\n\ncoef(cv_lasso, s = \"lambda.1se\")\n\n21 x 1 sparse Matrix of class \"dgCMatrix\"\n                    s1\n(Intercept)  0.1536661\nV1           1.3019575\nV2           .        \nV3           0.6422426\nV4           .        \nV5          -0.7892388\nV6           0.4944794\nV7           .        \nV8           0.2943189\nV9           .        \nV10          .        \nV11          0.1058440\nV12          .        \nV13          .        \nV14         -1.0402312\nV15          .        \nV16          .        \nV17          .        \nV18          .        \nV19          .        \nV20         -0.9791172\n\n\n\n\n\n\n\n\nNote\n\n\n\nglmnet() can be used to much broader class of models (e.g., Logistic regression, Poisson regression, Cox regression, etc). As the name suggests it’s elastic  net  methods for  generalized  linear  model."
  },
  {
    "objectID": "lasso.html#lasso-implementation-python",
    "href": "lasso.html#lasso-implementation-python",
    "title": "\n3  Regression Shrinkage Methods: LASSO (Ridge and Elastic Net)\n",
    "section": "\n3.4 LASSO implementation: Python",
    "text": "3.4 LASSO implementation: Python\nComing later."
  },
  {
    "objectID": "bias-variance-tradeoff.html",
    "href": "bias-variance-tradeoff.html",
    "title": "\n1  Variance-Bias Trade-off\n",
    "section": "",
    "text": "library(data.table)\nlibrary(tidyverse)\nlibrary(mgcv)\nlibrary(caret)\nlibrary(parallel)"
  },
  {
    "objectID": "tree-based-ml.html",
    "href": "tree-based-ml.html",
    "title": "\n5  Tree-based Regression\n",
    "section": "",
    "text": "Codelibrary(tidyverse)\nlibrary(data.table)\nlibrary(rpart)\nlibrary(rattle)\nlibrary(wooldridge)\nlibrary(reticulate)\nuse_python(\"/usr/local/bin/python3\")"
  },
  {
    "objectID": "tree-based-ml.html#sec-rt",
    "href": "tree-based-ml.html#sec-rt",
    "title": "\n5  Tree-based Regression\n",
    "section": "\n5.1 Regression tree",
    "text": "5.1 Regression tree\n\n5.1.1 What is it?\nHere is an example of regression tree to explain logged salary (lsalary) using the mlb1 data from the wooldridge package.\n\nCode#=== get mlb1 data (from wooldridge) ===#\ndata(mlb1)\n\n#=== build a simple tree ===#\nsimple_tree <-\n  rpart(\n    lsalary ~ hits + runsyr, \n    data = mlb1, \n    control = rpart.control(minsplit = 200)\n  )\n\nfancyRpartPlot(simple_tree)\n\n\n\n\nHere is how you read the figure. At the first node, all the observations belong to it (\\(n=353\\)) and the estimate of lsalary is 13. Now, the whole datasets are split into two based on the criteria of whether hits is less than 262 or not. If yes, then such observations will be grouped into the node with “2” on top (the leftmost node), and the estimated lsalary for all the observations in that group (\\(n = 132\\)) is 12. If no, then such observations will be grouped into the node with “3” on top, and the estimated lsalary for all the observations in that group (\\(n = 221\\)) is 14. This node is further split into two groups based on whether runsyr is less than 44 or not. For those observations with runsyr \\(< 44\\) (second node a the bottom), estimated lsalary is 14. For those with runsyr \\(>= 44\\) (rightmost node at the bottom), estimated lsalary is 15. The nodes that do not have any further bifurcations below are called terminal nodes or leafs.\nAs illustrated in the figure above, a regression tree splits the data into groups based on the value of explanatory variables, and all the observations in the same group will be assigned the same estimate (the sample average of the dependent variable of the group).\nAnother way of illustrating this grouping is shown below:\n\nCodeggplot(mlb1) +\n  geom_point(aes(y = hits, x = runsyr, color = lsalary)) +\n  scale_color_viridis_c() +\n  geom_hline(yintercept = 262) +\n  geom_line(\n    data = data.table(x = 44, y = seq(262, max(mlb1$hits), length = 100)), \n    aes(y = y, x = x)\n  ) +\n  annotate(\n    \"text\", \n    x = 44, y = 111, \n    label = \"Region 2\", \n    color = \"red\"\n  ) +\n  annotate(\n    \"text\", \n    x = 22, y = 1500, \n    label = \"Region 6\", \n    color = \"red\"\n  ) +\n  annotate(\n    \"text\", \n    x = 75, y = 1500, \n    label = \"Region 7\", \n    color = \"red\"\n  )\n\n\n\n\nThe mechanism called recursive binary splitting is used to split the predictor space like the example above. Suppose you have K explanatory variables (\\(X_1, \\dots, X_k\\)). Further, let \\(c\\) denote the cutpoint that splits the sample into two regions: {\\(X|X_k < c\\)} and {\\(X|X_k \\geq c\\)}.\n\n\n{\\(X|X_k < c\\)} means observations that satisfy the condition stated right to the vertical bar (|). Here, it means all the observations for which its \\(X_k\\) value is less than \\(c\\).\n\nStep 1: For each of the explanatory variables (\\(X_1\\) through \\(X_K\\)), find the cutpoint that leads to the lowest sum of the squared residuals.\nStep 2: Among all the splits (as many as the number of explanatory variables), pick the variable-cutpoint combination that leads to the lowest sum of the squared residuals.\n\nThe data is then split according to the chosen criteria and then the same process is repeated for each of the branches, ad infinitum until the user-specified stopping criteria is met.\nLet’s try to write (an inefficient version) this process for the first split from the beginning node using the mlb1 data as an illustration based on a simple grid search to find the optimal cutpoints (Step 1).\n\nCode#=== get data ===#\nlibrary(wooldridge)\ndata(mlb1)\n\nmlb1_dt <- \n  mlb1 %>% \n  data.table() %>% # turn into data.table \n  .[, salary := NULL] %>% # remove salary (use lsalary instead)\n  na.omit() # remove observations with NA in any of the variables\n\n\nLet’s work on splitting based on hruns. First, we define a sequence of values of cutpoints for hruns.\n\nCodevalue_seq <- \n  quantile(\n    mlb1_dt$hruns, \n    prob = seq(0.001, 0.999, length = 100)\n  ) %>% \n  unique()\n\n\nFor each value in value_seq, we find the RSS. For example, for the 50th value in value_seq,\n\nCodecopy(mlb1_dt) %>% \n  #=== find the mean of lsalary by whether hruns is less than the cutpoint or not ===#\n  .[, y_hat := mean(lsalary), by = (hruns < value_seq[50])] %>% \n  #=== get squared residuals ===#\n  .[, (lsalary - y_hat)^2] %>% \n  #=== get RSS ===#\n  sum()\n\n[1] 318.2431\n\n\nHow about 70th value in value_seq?\n\nCodecopy(mlb1_dt) %>% \n  #=== find the mean of lsalary by whether hruns is less than the cutpoint or not ===#\n  .[, y_hat := mean(lsalary), by = (hruns < value_seq[70])] %>% \n  #=== get squared residuals ===#\n  .[, (lsalary - y_hat)^2] %>% \n  #=== get RSS ===#\n  sum()\n\n[1] 419.1821\n\n\nThis means value_seq[70] (209.0880707) is a better cutpoint than value_seq[50] (71.7563535).\nOkay, let’s consider all the candidate values, not just 50th and 70th, and then pick the best.\n\nCodeget_rss <- function(i, var_name, value_seq, data)\n{\n  rss <-\n    copy(data) %>% \n    setnames(var_name, \"var\") %>% \n    .[, y_hat := mean(lsalary), by = (var < value_seq[i])] %>% \n    .[, (lsalary - y_hat)^2] %>% \n    sum()\n\n  return_data <-\n    data.table(\n      rss = rss,\n      var_name = var_name,\n      var_value = value_seq[i]\n    )\n\n  return(return_data)\n}\n\n\nHere are RSS values at every value in value_seq.\n\nCoderss_value <-\n  lapply(\n    seq_len(length(value_seq)),\n    function(x) get_rss(x, \"hruns\", value_seq, mlb1_dt) \n  ) %>% \n  rbindlist()\n\nhead(rss_value)\n\n        rss var_name var_value\n1: 445.0615    hruns         0\n2: 396.8287    hruns         1\n3: 383.2393    hruns         2\n4: 362.5779    hruns         3\n5: 337.0760    hruns         4\n6: 325.4865    hruns         5\n\nCodetail(rss_value)\n\n        rss var_name var_value\n1: 419.1821    hruns  209.0881\n2: 419.5111    hruns  231.0000\n3: 428.4163    hruns  242.4425\n4: 434.3318    hruns  258.5674\n5: 441.9307    hruns  333.4414\n6: 443.3740    hruns  426.0780\n\n\nFinding the cutpoint value that minimizes RSS,\n\nCoderss_value[which.min(rss), ]\n\n        rss var_name var_value\n1: 260.3094    hruns  28.47488\n\n\nOkay, so, the best cutpoint for hruns is 28.475\nSuppose we are considering only five explanatory variables in building a regression tree: hruns, years, rbisyr, allstar, runsyr, hits, and bavg. We do the same operation we did for hruns for all the variables.\n\nCodeget_rss_by_var <- function(var_name, data)\n{\n  temp_data <- copy(data) \n\n  #=== define a sequence of values of hruns ===#\n  value_seq <- \n    quantile(\n      temp_data[, ..var_name] %>% unlist(), \n      prob = seq(0.001, 0.999, length = 100)\n    ) %>% \n    unique()\n\n  #=== get RSS ===#\n  rss_value <-\n    lapply(\n      seq_len(length(value_seq)),\n      function(x) get_rss(x, var_name, value_seq, temp_data) \n    ) %>% \n    rbindlist() %>% \n    .[which.min(rss),]\n\n  return(rss_value)\n}\n\n\nLooping over the set of variables,\n\nCode(\nmin_rss_by_var <-\n  lapply(\n    c(\"hruns\", \"years\", \"rbisyr\", \"allstar\", \"runsyr\", \"hits\", \"bavg\"),\n    function(x) get_rss_by_var(x, mlb1_dt)\n  ) %>% \n  rbindlist()\n)\n\n        rss var_name  var_value\n1: 260.3094    hruns  28.474879\n2: 249.9090    years   4.000000\n3: 265.1082   rbisyr  32.774949\n4: 279.7165  allstar   8.209356\n5: 251.6343   runsyr  38.079145\n6: 205.1488     hits 354.811444\n7: 375.0281     bavg 252.359257\n\n\nSo, the variable-cutpoint combination that minimizes RSS is hits - 354.81. We now have the first split. This tree is developed further by splitting nodes like this.\n\n5.1.2 Training a regression tree in R\nYou can fit a regression tree using rpart() from the rpart package. Its syntax is similar to that of lm() for a quick fitting.\n\nCoderpart(\n  formula,\n  data\n)\n\n\nUsing mlb1, let’s fit a regression tree where lsalary is the dependent variable and hruns, years, rbisyr, allstar, runsyr, hits, and bavg are the explanatory variables.\n\nCode#=== fit a tree ===#\nfitted_tree <-\n  rpart(\n    lsalary ~ hruns + years + rbisyr + allstar + runsyr + hits + bavg, \n    data = mlb1_dt\n  )\n\n\nHere is the visualization of the fitted tree using fancyRpartPlot() from the rattle package.\n\nCodefancyRpartPlot(fitted_tree)\n\n\n\n\nNow, you may wonder why rpart() is not building a tree that has as many leaves as the number of observations so that we have a perfect prediction for the train data (mlb1). If we are simply implementing recursive binary splitting, then it should not have stopped where it stopped. This is because rpart() sets parameter values that control the development of a tree by default. Those default parameters can be seen below:\n\nCoderpart.control()\n\n$minsplit\n[1] 20\n\n$minbucket\n[1] 7\n\n$cp\n[1] 0.01\n\n$maxcompete\n[1] 4\n\n$maxsurrogate\n[1] 5\n\n$usesurrogate\n[1] 2\n\n$surrogatestyle\n[1] 0\n\n$maxdepth\n[1] 30\n\n$xval\n[1] 10\n\n\nFor example, minsplit is the minimum number of observations that must exist in a node in order for a split to be attempted. cp refers to the complexity parameter. For a given value of cp, a tree is build to minimize the following:\n\\[\n\\sum_{t=1}^T\\sum_{x_i\\in R_t} (y_i - \\hat{y}_{R_t})^2 + cp\\cdot T\n\\]\nwhere \\(R_t\\) is the \\(t\\)th region and \\(\\hat{y_{R_t}}\\) is the estimate of \\(y\\) for all the observations that reside in \\(R_t\\). So, the first term is RSS. The objective function has a penalization term (the second term) just like shrinkage methods we saw in Section 3.1. A higher value of cp leads to a less complex tree with less leaves.\nIf you want to build a much deeper tree that has many leaves, then you can do so using the control option like below.\n\nCodefull_tree <-\n  rpart(\n    lsalary ~ hruns + years + rbisyr + allstar + runsyr + hits + bavg, # formula\n    data = mlb1_dt, # data\n    control = # control of the hyper parameters\n      rpart.control(\n        minsplit = 2, \n        cp = 0 # complexity parameter\n      )\n  )\n\n\nLet’s see how amazing this tree is by comparing the observed and fitted lsalary values.\n\nCode#=== get fitted values ===#\nmlb1_dt[, y_hat := predict(full_tree, newdata = mlb1_dt)]\n\n#=== visualize the fit ===#\nggplot(data = mlb1_dt) +\n  geom_point(aes(y = lsalary, x = y_hat)) +\n  geom_abline(slope = 1, color = \"red\")\n\n\n\n\nYes, perfect prediction accuracy! At least for the train data anyway. But, we all know we want nothing to do with this kind of model. It is clearly over-fitting the train data.\nIn order to find a reasonable model, we can use KCV over cp. Fortunately, when we run rpart(), it automatically builds multiple trees at different values of cp that controls the number of leaves and conduct KCV. You can visualize this using plotcp().\n\nCodeplotcp(fitted_tree)\n\n\n\n\nMSE and cp are presented on the y- and x-axis, respectively. According to the KCV results, cp \\(= 0.018\\) provides the tree with the smallest number of leaves (the most simple) where the MSE value is within one standard deviation from the lowest MSE. You can access the tree built under cp \\(= 0.018\\) like below.\n\nCode#=== get the best tree ===#\nbest_tree <- prune(full_tree, cp = 0.018)\n\n#=== visualize it ===#\nfancyRpartPlot(best_tree)\n\n\n\n\nEven though how a regression tree is build in R. In practice, you never use a regression tree itself as the final model for your research as its performance is rather poor and tend to over-fit compared to other competitive methods. But, understanding how building a regression tree is important to understand its derivatives like random forest, boosted regression forest."
  },
  {
    "objectID": "tree-based-ml.html#sec-rf",
    "href": "tree-based-ml.html#sec-rf",
    "title": "\n5  Tree-based Regression\n",
    "section": "\n5.2 Random Forest (RF)",
    "text": "5.2 Random Forest (RF)\nRegression tree approach is often not robust and suffers from high variance. Here, we look at the process called  bagging  and how it can be used to train RF model, which is much more robust than a regression tree.\n\n5.2.1 Bagging (Bootstrap aggregation)\nConsider two random variables \\(x_1\\) and \\(x_2\\) from the identical distribution, where \\(E[x_i] = \\alpha\\) and \\(Var(x_i) = \\sigma^2\\). You are interested in estimating \\(E[x_i]\\). We all know that the following relationship holds in general:\n\\[\n\\begin{aligned}\nVar(\\frac{x_1 + x_2}{2}) & = \\frac{Var(x_1)}{4} + \\frac{Var(x_2)}{4} + \\frac{Cov(x_1, x_2)}{2} \\\\\n& = \\frac{\\sigma^2}{2} + \\frac{Cov(x_1, x_2)}{2}\n\\end{aligned}\n\\]\nSo, instead of using a single draw from \\(x_1\\) and using it as an estimate for \\(E[x_i]\\), it is better to use the values from both \\(x_1\\) and \\(x_2\\) and average them to obtain an estimate for \\(E[x_i]\\) as long as \\(x_1\\) and \\(x_2\\) are not perfectly  positively  correlated (in this case \\(Cov(x_1, x_2) = Var(x_1) = Var(x_1) = \\sigma^2\\)). The benefit of averaging is greater when the value of \\(Cov(x_1, x_2)\\) is smaller.\nLet’s do a little experiment to see this. We consider three cases:\n\nCode#=== set the number of observations to 1000 ===#\nN <- 1000\n\n\n\nCode#=== first case (no correlation) ===#\nx_1 <- rnorm(N)\nx_2 <- rnorm(N)\n\ncor(x_1, x_2)\n\n[1] 0.03488201\n\nCode#=== second case (positively correlated) ===#\nx_1 <- rnorm(N)\nx_2 <- 0.5 * x_1 + sqrt(1-(0.5)^2) * rnorm(N)\n\ncor(x_1, x_2)\n\n[1] 0.4930989\n\nCode#=== third case (negatively correlated) ===#\nx_1 <- rnorm(N)\nx_2 <- - 0.8 * x_1 - sqrt(1-(0.8)^2) * rnorm(N)\n\ncor(x_1, x_2)\n\n[1] -0.7898339\n\n\n\nCodeget_alpha <- function(i)\n{\n  #=== base case ===#\n  alpha_hat_0 <- rnorm(1)\n\n  #=== first case (no correlation) ===#\n  x_1 <- rnorm(1)\n  x_2 <- rnorm(1)\n\n  alpha_hat_1 <- (x_1 + x_2) / 2\n\n  #=== second case (positively correlated) ===#\n  x_1 <- rnorm(1)\n  x_2 <- 0.5 * x_1 + sqrt(1-(0.5)^2) * rnorm(1)\n\n  alpha_hat_2 <- (x_1 + x_2) / 2\n\n  #=== third case (negatively correlated) ===#\n  x_1 <- rnorm(1)\n  x_2 <- - 0.8 * x_1 - sqrt(1-(0.8)^2) * rnorm(1)\n\n  alpha_hat_3 <- (x_1 + x_2) / 2\n\n  return_data <-\n    data.table(\n      alpha_hat_0 = alpha_hat_0,\n      alpha_hat_1 = alpha_hat_1,\n      alpha_hat_2 = alpha_hat_2,\n      alpha_hat_3 = alpha_hat_3\n    )\n\n  return(return_data)\n\n} \n\n\n\nCodeset.seed(234934)\n\nsim_results <-\n  lapply(\n    1:1000,\n    get_alpha\n  ) %>% \n  rbindlist() %>% \n  melt()\n\nWarning in melt.data.table(.): id.vars and measure.vars are internally\nguessed when both are 'NULL'. All non-numeric/integer/logical type columns are\nconsidered id.vars, which in this case are columns []. Consider providing at\nleast one of 'id' or 'measure' vars in future.\n\n\nAs you can see below, they are all pretty much unbiased. However, all the cases that averaged two values (cases 1, 2, and 3) outperformed the base case that relied on a single value each iteration. You can see that when the random variables are negatively correlated, the power of averaging is greater compared to when they are independent or positively correlated. The independent case (case 1) is better than the positive correlation case (case 2).\n\nCode#=== expected value ===#\nsim_results[, mean(value), by = variable]\n\n      variable          V1\n1: alpha_hat_0 0.037836191\n2: alpha_hat_1 0.008031971\n3: alpha_hat_2 0.008486035\n4: alpha_hat_3 0.004259832\n\nCode#=== standard error ===#\nsim_results[, sd(value), by = variable]\n\n      variable        V1\n1: alpha_hat_0 0.9840232\n2: alpha_hat_1 0.6969730\n3: alpha_hat_2 0.8660699\n4: alpha_hat_3 0.3103408\n\n\nBagging takes advantage of the power of averaging. Specifically, bagging takes the following steps:\n\nGenerate many bootstrapped datasets (say \\(B\\) datasets)\nTrain a model on each of the bootstrapped datasets (\\(\\hat{f}^1, \\dots, \\hat{f}^B\\))\nAverage the estimates from all the trained models to come up with an estimate\n\n\\[\n\\hat{f}(X) = \\frac{\\hat{f}^1(X) + \\dots + \\hat{f}^B(X)}{B}\n\\]\nLet’s implement this for \\(B = 10\\) using mlb1_dt. First, define a function (named train_a_tree()) that bootstrap data, fit a regression tree, and then return the fitted values.\n\nCodetrain_a_tree <- function(i, data)\n{\n  #=== number of observations ===#\n  N <- nrow(data)\n\n  #=== bootstrapped data ===#\n  boot_data <- data[sample(1:N, N, replace = TRUE), ]\n\n  #=== train a regression tree ===#\n  rpart <-\n    rpart(\n      lsalary ~ hruns + years + rbisyr + allstar + runsyr + hits + bavg, \n      data = boot_data\n    )\n\n  #=== predict ===#\n  return_data <-\n    copy(data) %>% \n    .[, y_hat := predict(rpart, newdata = data)] %>% \n    .[, .(id, y_hat)]\n\n  return(return_data)\n}\n\n\nWe now repeat train_a_tree() 10 times.\n\nCode#=== create observation id for later group-by averaging ===#\nmlb1_dt[, id := 1:.N]\n\n(\ny_estimates <-\n  lapply(\n    1:10,\n    function(x) train_a_tree(x, mlb1_dt) \n  ) %>% \n  rbindlist()\n)\n\n       id    y_hat\n   1:   1 15.15792\n   2:   2 14.23816\n   3:   3 15.15792\n   4:   4 14.23816\n   5:   5 14.23816\n  ---             \n3296: 326 13.17676\n3297: 327 12.64863\n3298: 328 12.64863\n3299: 329 13.61157\n3300: 330 12.02302\n\n\nBy averaging \\(y\\) estimates by id, we can get bagging estimates.\n\nCodey_estimates[, mean(y_hat), by = id]\n\n      id       V1\n  1:   1 15.09137\n  2:   2 14.66143\n  3:   3 14.61425\n  4:   4 14.31035\n  5:   5 13.74852\n ---             \n326: 326 13.04455\n327: 327 12.79717\n328: 328 12.79717\n329: 329 13.67179\n330: 330 12.02051\n\n\nNow, let’s take a look at the individual estimates of \\(y\\) for the first observation.\n\nCodey_estimates[id == 1, ]\n\n    id    y_hat\n 1:  1 15.15792\n 2:  1 15.04446\n 3:  1 15.21238\n 4:  1 15.07311\n 5:  1 14.92840\n 6:  1 14.98571\n 7:  1 15.12060\n 8:  1 15.14064\n 9:  1 15.05676\n10:  1 15.19375\n\n\nHmm, the estimates look very similar. Actually, that is the case for all the observations. This is because the trained trees are very similar for many reasons, and the trees are highly “positively” correlated with each other. From our very simple experiment above, we know that the power of bagging is not very high when that is the case. While RF does use bagging, popular R and python packages does it in a much better way than I demonstrated here. We see this next.\n\n5.2.2 Random Forest (RF)\nUnlike a naive bagging approach I demonstrated above, RF does it in a clever way to decorrelate trees. Specifically, for any leave of any tree, they consider only a randomly select subset of the explanatory variables when deciding how to split a leave. A typical choice of the number of variables considered at each split is \\(\\sqrt{K}\\), where \\(K\\) is the number of the explanatory variables specified by the user. In the naive example above, all \\(K\\) variables are considered for all the split decisions of all the trees. Some variables are more influential than others and they get to be picked as the splitting variable at similar places, which can result in highly correlated trees. Instead, RF gives other variables a chance, which helps decorrelate the trees.\nWe can use ranger() from the ranger package to train an RF model.\n\n\nAnother compelling R package for RF is the randomForest package.\nThe ranger() function has many options you can specify that determine how trees are built. Here are some of the important ones (see here for the complete description of the hyper-parameters.):\n\n\nmtry: the number of variables considered in each split (default is the square root of the total numbers of explanatory variables rounded down.)\n\nnum.trees: the number of tree to be built (default is 500)\n\nmin.node.size: minimum number of observations in each node (default varies based on the the type of analysis)\n\nreplace: where sample with or without replacement when bootstrapping samples (default is TRUE)\n\nsample.fraction: the fraction of the entire observations that are used in each tree (default is 1 if sampling with replacement, 0.632 if sampling without replacement)\n\nLet’s try fitting an RF with ranger() with the default parameters.\n\nCode#=== load the package ===#\nlibrary(ranger)\n\n\nAttaching package: 'ranger'\n\n\nThe following object is masked from 'package:rattle':\n\n    importance\n\nCode#=== fit and RF ===#\n(\nrf_fit <- \n  ranger(\n    lsalary ~ hruns + years + rbisyr + allstar + runsyr + hits + bavg, \n    data = mlb1_dt\n  )\n)\n\nRanger result\n\nCall:\n ranger(lsalary ~ hruns + years + rbisyr + allstar + runsyr +      hits + bavg, data = mlb1_dt) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      330 \nNumber of independent variables:  7 \nMtry:                             2 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       0.3668547 \nR squared (OOB):                  0.7288123 \n\n\n\n\nSince we have many trees, it is no longer possible to have a nice graphical representation of the trained RF model like we did with a regression tree.\nIn the output, you can see OOB prediction error (MSE). OOB stands for  out-of-bag. When bootstrapping (whether you do it with replacement or not), some of the train data will not be used to build a tree.\n\nCoden_obs <- nrow(mlb1_dt)\n\n#=== bootstrapped data ===#\nboot_data <- mlb1_dt[sample(1:n_obs, n_obs, replace = TRUE), ]\n\n#=== which rows (observations) from the original datasets are missing? ===#\nmlb1_dt[, id %in% unique(boot_data$id)] %>% mean()\n\n[1] 0.6212121\n\n\nSo, only \\(65\\%\\) of the rows from the original data (mlb1_dt) in this bootstrapped sample (many duplicates of the original observations). The observations that are NOT included in the bootstrapped sample is called out-of-bag observations. This provides a great opportunity to estimate test MSE while training an RF model! For a given regression tree, you can apply it to the out-of-bag samples to calculate MSE. You can repeat this for all the trees and average the MSEs, effectively conducting cross-validation. When the number of trees is large enough, OOB MSE is almost equivalent to MSE from LOOCV [@james2013introduction]. This means that we can tune hyper-parameters by comparing OOB MSEs of different configurations of them.\nYou can use a simple grid-search to find the best hyper-parameters. Grid-search is simply a brute-force optimization methods that goes through all the combinations of hyper-parameters and see which combination comes at the top. The computational intensity of grid-search depends on how many hyper-parameters you want to vary and how many values you would like to look at for each of the hyper-parameters. Here, let’s tune mtry, min.node.size, and sample.fraction.\n\nCode#=== define set of values you want to look at ===#\nmtry_seq <- c(2, 4, 7)\nmin_node_size_seq <- c(2, 5, 10)\nsample_fraction_seq <- c(0.5, 0.75, 1)\n\n#=== create a complete combinations of the three parameters ===#\n(\nparameters <-\n  data.table::CJ(\n    mtry = mtry_seq,\n    min_node_size = min_node_size_seq,\n    sample_fraction = sample_fraction_seq\n  )\n)\n\n    mtry min_node_size sample_fraction\n 1:    2             2            0.50\n 2:    2             2            0.75\n 3:    2             2            1.00\n 4:    2             5            0.50\n 5:    2             5            0.75\n 6:    2             5            1.00\n 7:    2            10            0.50\n 8:    2            10            0.75\n 9:    2            10            1.00\n10:    4             2            0.50\n11:    4             2            0.75\n12:    4             2            1.00\n13:    4             5            0.50\n14:    4             5            0.75\n15:    4             5            1.00\n16:    4            10            0.50\n17:    4            10            0.75\n18:    4            10            1.00\n19:    7             2            0.50\n20:    7             2            0.75\n21:    7             2            1.00\n22:    7             5            0.50\n23:    7             5            0.75\n24:    7             5            1.00\n25:    7            10            0.50\n26:    7            10            0.75\n27:    7            10            1.00\n    mtry min_node_size sample_fraction\n\n\nIn total, we have 27 (\\(3 \\times 3 \\times 3\\)) cases. You can see how quickly the number of cases increases as you increase the number of parameters to tune and the values of each parameter. We can now loop over the rows of this parameter data (parameters) and get OOB MSE for each of them.\n\nCodeoob_mse_all <-\n  lapply(\n    seq_len(nrow(parameters)),\n    function(x) {\n\n      #=== Fit the mode ===#\n      rf_fit <- \n        ranger(\n          lsalary ~ hruns + years + rbisyr + allstar + runsyr + hits + bavg, \n          data = mlb1_dt,\n          num.trees = 1000,\n          mtry = parameters[x, mtry],\n          min.node.size = parameters[x, min_node_size],\n          sample.fraction = parameters[x, sample_fraction]\n        )\n\n      #=== return OOB SME ===#\n      return(rf_fit$prediction.error)\n      \n    }\n  ) %>% \n  unlist()\n\n#=== assign OOB MSE to the parameters data ===#\nparameters[, oob_mse := oob_mse_all]\n\n#=== take a look ===#\nparameters\n\n    mtry min_node_size sample_fraction   oob_mse\n 1:    2             2            0.50 0.3634654\n 2:    2             2            0.75 0.3632539\n 3:    2             2            1.00 0.3703735\n 4:    2             5            0.50 0.3609772\n 5:    2             5            0.75 0.3640391\n 6:    2             5            1.00 0.3657121\n 7:    2            10            0.50 0.3632176\n 8:    2            10            0.75 0.3590104\n 9:    2            10            1.00 0.3647577\n10:    4             2            0.50 0.3639452\n11:    4             2            0.75 0.3678789\n12:    4             2            1.00 0.3762502\n13:    4             5            0.50 0.3625460\n14:    4             5            0.75 0.3682806\n15:    4             5            1.00 0.3696961\n16:    4            10            0.50 0.3632466\n17:    4            10            0.75 0.3641201\n18:    4            10            1.00 0.3693227\n19:    7             2            0.50 0.3697563\n20:    7             2            0.75 0.3773171\n21:    7             2            1.00 0.3807321\n22:    7             5            0.50 0.3749875\n23:    7             5            0.75 0.3731546\n24:    7             5            1.00 0.3857696\n25:    7            10            0.50 0.3688267\n26:    7            10            0.75 0.3758684\n27:    7            10            1.00 0.3764331\n    mtry min_node_size sample_fraction   oob_mse\n\n\nSo, the best choice among the ones tried is:\n\nCodeparameters[which.min(oob_mse), ]\n\n   mtry min_node_size sample_fraction   oob_mse\n1:    2            10            0.75 0.3590104"
  },
  {
    "objectID": "tree-based-ml.html#boosted-regression-forest",
    "href": "tree-based-ml.html#boosted-regression-forest",
    "title": "\n5  Tree-based Regression\n",
    "section": "\n5.3 Boosted Regression Forest",
    "text": "5.3 Boosted Regression Forest\n\n5.3.1 Gradient Boosting\nIn training RF that uses the idea of bagging, the original data is used to generate many bootstrapped datasets, a regression tree is trained on each of them  independently , and then they are averaged to reach the final model. Boosting is similar to bagging (bootstrap aggregation) in that it trains many statistical models and then combine them to reach the final model. However, instead of building many trees independently, it builds trees  sequentially  in a manner that improves prediction step by step.\nWhile there are many variants of boosting methods (see Chapter 10 of @hastie2009elements), we will look at gradient boosting using trees for regression in particular (Algorithm 10.3 in @hastie2009elements presents the generic gradient tree boosting algorithm), where squared error is used as the loss function.\n\nSet \\(f_0(X_i) = \\frac{\\sum_{i=1}^N y_i}{N}\\) for all \\(i = 1, \\dots, N\\)\n\nFor b = 1 to B,\n\n\nFor \\(i = 1, \\dots, N\\), calculate \\[\n    r_{i,b} =  (y_i - f_{b-1}(X_i))\n    \\]\n\nFit a regression tree to \\(r_{i, b}\\), which generates terminal regions \\(R_{j,b}\\), \\(j = 1, \\dots, J\\), and denote the predicted value of region \\(R_{j,b}\\) as \\(\\gamma_{j,b}\\).\nSet \\(f_b(X_i) = f_{b-1}(X_i) + \\lambda \\cdot \\sum_{j=1}^J\\gamma_{j, b}\\cdot I(X_i \\in R_{j,b})\\)\n\n\n\nFinally, \\(\\hat{f}(X_i) = f_B(X_i)\\)\n\n\nLet’s try to go through this algorithm a bit to have it sink in for you.\nStep 1\nStep 1 finds the mean of the dependent variable. This quantity is used as the starting estimate for the dependent variable.\n\nCode(\nf_0 <- mean(mlb1_dt$lsalary)\n)\n\n[1] 13.51172\n\n\nStep 2: \\(b = 1\\)\nNow, we get residuals:\n\nCodemlb1_dt[, resid_1 := lsalary - f_0]\n\n\nThe residuals contain information in lsalary that was left unexplained. By training a regression tree using the residuals as the dependent variable, we are finding a tree that can explain the unexplained parts of lsalary using the explanatory variables.\n\nCodetree_fit_b1 <- \n  rpart(\n    resid_1 ~ ., # . means all variables\n    data = mlb1_dt \n  )\n\n\nHere is the fitted value of the residuals (\\(\\sum_{j=1}^J\\gamma_{j, b}\\cdot I(X_i \\in R_{j,b})\\))\n\nCoderesid_1_hat <- predict(tree_fit_b1, newdata = mlb1_dt)\nhead(resid_1_hat)\n\n         1          2          3          4          5          6 \n 1.7134881  1.7134881  1.2414996  1.2414996  0.5054178 -0.1851016 \n\n\nNow, we update our prediction according to \\(f_b(X_i) = f_{b-1}(X_i) + \\lambda \\cdot \\sum_{j=1}^J\\gamma_{j, b}\\cdot I(X_i \\in R_{j,b})\\). We set \\(\\lambda\\) to be \\(0.2\\) in this illustration.\n\nCodelambda <- 0.2\nf_1 <- f_0 + lambda * resid_1_hat\nhead(f_1)\n\n       1        2        3        4        5        6 \n13.85441 13.85441 13.76002 13.76002 13.61280 13.47470 \n\n\nDid we actually improve prediction accuracy? Let’s compare f_0 and f_1.\n\nCodesum((mlb1_dt$lsalary - f_0)^2)\n\n[1] 445.0615\n\nCodesum((mlb1_dt$lsalary - f_1)^2)\n\n[1] 288.3205\n\n\nGreat. Let’s move on to \\(b = 2\\).\n\nCode#=== get negative of the residuals ===#\nmlb1_dt[, resid_2 := lsalary - f_1]\n\n#=== fit a regression tree ===#\ntree_fit_b2 <- \n  rpart(\n    resid_2 ~ ., # . means all variables\n    data = mlb1_dt \n  )\n\n#=== get predicted values ===#\nresid_2_hat <- predict(tree_fit_b2, newdata = mlb1_dt)\n\n#=== update ===#\nf_2 <- f_1 + lambda * resid_2_hat\n\n\n\nCodesum((mlb1_dt$lsalary - f_1)^2)\n\n[1] 288.3205\n\nCodesum((mlb1_dt$lsalary - f_2)^2)\n\n[1] 186.9229\n\n\nWe further improved our predictions. We repeat this process until certain user-specified stopping criteria is met.\nAs you probably have noticed, there are several key parameters in the process above that controls the performance of gradient boosting forest. \\(\\lambda\\) controls the speed of learning. The lower \\(\\lambda\\) is, slower the learning speed is. \\(B\\) (the number of trees) determines how many times we want to make small improvements to the original prediction. When you increase the value of \\(\\lambda\\), you should decrease the value of \\(B\\). Too high values of \\(\\lambda\\) and \\(B\\) can lead to over-fitting.\nYou may have been wondering why this algorithm is called Gradient boosting. Gradient boosting is a much more general than the one described here particularly for gradient tree boosting for regression. It can be applied to both regression and classification1. In general, Step 2.a can be written as follows:\n\\[\nr_{i,b} = - \\huge[\\normalsize\\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)}\\huge]\\normalsize_{f = f_{b-1}}\n\\]\nwhere \\(L(y_i, f(x_i))\\) is the loss function. For regression, the loss function is almost always squared error: \\((y_i - f(x_i))^2\\). For, \\(L(y_i, f(x_i)) = (y_i - f(x_i))^2\\), the negative of the derivative of the loss function with respect to \\(f(x_i)\\) is\n\\[\n- \\huge[\\normalsize\\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)}\\huge]\\normalsize_{f = f_{b-1}} = - (- 2 (y_i - f(x_i))) = 2 (y_i - f(x_i))\n\\]\nThis is why we have \\(r_{i,b} = (y_i - f_{b-1}(X_i))\\) at Step 2.a. And, as you just saw, we are using the gradient of the loss function for model updating, which is why it is called  gradient  boosting. Note that it does not really matter whether you have \\(2\\) in front of the residuals or not the fitted residuals is multiplied (scaled) by \\(\\lambda\\) to when updating the model. You can always find the same \\(\\lambda\\) that would result in the same results as when just non-scaled residuals are used.\nMost R and python packages allow you to use a fraction of the train sample that are randomly selected and/or to use a subset of the included variables in building a tree within Step 2. This generate randomness in the algorithm and they are referred to as  stochastic gradient boosting.\n\n5.3.2 Implementation in R\nWe can use the gbm package to train a gradient boosting regression. Just like ranger(), gbm takes formula and data like below.\n\nCodelibrary(gbm)\n\nLoaded gbm 2.1.8\n\nCode#=== fit a gbm model ===#\ngbm_fit <- \n  gbm(\n    lsalary ~ hruns + years + rbisyr + allstar + runsyr + hits + bavg, \n    data = mlb1_dt \n  )\n\nDistribution not specified, assuming gaussian ...\n\n\nHere is the list of some parameters to be aware of:\n\n\nn.trees: Number of trees (\\(B\\)). Default is \\(100\\).\n\ninteraction.depth: 1 implies an additive model without interactions between included variables2, 2 implies a model with 2-way interactions. Default is 1.\n\nn.minobsinnode: Minimum number of observations in the terminal nodes.\n\nshrinkage: Learning rate (\\(\\lambda\\)). Default is 0.1.\n\nbag.fraction: The fraction of the train data observations that are select randomly in building a tree. Default is 0.5.\n\ncv.folds: The number of folds in conducting KCV\n\nBy specifying cv.folds, gbm() automatically conducts cross-validation for you.\n\nCode#=== gbm fit with CV ===#\ngbm_fit <- \n  gbm(\n    lsalary ~ hruns + years + rbisyr + allstar + runsyr + hits + bavg, # . means all variables\n    data = mlb1_dt,\n    cv.folds = 5,\n  )\n\nDistribution not specified, assuming gaussian ...\n\nCode#=== see the MSE history ===#  \ngbm_fit$cv.error\n\n  [1] 1.2262241 1.1179341 1.0228205 0.9368023 0.8703427 0.8081771 0.7587331\n  [8] 0.7184427 0.6784681 0.6488203 0.6220356 0.5944452 0.5684697 0.5462641\n [15] 0.5307501 0.5141117 0.4979657 0.4827620 0.4730010 0.4581580 0.4504190\n [22] 0.4443921 0.4371236 0.4309866 0.4229345 0.4173295 0.4134676 0.4087607\n [29] 0.4060998 0.4026218 0.3994747 0.3964138 0.3928210 0.3916108 0.3900712\n [36] 0.3871619 0.3863775 0.3846834 0.3842239 0.3805245 0.3795898 0.3791349\n [43] 0.3755839 0.3744459 0.3740001 0.3739954 0.3732254 0.3739662 0.3735692\n [50] 0.3727525 0.3696160 0.3695747 0.3697210 0.3678560 0.3674522 0.3653063\n [57] 0.3658336 0.3640528 0.3627853 0.3626865 0.3632326 0.3637658 0.3628768\n [64] 0.3606575 0.3606048 0.3615596 0.3613531 0.3611161 0.3618721 0.3626175\n [71] 0.3622998 0.3624336 0.3617014 0.3624890 0.3621559 0.3625732 0.3626680\n [78] 0.3625827 0.3620569 0.3613892 0.3611959 0.3609769 0.3610368 0.3599778\n [85] 0.3599650 0.3598605 0.3612187 0.3607883 0.3617094 0.3611722 0.3600410\n [92] 0.3604629 0.3595301 0.3591031 0.3587180 0.3575208 0.3580529 0.3586332\n [99] 0.3584507 0.3585487\n\n\nYou can visualize the CV results using gbm.perf().\n\nCodegbm.perf(gbm_fit)\n\n\n\n\n[1] 96\n\n\nNote that it will tell you what the optimal number of trees is  given  the values of the other hyper-parameters (here default values). If you want to tune other parameters as well, you need to program it yourself."
  },
  {
    "objectID": "tree-based-ml.html#over-fitting",
    "href": "tree-based-ml.html#over-fitting",
    "title": "\n5  Tree-based Regression\n",
    "section": "\n5.5 Over-fitting",
    "text": "5.5 Over-fitting"
  },
  {
    "objectID": "tree-based-ml.html#resources",
    "href": "tree-based-ml.html#resources",
    "title": "\n5  Tree-based Regression\n",
    "section": "\n5.6 Resources",
    "text": "5.6 Resources\n\n\nGradient Boosting with Scikit-Learn, XGBoost, LightGBM, and CatBoost by Jason Brownlee\nA Gentle Introduction to XGBoost for Applied Machine Learning"
  },
  {
    "objectID": "tree-based-ml.html#extreme-gradient-boosting",
    "href": "tree-based-ml.html#extreme-gradient-boosting",
    "title": "\n5  Tree-based Regression\n",
    "section": "\n5.4 Extreme Gradient Boosting",
    "text": "5.4 Extreme Gradient Boosting\nExtreme gradient boosting (XGB) is a variant of gradient boosting that has been extremely popular due to its superb performance. The basic concept is the same as the gradient boosting algorithm described above, however, it has its own way of building a tree, which is more mindful of avoiding over-fitting trees.\n\n5.4.1 Tree updating in XGB (general)\nLet \\(f_{i,b}(x_i)\\) be the prediction for the \\(i\\)th observation at the \\(b\\)-th iteration. Further, let \\(w_t(x_i)\\) is the term that is added to \\(f_{i,b}(x_i)\\) to obtain \\(f_{i,b+1}(x_i)\\). In XGB, \\(w_t(x_i)\\) is such that it minimizes the following objective:\n\\[\n\\Psi_t = \\sum_{i=1}^N [L(y_i, f_{i,b}(x_i) + w_t(x_i))] + \\Omega(w_t)\n\\tag{5.1}\\]\nwhere \\(L()\\) is the user-specified loss-function that is differentiable and \\(\\Omega(w_t)\\) is the regularization term. Instead of Equation 5.1, XGB uses the second order Taylor expansion of \\(L()\\) about \\(w\\)3.\n\\[\n\\tilde{\\Psi}_t = \\sum_{i=1}^N [L(y_i, f_{i,b}(x_i)) + g_i w_t(x_i) + \\frac{1}{2}h_i w_t(x_i)^2] + \\Omega(w_t)\n\\tag{5.2}\\]\nwhere \\(g_i = \\frac{\\partial L(y_i, p_i)}{\\partial p_i}\\) (first-order derivative) and \\(h_i = \\frac{\\partial^2 L(y_i, p_i)}{\\partial p_i^2}\\) (second-order derivative). Since \\(L(y_i, f_{i,b}(x_i))\\) is just a constant, we can safely remove it from the objective function, which leads to\n\\[\n\\tilde{\\Psi}_t = \\sum_{i=1}^N [g_i w_t(x_i) + \\frac{1}{2}h_i w_t(x_i)^2] + \\Omega(w_t)\n\\tag{5.3}\\]\nLet \\(I_j\\) denote a set of observations that belong to leaf \\(j\\) (\\(j = 1, \\dots, J\\)). Then, Equation 5.3 is written as follows:\n\\[\n\\tilde{\\Psi}_t = \\sum_{j=1}^J\\huge[\\normalsize (\\sum_{i\\in I_j}g_i)w_j + \\frac{1}{2}(\\sum_{i\\in I_j}h_i + \\lambda)w_j^2 \\huge]\\normalsize + \\gamma J\n\\tag{5.4}\\]\n\n\nRemember that all the observations in the same leaf shares the same prediction. So, for all \\(i\\)s that belong to leaf \\(j\\), the prediction is denoted as \\(w_j\\) in Equation 5.4. That is, \\(w_t(x_i)\\) that belongs to leaf \\(j\\) is \\(w_j\\).\nFor a given tree structure (denoted as \\(q(x)\\)), the leaves can be treated independently in minimizing this objective.\nTaking the derivative of \\(\\tilde{\\Psi}_t\\) w.r.t \\(w_j\\),\n\\[\n\\begin{aligned}\n(\\sum_{i\\in I_j}g_i) + (\\sum_{i\\in I_j}h_i + \\lambda)w_j = 0 \\\\\n\\Rightarrow w_j^* = \\frac{-\\sum_{i\\in I_j}g_i}{\\sum_{i\\in I_j}h_i + \\lambda}\n\\end{aligned}\n\\tag{5.5}\\]\nThe minimized value of \\(\\tilde{\\Psi}_t\\) is then (obtained by plugging \\(w_j^*\\) into Equation 5.4),\n\\[\n\\begin{aligned}\n\\tilde{\\Psi}_t(q)^* & = \\sum_{j=1}^J\\huge[\\normalsize (\\sum_{i\\in I_j}g_i)\\frac{-\\sum_{i\\in I_j}g_i}{\\sum_{i\\in I_j}h_i + \\lambda} + \\frac{1}{2}(\\sum_{i\\in I_j}h_i + \\lambda)(\\frac{-\\sum_{i\\in I_j}g_i}{\\sum_{i\\in I_j}h_i + \\lambda})^2 \\huge]\\normalsize + \\gamma J \\\\\n& = \\sum_{j=1}^J\\huge[\\normalsize \\frac{-(\\sum_{i\\in I_j}g_i)^2}{\\sum_{i\\in I_j}h_i + \\lambda} + \\frac{1}{2}\\frac{(\\sum_{i\\in I_j}g_i)^2}{\\sum_{i\\in I_j}h_i + \\lambda} \\huge]\\normalsize + \\gamma J \\\\\n& = -\\frac{1}{2} \\sum_{j=1}^J \\huge[\\normalsize\\frac{(\\sum_{i\\in I_j}g_i)^2}{\\sum_{i\\in I_j}h_i + \\lambda}\\huge]\\normalsize + \\gamma J\n\\end{aligned}\n\\tag{5.6}\\]\nFor notatinal convenience, we call \\(\\frac{(\\sum_{i\\in I_j}g_i)^2}{\\sum_{i\\in I_j}h_i + \\lambda}\\) quality score and denote it by \\(Q_j\\) ( Quality score for leaf \\(j\\)).\nWe could find the best tree structure by finding \\(w_j^*(q)\\) according to Equation 5.4 and calculate \\(\\tilde{\\Psi}_t(q)^*\\) according to Equation 5.6 for each of all the possible tree structures, and then pick the tree structure q(x) that has the lowest \\(\\tilde{\\Psi}_t(q)^*\\).\nHowever, it is impossible to consider all possible tree structures practically. So, a greedy (myopic) approach that starts from a single leaf and iteratively splits leaves is used instead.\nConsider splitting an existing leaf \\(s\\) (where in the tree it may be located) into two leaves \\(L\\) and \\(R\\) when there are \\(J\\) existing leaves. Then, we find \\(w_j^*\\) and calculate \\(\\tilde{\\Psi}_t(q)^*\\) for each leaf, and the resulting minimized objective is\n\\[\n-\\frac{1}{2} \\huge[\\normalsize Q_L + Q_R + \\Gamma \\huge]\\normalsize + \\gamma(J+1)\n\\]\nwhere \\(\\Gamma\\) is the sum of quality scores for all the leaves except \\(L\\) and \\(R\\).\n\n\n\\[\n\\Gamma = \\sum_{j\\ne \\{L, R\\}}^J Q_j\n\\]\nThe minimized objective before splitting is\n\\[\n-\\frac{1}{2} \\huge[\\normalsize Q_s + \\Gamma \\huge]\\normalsize + \\gamma J\n\\]\nSo, the reduction  in loss after the split is\n\\[\nG(s, L, R) = \\frac{1}{2} \\huge[\\normalsize Q_L + Q_R - Q_s \\huge]\\normalsize - \\gamma\n\\]\nLet’s call \\(G(s, L, R)\\) simply a gain (of the split).\n\n\nA more positive value of gain (\\(G(s, L, R)\\)) means a more successful split.\nWe can try many different patterns of \\(I_L\\) and \\(I_R\\) (how to split tree \\(s\\)), calculate the gain for each of them and pick the split that has the highest gain.\n\n\nDifferent patterns of \\(I_L\\) and \\(I_R\\) arise from different variable-cutpoint combinations\nIf the highest gain is negative, then the leaf under consideration for splitting is not split.\nOnce the best tree is chosen (the tree that has the highest gain among the ones investigated), then we update our prediction based on \\(w^*\\) of the tree. For observation \\(i\\) that belongs to leaf \\(j\\) of the tree,\n\\[\n\\begin{aligned}\nf_{i,b} = f_{i,b-1} + \\eta \\cdot w_j^*\n\\end{aligned}\n\\tag{5.7}\\]\nwhere \\(\\eta\\) is the learning rate.\n\n5.4.2 Tree updating in XGB (regression)\nWe now make the general tree updating algorithm specific to regression problems, where the loss function is squared error: \\(L(y_i, p_i) = \\frac{1}{2}(y_i - p_i)^2\\), where \\(p_i\\) is the predicted value for \\(i\\).\n\nFirst, let’s find \\(g_i\\) and \\(h_i\\) for \\(L(y_i, p_i) = \\frac{1}{2}(y_i - p_i)^2\\).\n\\[\n\\begin{aligned}\ng_i = \\frac{\\partial L(y_i, p_i)}{\\partial p_i}  = -(y_i - p_i)\\\\\nh_i = \\frac{\\partial^2 L(y_i, p_i)}{\\partial p_i^2} = 1 \\\\\n\\end{aligned}\n\\]\nSo, \\(g_i\\) is simply the negative of the residual for \\(i\\).\nNow, suppose your are at iteration \\(b\\) and the predicted value for \\(i\\) is denoted as \\(f_{i,b}(x_i)\\). Further, let \\(r_{i,b}\\) denote the residual (\\(y_i - f_{i,b}(x_i)\\)).\nPlugging these into Equation 5.5,\n\\[\n\\begin{aligned}\nw_j^* & = \\frac{\\sum_{i\\in I_j}r_{i,b}}{\\sum_{i\\in I_j}1 + \\lambda} \\\\\n      & = \\frac{\\sum_{i\\in I_j}r_{i,b}}{N_j + \\lambda}\n\\end{aligned}\n\\tag{5.8}\\]\nThat is for a given leaf \\(j\\), the optimal predicted value (\\(w_j^*\\)) is the sum of the residuals of all the observations in leaf \\(j\\) divided by the number of observations in leaf \\(j\\) plus \\(\\lambda\\). When \\(\\lambda = 0\\), the optimal predicted value (\\(w_j^*\\)) is simply the mean of the residuals.\nThe quality score for leaf \\(j\\) is then,\n\\[\nQ_j = \\frac{(\\sum_{i\\in I_j}r_{i,b})^2}{N_j + \\lambda}\n\\tag{5.9}\\]\n\n5.4.3 Illustration of XGB for regression\nIn order to further our understanding of the entire XGB algorithm, let’s take a lookt at a simple regression problem as an illustration. We consider a four-observation data as follows:\n\nCode(\ndata <-\n  data.table(\n    y = c(-3, 7, 8, 12),\n    x = c(1, 4, 6, 8)\n  )\n)\n\n    y x\n1: -3 1\n2:  7 4\n3:  8 6\n4: 12 8\n\n\n\nCode(\ng_0 <-\n  ggplot(data) +\n  geom_point(aes(y = y, x = x))\n)\n\n\n\n\nFirst step (\\(b = 0\\)) is to make an initial prediction. This can be any number, but let’s use the mean of y and set it as the predicted value for all the observations.\n\nCode(\nf_0 <- mean(data$y) # f_0: the predicted value for all the observations\n)\n\n[1] 6\n\n\nLet’s set \\(\\gamma\\), \\(\\lambda\\), and \\(\\eta\\) to \\(10\\), \\(1\\), and \\(0.3\\), respectively.\n\nCodegamma <- 10\nlambda <- 1\neta <- 0.3\n\n\nWe have a single-leaf tree at the moment. And the quality score for this leaf is\n\n\nquality score for leaf \\(j\\) is \\(\\frac{(\\sum_{i\\in I_j}r_{i,b})^2}{N_j + \\lambda}\\)\n\nCode#=== get residuals ===#\ndata[, resid := y - f_0]\n\n#=== get quality score ===#\n(\nq_0 <- (sum(data$resid))^2/(nrow(data) + 1)\n)\n\n[1] 0\n\n\nQuality score of the leaf is 0.\n\n\nSince we are using the mean of \\(y\\) as the prediction, of course, the sum of the residuals is zero, which then means that the quality score is zero.\nNow, we have three potential to split patterns: {x, 2}, {x, 5}, {x, 7}.\n\n\n{x, 2} means the leaf is split into two leaves: \\({x | x <2}\\) and \\({x | x >= 2}\\). Note that any number between \\(1\\) and \\(4\\) will result in the same split results.\nLet’s consider them one by one.\n\n5.4.3.1  Split: {x, 2} \n\nHere are graphical representations of the split:\n\nCodeg_0 +\n  geom_vline(xintercept = 2, color = \"red\") +\n  annotate(\"text\", x = 1.25, y = 6, label = \"leaf L\", color = \"red\") +\n  annotate(\"text\", x = 5, y = 6, label = \"leaf R\", color = \"red\")\n\n\n\n\n\n\n\nCodeDiagrammeR::grViz(\n\"\ndigraph {\n  graph [ranksep = 0.2]\n  node [shape = box]\n    T1R [label = 'L: -9']\n    T1L [label = 'R: 1 , 2 , 6']\n    T0 [label = '-9, 1 , 2 , 6']\n  edge [minlen = 2]\n    T0->T1L\n    T0->T1R\n  { rank = same; T1R; T1L}\n}\n\"\n)\n\n\n\n\n\nLet’s split the data.\n\nCode#=== leaf L ===#\n(\ndata_L_1 <- data[x < 2, ]\n)\n\n    y x resid\n1: -3 1    -9\n\nCode#=== leaf R ===#\n(\ndata_R_1 <- data[x >= 2, ]\n)\n\n    y x resid\n1:  7 4     1\n2:  8 6     2\n3: 12 8     6\n\n\nUsing Equation 5.8,\n\n\n\\(w_j^* = \\frac{\\sum_{i\\in I_j}r_{i,b}}{N_j + \\lambda}\\)\n\nCodew_L <- (sum(data_L_1$resid))/(nrow(data_L_1) + lambda)\nw_R <- (sum(data_R_1$resid))/(nrow(data_R_1) + lambda)\n\n\n\\[\n\\begin{aligned}\nw_L^* & = -9 / (1 + 1) = -4.5 \\\\\nw_R^* & = 1 + 2 + 6 / (3 + 1) = 2.25\n\\end{aligned}\n\\]\nUsing Equation 5.9, the quality scores for the leaves are\n\n\n\\(Q_j = \\frac{(\\sum_{i\\in I_j}r_{i,b})^2}{N_j + \\lambda}\\)\n\nCodeq_L <- (sum(data_L_1$resid))^2/(nrow(data_L_1) + lambda)\nq_R <- (sum(data_R_1$resid))^2/(nrow(data_R_1) + lambda)\n\n\n\n\n\nCodeDiagrammeR::grViz(\n  paste0(\n  \"\n  digraph {\n    graph [ranksep = 0.2]\n    node [shape = box]\n      T1R [label = 'L: -9 \\n Q score = \", round(q_L, digits = 2), \"']\n      T1L [label = 'R: 1 , 2 , 6 \\n Q score = \", round(q_R, digits = 2), \"']\n      T0 [label = '-9, 1 , 2 , 6']\n    edge [minlen = 2]\n      T0->T1L\n      T0->T1R\n    { rank = same; T1R; T1L}\n  }\n  \"\n  )\n\n)\n\n\n\n\n\n\\[\n\\begin{aligned}\nq_L^* & = (-9)^2 / (1 + 1) = 40.5 \\\\\nq_R^* & = (1 + 2 + 6)^2 / (3 + 1) = 20.25\n\\end{aligned}\n\\]\nNotice that residuals are first summed and then squared in the denominator of the quality score (the higher, the better). This means that if the prediction is off in the same direction (meaning they are similar) among the observations within the leaf, then the quality score is higher. On the other hand, if the prediction is off in both directions (meaning they are not similar), then the residuals cancel each other out, resulting in a lower quality score. Since we would like to create leaves consisting of similar observations, a more successful split has a higher quality score.\nFinally, the gain of this split is\n\n\n\\[\nG(s, L, R) = \\frac{1}{2} \\huge[\\normalsize Q_L + Q_R - Q_s \\huge]\\normalsize - \\gamma\n\\]\nwhere \\(s\\) is the leaf before split, \\(L\\) and \\(R\\) are leaves after the split of leaf \\(s\\).\n\nCodegain_1 <- (q_L + q_R - q_0)/2 - gamma\n\n\n\\[\nG_1 = \\frac{40.5 + 20.25 - 0}{2} - 10 = 20.375\n\\]\nNow that we have gone through the process of finding update value (\\(w\\)), quality score (\\(q\\)), and gain (\\(G\\)) for a given split structure, let’s write a function that returns the values of these measures by feeding the cutpoint before moving onto the next split candidate.\n\nCodeget_info <- function(data, cutpoint, lambda, gamma)\n{\n  q_0 <- (sum(data$resid))^2/(nrow(data) + lambda)\n\n  data_L <- data[x < cutpoint, ]\n  data_R <- data[x >= cutpoint, ]\n\n  w_L <- (sum(data_L$resid))/(nrow(data_L) + lambda)\n  w_R <- (sum(data_R$resid))/(nrow(data_R) + lambda)\n\n  q_L <- (sum(data_L$resid))^2/(nrow(data_L) + lambda)\n  q_R <- (sum(data_R$resid))^2/(nrow(data_R) + lambda)\n\n  gain <- (q_L + q_R - q_0)/2 - gamma\n\n  return(list(\n    w_L = w_L, \n    w_R = w_R, \n    q_L = q_L, \n    q_R = q_R, \n    gain = gain \n  ))\n}\n\n\n\n5.4.3.2  Split: {x, 5} \n\n\nCodemeasures_2 <- get_info(data, 5, lambda, gamma)\n\n\n\nCodeg_0 +\n  geom_vline(xintercept = 5, color = \"red\") +\n  annotate(\"text\", x = 3, y = 6, label = \"leaf L\", color = \"red\") +\n  annotate(\"text\", x = 7, y = 6, label = \"leaf R\", color = \"red\")\n\n\n\n\n\n\n\nCodeDiagrammeR::grViz(\n  paste0(\n    \"\n    digraph {\n      graph [ranksep = 0.2]\n      node [shape = box]\n        T1R [label = 'L: -9, 1 \\n Q score = \", round(measures_2$q_L, digits = 2), \"']\n        T1L [label = 'R: 2 , 6 \\n Q score = \", round(measures_2$q_R, digits = 2), \"']\n        T0 [label = '-9, 1 , 2 , 6']\n      edge [minlen = 2]\n        T0->T1L\n        T0->T1R\n      { rank = same; T1R; T1L}\n    }\n    \"\n  )\n)\n\n\n\n\n\n\\[\n\\begin{aligned}\nq_L^* & = (-9)^2 / (2 + 1) = 21.33 \\\\\nq_R^* & = (1 + 2 + 6)^2 / (2 + 1) = 21.33\n\\end{aligned}\n\\]\n\\[\nG_2 = \\frac{21.33 + 21.33 - 0}{2} - 10 = 11.3333333\n\\]\n\n5.4.3.3  Split: {x, 7} \n\n\nCodemeasures_3 <- get_info(data, 7, lambda, gamma)\n\n\n\nCodeg_0 +\n  geom_vline(xintercept = 7, color = \"red\") +\n  annotate(\"text\", x = 4, y = 6, label = \"leaf L\", color = \"red\") +\n  annotate(\"text\", x = 8, y = 6, label = \"leaf R\", color = \"red\")\n\n\n\n\n\n\n\nCodeDiagrammeR::grViz(\n  paste0(\n    \"\n    digraph {\n      graph [ranksep = 0.2]\n      node [shape = box]\n        T1R [label = 'L: -9, 1, 2 \\n Q score = \", round(measures_3$q_L, digits = 2), \"']\n        T1L [label = 'R: 6 \\n Q score = \", round(measures_3$q_R, digits = 2), \"']\n        T0 [label = '-9, 1 , 2 , 6']\n      edge [minlen = 2]\n        T0->T1L\n        T0->T1R\n      { rank = same; T1R; T1L}\n    }\n    \"\n  )\n)\n\n\n\n\n\n\\[\n\\begin{aligned}\nq_L^* & = (-9)^2 / (1 + 1) = 9 \\\\\nq_R^* & = (1 + 2 + 6)^2 / (3 + 1) = 18\n\\end{aligned}\n\\]\n\\[\nG_3 = \\frac{9 + 18 - 0}{2} - 10 = 3.5\n\\]\nAmong all the splits we considered, the first case (Split: {x, 2}) has the highest score. This is easy to confirm visually and shows picking a split based on the gain measure indeed makes sense.\nNow we consider how to split leaf R (leaf L cannot be split further as it has only one observation). We have two split candidates: {x, 5} and {x, 7}. Let’s get the gain measures using get_info().\n\nCode#=== first split ===#\nget_info(data_R_1, 5, lambda, gamma)$gain \n\n[1] -9.208333\n\nCode#=== second split ===#\nget_info(data_R_1, 7, lambda, gamma)$gain\n\n[1] -9.625\n\n\nSo, neither of the splits has a positive gain value. Therefore, we do not adopt either of the splits. For this iteration (\\(b=1\\)), this is the end of tree building.\n\n\n\n\n\n\nNote\n\n\n\nIf the value of \\(\\gamma\\) is lower (say, 0), then we would have adopted the second split.\n\nCodeget_info(data_R_1, 5, lambda, 0)$gain # first split\n\n[1] 0.7916667\n\nCodeget_info(data_R_1, 7, lambda, 0)$gain # second split\n\n[1] 0.375\n\n\nAs you can see, a higher value of \\(\\gamma\\) leads to a more aggressive tree pruning.\n\n\nSo, the final tree for this iteration (\\(b = 1\\)) is\n\nCodeDiagrammeR::grViz(\n  paste0(\n  \"\n  digraph {\n    graph [ranksep = 0.2]\n    node [shape = box, width = 0.3, height = 0.15, fontsize = 3, fixedsize = TRUE, penwidth = 0.2]\n      T1R [label = 'L: -9 \\n w* = \", round(w_L, digits = 2), \"']\n      T1L [label = 'R: 1 , 2 , 6 \\n w* = \", round(w_R, digits = 2), \"']\n      T0 [label = '-9, 1 , 2 , 6']\n    edge [penwidth = 0.2, arrowsize = 0.3, len = 0.3]\n      T0->T1L\n      T0->T1R\n    { rank = same; T1R; T1L}\n  }\n  \"\n  )\n\n)\n\n\n\n\n\nWe now use \\(w^*\\) from this tree to update our prediction according to Equation 5.7.\n\n\n\\(f_{i,b} = f_{i,b-1} + \\eta \\cdot w_j^*\\)\n\nCodemeasures_1 <- get_info(data, 2, lambda, gamma)\n\n\nSince the first observation is in \\(L\\),\n\\[\nf_{i = 1,b = 1} = 6 + 0.3 \\times -4.5 = 4.65\n\\]\nSince the second, third, and fourth observations are in \\(R\\),\n\\[\n\\begin{aligned}\nf_{i = 2,b = 1} = 6 + 0.3 \\times 2.25 = 6.68 \\\\\nf_{i = 3,b = 1} = 6 + 0.3 \\times 2.25  = 6.68\\\\\nf_{i = 4,b = 1} = 6 + 0.3 \\times 2.25 = 6.68\n\\end{aligned}\n\\]\n\nCodedata %>% \n  .[, f_0 := f_0] %>% \n  .[1, f_1 := f_0 + measures_1$w_L * eta] %>%\n  .[2:4, f_1 := f_0 + measures_1$w_R * eta]\n\n\nThe prediction updates can be seen below. Though small, we made small improvements in our prediction.\n\nCodeggplot(data = data) +\n  geom_point(aes(y = y, x = x, color = \"observed\")) +\n  geom_point(aes(y = f_1, x = x, color = \"after (f1)\")) +\n  geom_point(aes(y = f_0, x = x, color = \"before (f0)\")) +\n  scale_color_manual(\n    values = \n      c(\n        \"before (f0)\" = \"blue\", \n        \"after (f1)\" = \"red\",\n        \"observed\" = \"black\"\n      ), \n    name = \"\"\n  ) +\n  geom_segment(\n    aes(y = f_0, x = x, yend = f_1, xend = x), \n    color = \"blue\",\n    arrow = arrow(length = unit(0.1, \"cm\"))\n  ) +\n  theme_bw()\n\n\n\n\nNow, we move on to \\(b=2\\). We first update residuals:\n\nCodedata[, resid := y - f_1]\n\ndata\n\n    y x  resid f_0   f_1\n1: -3 1 -7.650   6 4.650\n2:  7 4  0.325   6 6.675\n3:  8 6  1.325   6 6.675\n4: 12 8  5.325   6 6.675\n\n\nJust like at \\(b=1\\), all the possible splits are {x, 2}, {x, 5}, {x, 7}. Let’s find the gain for each split.\n\nCodelapply(\n  c(2, 5, 7),\n  function(x) get_info(data, x, lambda, gamma)$gain\n)\n\n[[1]]\n[1] 10.66639\n\n[[2]]\n[1] 6.267458\n\n[[3]]\n[1] 1.543344\n\n\nSo, the first split is again the best split. Should we split the right leaf, which has the observations except the first one?\n\nCodelapply(\n  c(5, 7),\n  function(x) get_info(data[2:3, ], x, lambda, gamma)$gain\n)\n\n[[1]]\n[1] -9.988437\n\n[[2]]\n[1] -10\n\n\nAll the splits have negative gains. So, we do not split this leaf just like at \\(b=1\\).\nSo, the final tree for this iteration (\\(b = 1\\)) is\n\nCodemeasures_b2 <- get_info(data, 2, lambda, gamma)\n\n#| code-fold: true\n#| fig-height: 2\n#| fig-width: 4\n\nDiagrammeR::grViz(\n  paste0(\n  \"\n  digraph {\n    graph [ranksep = 0.2]\n    node [shape = box, width = 0.4, height = 0.15, fontsize = 3, fixedsize = TRUE, penwidth = 0.2]\n      T1R [label = 'L: -8.18 \\n w* = \", round(measures_b2$w_L, digits = 2), \"']\n      T1L [label = 'R: 0.71 , 1.71 , 5.71 \\n w* = \", round(measures_b2$w_R, digits = 2), \"']\n      T0 [label = '-8.18, 0.71 , 1.71 , 5.71']\n    edge [penwidth = 0.2, arrowsize = 0.3, len = 0.3]\n      T0->T1L\n      T0->T1R\n    { rank = same; T1R; T1L}\n  }\n  \"\n  )\n\n)\n\n\n\n\n\nLet’s now update our predictions.\n\nCodedata %>% \n  .[1, f_2 := f_1 + measures_b2$w_L * eta] %>%  \n  .[2:4, f_2 := f_1 + measures_b2$w_R * eta] \n\n\n\nCodeggplot(data = data) +\n  geom_point(aes(y = y, x = x, color = \"observed\")) +\n  geom_point(aes(y = f_2, x = x, color = \"f2\")) +\n  geom_point(aes(y = f_1, x = x, color = \"f1\")) +\n  geom_point(aes(y = f_0, x = x, color = \"f0\")) +\n  scale_color_manual(\n    values = \n      c(\n        \"f0\" = \"blue\", \n        \"f1\" = \"red\",\n        \"f2\" = \"red\",\n        \"observed\" = \"black\"\n      ), \n    name = \"\"\n  ) +\n  geom_segment(\n    aes(y = f_0, x = x, yend = f_1, xend = x), \n    color = \"blue\",\n    arrow = arrow(length = unit(0.1, \"cm\"))\n  ) +\n  geom_segment(\n    aes(y = f_1, x = x, yend = f_2, xend = x), \n    color = \"blue\",\n    arrow = arrow(length = unit(0.1, \"cm\"))\n  ) +\n  theme_bw()\n\n\n\n\nAgain, we made small improvements in our predictions. This process continues until user-specified stopping criteria is met.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\\(\\lambda\\):\n\nA higher value of \\(\\lambda\\) leads to a lower value of prediction updates (\\(w^*\\)).\nA higher value of \\(\\lambda\\) leads to a lower value of quality score (\\(Q\\)), thus leading to a lower value of gain (\\(G\\)), which then leads to more aggressive pruning for a given value of \\(\\gamma\\).\n\n\n\n\\(\\gamma\\):\n\nA higher value of \\(\\gamma\\) leads to more aggressive pruning.\n\n\n\n\\(\\eta\\):\n\nA higher value of \\(\\eta\\) leads to faster learning.\n\n\n\n\n\n\n5.4.4 Implementation\n\nCodelibrary(xgboost)\n\n\nAttaching package: 'xgboost'\n\n\nThe following object is masked from 'package:rattle':\n\n    xgboost\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\nCodemlb1_dm_X <- \n  xgb.DMatrix(\n    data = as.matrix(mlb1_dt[, .(hruns, years, rbisyr, allstar, runsyr, hits, bavg)]),\n    label = as.matrix(mlb1_dt[, lsalary])\n  )\n\nxgb.train(\n  data = mlb1_dm_X, # independent variable\n  nrounds = 100,\n  eta = 1,\n  objective = \"reg:squarederror\"\n\n)\n\n##### xgb.Booster\nraw: 146.1 Kb \ncall:\n  xgb.train(data = mlb1_dm_X, nrounds = 100, eta = 1, objective = \"reg:squarederror\")\nparams (as set within xgb.train):\n  eta = \"1\", objective = \"reg:squarederror\", validate_parameters = \"TRUE\"\nxgb.attributes:\n  niter\ncallbacks:\n  cb.print.evaluation(period = print_every_n)\n# of features: 7 \nniter: 100\nnfeatures : 7 \n\n\n\n\nR\nPython\n\n\n\nBluf\n\nCodemtcars\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\n\n\nsfkewjf"
  },
  {
    "objectID": "causal-ml.html",
    "href": "causal-ml.html",
    "title": "Causal Machine Learning (CML) Methods",
    "section": "",
    "text": "\\[\nTE(X) = \\theta(X)\\cdot T\n\\]\n\\(\\theta(X)\\) is the impact of the treatment when \\(T\\) is binary and marginal impact of the treatment when \\(T\\) is continuous. \\(\\theta(X)\\) is a function of attributes (\\(X\\)), meaning that the impact of the treatment varies (heterogeneous) based on the value of the attributes.\n\n\n\\(T\\) is 0 if not treated, 1 if treated.\nCML considers the following model (following the documentation of the econml Python package)\n\\[\n\\begin{aligned}\nY & = \\theta(X)\\cdot T + g(X, W) + \\varepsilon \\\\\nT & = f(X, W) + \\eta\n\\end{aligned}\n\\]\n\\(W\\) are the collection of attributes that affect \\(Y\\) along with \\(X\\) (represented by \\(g(X, W)\\)), but not as drivers of the heterogeneity in the impact of the treatment. \\(X\\) not just affects \\(Y\\) as drivers of the heterogeneity in the impact of the treatment (\\(\\theta(X)\\cdot T\\)), but also directly along with \\(W\\).\nBoth \\(X\\) and \\(W\\) are potential confounders. While we do control for them (eliminating their influence) by partialing out \\(f(X, W)\\) and \\(g(X, W)\\), the sole focus is on the estimation of \\(\\theta(X)\\). This is in stark contrast to the focus of the ML methods we have seen in earlier sections, which primarily focuses on the accurate prediction of the  level of the dependent variable, rather than how the level of the dependent variable  changes  when treated like CML methods.\nUnderstanding the how treatment effects vary can be highly valuable in many circumstances.\n Example 1:  If we come to know a particular drug is effective on elderly people but detrimental to kids, then doctors can make a smart decision of prescribing the drug to elderly people, but not to kids.\n\n\nIn this example, the heterogeneity driver (\\(X\\)) is age.\n Example 3:  If we come to know fertilizer is more effective in increasing corn yield in soil type A than B, then farmers can apply more fertilized on the parts of the field where soil type is A but less on where soil type is B.\n\n\nIn this example, the heterogeneity driver (\\(X\\)) is soil type.\nAs you can see these examples, knowledge on the heterogeneity of the treatment effects and its causes can help decision makers smart-target the treatment."
  },
  {
    "objectID": "cf.html",
    "href": "cf.html",
    "title": "6  Causal Forest",
    "section": "",
    "text": "blur blur"
  },
  {
    "objectID": "bootstrap.html#how-it-works",
    "href": "bootstrap.html#how-it-works",
    "title": "\n4  Bootstrap\n",
    "section": "\n4.2 How it works",
    "text": "4.2 How it works\nHere are the general steps of a bootstrap:\n\nStep 1: Sample the data with replacement (You can sample the same observations more than one times. You draw a ball and then you put it back in the box.)\nStep 2: Run a statistical analysis to estimate whatever quantity you are interested in estimating\nRepeat Steps 1 and 2 many times and store the estimates\nDerive uncertainty measures from the collection of estimates obtained above\n\nLet’s demonstrate this using a very simple linear regression example.\nHere is the data generating process:\n\nCodeset.seed(89343)\nN <- 100\nx <- rnorm(N)\ny <- 2 * x + 2 * rnorm(N)\n\n\nWe would like to estimate the coefficient on \\(x\\) by applying OLS to the following model:\n\\[\ny = \\alpha + \\beta_x + \\mu\n\\]\nWe know from the econometric theory class that the SE of \\(\\hat{\\beta}_{OLS}\\) is \\(\\frac{\\sigma}{\\sqrt{SST_x}}\\), where \\(\\sigma^2\\) is the variance of the error term (\\(\\mu\\)) and \\(SST_x = \\sum_{i=1}^N (x_i - \\bar{x})^2\\) (\\(\\bar{x}\\) is the mean of \\(x\\)).\n\nCodemean_x <- mean(x)\nsst_x <- ((x-mean(x))^2) %>% sum()\n(\nse_bhat <- sqrt(4 / sst_x)\n)\n\n[1] 0.217933\n\n\nSo, we know that the true SE of \\(\\hat{\\beta}_{OLS}\\) is 0.217933. There is not really any point in using bootstrap in this case, but this is a good example to see if bootstrap works or not.\nLet’s implement a single iteration of the entire bootstrap steps (Steps 1 and 2).\n\nCode#=== set up a dataset ===#\ndata <-\n  data.table(\n    y = y,\n    x = x\n  )\n\n\nNow, draw observations with replacement so the resulting dataset has the same number of observations as the original dataset.\n\nCodenum_obs <- nrow(data)\n\n#=== draw row numbers ===#\n(\nrow_indices <- sample(seq_len(num_obs), num_obs, replace = TRUE)\n)\n\n  [1] 29 56 39 83 52 66 70 19  4 34 28 34 81 32  9 95 99 86  4 79 30 15 41 97 43\n [26] 89 60 41 16 19 66 96 34 91 86 67 75 28 74 50 71 95 74 87 58 27  9 65 80 41\n [51] 71 64 21 47 45 77 97 94 72 50 23 10 33 45 14 17 82 56 33 75 70 63 78 81 64\n [76] 16 84 90  2 17  5 46 53 37 93 85 72 63 10 35 42 20 70 49 74 32 25 73 76 32\n\n\nUse the sampled indices to create a bootstrapped dataset:\n\nCodetemp_data <- data[row_indices, ]\n\n\nNow, apply OLS to get a coefficient estimate on \\(x\\) using the bootstrapped dataset.\n\nCodelm(y ~ x, data = temp_data)$coefficient[\"x\"]\n\n       x \n2.040957 \n\n\nThis is the end of Steps 1 and 2. Now, let’s repeat this step 1000 times. First, we define a function that implements Steps 1 and 2.\n\nCodeget_beta <- function(i, data)\n{\n  num_obs <- nrow(data)\n\n  #=== sample row numbers ===#\n  row_indices <- sample(seq_len(num_obs), num_obs, replace = TRUE)\n\n  #=== bootstrapped data ===#\n  temp_data <- data[row_indices, ]\n\n  #=== get coefficient ===#\n  beta_hat <- lm(y ~ x, data = temp_data)$coefficient[\"x\"]\n\n  return(beta_hat)\n}\n\n\nNow repeat get_beta() many times:\n\nCodebeta_store <-\n  lapply(\n    1:1000,\n    function(x) get_beta(x, data)\n  ) %>% \n  unlist()\n\n\nCalculate standard deviation of \\(\\hat{\\beta}_{OLS}\\),\n\nCodesd(beta_store)\n\n[1] 0.2090611\n\n\nNot, bad. What if we make the number of observations to 1000 instead of 100?\n\nCodeset.seed(67343)\n\n#=== generate data ===#\nN <- 1000\nx <- rnorm(N)\ny <- 2 * x + 2 * rnorm(N)\n\n#=== set up a dataset ===#\ndata <-\n  data.table(\n    y = y,\n    x = x\n  )\n\n#=== true SE ===#\nmean_x <- mean(x)\nsst_x <- sum(((x-mean(x))^2))\n(\nse_bhat <- sqrt(4 / sst_x)\n)\n\n[1] 0.06243842\n\n\n\nCode#=== bootstrap-estimated SE ===#\nbeta_store <-\n  lapply(\n    1:1000,\n    function(x) get_beta(x, data)\n  ) %>% \n  unlist()\n\nsd(beta_store)\n\n[1] 0.06147708\n\n\nThis is just a single simulation. So, we cannot say bootstrap works better when the number of sample size is larger only from these experiments. But, it is generally true that bootstrap indeed works better when the number of sample size is larger."
  },
  {
    "objectID": "bootstrap.html#more-complicated-example",
    "href": "bootstrap.html#more-complicated-example",
    "title": "\n4  Bootstrap\n",
    "section": "\n4.3 More complicated example",
    "text": "4.3 More complicated example\nConsider a simple production function (yield response functions for agronomists):\n\\[\ny = \\beta_1 x + \\beta_2 x^2 + \\mu\n\\]\n\n\n\\(y\\): output\n\n\\(x\\): input\n\n\\(\\mu\\): error\n\nThe price of \\(y\\) is 5 and the cost of \\(x\\) is 2. Your objective is to identify the amount of input that maximizes profit. You do not know \\(\\beta_1\\) and \\(\\beta_2\\), and will be estimating them using the data you have collected. Letting \\(\\hat{\\beta_1}\\) and \\(\\hat{\\beta_2}\\) denote the estimates of \\(\\beta_1\\) and \\(\\beta_2\\), respectively, the mathematical expression of the optimization problem is:\n\\[\nMax_x 5(\\hat{\\beta}_1 x + \\hat{\\beta}_2 x^2) - 2 x\n\\]\nThe F.O.C is\n\\[\n5\\hat{\\beta}_1 + 10 \\hat{\\beta}_2 x - 2 = 0\n\\]\nSo, the estimated profit-maximizing input level is \\(\\hat{x}^* = \\frac{2-5\\hat{\\beta}_1}{10\\hat{\\beta}_2}\\). What we are interested in knowing is the SE of \\(x^*\\). As you can see, it is a non-linear function of the coefficients, which makes it slightly harder than simply getting the SE of \\(\\hat{\\beta_1}\\) or \\(\\hat{\\beta_2}\\). However, bootstrap can easily get us an estimate of the SE of \\(\\hat{x}^*\\)1. The bootstrap process will be very much the same as the first bootstrap example except that we will estimate \\(x^*\\) in each iteration instead of stopping at estimating just coefficients. Let’s work on a single iteration first.\nHere is the data generating process:\n\nCodeset.seed(894334)\n\nN <-  1000\nx <-  runif(N) * 3\ney <- 6 * x - 2 * x^2\nmu <- 2 * rnorm(N)\ny <- ey + mu\n\ndata <- \n  data.table(\n    x = x,\n    y = y,\n    ey = ey\n  )\n\n\nUnder the data generating process, here is the production function looks like:\n\nCodeggplot(data = data) +\n  geom_line(aes(y = ey, x = x))\n\n\n\n\n\nCodenum_obs <- nrow(data)\nrow_indices <- sample(seq_len(num_obs), num_obs, replace = TRUE)\nboot_data <- data[row_indices, ]\nreg <- lm(y ~ x + I(x^2), data = boot_data)\n\n\nNow that we have estimated \\(\\beta_1\\) and \\(\\beta_2\\), we can easily estimate \\(x^*\\) using its analytical formula.\n\nCode(\nx_star <- (2 - 5 * reg$coef[\"x\"])/ (10 * reg$coef[\"I(x^2)\"])\n)\n\n      x \n1.40625 \n\n\nWe can repeat this many times to get a collection of \\(x^*\\) estimates and calculate the standard deviation.\n\nCodeget_x_star <- function(i)\n{\n  row_indices <- sample(seq_len(num_obs), num_obs, replace = TRUE)\n  boot_data <- data[row_indices, ]\n  reg <- lm(y ~ x + I(x^2), data = boot_data)\n  x_star <- (2 - 5 * reg$coef[\"x\"])/ (10 * reg$coef[\"I(x^2)\"])\n}\n\n\n\nCodex_stars <- \n  lapply(\n    1:1000,\n    get_x_star\n  ) %>%\n  unlist()\n\n\nHere is the histogram:\n\nCodehist(x_stars, breaks = 30)\n\n\n\n\nSo, it seems to follow a normal distribution. You can get standard deviation of x_stars as an estimate of the SE of \\(\\hat{x}^*\\).\n\nCodesd(x_stars)\n\n[1] 0.01783721\n\n\nYou can get the 95% confidence interval (CI) like below:\n\nCodequantile(x_stars, prob = c(0.025, 0.975))\n\n    2.5%    97.5% \n1.364099 1.430915"
  },
  {
    "objectID": "bootstrap.html#one-more-example-with-a-non-parametric-model",
    "href": "bootstrap.html#one-more-example-with-a-non-parametric-model",
    "title": "\n4  Bootstrap\n",
    "section": "\n4.4 One more example with a non-parametric model",
    "text": "4.4 One more example with a non-parametric model\nWe now demonstrate how we can use bootstrap to get an estimate of the SE of \\(\\hat{x}^*\\) when we use random forest (RF) as our regression method instead of OLS. When RF is used, we do not have any coefficients like the OLS case above. Even then, bootstrap allows us to estimate the SE of \\(\\hat{x}^*\\).\nThe procedure is exactly the same except that we use RF to estiamte the production function and also that we need to conduct numerical optimization as no analytical formula is available unlike the case above.\nWe first implement a single iteration.\n\nCode#=== get bootstrapped data ===#\nrow_indices <- sample(seq_len(num_obs), num_obs, replace = TRUE)\nboot_data <- data[row_indices, ]\n\n#=== train RF ===#\nreg_rf <- ranger(y ~ x, data = boot_data)\n\n\nOnce you train RF, we can predict yield at a range of values of \\(x\\), calculate profit, and then pick the value of \\(x\\) that maximizes the estimated profit. Here is what the estimated production function looks like:\n\nCode#=== create series of x values at which yield will be predicted ===#\neval_data <- data.table(x = seq(0, 3, length = 1000))\n\n#=== predict yield based on the trained RF ===#\neval_data[, y_hat := predict(reg_rf, eval_data)$predictions]\n\n#=== plot ===#\nggplot(data = eval_data) +\n  geom_line(aes(y = y_hat, x = x))\n\n\n\n\nWell, it is very spiky (we need to tune hyper-parameters using KCV. But, more on this later. The quality of RF estimation has nothing to do with the goal of this section).\nWe can now predict profit at each value of \\(x\\).\n\nCode#=== calculate profit ===#\neval_data[, profit_hat := 5 * y_hat - 2 * x]\n\nhead(eval_data)\n\n             x      y_hat profit_hat\n1: 0.000000000 -0.4807528  -2.403764\n2: 0.003003003 -0.4807528  -2.409770\n3: 0.006006006 -0.4807528  -2.415776\n4: 0.009009009  1.0193781   5.078872\n5: 0.012012012  1.0193781   5.072866\n6: 0.015015015  1.2466564   6.203252\n\n\nThe only thing left for us to do is to find the \\(x\\) value that maximizes profit.\n\nCodeeval_data[which.max(profit_hat), ]\n\n          x    y_hat profit_hat\n1: 1.273273 8.675033   40.82862\n\n\nOkay, so 1.2732733 is the \\(\\hat{x}^*\\) from this iteration.\nAs you might have guessed already, we can just repeat this step to get an estimate of the SE of \\(\\hat{x}^*_{RF}\\).\n\nCodeget_x_star_rf <- function(i)\n{\n  print(i) # progress tracker\n  \n  #=== get bootstrapped data ===#\n  row_indices <- sample(seq_len(num_obs), num_obs, replace = TRUE)\n  boot_data <- data[row_indices, ]\n\n  #=== train RF ===#\n  reg_rf <- ranger(y ~ x, data = boot_data)\n\n  #=== create series of x values at which yield will be predicted ===#\n  eval_data <- data.table(x = seq(0, 3, length = 1000))\n\n  #=== predict yield based on the trained RF ===#\n  eval_data[, y_hat := predict(reg_rf, eval_data)$predictions]\n  \n  #=== calculate profit ===#\n  eval_data[, profit_hat := 5 * y_hat - 2 * x]\n  \n  #=== find x_star_hat ===#\n  x_star_hat <- eval_data[which.max(profit_hat), x]\n  \n  return(x_star_hat)\n}\n\n\n\nCodex_stars_rf <- \n  mclapply(\n    1:1000,\n    get_x_star_rf,\n    mc.cores = 12\n  ) %>%\n  unlist()\n\n#=== Windows user ===#\n# library(future.apply)\n# plan(\"multisession\", workers = detectCores() - 2)\n# x_stars_rf <- \n#   future_lapply(\n#     1:1000,\n#     get_x_star_rf\n#   ) %>%\n#   unlist()\n\n\nHere are the estimate of the SE of \\(\\hat{x}^*_{RF}\\) and 95% CI.\n\nCodesd(x_stars_rf)\n\n[1] 0.4091596\n\nCodequantile(x_stars_rf, prob = c(0.025, 0.975))\n\n     2.5%     97.5% \n0.5465465 2.0570571 \n\n\nAs you can see, the estimation of \\(x^*\\) is much more inaccurate than the previous OLS approach. This is likely due to the fact that we are not doing a good job of tuning the hyper-parameters of RF (but, again, more on this later).\nThis conclude the illustration of the power of using bootstrap to estimate the uncertainty of the statistics of interest (\\(x^*\\) here) when the analytical formula of the statistics is non-linear or not even known."
  },
  {
    "objectID": "reticulate.html",
    "href": "reticulate.html",
    "title": "Appendix B — Use Python from within R using the reticulate package",
    "section": "",
    "text": "class: middle"
  },
  {
    "objectID": "reticulate.html#python-set-up",
    "href": "reticulate.html#python-set-up",
    "title": "Appendix B — Use Python from within R using the reticulate package",
    "section": "B.1 Python set up",
    "text": "B.1 Python set up"
  },
  {
    "objectID": "reticulate.html#reticulate",
    "href": "reticulate.html#reticulate",
    "title": "Appendix B — Use Python from within R using the reticulate package",
    "section": "B.2 Reticulate",
    "text": "B.2 Reticulate"
  },
  {
    "objectID": "cross-validation.html#motivations",
    "href": "cross-validation.html#motivations",
    "title": "\n2  Cross-validation based on mean squared error (MSE)\n",
    "section": "\n2.1 Motivations",
    "text": "2.1 Motivations\nNo model works the best all the time, and searching for the best modeling approach and specifications is an essential part of modeling applications.\nFor example, we may consider five approaches with varying modeling specifications for each of the approaches:\n\nRandom Forest (RF)\n\nnumber of trees (1000, 2000)\nnumber of variables used in each tree (3, 5, 8)\nand many other hyper parameters\n\n\nLASSO\n\npenalty parameter (1, 2, 3, etc)\n\n\nGAM\n\nnumber of knots\npenalty parameter\n\n\nBoosted Regression Forest (BRF)\n\nnumber of trees (1000, 2000)\nnumber of variables used in each tree (3, 5, 8)\nand many other hyper parameters\n\n\nConvolutional Neural Network (CNN)\n\nconvolution matrix dimension\nthe order of convolution\nlearning rate\nand many other hyper parameters\n\n\n\nOur goal here is to find the model that would performs the best when applied to the data that has not been seen yet.\nWe saw earlier that training MSE is not appropriate for that purpose as picking the model with the lowest training MSE would very much likely to lead you to the over-fitted model. In this lecture, we consider a better way of selecting a model using only train data."
  },
  {
    "objectID": "cross-validation.html#leave-one-out-cross-validation-loocv",
    "href": "cross-validation.html#leave-one-out-cross-validation-loocv",
    "title": "\n2  Cross-validation based on mean squared error (MSE)\n",
    "section": "\n2.2 Leave-One-Out Cross-Validation (LOOCV)",
    "text": "2.2 Leave-One-Out Cross-Validation (LOOCV)\nConsider a dataset: \\(D = \\{X_1, y_1\\}, \\{X_2, y_2\\}, \\dots, \\{X_N, y_N\\}\\), where \\(X_i\\) is a collection of features and \\(y_i\\) is the dependent variable for \\(i\\)th observation. Further, suppose you use \\(D\\) for training ML models and \\(\\hat{f}()\\) is a trained model.\nLOOCV leaves out a single observation (say \\(i\\)), and train a model (say, GAM with the number of knots of 10) using the all the other observations (-\\(i\\)), and then find MSE for the left-out observation. This process is repeated for all the observations, and then the average of the individual MSEs is calculated.\n\n2.2.1 R demonstration using mgcv::gam()\n\nLet’s demonstrate this using R. Here is the dataset we use.\n\nCodeset.seed(943843)\n\n# define the data generating process\n# x fixed\ngen_data <- function(x) {\n  \n  ey <- (x - 2.5)^3 # E[y|x]\n  y <- ey + 3 * rnorm(length(x)) # y = E[y|x] + u\n  return_data <- data.table(y = y, x = x, ey = ey)\n\n  return(return_data)\n}\n\n## generate train data\ndata <- gen_data(x = runif(100) * 5)\n\n\nVisually, here is the relationship between \\(E[y]\\) and \\(x\\):\n\nCodeggplot(data = data) +\n  geom_line(aes(y = ey, x = x))\n\n\n\n\nFor example, for the case where the first observation is left out for validation,\n\nCode# leave out the first observation\nleft_out_observation <- data[1, ]\n\n# all the rest\ntrain_data <- data[-1, ]\n\n\nNow we train a gam model using the train_data, predict \\(y\\) for the first observation, and find the MSE.\n\nCode#=== train the model ===#\nfitted <- gam(y ~ s(x, k = 10), sp = 0, data = train_data)\n\n#=== predict y for the first observation ===#\ny_fitted <- predict(fitted, newdata = left_out_observation)\n\n#=== get MSE ===#\nMSE <- (left_out_observation[, y] - y_fitted) ^ 2\n\n\nAs described above, LOOCV repeats this process for every single observation of the data. Now, let’s write a function that does the above process for any \\(i\\) you specify.\n\nCode#=== define the modeling approach ===#\ngam_k_10 <- function(train_data) \n{\n  gam(y ~ s(x, k = 30), sp = 0, data = train_data)\n}\n\n#=== define the process of getting MSE for ith observation ===#\nget_mse <- function(i, model)\n{\n  left_out_observation <- data[i, ]\n\n  # all the rest\n  train_data <- data[-i, ]\n\n  #=== train the model ===#\n  fitted <- model(train_data)\n\n  #=== predict y for the first observation ===#\n  y_fitted <- predict(fitted, newdata = left_out_observation)\n\n  #=== get MSE ===#\n  MSE <- (left_out_observation[, y] - y_fitted) ^ 2 \n\n  return(MSE)\n} \n\n\nFor example, this gets MSE for the 10th observation.\n\nCodeget_mse(10, gam_k_10)\n\n       1 \n1.523446 \n\n\nLet’s now loop over \\(i = 1:100\\).\n\nCodemse_indiv <-\n  lapply(\n    1:100,\n    function(x) get_mse(x, gam_k_10)\n  ) %>% \n  #=== list to a vector ===#\n  unlist() \n\n\nHere is the distribution of MSEs.\n\nCodehist(mse_indiv)\n\n\n\n\nWe now get the average MSE.\n\nCodemse_average <- mean(mse_indiv)\n\n\n\n2.2.2 Selecting the best GAM specification: Illustration\nNow, let’s try to find the best (among the ones we try) GAM specification using LOOCV. We will try ten different GAM specifications which vary in penalization parameter. Penalization parameter can be set using the sp option for mgcv::gam(). A greater value of sp leads to a more smooth fitted curve.\n\nCodespecify_gam <- function(sp) {\n  function(train_data) {\n    gam(y ~ s(x, k = 30), sp = sp, data = train_data)\n  }\n}\n\nget_mse_by_sp <- function(sp)\n{\n\n  temp_gam <- specify_gam(sp)\n\n  mse_indiv <-\n    lapply(\n      1:100,\n      function(x) get_mse(x, temp_gam)\n    ) %>% \n    #=== list to a vector ===#\n    unlist() %>% \n    mean()\n\n  return_data <-\n    data.table(\n      mse = mse_indiv,\n      sp = sp\n    )\n\n  return(return_data)\n}\n\n\nFor example, the following code gets you the average MSE for sp \\(= 3\\).\n\nCodeget_mse_by_sp(3)\n\n        mse sp\n1: 11.56747  3\n\n\nNow, let’s loop over ten values of sp: 0, 0.2, 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2.\n\nCode(\nmse_data <- \n  lapply(\n    seq(0, 2, by = 0.2),\n    function(x) get_mse_by_sp(x)\n  ) %>% \n  rbindlist()\n)\n\n          mse  sp\n 1: 12.460156 0.0\n 2:  9.909992 0.2\n 3:  9.957858 0.4\n 4: 10.049327 0.6\n 5: 10.164749 0.8\n 6: 10.293142 1.0\n 7: 10.427948 1.2\n 8: 10.565081 1.4\n 9: 10.701933 1.6\n10: 10.836829 1.8\n11: 10.968701 2.0\n\n\n\n\n\nSo, according to the LOOCV, we should pick sp \\(= 0.2\\) as the penalty parameter.\nNow, that we know sp \\(= 0.2\\) produces the lowest LOOCV MSE, we rerun gam() using the entire dataset (not leaving out any of the observations) and make it our final trained model.\n\nCodefinal_gam_spec <- specify_gam(sp = 1)\n\nfit_gam <- final_gam_spec(train_data)\n\n\nHere is what the fitted curve looks like:\n\nCodeplot(fit_gam)\n\n\n\n\nLooks good. By the way, here are the fitted curves for some other sp values.\n\nCodefitted_curves <- \n  lapply(\n    c(0, 0.6, 1, 2),\n    function(x) {\n      temp_gam <- specify_gam(sp = x)\n      fit_gam <- temp_gam(train_data)   \n    }\n  )  \n\nfor (plot in fitted_curves) {\n  plot(plot)\n}\n\n\n\n\n\n(a) k = 0\n\n\n\n\n\n\n(b) k = 0.6\n\n\n\n\n\n\n\n\n(c) k = 1\n\n\n\n\n\n\n(d) k = 2\n\n\n\n\nFigure 2.1: Fitted curves at various penalization parameters\n\n\n\nLOOCV is perfectly general and can be applied to any statistical methods. However, it can be extremely computationally burdensome because you need to fit the same model for as many as the number of observations. So, if you have 10,000 observations, then you need to fit the model 10,000 times, which can take a long long time.\n\n2.2.3 Summary\n\n\n\n\n\n\nNote\n\n\n\nLOOCV is perfectly general and can be applied to any statistical methods.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nLOOCV can be highly computation-intensive when the dataset is large"
  },
  {
    "objectID": "cross-validation.html#k-fold-cross-validation-kcv",
    "href": "cross-validation.html#k-fold-cross-validation-kcv",
    "title": "\n2  Cross-validation based on mean squared error (MSE)\n",
    "section": "\n2.3 K-fold Cross-Validation (KCV)",
    "text": "2.3 K-fold Cross-Validation (KCV)\nKCV is a type of cross-validation that overcomes the LOOCV’s drawback of being computationally too intensive when the dataset is large. KCV first splits the entire dataset intro \\(K\\) folds (K groups) randomly. It then leaves out a chunk of observations that belongs to a fold (group), trains the model using the rest of the observations in the other folds, evaluate the trained model using the left-out group. It repeats this process for all the groups and average the MSEs obtained for each group.\nLet’s demonstrate this process using R.\n\nCodeset.seed(89534)\ndata <- gen_data(x = runif(500) * 5)\n\n\nYou can use caret::createFolds() to split the data into groups.\n\nCode#=== split into 5 groups ===#\nfolds <- caret::createFolds(data$y, k = 5, list = TRUE, returnTrain = FALSE)\n\n\nThe resulting output is a list of indices for each fold. For example, fold 1 consists of observations whose row indices are the following:\n\nCodefolds[[1]]\n\n  [1]   2   4   7  10  17  22  24  27  45  63  67  72  77  90  95 110 112 113\n [19] 115 119 121 127 136 137 146 155 167 169 176 180 181 186 199 212 215 221\n [37] 224 229 231 234 235 240 243 261 262 275 283 285 286 291 300 308 309 312\n [55] 313 321 327 347 348 353 354 359 360 361 366 367 368 370 372 377 378 382\n [73] 384 399 402 409 410 418 420 421 428 432 433 438 444 451 456 457 461 465\n [91] 468 474 475 479 483 485 487 490 492 498\n\n\nNow, let’s get MSE for the first fold.\n\nCode#=== define test and train data ===#\ntest_data <- data[folds[[1]], ]\ntrain_data <- data[-folds[[1]], ]\n\n#=== train the model ===#\nfitted_model <- gam(y ~ s(x, k = 30), sp = 0, data = train_data)\n\n#=== predict y for the test data ===#\ny_hat <- predict(fitted_model, test_data)\n\n#=== calculate MSE for the fold ===#\n(test_data[, y] - y_hat)^2 %>% mean()\n\n[1] 10.84234\n\n\nNow that we know how to get MSE for a single fold, let’s loop over folds and get MSE for each of the folds. We first create a function that gets us MSE for a single fold.\n\nCodeget_mse_by_fold <- function(data, fold, model)\n{\n  test_data <- data[folds[[fold]], ]\n  train_data <- data[-folds[[fold]], ]\n\n  #=== train the model ===#\n  fitted_model <- model(train_data)\n\n  #=== predict y for the test data ===#\n  y_hat <- predict(fitted_model, test_data)\n\n  #=== calculate MSE for the fold ===#\n  mse <- (test_data[, y] - y_hat)^2 %>% mean() \n\n  return_data <- \n    data.table(\n      k = fold, \n      mse = mse\n    )\n\n  return(return_data)\n}\n\n\nThis will get you MSE for the third fold.\n\nCodeget_mse_by_fold(data, 3, gam_k_10)\n\n   k      mse\n1: 3 10.35178\n\n\nNow, let’s loop over the folds.\n\nCodemse_all <-\n  lapply(\n    seq_len(length(folds)),\n    function(x) get_mse_by_fold(data, x, gam_k_10)\n  ) %>% \n  rbindlist()\n\n\nBy averaging MSE values, we get\n\nCodemse_all[, mean(mse)]\n\n[1] 10.63968\n\n\n\n2.3.1 Selecting the best GAM specification: Illustration\nJust like we found the best gam specification (choice of penalization parameter), we do the same now using KCV.\n\nCodeget_mse_by_sp_kcv <- function(sp)\n{\n\n  temp_gam <- specify_gam(sp)\n\n  mse_by_k <-\n    lapply(\n      seq_len(length(folds)),\n      function(x) get_mse_by_fold(train_data, x, temp_gam)\n    ) %>% \n    rbindlist()\n\n  return_data <-\n    mse_by_k %>% \n    .[, sp := sp]\n\n  return(return_data[])\n}\n\n\nFor example, the following code gets you the MSE for all the folds for sp \\(= 3\\).\n\nCodeget_mse_by_sp_kcv(3)\n\nWarning in `[.data.table`(data, -folds[[fold]], ): Item 75 of i is -402 but\nthere are only 400 rows. Ignoring this and 25 more like it out of 100.\n\n\nWarning in `[.data.table`(data, -folds[[fold]], ): Item 87 of i is -404 but\nthere are only 400 rows. Ignoring this and 13 more like it out of 100.\n\n\nWarning in `[.data.table`(data, -folds[[fold]], ): Item 77 of i is -406 but\nthere are only 400 rows. Ignoring this and 23 more like it out of 100.\n\n\nWarning in `[.data.table`(data, -folds[[fold]], ): Item 82 of i is -401 but\nthere are only 400 rows. Ignoring this and 18 more like it out of 100.\n\n\nWarning in `[.data.table`(data, -folds[[fold]], ): Item 84 of i is -407 but\nthere are only 400 rows. Ignoring this and 16 more like it out of 100.\n\n\n   k mse sp\n1: 1  NA  3\n2: 2  NA  3\n3: 3  NA  3\n4: 4  NA  3\n5: 5  NA  3\n\n\nNow, let’s loop over ten values of sp: 0, 0.2, 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2.\n\nCode(\nmse_results <- \n  lapply(\n    seq(0, 2, by = 0.2),\n    function(x) get_mse_by_sp_kcv(x)\n  ) %>% \n  rbindlist()\n)\n\n    k       mse  sp\n 1: 1 10.842343 0.0\n 2: 2 10.276725 0.0\n 3: 3 10.351780 0.0\n 4: 4 10.468232 0.0\n 5: 5 11.259309 0.0\n 6: 1 10.640723 0.2\n 7: 2  9.474846 0.2\n 8: 3 10.358743 0.2\n 9: 4  9.391190 0.2\n10: 5 10.858930 0.2\n11: 1 10.777523 0.4\n12: 2  9.617963 0.4\n13: 3 10.283409 0.4\n14: 4  9.523628 0.4\n15: 5 10.908875 0.4\n16: 1 10.913666 0.6\n17: 2  9.765395 0.6\n18: 3 10.277063 0.6\n19: 4  9.659668 0.6\n20: 5 10.966567 0.6\n21: 1 11.051282 0.8\n22: 2  9.913271 0.8\n23: 3 10.308488 0.8\n24: 4  9.797650 0.8\n25: 5 11.033462 0.8\n26: 1 11.189673 1.0\n27: 2 10.060950 1.0\n28: 3 10.363166 1.0\n29: 4  9.936116 1.0\n30: 5 11.108143 1.0\n31: 1 11.327745 1.2\n32: 2 10.207548 1.2\n33: 3 10.432741 1.2\n34: 4 10.073552 1.2\n35: 5 11.188659 1.2\n36: 1 11.464489 1.4\n37: 2 10.352164 1.4\n38: 3 10.511860 1.4\n39: 4 10.208725 1.4\n40: 5 11.273202 1.4\n41: 1 11.599096 1.6\n42: 2 10.494040 1.6\n43: 3 10.596908 1.6\n44: 4 10.340731 1.6\n45: 5 11.360265 1.6\n46: 1 11.730962 1.8\n47: 2 10.632602 1.8\n48: 3 10.685372 1.8\n49: 4 10.468961 1.8\n50: 5 11.448649 1.8\n51: 1 11.859656 2.0\n52: 2 10.767445 2.0\n53: 3 10.775473 2.0\n54: 4 10.593034 2.0\n55: 5 11.537423 2.0\n    k       mse  sp\n\n\nLet’s now get the average MSE by sp:\n\nCode(\nmean_mse_data <- mse_results[, .(mean_mse = mean(mse)), by = sp]\n)\n\n     sp mean_mse\n 1: 0.0 10.63968\n 2: 0.2 10.14489\n 3: 0.4 10.22228\n 4: 0.6 10.31647\n 5: 0.8 10.42083\n 6: 1.0 10.53161\n 7: 1.2 10.64605\n 8: 1.4 10.76209\n 9: 1.6 10.87821\n10: 1.8 10.99331\n11: 2.0 11.10661\n\n\n\n\n\nSo, according to the KCV, we should pick sp \\(= 0.2\\) as the penalty parameter.\nBy the way, here is what MSE values look like for each fold based on the value of sp.\n\nCodeggplot(data = mse_results) +\n  geom_line(aes(y = mse, x = sp, color = factor(k))) +\n  scale_color_discrete(name = \"\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nEven though we compared different specification of the same approach (GAM), we can compare across different models as well. For example, you can find KCV for an RF model with a particular specifications of its hyper-parameters and compare the KCV with those of the GAM model specifications and see what comes at the top."
  },
  {
    "objectID": "cross-validation.html#does-kcv-really-work",
    "href": "cross-validation.html#does-kcv-really-work",
    "title": "\n2  Cross-validation based on mean squared error (MSE)\n",
    "section": "\n2.4 Does KCV really work?",
    "text": "2.4 Does KCV really work?\nLOOCV and KCV use the train data to estimate test MSE. But, does it really work? In other words, does it let us pick the parameter that minimizes the test MSE? We will run a simple MC simulations to test this. We continue to use the same data generating process and gam models for this simulation as well.\nNow, since we know the data generating process, we can actually use the following metric instead of MSE.\n\\[\n\\sum_{i=1}^N(\\hat{f}(x_i) - E[y|x_i])^2\n\\]\nThis measure removes the influence of the error term that appears in the test MSE. Your objective is to minimize this measure. Of course, you cannot do this in practice because you do not observe \\(E[y|x]\\). Let’s call this “pure” MSE.\nFirst, we define a function that gets you MSE from KCV and MSE using the test data as a function of sp.\n\nCodeget_mse_by_sp <- function(sp)\n{\n\n  #=== generate train data and test data ===#\n  train_data <- gen_data(x = runif(500) * 5)\n  test_data <- gen_data(x = runif(500) * 5)\n\n  #/*----------------------------------*/\n  #' ## MSE from KCV \n  #/*----------------------------------*/\n  #=== split train_data into 5 groups ===#\n  folds <- caret::createFolds(train_data$y, k = 5, list = TRUE, returnTrain = FALSE)\n  \n  #=== specify the model ===#\n  temp_gam <- specify_gam(sp)\n\n  #=== get MSE by fold ===#\n  mse_by_k <-\n    lapply(\n      seq_len(length(folds)),\n      function(x) get_mse_by_fold(train_data, x, temp_gam)\n    ) %>% \n    rbindlist()\n\n  #=== find the average MSE (over folds) ===#\n  mse_kcv <-\n    mse_by_k %>% \n    .[, .(mse = mean(mse))] %>% \n    .[, sp := sp] %>% \n    .[, type := \"KCV\"]\n\n  #/*----------------------------------*/\n  #' ## pure MSE from the test data  \n  #/*----------------------------------*/\n  #=== train using the entire train dataset ===#\n  fitted <- temp_gam(train_data)\n\n  #=== find the average MSE (over observations) ===#\n  mse_test <- \n    test_data %>% \n    #=== predict y ===#\n    .[, y_hat := predict(fitted, newdata = .)] %>% \n    .[, .(mse = mean((ey - y_hat)^2))] %>% \n    .[, sp := sp] %>% \n    .[, type := \"Pure\"]\n\n  #/*----------------------------------*/\n  #' ## Combine and return\n  #/*----------------------------------*/\n  return_data <- rbind(mse_kcv, mse_test)\n\n  return(return_data)\n}\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNote that you do not have to use an independent test data to obtain pure MSE above even though the code does it so. You could just use the train_data in getting pure MSE and the results would be essentially the same.\n\n\nFor example, this will give you MSE from KCV and MSE using the test data for sp \\(= 2\\).\n\nCodeget_mse_by_sp(2)\n\n        mse sp type\n1: 9.848602  2  KCV\n2: 1.240874  2 Pure\n\n\nNow, we define a function that loops over all the sp values we test.\n\nCodesp_seq <- seq(0, 2, by = 0.2)\n\nget_mse <- function(i)\n{ \n  print(i) # progress tracker\n\n  lapply(\n    sp_seq,\n    function(x) get_mse_by_sp(x)\n  ) %>% \n  rbindlist()\n}\n\n\nFor example, the following gives you MSE values for all the sp values for a single iteration.\n\nCodeget_mse(1)\n\n[1] 1\n\n\n           mse  sp type\n 1:  9.0044389 0.0  KCV\n 2:  0.5807781 0.0 Pure\n 3:  9.3461309 0.2  KCV\n 4:  0.1725085 0.2 Pure\n 5:  8.7595338 0.4  KCV\n 6:  0.1609511 0.4 Pure\n 7:  9.2329647 0.6  KCV\n 8:  0.2635158 0.6 Pure\n 9:  9.8506984 0.8  KCV\n10:  0.3720716 0.8 Pure\n11:  9.7156699 1.0  KCV\n12:  0.3158445 1.0 Pure\n13: 10.0551476 1.2  KCV\n14:  0.6563347 1.2 Pure\n15:  9.6189328 1.4  KCV\n16:  0.2748940 1.4 Pure\n17:  9.3173610 1.6  KCV\n18:  0.2448977 1.6 Pure\n19: 10.2279545 1.8  KCV\n20:  0.3988145 1.8 Pure\n21: 10.8770487 2.0  KCV\n22:  0.9743496 2.0 Pure\n           mse  sp type\n\n\nFinally, we run get_mse() 500 times.\n\nCodemse_results <-\n  mclapply(\n    1:500,\n    get_mse,\n    mc.cores = 12\n  ) %>% \n  rbindlist(idcol = \"sim_id\")\n\n#=== use this if you are a Windows user ===#\n# mse_results <-\n#   lapply(\n#     1:500,\n#     get_mse\n#   )\n\n\nFor each simulation round, let’s find the best sp using KCV and pure MSE.\n\nCode(\nwhich_sp_optimal <-\n  mse_results %>% \n  .[, .SD[which.min(mse), ], by = .(type, sim_id)] %>% \n  #=== drop mse ===#\n  .[, mse := NULL] %>% \n  dcast(sim_id ~ type, value.var = \"sp\") %>% \n  .[, num_comb := .N, by = .(KCV, Pure)]\n)\n\n\nFor example, for the first simulation, sp that would have minimized pure MSE (least error relative to the true \\(E[y|x]\\) across varying values of \\(x\\)) was 0.2. On the other hand, if you relied on KCV, you would have chosen 0.8.\nThe following figure shows the relationship between KCV-based and pure MSE-based sp values.\n\nCode#=== plot the frequency of sp chosen by KCV ===#\nggplot(data = which_sp_optimal) +\n  geom_point(aes(y = Pure, x = KCV, size = num_comb)) +\n  scale_y_continuous(breaks = sp_seq) +\n  scale_x_continuous(breaks = sp_seq) \n\n\n\n\nAs you can see KCV gives you sp that is close to the sp based on pure MSE in many cases. But, you can also see that KCV can suggest you a very different number as well. KCV is not perfect, which is kind of obvious."
  },
  {
    "objectID": "bias-variance-tradeoff.html#general-mc-application-steps",
    "href": "bias-variance-tradeoff.html#general-mc-application-steps",
    "title": "\n1  Variance-Bias Trade-off\n",
    "section": "\n1.1 General MC application steps",
    "text": "1.1 General MC application steps\nSuppose you have a dataset: \\(D = \\{X_1, y_1\\}, \\{X_2, y_2\\}, \\dots, \\{X_N, y_N\\}\\), where \\(X_i\\) is a collection of features and \\(y_i\\) is the dependent variable for \\(i\\)th observation. Further, suppose you use \\(D\\) for training ML models and \\(\\hat{f}()\\) is a trained model.\nA most common measure of how good a model is mean squared error (MSE) defined as below:\n\\[\nMSE = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{f}(x_i))^2\n\\]\n\\(f^(x_i)\\) is the value of \\(y\\) predicted by the trained model \\(\\hat{f}()\\), so \\(y_i - \\hat{f}(x_i)\\) is the residual (termed error more often).\nWhen you get the MSE of a trained model for the very data that is used to train the model, then we may call it training MSE.\nHowever, we are typically interested in how the trained model performs for the data that we have not seen. Let \\(D^{test} = \\{X^{test}_1, y^{test}_1\\}, \\{X^{test}_2, y^{test}_2\\}, \\dots, \\{X^{test}_M, y^{test}_M\\}\\) denote new data set with \\(M\\) data points. Then the test MSE would be:\n\\[\nMSE_{test} = \\frac{1}{M} \\sum_{i=1}^M (y^{test}_{i} - \\hat{f}(x^{test}_{i}))^2\n\\]\nTypically, we try different ML approaches (Random Forest, Support Vector Machine, Causal Forest, Boosted Regression Forest, Neural Network, etc). We also try different values of hyper-parameters for the same approach (e.g., tree depth and minimum observations per leaf for RF). Ideally, we would like to pick the model that has the smallest test \\(MSE\\) among all the models.\nSuppose you do not have a sufficiently large dataset to split to a train and test datasets (often the case). So, you used all the available observations to train a model. That means you can get only training \\(MSE\\). In this case, can we trust the model that has the lowest training \\(MSE\\)? The quick answer is no. The problem is that the model with the lowest training \\(MSE\\) does not necessarily achieve the lowest test \\(MSE\\).\nLet’s run some simulations to see this. The data generating process is as follows:\n\\[\ny  = (x - 2.5)^3 + \\mu\n\\]\nwhere \\(\\mu\\) is the error term.\n\nset.seed(943843)\n\n# define the data generating process\n# x fixed\ngen_data <- function(x) {\n  \n  ey <- (x - 2.5)^3 # E[y|x]\n  y <- ey + 3 * rnorm(length(x)) # y = E[y|x] + u\n  return_data <- data.table(y = y, x = x, ey = ey)\n\n  return(return_data)\n}\n\n## generate train data\ntrain_data <- gen_data(x = runif(100) * 5)\n\n  \n## generate test data\n# test data is large to stabilize test MSE \ntest_data <- gen_data(x = runif(10000) * 5)\n\nVisually, here is the relationship between \\(E[y]\\) and \\(x\\):\n\nggplot(data = train_data) +\n  geom_line(aes(y = ey, x = x))\n\n\n\n\nNow, let’s define a function that runs regression with different levels of flexibilities using a generalized additive model from the mgcv package, predict \\(y\\) for both the train and test datasets, and find train and test MSEs.\n\nest_gam <- function(k, train_data, test_data) \n{\n\n  # define train and test data\n  train_data <- copy(train_data)\n  test_data <- copy(test_data)\n\n  #--- train a model ---#  \n  trained_model <- gam(y ~ s(x, k = k), sp = 0, data = train_data)\n\n  #--- predict y for the train and test datasets ---#\n  train_data[, y_hat := predict(trained_model, newdata = train_data)] \n  train_data[, type := \"Train\"]\n  test_data[, y_hat := predict(trained_model, newdata = test_data)] \n  test_data[, type := \"Test\"]\n\n  #--- combine before returning ---#\n  return_data <- \n    rbind(train_data, test_data) %>%\n    .[, num_knots := k]\n\n  return(return_data)\n}\n\nWe now loop over the number of knots (num_knots).\n\nsim_results <- \n  lapply(1:50, function(x) est_gam(x, train_data, test_data)) %>%\n  rbindlist()\n\nFigure 1.1 presents the fitted regression lines for num_knots \\(= 1, 4, 5, 15, 25\\), and \\(50\\), along with the observed data points in the train dataset.\n\nCodeggplot(sim_results[num_knots  %in% c(1, 4, 15, 25, 50), ]) +\n  geom_point(data = train_data, aes(y = y, x = x), color = \"grey\") +\n  geom_line(aes(y = y_hat, x = x, color = factor(num_knots))) \n\n\n\nFigure 1.1: Fitted curves by gam() with different numbers of knots\n\n\n\n\nWhen the number of knots is \\(1\\), gam is not flexible enough to capture the underlying cubic function. However, once the number of knots becomes \\(4\\), it is capable of capturing the underlying cubic curve. However, when you increase the number of knots to 15, you see that the fitted curve is very wiggly (sudden and large changes in \\(y\\) when \\(x\\) is changed slighly). When num_knots \\(= 50\\), the fitted curve looks crazy and does not resemble the underlying smooth cubic curve.\nNow, let’s check how train and test MSEs change as k changes. As you can see in Figure 1.2 below, train MSE goes down as k increases (the more complex the model is, the better fit you will get for the train data). However, test MSE is the lowest when num_knots \\(= 4\\), and it goes up afterward instead of going down. As we saw earlier, when model is made too flexible, it is trained to fit the trained data too well and lose generalizability (predict well for the dataset that has not been seen). This phenomenon is called over-fitting.\n\nCode#--- get train and test MSEs by k ---#\nsummary <- \n  sim_results[, .(mse = mean((y_hat - y)^2)), by = .(num_knots, type)]\n\nggplot(summary) +\n  geom_line(aes(y = mse, x = num_knots, color = type)) +\n  geom_point(aes(y = mse, x = num_knots, color = type)) +\n  xlab(\"Number of knots\") +\n  ylab(\"MSE\") +\n  scale_color_discrete(name = \"\")\n\n\n\nFigure 1.2: Train and test MSEs as a function of the number of knots\n\n\n\n\nThis clearly tells us that we should NOT use training MSE to pick the best model."
  },
  {
    "objectID": "bias-variance-tradeoff.html#variance-bias-trade-off",
    "href": "bias-variance-tradeoff.html#variance-bias-trade-off",
    "title": "\n1  Variance-Bias Trade-off\n",
    "section": "\n1.2 Variance-bias trade-off",
    "text": "1.2 Variance-bias trade-off\nExpected test MSE at \\(x = x_0\\) can be written in general (no matter what the trained model is) as follows:\n\\[\nE[(y_0 - \\hat{f}(x_0))^2] = Var(\\hat{f}(x_0)) + E[y - \\hat{f}(x_0)]^2 + Var(\\mu)\n\\]\nThe first term is the variance of predicted value at \\(x_0\\), the second term is the squared bias of \\(\\hat{f}(x_0)\\) (how much \\(\\hat{f}(x_0)\\) differs from \\(E[y_0]\\) on average), and \\(Var(\\mu)\\) is the variance of the error term.\nTo illustrate this trade-off, we will run Monte Carlo simulations. We repeat the folowing steps 500 times.\n\nstep 1: generate train and test datasets\nstep 2: train gam with different values of \\(k\\) \\((1, 5, 15, 25, 50)\\) using the train dataset\nstep 3: predict \\(y\\) using the test dataset\n\nOnce all the iterations are completed, simulation results are summarized to estimate \\(Var(\\hat{f}(x_0))\\) and \\(E[y - \\hat{f}(x_0)]^2\\) for all the values of \\(x_0\\) (all the \\(x\\) values observed in the test dataset) by \\(k\\). We then average them to find the overall \\(Var(\\hat{f}(x_0))\\) and \\(E[y - \\hat{f}(x_0)]^2\\) by \\(k\\).\n\nx_train <- runif(100) * 5\n# x_test is fixed to make it easier to get average conditonal on a given value of x later  \nx_test <- runif(100) * 5\n\n# function that performs steps 1 ~ 3 (a single iteration) \nrun_mc <- function(i, x_train, x_test)\n{\n  print(i) # track progress\n  train_data <- gen_data(x_train) # generate data\n  test_data <- gen_data(x_test) # generate data\n  # run gam for K = 1, ..., 50\n  sim_results <- \n    lapply(\n      c(1, 5, 15, 25, 50), \n      function(x) est_gam(x, train_data, test_data)\n    ) %>%\n    rbindlist()\n\n  return(sim_results)\n}\n\n# runs run_mc 500 times\nmc_results <-\n  mclapply(\n    1:500,\n    function(x) run_mc(x, x_train, x_test),\n    mc.cores = 12\n  ) %>%\n  rbindlist(idcol = \"sim_id\") \n\nFigure 1.3 shows plots fitted curves for all the 500 simulations by \\(k\\) (grey lines). The blue line is the true \\(E[y|x]\\). The red line is \\(E[\\hat{f}(x)]\\)1. Figure Figure 1.4 plots the average2 \\(Var(\\hat{f}(x))\\) (red), \\(E[y - \\hat{f}(x)]^2\\) (blue), and MSE (darkgreen) from the test datasets for different values of \\(k\\).\nAs you can see in Figure 1.3, when \\(k = 1\\), it clealy has a significant bias in estimating \\(E[y|x]\\) except for several values of \\(x\\) at which \\(E[f^(x)]\\) only happens to be unbiased. The model is simply too restrictive and suffers significant bias. However, the variance of \\(\\hat{f}(x)\\) is the smallest as shown in Figure 1.4. As we increase the value of \\(k\\) (making the model more flexible), bias dramatically reduces. However, the variance of \\(\\hat{f}(x)\\) slightly increases. Going from \\(k = 5\\) to \\(k = 10\\) further reduces bias. That is, even though individual fitted curves may look very bad, on average they perform well (as you know that what bias measures). However, the variance of \\(\\hat{f}(x)\\) dramatically increases (this is why individual fitted curves look terrible). Moving to a even higher value of \\(k\\) does not reduce bias, but increases the variance of \\(\\hat{f}(x)\\) even further.\nAccording to MSE presented in Figure 1.4, \\(k = 5\\) is the best model among all the models tried in this experiment. In this experiment, we had test datasets available. However, in practice, we need to pick the best model when test datasets are not available most of the time. For such a case, we would like a clever way to estimate test MSE even when test datasets are not available. We will later talk about cross-validation as a means to do so.\n\nCodemc_results_sum <- \n    mc_results %>%\n    .[type == \"Test\", ] %>% \n    .[, .(mean_y_hat = mean(y_hat)), by = .(x, ey, num_knots)]\n\nggplot() +\n    geom_line(data = mc_results[type == \"Test\", ], aes(y = y_hat, x = x, group = sim_id), color = \"gray\") +\n    geom_line(data = mc_results_sum, aes(y = mean_y_hat, x = x), color = \"red\") +\n    geom_line(data = mc_results_sum, aes(y = ey, x= x), color = \"blue\") +\n    facet_wrap(. ~ num_knots, ncol = 5)\n\n\n\nFigure 1.3: Bias-variance trade-off of GAM models with differing number of knots\n\n\n\n\n\nCodesum_stat <- \n  mc_results %>%\n  .[type == \"Test\", ] %>% \n  .[\n    , \n    .(\n      var_hat = var(y_hat), # varianc of y_hat\n      bias_sq = mean(y_hat - ey)^2, # squared bias\n      mse = mean((y - y_hat)^2)\n    ),\n    by = .(x, num_knots)\n  ] %>%\n  .[\n    ,\n    .(\n      mean_var_hat = mean(var_hat),\n      mean_bias_sq = mean(bias_sq),\n      mean_mse = mean(mse)\n    ),\n    by = .(num_knots)\n  ]\n\nggplot(data = sum_stat) +\n  geom_line(aes(y = mean_var_hat, x = num_knots), color = \"red\") +\n  geom_point(aes(y = mean_var_hat, x = num_knots), color = \"red\") +\n  geom_line(aes(y = mean_bias_sq, x = num_knots), color = \"blue\") +\n  geom_point(aes(y = mean_bias_sq, x = num_knots), color = \"blue\") +\n  geom_line(aes(y = mean_mse, x = num_knots), color = \"darkgreen\") +\n  geom_point(aes(y = mean_mse, x = num_knots), color = \"darkgreen\") +\n  ylab(\"\") +\n  xlab(\"Number of knots\")\n\n\n\nFigure 1.4: Expected variance of predicted values and bias"
  },
  {
    "objectID": "bias-variance-tradeoff.html#additional-example-k-nearest-neighbor-regression",
    "href": "bias-variance-tradeoff.html#additional-example-k-nearest-neighbor-regression",
    "title": "\n1  Variance-Bias Trade-off\n",
    "section": "\n1.3 Additional Example (K-nearest neighbor regression)",
    "text": "1.3 Additional Example (K-nearest neighbor regression)\n\nfit_knn <- function(k, train_data, test_data) \n{\n\n  # define train and test data\n  train_data <- copy(train_data)\n  test_data <- copy(test_data)\n\n  #--- train a model ---#  \n  trained_model <- knnreg(y ~ x, k = k, data = train_data)\n\n  #--- predict y for the train and test datasets ---#\n  train_data[, y_hat := predict(trained_model, newdata = train_data)] \n  train_data[, type := \"Train\"]\n  test_data[, y_hat := predict(trained_model, newdata = test_data)] \n  test_data[, type := \"Test\"]\n\n  #--- combine before returning ---#\n  return_data <- \n    rbind(train_data, test_data) %>%\n    .[, num_nbs := k]\n\n  return(return_data)\n}\n\nWe now loop over the number of neighbors (k).\n\n## generate train data\ntrain_data <- gen_data(x = runif(1000) * 5)\n  \n## generate test data\ntest_data <- gen_data(x = runif(1000) * 5)\n\nsim_results <- \n  lapply(1:50, function(x) fit_knn(x, train_data, test_data)) %>%\n  rbindlist()\n\nFigure 1.5 presents the fitted regression lines for \\(k = 1, 5, 15, 25\\), and \\(50\\) using knnreg(), along with the observed data points in the train dataset.\n\nCodeggplot(sim_results[num_nbs  %in% c(1, 5, 15, 25, 50), ]) +\n  geom_point(data = train_data, aes(y = y, x = x), color = \"gray\") +\n  geom_line(aes(y = y_hat, x = x), color = \"red\") +\n  facet_grid(num_nbs ~ .)\n\n\n\nFigure 1.5: Fitted curves by knnreg() with different numbers of neighbors\n\n\n\n\n\nCode#--- get train and test MSEs by k ---#\nsummary <- \n  sim_results[, .(mse = mean((y_hat - y)^2)), by = .(num_nbs, type)]\n\nggplot(summary) +\n  geom_line(aes(y = mse, x = num_nbs, color = type)) +\n  geom_point(aes(y = mse, x = num_nbs, color = type)) +\n  xlab(\"Number of neighbors\") +\n  ylab(\"MSE\") +\n  scale_color_discrete(name = \"\")\n\n\n\nFigure 1.6: Train and test MSEs as a function of the number of knots"
  },
  {
    "objectID": "P03-xgb.html",
    "href": "P03-xgb.html",
    "title": "\n9  Extreme Gradient Boosting\n",
    "section": "",
    "text": "Extreme gradient boosting (XGB) is a variant of gradient boosting that has been extremely popular due to its superb performance. The basic concept is the same as the gradient boosting algorithm described above, however, it has its own way of building a tree, which is more mindful of avoiding over-fitting trees."
  },
  {
    "objectID": "P03-xgb.html#preparation",
    "href": "P03-xgb.html#preparation",
    "title": "\n9  Extreme Gradient Boosting\n",
    "section": "Preparation",
    "text": "Preparation\n\nlibrary(tidyverse)\nlibrary(data.table)"
  },
  {
    "objectID": "P03-xgb.html#tree-updating-in-xgb-general",
    "href": "P03-xgb.html#tree-updating-in-xgb-general",
    "title": "\n9  Extreme Gradient Boosting\n",
    "section": "\n9.1 Tree updating in XGB (general)",
    "text": "9.1 Tree updating in XGB (general)\nLet \\(f_{i,b}(x_i)\\) be the prediction for the \\(i\\)th observation at the \\(b\\)-th iteration. Further, let \\(w_t(x_i)\\) is the term that is added to \\(f_{i,b}(x_i)\\) to obtain \\(f_{i,b+1}(x_i)\\). In XGB, \\(w_t(x_i)\\) is such that it minimizes the following objective:\n\\[\n\\Psi_t = \\sum_{i=1}^N [L(y_i, f_{i,b}(x_i) + w_t(x_i))] + \\Omega(w_t)\n\\tag{9.1}\\]\nwhere \\(L()\\) is the user-specified loss-function that is differentiable and \\(\\Omega(w_t)\\) is the regularization term. Instead of Equation 9.1, XGB uses the second order Taylor expansion of \\(L()\\) about \\(w\\)1.\n\\[\n\\tilde{\\Psi}_t = \\sum_{i=1}^N [L(y_i, f_{i,b}(x_i)) + g_i w_t(x_i) + \\frac{1}{2}h_i w_t(x_i)^2] + \\Omega(w_t)\n\\tag{9.2}\\]\nwhere \\(g_i = \\frac{\\partial L(y_i, p_i)}{\\partial p_i}\\) (first-order derivative) and \\(h_i = \\frac{\\partial^2 L(y_i, p_i)}{\\partial p_i^2}\\) (second-order derivative). Since \\(L(y_i, f_{i,b}(x_i))\\) is just a constant, we can safely remove it from the objective function, which leads to\n\\[\n\\tilde{\\Psi}_t = \\sum_{i=1}^N [g_i w_t(x_i) + \\frac{1}{2}h_i w_t(x_i)^2] + \\Omega(w_t)\n\\tag{9.3}\\]\nLet \\(I_j\\) denote a set of observations that belong to leaf \\(j\\) (\\(j = 1, \\dots, J\\)). Then, Equation 9.3 is written as follows:\n\\[\n\\tilde{\\Psi}_t = \\sum_{j=1}^J\\huge[\\normalsize (\\sum_{i\\in I_j}g_i)w_j + \\frac{1}{2}(\\sum_{i\\in I_j}h_i + \\lambda)w_j^2 \\huge]\\normalsize + \\gamma J\n\\tag{9.4}\\]\n\n\nRemember that all the observations in the same leaf shares the same prediction. So, for all \\(i\\)s that belong to leaf \\(j\\), the prediction is denoted as \\(w_j\\) in Equation 9.4. That is, \\(w_t(x_i)\\) that belongs to leaf \\(j\\) is \\(w_j\\).\nFor a given tree structure (denoted as \\(q(x)\\)), the leaves can be treated independently in minimizing this objective.\nTaking the derivative of \\(\\tilde{\\Psi}_t\\) w.r.t \\(w_j\\),\n\\[\n\\begin{aligned}\n(\\sum_{i\\in I_j}g_i) + (\\sum_{i\\in I_j}h_i + \\lambda)w_j = 0 \\\\\n\\Rightarrow w_j^* = \\frac{-\\sum_{i\\in I_j}g_i}{\\sum_{i\\in I_j}h_i + \\lambda}\n\\end{aligned}\n\\tag{9.5}\\]\nThe minimized value of \\(\\tilde{\\Psi}_t\\) is then (obtained by plugging \\(w_j^*\\) into Equation 9.4),\n\\[\n\\begin{aligned}\n\\tilde{\\Psi}_t(q)^* & = \\sum_{j=1}^J\\huge[\\normalsize (\\sum_{i\\in I_j}g_i)\\frac{-\\sum_{i\\in I_j}g_i}{\\sum_{i\\in I_j}h_i + \\lambda} + \\frac{1}{2}(\\sum_{i\\in I_j}h_i + \\lambda)(\\frac{-\\sum_{i\\in I_j}g_i}{\\sum_{i\\in I_j}h_i + \\lambda})^2 \\huge]\\normalsize + \\gamma J \\\\\n& = \\sum_{j=1}^J\\huge[\\normalsize \\frac{-(\\sum_{i\\in I_j}g_i)^2}{\\sum_{i\\in I_j}h_i + \\lambda} + \\frac{1}{2}\\frac{(\\sum_{i\\in I_j}g_i)^2}{\\sum_{i\\in I_j}h_i + \\lambda} \\huge]\\normalsize + \\gamma J \\\\\n& = -\\frac{1}{2} \\sum_{j=1}^J \\huge[\\normalsize\\frac{(\\sum_{i\\in I_j}g_i)^2}{\\sum_{i\\in I_j}h_i + \\lambda}\\huge]\\normalsize + \\gamma J\n\\end{aligned}\n\\tag{9.6}\\]\nFor notatinal convenience, we call \\(\\frac{(\\sum_{i\\in I_j}g_i)^2}{\\sum_{i\\in I_j}h_i + \\lambda}\\) quality score and denote it by \\(Q_j\\) ( Quality score for leaf \\(j\\)).\nWe could find the best tree structure by finding \\(w_j^*(q)\\) according to Equation 9.4 and calculate \\(\\tilde{\\Psi}_t(q)^*\\) according to Equation 9.6 for each of all the possible tree structures, and then pick the tree structure q(x) that has the lowest \\(\\tilde{\\Psi}_t(q)^*\\).\nHowever, it is impossible to consider all possible tree structures practically. So, a greedy (myopic) approach that starts from a single leaf and iteratively splits leaves is used instead.\nConsider splitting an existing leaf \\(s\\) (where in the tree it may be located) into two leaves \\(L\\) and \\(R\\) when there are \\(J\\) existing leaves. Then, we find \\(w_j^*\\) and calculate \\(\\tilde{\\Psi}_t(q)^*\\) for each leaf, and the resulting minimized objective is\n\\[\n-\\frac{1}{2} \\huge[\\normalsize Q_L + Q_R + \\Gamma \\huge]\\normalsize + \\gamma(J+1)\n\\]\nwhere \\(\\Gamma\\) is the sum of quality scores for all the leaves except \\(L\\) and \\(R\\).\n\n\n\\[\n\\Gamma = \\sum_{j\\ne \\{L, R\\}}^J Q_j\n\\]\nThe minimized objective before splitting is\n\\[\n-\\frac{1}{2} \\huge[\\normalsize Q_s + \\Gamma \\huge]\\normalsize + \\gamma J\n\\]\nSo, the reduction  in loss after the split is\n\\[\nG(s, L, R) = \\frac{1}{2} \\huge[\\normalsize Q_L + Q_R - Q_s \\huge]\\normalsize - \\gamma\n\\]\nLet’s call \\(G(s, L, R)\\) simply a gain (of the split).\n\n\nA more positive value of gain (\\(G(s, L, R)\\)) means a more successful split.\nWe can try many different patterns of \\(I_L\\) and \\(I_R\\) (how to split tree \\(s\\)), calculate the gain for each of them and pick the split that has the highest gain.\n\n\nDifferent patterns of \\(I_L\\) and \\(I_R\\) arise from different variable-cutpoint combinations\nIf the highest gain is negative, then the leaf under consideration for splitting is not split.\nOnce the best tree is chosen (the tree that has the highest gain among the ones investigated), then we update our prediction based on \\(w^*\\) of the tree. For observation \\(i\\) that belongs to leaf \\(j\\) of the tree,\n\\[\n\\begin{aligned}\nf_{i,b} = f_{i,b-1} + \\eta \\cdot w_j^*\n\\end{aligned}\n\\tag{9.7}\\]\nwhere \\(\\eta\\) is the learning rate."
  },
  {
    "objectID": "P03-xgb.html#tree-updating-in-xgb-regression",
    "href": "P03-xgb.html#tree-updating-in-xgb-regression",
    "title": "\n9  Extreme Gradient Boosting\n",
    "section": "\n9.2 Tree updating in XGB (regression)",
    "text": "9.2 Tree updating in XGB (regression)\nWe now make the general tree updating algorithm specific to regression problems, where the loss function is squared error: \\(L(y_i, p_i) = \\frac{1}{2}(y_i - p_i)^2\\), where \\(p_i\\) is the predicted value for \\(i\\).\n\nFirst, let’s find \\(g_i\\) and \\(h_i\\) for \\(L(y_i, p_i) = \\frac{1}{2}(y_i - p_i)^2\\).\n\\[\n\\begin{aligned}\ng_i = \\frac{\\partial L(y_i, p_i)}{\\partial p_i}  = -(y_i - p_i)\\\\\nh_i = \\frac{\\partial^2 L(y_i, p_i)}{\\partial p_i^2} = 1 \\\\\n\\end{aligned}\n\\]\nSo, \\(g_i\\) is simply the negative of the residual for \\(i\\).\nNow, suppose your are at iteration \\(b\\) and the predicted value for \\(i\\) is denoted as \\(f_{i,b}(x_i)\\). Further, let \\(r_{i,b}\\) denote the residual (\\(y_i - f_{i,b}(x_i)\\)).\nPlugging these into Equation 9.5,\n\\[\n\\begin{aligned}\nw_j^* & = \\frac{\\sum_{i\\in I_j}r_{i,b}}{\\sum_{i\\in I_j}1 + \\lambda} \\\\\n      & = \\frac{\\sum_{i\\in I_j}r_{i,b}}{N_j + \\lambda}\n\\end{aligned}\n\\tag{9.8}\\]\nThat is for a given leaf \\(j\\), the optimal predicted value (\\(w_j^*\\)) is the sum of the residuals of all the observations in leaf \\(j\\) divided by the number of observations in leaf \\(j\\) plus \\(\\lambda\\). When \\(\\lambda = 0\\), the optimal predicted value (\\(w_j^*\\)) is simply the mean of the residuals.\nThe quality score for leaf \\(j\\) is then,\n\\[\nQ_j = \\frac{(\\sum_{i\\in I_j}r_{i,b})^2}{N_j + \\lambda}\n\\tag{9.9}\\]"
  },
  {
    "objectID": "P03-xgb.html#illustration-of-xgb-for-regression",
    "href": "P03-xgb.html#illustration-of-xgb-for-regression",
    "title": "\n9  Extreme Gradient Boosting\n",
    "section": "\n9.3 Illustration of XGB for regression",
    "text": "9.3 Illustration of XGB for regression\nIn order to further our understanding of the entire XGB algorithm, let’s take a lookt at a simple regression problem as an illustration. We consider a four-observation data as follows:\n\n(\ndata <-\n  data.table(\n    y = c(-3, 7, 8, 12),\n    x = c(1, 4, 6, 8)\n  )\n)\n\n    y x\n1: -3 1\n2:  7 4\n3:  8 6\n4: 12 8\n\n\n\n(\ng_0 <-\n  ggplot(data) +\n  geom_point(aes(y = y, x = x))\n)\n\n\n\n\nFirst step (\\(b = 0\\)) is to make an initial prediction. This can be any number, but let’s use the mean of y and set it as the predicted value for all the observations.\n\n(\nf_0 <- mean(data$y) # f_0: the predicted value for all the observations\n)\n\n[1] 6\n\n\nLet’s set \\(\\gamma\\), \\(\\lambda\\), and \\(\\eta\\) to \\(10\\), \\(1\\), and \\(0.3\\), respectively.\n\ngamma <- 10\nlambda <- 1\neta <- 0.3\n\nWe have a single-leaf tree at the moment. And the quality score for this leaf is\n\n\nquality score for leaf \\(j\\) is \\(\\frac{(\\sum_{i\\in I_j}r_{i,b})^2}{N_j + \\lambda}\\)\n\n#=== get residuals ===#\ndata[, resid := y - f_0]\n\n#=== get quality score ===#\n(\nq_0 <- (sum(data$resid))^2/(nrow(data) + 1)\n)\n\n[1] 0\n\n\nQuality score of the leaf is 0.\n\n\nSince we are using the mean of \\(y\\) as the prediction, of course, the sum of the residuals is zero, which then means that the quality score is zero.\nNow, we have three potential to split patterns: {x, 2}, {x, 5}, {x, 7}.\n\n\n{x, 2} means the leaf is split into two leaves: \\({x | x <2}\\) and \\({x | x >= 2}\\). Note that any number between \\(1\\) and \\(4\\) will result in the same split results.\nLet’s consider them one by one.\n\n9.3.0.1  Split: {x, 2} \n\nHere are graphical representations of the split:\n\ng_0 +\n  geom_vline(xintercept = 2, color = \"red\") +\n  annotate(\"text\", x = 1.25, y = 6, label = \"leaf L\", color = \"red\") +\n  annotate(\"text\", x = 5, y = 6, label = \"leaf R\", color = \"red\")\n\n\n\n\n\n\n\nCodeDiagrammeR::grViz(\n\"\ndigraph {\n  graph [ranksep = 0.2]\n  node [shape = box]\n    T1R [label = 'L: -9']\n    T1L [label = 'R: 1 , 2 , 6']\n    T0 [label = '-9, 1 , 2 , 6']\n  edge [minlen = 2]\n    T0->T1L\n    T0->T1R\n  { rank = same; T1R; T1L}\n}\n\"\n)\n\n\n\n\n\nLet’s split the data.\n\n#=== leaf L ===#\n(\ndata_L_1 <- data[x < 2, ]\n)\n\n    y x resid\n1: -3 1    -9\n\n#=== leaf R ===#\n(\ndata_R_1 <- data[x >= 2, ]\n)\n\n    y x resid\n1:  7 4     1\n2:  8 6     2\n3: 12 8     6\n\n\nUsing Equation 9.8,\n\n\n\\(w_j^* = \\frac{\\sum_{i\\in I_j}r_{i,b}}{N_j + \\lambda}\\)\n\nw_L <- (sum(data_L_1$resid))/(nrow(data_L_1) + lambda)\nw_R <- (sum(data_R_1$resid))/(nrow(data_R_1) + lambda)\n\n\\[\n\\begin{aligned}\nw_L^* & = -9 / (1 + 1) = -4.5 \\\\\nw_R^* & = 1 + 2 + 6 / (3 + 1) = 2.25\n\\end{aligned}\n\\]\nUsing Equation 9.9, the quality scores for the leaves are\n\n\n\\(Q_j = \\frac{(\\sum_{i\\in I_j}r_{i,b})^2}{N_j + \\lambda}\\)\n\nq_L <- (sum(data_L_1$resid))^2/(nrow(data_L_1) + lambda)\nq_R <- (sum(data_R_1$resid))^2/(nrow(data_R_1) + lambda)\n\n\n\n\nCodeDiagrammeR::grViz(\n  paste0(\n  \"\n  digraph {\n    graph [ranksep = 0.2]\n    node [shape = box]\n      T1R [label = 'L: -9 \\n Q score = \", round(q_L, digits = 2), \"']\n      T1L [label = 'R: 1 , 2 , 6 \\n Q score = \", round(q_R, digits = 2), \"']\n      T0 [label = '-9, 1 , 2 , 6']\n    edge [minlen = 2]\n      T0->T1L\n      T0->T1R\n    { rank = same; T1R; T1L}\n  }\n  \"\n  )\n\n)\n\n\n\n\n\n\\[\n\\begin{aligned}\nq_L^* & = (-9)^2 / (1 + 1) = 40.5 \\\\\nq_R^* & = (1 + 2 + 6)^2 / (3 + 1) = 20.25\n\\end{aligned}\n\\]\nNotice that residuals are first summed and then squared in the denominator of the quality score (the higher, the better). This means that if the prediction is off in the same direction (meaning they are similar) among the observations within the leaf, then the quality score is higher. On the other hand, if the prediction is off in both directions (meaning they are not similar), then the residuals cancel each other out, resulting in a lower quality score. Since we would like to create leaves consisting of similar observations, a more successful split has a higher quality score.\nFinally, the gain of this split is\n\n\n\\[\nG(s, L, R) = \\frac{1}{2} \\huge[\\normalsize Q_L + Q_R - Q_s \\huge]\\normalsize - \\gamma\n\\]\nwhere \\(s\\) is the leaf before split, \\(L\\) and \\(R\\) are leaves after the split of leaf \\(s\\).\n\ngain_1 <- (q_L + q_R - q_0)/2 - gamma\n\n\\[\nG_1 = \\frac{40.5 + 20.25 - 0}{2} - 10 = 20.375\n\\]\nNow that we have gone through the process of finding update value (\\(w\\)), quality score (\\(q\\)), and gain (\\(G\\)) for a given split structure, let’s write a function that returns the values of these measures by feeding the cutpoint before moving onto the next split candidate.\n\nget_info <- function(data, cutpoint, lambda, gamma)\n{\n  q_0 <- (sum(data$resid))^2/(nrow(data) + lambda)\n\n  data_L <- data[x < cutpoint, ]\n  data_R <- data[x >= cutpoint, ]\n\n  w_L <- (sum(data_L$resid))/(nrow(data_L) + lambda)\n  w_R <- (sum(data_R$resid))/(nrow(data_R) + lambda)\n\n  q_L <- (sum(data_L$resid))^2/(nrow(data_L) + lambda)\n  q_R <- (sum(data_R$resid))^2/(nrow(data_R) + lambda)\n\n  gain <- (q_L + q_R - q_0)/2 - gamma\n\n  return(list(\n    w_L = w_L, \n    w_R = w_R, \n    q_L = q_L, \n    q_R = q_R, \n    gain = gain \n  ))\n}\n\n\n9.3.0.2  Split: {x, 5} \n\n\nmeasures_2 <- get_info(data, 5, lambda, gamma)\n\n\ng_0 +\n  geom_vline(xintercept = 5, color = \"red\") +\n  annotate(\"text\", x = 3, y = 6, label = \"leaf L\", color = \"red\") +\n  annotate(\"text\", x = 7, y = 6, label = \"leaf R\", color = \"red\")\n\n\n\n\n\n\n\nCodeDiagrammeR::grViz(\n  paste0(\n    \"\n    digraph {\n      graph [ranksep = 0.2]\n      node [shape = box]\n        T1R [label = 'L: -9, 1 \\n Q score = \", round(measures_2$q_L, digits = 2), \"']\n        T1L [label = 'R: 2 , 6 \\n Q score = \", round(measures_2$q_R, digits = 2), \"']\n        T0 [label = '-9, 1 , 2 , 6']\n      edge [minlen = 2]\n        T0->T1L\n        T0->T1R\n      { rank = same; T1R; T1L}\n    }\n    \"\n  )\n)\n\n\n\n\n\n\\[\n\\begin{aligned}\nq_L^* & = (-9)^2 / (2 + 1) = 21.33 \\\\\nq_R^* & = (1 + 2 + 6)^2 / (2 + 1) = 21.33\n\\end{aligned}\n\\]\n\\[\nG_2 = \\frac{21.33 + 21.33 - 0}{2} - 10 = 11.3333333\n\\]\n\n9.3.0.3  Split: {x, 7} \n\n\nmeasures_3 <- get_info(data, 7, lambda, gamma)\n\n\ng_0 +\n  geom_vline(xintercept = 7, color = \"red\") +\n  annotate(\"text\", x = 4, y = 6, label = \"leaf L\", color = \"red\") +\n  annotate(\"text\", x = 8, y = 6, label = \"leaf R\", color = \"red\")\n\n\n\n\n\n\n\nCodeDiagrammeR::grViz(\n  paste0(\n    \"\n    digraph {\n      graph [ranksep = 0.2]\n      node [shape = box]\n        T1R [label = 'L: -9, 1, 2 \\n Q score = \", round(measures_3$q_L, digits = 2), \"']\n        T1L [label = 'R: 6 \\n Q score = \", round(measures_3$q_R, digits = 2), \"']\n        T0 [label = '-9, 1 , 2 , 6']\n      edge [minlen = 2]\n        T0->T1L\n        T0->T1R\n      { rank = same; T1R; T1L}\n    }\n    \"\n  )\n)\n\n\n\n\n\n\\[\n\\begin{aligned}\nq_L^* & = (-9)^2 / (1 + 1) = 9 \\\\\nq_R^* & = (1 + 2 + 6)^2 / (3 + 1) = 18\n\\end{aligned}\n\\]\n\\[\nG_3 = \\frac{9 + 18 - 0}{2} - 10 = 3.5\n\\]\nAmong all the splits we considered, the first case (Split: {x, 2}) has the highest score. This is easy to confirm visually and shows picking a split based on the gain measure indeed makes sense.\nNow we consider how to split leaf R (leaf L cannot be split further as it has only one observation). We have two split candidates: {x, 5} and {x, 7}. Let’s get the gain measures using get_info().\n\n#=== first split ===#\nget_info(data_R_1, 5, lambda, gamma)$gain \n\n[1] -9.208333\n\n#=== second split ===#\nget_info(data_R_1, 7, lambda, gamma)$gain\n\n[1] -9.625\n\n\nSo, neither of the splits has a positive gain value. Therefore, we do not adopt either of the splits. For this iteration (\\(b=1\\)), this is the end of tree building.\n\n\n\n\n\n\nNote\n\n\n\nIf the value of \\(\\gamma\\) is lower (say, 0), then we would have adopted the second split.\n\nget_info(data_R_1, 5, lambda, 0)$gain # first split\n\n[1] 0.7916667\n\nget_info(data_R_1, 7, lambda, 0)$gain # second split\n\n[1] 0.375\n\n\nAs you can see, a higher value of \\(\\gamma\\) leads to a more aggressive tree pruning.\n\n\nSo, the final tree for this iteration (\\(b = 1\\)) is\n\nCodeDiagrammeR::grViz(\n  paste0(\n  \"\n  digraph {\n    graph [ranksep = 0.2]\n    node [shape = box, width = 0.3, height = 0.15, fontsize = 3, fixedsize = TRUE, penwidth = 0.2]\n      T1R [label = 'L: -9 \\n w* = \", round(w_L, digits = 2), \"']\n      T1L [label = 'R: 1 , 2 , 6 \\n w* = \", round(w_R, digits = 2), \"']\n      T0 [label = '-9, 1 , 2 , 6']\n    edge [penwidth = 0.2, arrowsize = 0.3, len = 0.3]\n      T0->T1L\n      T0->T1R\n    { rank = same; T1R; T1L}\n  }\n  \"\n  )\n\n)\n\n\n\n\n\nWe now use \\(w^*\\) from this tree to update our prediction according to Equation 9.7.\n\n\n\\(f_{i,b} = f_{i,b-1} + \\eta \\cdot w_j^*\\)\n\nmeasures_1 <- get_info(data, 2, lambda, gamma)\n\nSince the first observation is in \\(L\\),\n\\[\nf_{i = 1,b = 1} = 6 + 0.3 \\times -4.5 = 4.65\n\\]\nSince the second, third, and fourth observations are in \\(R\\),\n\\[\n\\begin{aligned}\nf_{i = 2,b = 1} = 6 + 0.3 \\times 2.25 = 6.68 \\\\\nf_{i = 3,b = 1} = 6 + 0.3 \\times 2.25  = 6.68\\\\\nf_{i = 4,b = 1} = 6 + 0.3 \\times 2.25 = 6.68\n\\end{aligned}\n\\]\n\ndata %>% \n  .[, f_0 := f_0] %>% \n  .[1, f_1 := f_0 + measures_1$w_L * eta] %>%\n  .[2:4, f_1 := f_0 + measures_1$w_R * eta]\n\nThe prediction updates can be seen below. Though small, we made small improvements in our prediction.\n\nCodeggplot(data = data) +\n  geom_point(aes(y = y, x = x, color = \"observed\")) +\n  geom_point(aes(y = f_1, x = x, color = \"after (f1)\")) +\n  geom_point(aes(y = f_0, x = x, color = \"before (f0)\")) +\n  scale_color_manual(\n    values = \n      c(\n        \"before (f0)\" = \"blue\", \n        \"after (f1)\" = \"red\",\n        \"observed\" = \"black\"\n      ), \n    name = \"\"\n  ) +\n  geom_segment(\n    aes(y = f_0, x = x, yend = f_1, xend = x), \n    color = \"blue\",\n    arrow = arrow(length = unit(0.1, \"cm\"))\n  ) +\n  theme_bw()\n\n\n\n\nNow, we move on to \\(b=2\\). We first update residuals:\n\ndata[, resid := y - f_1]\n\ndata\n\n    y x  resid f_0   f_1\n1: -3 1 -7.650   6 4.650\n2:  7 4  0.325   6 6.675\n3:  8 6  1.325   6 6.675\n4: 12 8  5.325   6 6.675\n\n\nJust like at \\(b=1\\), all the possible splits are {x, 2}, {x, 5}, {x, 7}. Let’s find the gain for each split.\n\nlapply(\n  c(2, 5, 7),\n  function(x) get_info(data, x, lambda, gamma)$gain\n)\n\n[[1]]\n[1] 10.66639\n\n[[2]]\n[1] 6.267458\n\n[[3]]\n[1] 1.543344\n\n\nSo, the first split is again the best split. Should we split the right leaf, which has the observations except the first one?\n\nlapply(\n  c(5, 7),\n  function(x) get_info(data[2:3, ], x, lambda, gamma)$gain\n)\n\n[[1]]\n[1] -9.988437\n\n[[2]]\n[1] -10\n\n\nAll the splits have negative gains. So, we do not split this leaf just like at \\(b=1\\).\nSo, the final tree for this iteration (\\(b = 1\\)) is\n\nmeasures_b2 <- get_info(data, 2, lambda, gamma)\n\n#| code-fold: true\n#| fig-height: 2\n#| fig-width: 4\n\nDiagrammeR::grViz(\n  paste0(\n  \"\n  digraph {\n    graph [ranksep = 0.2]\n    node [shape = box, width = 0.4, height = 0.15, fontsize = 3, fixedsize = TRUE, penwidth = 0.2]\n      T1R [label = 'L: -8.18 \\n w* = \", round(measures_b2$w_L, digits = 2), \"']\n      T1L [label = 'R: 0.71 , 1.71 , 5.71 \\n w* = \", round(measures_b2$w_R, digits = 2), \"']\n      T0 [label = '-8.18, 0.71 , 1.71 , 5.71']\n    edge [penwidth = 0.2, arrowsize = 0.3, len = 0.3]\n      T0->T1L\n      T0->T1R\n    { rank = same; T1R; T1L}\n  }\n  \"\n  )\n\n)\n\n\n\n\n\nLet’s now update our predictions.\n\ndata %>% \n  .[1, f_2 := f_1 + measures_b2$w_L * eta] %>%  \n  .[2:4, f_2 := f_1 + measures_b2$w_R * eta] \n\n\nCodeggplot(data = data) +\n  geom_point(aes(y = y, x = x, color = \"observed\")) +\n  geom_point(aes(y = f_2, x = x, color = \"f2\")) +\n  geom_point(aes(y = f_1, x = x, color = \"f1\")) +\n  geom_point(aes(y = f_0, x = x, color = \"f0\")) +\n  scale_color_manual(\n    values = \n      c(\n        \"f0\" = \"blue\", \n        \"f1\" = \"red\",\n        \"f2\" = \"red\",\n        \"observed\" = \"black\"\n      ), \n    name = \"\"\n  ) +\n  geom_segment(\n    aes(y = f_0, x = x, yend = f_1, xend = x), \n    color = \"blue\",\n    arrow = arrow(length = unit(0.1, \"cm\"))\n  ) +\n  geom_segment(\n    aes(y = f_1, x = x, yend = f_2, xend = x), \n    color = \"blue\",\n    arrow = arrow(length = unit(0.1, \"cm\"))\n  ) +\n  theme_bw()\n\n\n\n\nAgain, we made small improvements in our predictions. This process continues until user-specified stopping criteria is met.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\\(\\lambda\\):\n\nA higher value of \\(\\lambda\\) leads to a lower value of prediction updates (\\(w^*\\)).\nA higher value of \\(\\lambda\\) leads to a lower value of quality score (\\(Q\\)), thus leading to a lower value of gain (\\(G\\)), which then leads to more aggressive pruning for a given value of \\(\\gamma\\).\n\n\n\n\\(\\gamma\\):\n\nA higher value of \\(\\gamma\\) leads to more aggressive pruning.\n\n\n\n\\(\\eta\\):\n\nA higher value of \\(\\eta\\) leads to faster learning."
  },
  {
    "objectID": "P03-xgb.html#implementation",
    "href": "P03-xgb.html#implementation",
    "title": "\n9  Extreme Gradient Boosting\n",
    "section": "\n9.4 Implementation",
    "text": "9.4 Implementation\n\n\nR\nPython\n\n\n\nYou can use the xgboost package to implement XGB modeling.\n\nlibrary(xgboost)\n\n\nAttaching package: 'xgboost'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\n\nThe first task is to create a class of matrix called xgb.DMatrix using the xgb.DMatrix() function. You provide the explanatory variable data matrix to the data option and the dependent variable matrix (vector) to the label option in xgb.DMatrix() like below.\nLet’s get the mlb1 data from the wooldridge package for demonstration.\n\nlibrary(wooldridge)\ndata(mlb1)\n\nmlb1_dt <- \n  mlb1 %>% \n  data.table() %>% # turn into data.table \n  .[, salary := NULL] %>% # remove salary (use lsalary instead)\n  na.omit() # remove observations with NA in any of the variables\n\n\nmlb1_dm_X <- \n  xgb.DMatrix(\n    data = as.matrix(mlb1_dt[, .(hruns, years, rbisyr, allstar, runsyr, hits, bavg)]),\n    label = as.matrix(mlb1_dt[, lsalary])\n  )\n\nWe can then use xgb.train() to train a model using the XGB algorithm.\n\nxgb_fit <-\n  xgb.train(\n    data = mlb1_dm_X, # independent variable\n    nrounds = 100, # number of iterations (trees to add)\n    eta = 1, # learning rate\n    objective = \"reg:squarederror\" # objective function\n  )"
  },
  {
    "objectID": "A01-mc-simulation.html",
    "href": "A01-mc-simulation.html",
    "title": "Appendix A — Monte Carlo (MC) Simulation",
    "section": "",
    "text": "Monte Carlo (MC) simulation is an important tool to test econometric hypothesis  numerically and it is highly desirable that you can conduct your own MC simulations that fit your need. Suppose you are interested in learning whether the OSL estimator of a simple linear-in-parameter model is unbiased when the error term is correlated with one of the explanatory variables. Well, it has been theoretically proven that the OLS estimator is biased under such a data generating process. So, we do not really have to show this numerically. But, what if you are facing with a more complex econometric task for which the answer is not clear for you? For example, what is the impact of over-fitting the first stage estimations in a double machine learning approach to the bias and efficiency of the estimation of treatment effect in the second stage? We can partially answer to this question (though not generalizable unlike theoretical expositions) using MC simulations. Indeed, this book uses MC simulations often to get insights into econometric problems for which the answers are not clear or to just confirm if econometric theories are correct.\nIt is important to first recognize that it is  impossible  to test econometric theory using real-wold data. That is simply because you never know the underlying data generating process of real-world data. In MC simulations, we generate data according to the data generating process we specify. This allows us to check if the econometric outcome is consistent with the data generating process or not. This is the reason every journal article with newly developed statistical procedures published in an econometric journal has MC simulations to check the new econometric theories are indeed correct (e.g., whether the new estimator is unbiased or not).\nHere, we learn how to program MC simulations using a very simple econometric example. Specifically, suppose you are interested in checking what happens to OLS estimators if \\(E[u|x]=0\\) (the error term and \\(x\\) are not correlated) is violated in estimating \\(y = \\alpha + \\beta x + u\\)."
  },
  {
    "objectID": "A01-mc-simulation.html#random-number-generator",
    "href": "A01-mc-simulation.html#random-number-generator",
    "title": "Appendix A — Monte Carlo (MC) Simulation",
    "section": "\nA.1 Random number generator",
    "text": "A.1 Random number generator\nTo create a dateset, we use pseudo random number generators. In most cases, runif() and rnorm() are sufficient.\n\n#=== uniform ===#\nx_u <- runif(1000) \n\nhead(x_u)\n\n[1] 0.23398740 0.91079122 0.21408432 0.76835379 0.03708686 0.64953793\n\nhist(x_u)\n\n\n\n\n\n#=== normal ===#\nx_n <- rnorm(1000, mean = 0, sd = 1)\n\nhead(x_n)\n\n[1] -0.57114030  0.88749952 -0.04288461  0.06277424  1.01654429 -0.75117239\n\nhist(x_n)\n\n\n\n\nWe can use runif() to draw from the Bernouli distribution, which can be useful in generating a treatment variable.\n\n#=== Bernouli (0.7) ===#\nrunif(30) > 0.3\n\n [1]  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE\n[13]  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE\n[25]  TRUE  TRUE FALSE  TRUE FALSE  TRUE\n\n\nThey are called  pseudo  random number generators because they are not truly random. What sequence of numbers you get is determined by  seed . In R, you can use set.seed() to set seed.\n\nset.seed(43230)\nrunif(10)\n\n [1] 0.78412711 0.06118740 0.02052893 0.80489633 0.69706142 0.65549966\n [7] 0.18618487 0.87417016 0.39325872 0.06729849\n\n\nIf you run the code on your computer, then you would get exactly the same set of numbers. So, pseudo random generators generate random-looking numbers, but it is not truly random. You are simply drawing from a pre-determined sequence of number that  act like random numbers. This is a very important feature of pseudo random number generators. The fact that  anybody can generate the same sequence of numbers mean that any results based on pseudo random number generators can be reproducible. When you use MC simulations, you  must set a seed so that your results are reproducible."
  },
  {
    "objectID": "A01-mc-simulation.html#mc-simulation-steps",
    "href": "A01-mc-simulation.html#mc-simulation-steps",
    "title": "Appendix A — Monte Carlo (MC) Simulation",
    "section": "\nA.2 MC simulation steps",
    "text": "A.2 MC simulation steps\n\nStep 1: Generate data based on the data generating process  you  specify\nStep 2: Get an estimate based on the generated data (e.g. OLS, mean)\nStep 3: Repeat Steps 1 and 2 many times (e.g., 1000)\nStep 4: Compare your estimates with the true parameter specified in Step 1\n\nGoing though Steps 1 and 2 only once is not going to give you an idea of how the estimator of interest performs. So, you repeat Steps 1 and 2 many times to what you can expect form the estimator on average.\nLet’s use a very simple example to better understand the MC steps. The statistical question of interest here is whether sample mean is an unbiased estimator of the expected value: \\(E[\\frac{1}{n}\\sum_{i=1}^n x_i] = E[x]\\), where \\(x_i\\) is an independent random draw from the same distribution.\n\n\nOf course, \\(x_i\\) does not have to be independent. But, just making things as simple as possible.\nHere is Steps 1.\n\nx <- runif(100) \n\nHere, \\(x\\) follows \\(Unif(0, 1)\\) and \\(E[x] = 0.5\\). This is the data generating process. And, data (x) has been generated using x <- runif(100).\nStep 2 is the estimation of \\(E[x]\\). The estimator is the mean of the observed values of x.\n\n(\nmean_x <- mean(x)\n)\n\n[1] 0.5226597\n\n\nOkay, pretty close. But, remember this is just a single realization of the estimator. Let’s move on to Step 3 (repeating the above many times). Let’s write a function that does Steps 1 and 2.\n\nget_estimate <- function()\n{\n  x <- runif(100) \n  mean_x <- mean(x)\n  return(mean_x)\n}\n\nYou can now repeat get_estimate() many times. There are numerous ways to do this in R. But, let’s use lapply() here.\n\nestimates <-\n  lapply(\n    1:1000,\n    function(x) get_estimate()\n  ) %>% \n  unlist()\n\nHere is the mean of the estimates (the estimate of \\(E[\\frac{1}{n}\\sum_{i=1}^n x_i]\\)).\n\nmean(estimates)\n\n[1] 0.4991132\n\n\nVery close. Of course, you will not get the exact number you are hoping to get, which is \\(0.5\\) in this case as MC simulation is a random process.\nWhile this example may seem excessively simple, no matter what you are trying to test, the basic steps will be exactly the same."
  },
  {
    "objectID": "A01-mc-simulation.html#another-example",
    "href": "A01-mc-simulation.html#another-example",
    "title": "Appendix A — Monte Carlo (MC) Simulation",
    "section": "\nA.3 Another Example",
    "text": "A.3 Another Example\nLet’s work on a slightly more complex MC simulations. We are interested in understanding what happens to \\(\\beta_1\\) if \\(E[u|x]\\ne 0\\) when estimating \\(y=\\beta_0+\\beta_1 x + u\\) (classic endogeneity problem).\nLet’s set some parameters first.\n\nB <- 1000 # the number of iterations\nN <- 100 # sample size\n\nLet’s write a code to generate data for a single iteration (Step 1).\n\nmu <- rnorm(N) # the common term shared by both x and u\nx <- rnorm(N) + mu # independent variable\nu <- rnorm(N) + mu # error\ny <- 1 + x + u # dependent variable\ndata <- data.frame(y = y, x = x)\n\nSo, the target parameter (\\(\\beta_1\\)) is 1 in this data generating process. x and u are correlated because they share the common term mu.\n\ncor(x, u)\n\n[1] 0.5222601\n\n\nThis code gets the OLS estimate of \\(\\beta_1\\) (Step 2).\n\nlm(y ~ x, data = data)$coefficient[\"x\"]\n\n       x \n1.518179 \n\n\nOkay, things are not looking good for OLS already.\nLet’s repeat Steps 1 and 2 many times (Step 3).\n\nget_ols_estimate <- function()\n{\n  mu <- rnorm(N) # the common term shared by both x and u\n  x <- rnorm(N) + mu # independent variable\n  u <- rnorm(N) + mu # error\n  y <- 1 + x + u # dependent variable\n  data <- data.frame(y = y, x = x)\n\n  beta_hat <- lm(y ~ x, data = data)$coefficient[\"x\"]\n\n  return(beta_hat)\n}\n\nestimates <-\n  lapply(\n    1:1000,\n    function(x) get_ols_estimate()\n  ) %>% \n  unlist()\n\nYes, the OLS estimator of \\(\\beta_1\\) is biased as we expected.\n\nmean(estimates)\n\n[1] 1.50085\n\nhist(estimates)"
  }
]