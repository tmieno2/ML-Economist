[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Machine Learning for Economists (Under Construction)",
    "section": "",
    "text": "This book provides an introduction to machine learning methods for economists with a strong focus on causal machine learning methods. It is still under development and it still has a long way to go before it is completed. One thing that may distinguish this book from the books on machine learning methods on the market is that it attempts to be illustrative and easy to understand for those who are not familiar with machine learning methods. This is done by providing step-by-step R codes to enhance understanding of various algorithms. This, however, does not mean that anybody can read the book and understand the materials. It is desirable that you have gone through 1st-year Ph.D. econometrics sequence (or equivalent)."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Introduction to Machine Learning for Economists (Under Construction)",
    "section": "License",
    "text": "License\nThis website is (and will always be) free to use, and is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 4.0 License."
  },
  {
    "objectID": "H00-preface.html",
    "href": "H00-preface.html",
    "title": "Preface",
    "section": "",
    "text": "It is critical to understand the distinctions between  prediction  and  causal inference  for anybody who is interested in using machine learning (ML) methods. This is because a method designed to for the former objective may not work well for the latter objective, and vice versa.\n\n\n\n\n\n\nImportant\n\n\n\n\n Prediction: it aims at predicting accurately the level of the variable of interest (the dependent variable) well based on explanatory variables.\n Causal Inference: it aims at predicting the change in the dependent variable when an explanatory variable of interest changes its value.\n\n\n\nExamples where  prediction matters:\n\nprediction of the future price of corn when the modeler is interested in using the predicted price to make money in the futures market\nprediction of the crop yield by field when the modeler is interested in using the field-level predicted crop yields as an dependent or explanatory variable in a regression analysis (e.g., the impact of weather on crop yield)\nprediction of what is in the vicinity of a self-driving car (the user)\n\nWhat is common among these examples is that the users wants to use the  level  or state of the dependent variable to drive their decisions.\nExamples where  causal inference matters:\n\nunderstand the impact of a micro-finance program on welfare in developing countries when the modelers is interested in whether they should implement such a program or not (does the benefit of implementing the program worth the cost?). The modelers do not care about what level of welfare people are gonna be at. They care about how much improvement (change) in welfare the program would make.\nunderstand the impact of water use limits for farmers on groundwater usage when the modeler (water managers) are interested in predicting how much water use reduction (change) they can expect.\nunderstand the impact of fertilizer on yield for when the modelers are interested in identifying the profit-maximizing fertilizer level. The modelers do not care about what the yield levels are going to be at different fertilizer levels. They care about how much yield improvement (change) can be achieved when more fertilizer is applied.\n\nWhat is common among these examples is that the users wants to use the information about the  change  in the dependent variable after changing the value of an explanatory variable (implementing a policy) in driving their decisions.\nNow, you may think that once you can predict the  level  of the dependent variable as a function of explanatory variables \\(X\\), say \\(\\hat{f}(X)\\), where \\(\\hat{f}(\\cdot)\\) is the trained model, then you can simply take the difference in the predicted values of the dependent variable evaluated at \\(X\\) before (\\(X_0\\)) and after (\\(X_1\\)) to find the change in the dependent variable caused by the change in \\(X\\).\n\\[\n\\begin{aligned}\n\\hat{f}(X_1) - \\hat{f}(X_0)\n\\end{aligned}\n\\]\nYou are indeed right and you can predict the change in the dependent variable when the value of an explanatory variable changes once the model is trained to predict the level of the dependent variable. However, this way of predicting the impact of \\(X\\) (the continuous treatment version of the so-called S-learner) is often biased. Instead, (most of) causal machine learning methods razor-focus on  directly  estimating the change in the dependent variable when the value of an explanatory variable changes and typically performs better.\nTraditionally, the vast majority of ML methods focused on prediction, rather than causal inference. It is only recently (I would say around 2015) that academics and practitioners in industry started to realize the limitation of prediction-oriented methods for many of the research and business problems they need to solve. In response, there are now an emerging tide of new kinds of machine learning methods called causal machine learning methods, which focus on causal identification of a treatment (e.g., pricing of a product, policy intervention, etc). The goal of this book is to learn such causal machine learning methods and add them to your econometric tool box for practical applications. This, however, does not mean we do not learn any prediction-oriented (traditional) machine learning methods. Indeed, it is essential to understand them because the prominent causal machine learning methods do use prediction-oriented ML methods in its process as we will see later. It is just that we do not use prediction-oriented ML methods by themselves for the task of identifying the causal impact of a treatment."
  },
  {
    "objectID": "H00-preface.html#r-and-python",
    "href": "H00-preface.html#r-and-python",
    "title": "Preface",
    "section": "R and Python",
    "text": "R and Python\n\nR reticulate package: can call python functions from within R"
  },
  {
    "objectID": "B01-nonlinear.html",
    "href": "B01-nonlinear.html",
    "title": "1  Non-linear function estimation",
    "section": "",
    "text": "The purpose of this section is to introduce you to the idea of semi-parametric and non-parametric regression methods. The world of semi-parametric and non-parametric regression is vast. But, we only scratch the surface by just looking at smoothing splines and local regression methods in this chapter. Learning these methods also provide us with excellent opportunities to learn the phenomenon called over-fitting and also the importance of hyper-parameters. Understanding local regression in particular is essential in understanding generalized random forest covered in Chapter 17."
  },
  {
    "objectID": "B01-nonlinear.html#sec-functional-form",
    "href": "B01-nonlinear.html#sec-functional-form",
    "title": "1  Non-linear function estimation",
    "section": "1.1 Flexible functional form estimation",
    "text": "1.1 Flexible functional form estimation\n\n\nPackages to load for replications\n\nlibrary(data.table)\nlibrary(tidyverse)\nlibrary(mgcv)\nlibrary(parallel)\nlibrary(patchwork)\nlibrary(caret)\n\nThere is a clear limit to liner (in parameter) parametric models in flexibility to represent quantitative relationships between variables. For example, consider crop yield response to fertilizer. Typically, yield increases at the diminishing rate as fertilizer rate increases. However, at a high enough fertilizer rate, yield stops increasing (fertilizer is not a limiting factor at that point). This relationship is illustrated in the figure below.\n\nset.seed(83944)\n\n#=== generate data ===#\nN <- 400 # number of observations\nx <- seq(1, 250, length = N) \ny_det <- 240 * (1 - 0.4 * exp(- 0.03 * x))\ne <- 25 * rnorm(N) # error\ndata <- data.table(x = x, y = y_det + e, y_det = y_det)\n\n#=== plot ===#\n(\ng_base <- ggplot(data) +\n  geom_line(aes(y = y_det, x = x)) +\n  theme_bw()\n)\n\n\n\n\n\n\n\n\nLet’s try to fit this data using linear parametric models with \\(sqrt(x)\\), \\(log(x)\\), and \\(x + x^2\\), where the dependent variable is y_det, which is \\(E[y|x]\\) (no error added).\n\n#=== sqrt ===#\nlm_sq <- lm(y_det ~ sqrt(x), data = data)\ndata[, y_hat_sqrt := lm_sq$fit]\n\n#=== log ===#\nlm_log <- lm(y_det ~ log(x), data = data)\ndata[, y_hat_log := lm_log$fit]\n\n#=== quadratic ===#\nlm_quad <- lm(y_det ~ x + x^2, data = data)\ndata[, y_hat_quad := lm_quad$fit]\n\n\n\nCode\nplot_data <-\n  melt(data, id.var = \"x\") %>% \n  .[variable != \"y\", ] %>% \n  .[, fit_case := fcase(\n    variable == \"y_det\", \"True response\",\n    variable == \"y_hat_sqrt\", \"sqrt\",\n    variable == \"y_hat_log\", \"log\",\n    variable == \"y_hat_quad\", \"quadratic\"\n  )]\n\nggplot(plot_data) +\n  geom_line(aes(y = value, x = x, color = fit_case)) +\n  scale_color_discrete(name = \"\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nNone of the specifications do quite well. Indeed, you cannot represent the relationship well using well-known popular functional forms. Let’s now look at methods that are flexible enough to capture the relationship. First, smoothing splines, and then K-nearest neighbor next."
  },
  {
    "objectID": "B01-nonlinear.html#sec-smoothing-splines",
    "href": "B01-nonlinear.html#sec-smoothing-splines",
    "title": "1  Non-linear function estimation",
    "section": "1.2 Smoothing Splines (semi-parametric)",
    "text": "1.2 Smoothing Splines (semi-parametric)\nDetailed discussion of smoothing splines is out of the scope of this book. Only its basic ideas will be presented in this chapter. See Wood (2006) for a fuller treatment of this topic.\nConsider a simple quantitative relationship of two variables \\(y\\) and \\(x\\): \\(y = f(x)\\).\n\\[\n\\begin{aligned}\ny = f(x)\n\\end{aligned}\n\\]\nIt is possible to characterize this function by using many functions in additive manner: \\(b_1(x), \\dots, b_K(x)\\).\n\\[\n\\begin{aligned}\ny = \\sum_{k=1}^K \\beta_k b_k(x)\n\\end{aligned}\n\\]\nwhere \\(\\beta_k\\) is the coefficient on \\(b_k(x)\\).\nHere are what \\(b_1(x), \\dots, b_K(x)\\) may look like (1 intercept and 9 cubic spline functions).\n\n\nCode\nbasis_data <-\n  gam(y_det ~ s(x, k = 10, bs = \"cr\"), data = data) %>% \n  predict(., type = \"lpmatrix\") %>% \n  data.table() %>% \n  .[, x := data[, x]] %>% \n  melt(id.var = \"x\")\n\nggplot(data = basis_data) +\n  geom_line(aes(y = value, x = x)) +\n  facet_grid(variable ~ .) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nBy assigning different values to \\(b_1(x), \\dots, b_K(x)\\), their summation can represent different functional relationships.\nHere is what \\(\\sum_{k=1}^K \\beta_k b_k(x)\\) looks like when \\(\\beta_1\\) through \\(\\beta_{10}\\) are all \\(200\\).\n\n\n\\[\ny = \\sum_{k=1}^{10} 200 b_k(x)\n\\]\n\n\nCode\ndata.table(\n  variable = unique(basis_data$variable),\n  coef = rep(200, 10)\n) %>% \n.[basis_data, on = \"variable\"] %>% \n.[, sum(coef * value), by = x] %>% \nggplot(data = .) +\n  geom_line(aes(y = V1, x = x)) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nHere is what \\(\\sum_{k=1}^K \\beta_k b_k(x)\\) looks like when \\(\\beta_1\\) through \\(\\beta_4\\) are all \\(50\\) and \\(\\beta_5\\) through \\(\\beta_9\\) are all \\(200\\).\n\n\n\\[\ny = \\sum_{k=1}^5 50 b_k(x) + \\sum_{k=6}^{10} 200 b_k(x)\n\\]\n\n\nCode\ndata.table(\n  variable = unique(basis_data$variable),\n  coef = c(rep(50, 5), rep(200, 5))\n) %>% \n.[basis_data, on = \"variable\"] %>% \n.[, sum(coef * value), by = x] %>% \nggplot(data = .) +\n  geom_line(aes(y = V1, x = x)) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nIn practice, we fit the model to a dataset to find coefficient estimates that fit the data well. Here, we use the gam() function from the mgcv package. Note that, we use \\(E[y|x]\\) (y_det) as the dependent variable to demonstrate the ability of smoothing splines to imitate the true function.\n\n\ngam stands for  Generalized Additive Model. It is a much wider class of model than our examples in this section. See Wood (2006) for more details.\n\ngam_fit <- gam(y_det ~ s(x, k = 10, bs = \"cr\"), data = data)\n\ns(x, k = 10, bs = \"cr\") in the regression formula tells gam() to use 10 knots, which results in an intercept and nine spline basis functions. bs = \"cr\" tells gam() to use cubic spline basis functions.\n\n\nThere are many other spline basis options offered by the mgcv package. Interested readers are referred to Wood (2006).\nHere are the coefficient estimates:\n\ngam_fit$coefficient\n\n(Intercept)      s(x).1      s(x).2      s(x).3      s(x).4      s(x).5 \n 227.449797   -1.487811   16.749494   28.308769   32.116284   34.173014 \n     s(x).6      s(x).7      s(x).8      s(x).9 \n  35.179359   34.540353   38.592136   21.874699 \n\n\nThis translate into the following fitted curve.\n\n\nCode\ndata.table(\n  variable = unique(basis_data$variable)[-1],\n  coef = gam_fit$coefficient[-1]\n) %>% \n.[basis_data, on = \"variable\"] %>% \n.[, .(y_no_int = sum(coef * value)), by = x] %>% \n.[, y_hat := gam_fit$coefficient[1] + y_no_int] %>% \nggplot(data = .) +\n  geom_line(aes(y = y_hat, x = x, color = \"gam-fitted\")) +\n  geom_line(data = data, aes(y = y_det, x = x, color = \"True\")) +\n  scale_color_manual(\n    name = \"\",\n    values = c(\"gam-fitted\" = \"red\", \"True\" = \"blue\")\n  ) +\n  ylab(\"y\") +\n  xlab(\"x\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nAs you can see, the trained model is almost perfect in representing the functional relationship of \\(y\\) and \\(x\\).\nNow, when gam() fits a model to a dataset, it penalizes the wiggliness (how wavy the curve is) of the estimated function to safe-guard against fitting the model too well to the data. Specifically, it finds coefficients that minimizes the sum of the squared residuals (for regression) plus an additional term that captures how wavy the resulting function is.\n\n\nHere is an example of wiggly (first) v.s. smooth (second) functions.\n\n\nCode\ngam_fit_wiggly <- gam(y ~ s(x, k = 40, bs = \"cr\", sp = 0), data = data)\nplot(gam_fit_wiggly, se = FALSE)\n\n\n\n\n\nCode\ngam_fit_smooth <- gam(y ~ s(x, k = 5, bs = \"cr\"), data = data)\nplot(gam_fit_smooth, se = FALSE)\n\n\n\n\n\n\\[\n\\begin{aligned}\nMin_{\\hat{f}(x)} \\sum_{i=1}^N(y_i - \\hat{f}(x_i))^2 + \\lambda \\Omega(\\hat{f}(x))\n\\end{aligned}\n\\]\nwhere \\(\\Omega(\\hat{f}(x)) > 0\\) is a function that captures how wavy the resulting function is. It takes a higher value when \\(\\hat{f}(x)\\) is more wiggly. \\(\\lambda > 0\\) is the penalization parameter. As \\(\\lambda\\) gets larger, a greater penalty on the wiggliness of \\(\\hat{f}(x)\\), thus resulting in a smoother curve.\nYou can specify \\(\\lambda\\) by sp parameter in gam(). When sp is not specified by the user, gam() finds the optimal value of sp internally using cross-validation (cross-validation will be introduce formally in Chapter 3). For now, just consider it as a method to find parameters that make the trained model a good representation of the underlying conditional mean function (\\(E[y|x]\\)).\n\n\nMore specifically, it uses generalized cross-validation (GCV). A special type of cross-validation that can be done when the model is linear in parameter.\nIf you do not pick the value of sp well, the estimated curve will be very wiggly. Let’s see an example by setting the value of sp to 0, meaning no punishment for being very wiggly. We also set the number of splines to \\(39\\) so that \\(\\sum_{k=1}^K \\beta_k b_k(x)\\) is  very flexible.\n\n\nCode\n#=== fit ===#\ngam_fit_wiggly <- gam(y ~ s(x, k = 40, bs = \"cr\", sp = 0), data = data)\n\n#=== assign the fitted values to a variable ===#\ndata[, y_hat := gam_fit_wiggly$fitted.values]\n\n#=== plot ===#\nggplot(data = data) +\n  geom_line(aes(y = y_hat, x = x), color = \"red\") +\n  geom_point(aes(y = y, x = x), size = 0.7) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nWe call this phenomenon over-fitting (of the data by the model). An over-fitted model does well in predicting \\(y\\) when applied to the data the model used to train itself. However, it would do a terrible job in prediction on the data it has never seen clearly because it is not predicting \\(E[y|x]\\) well.\n\n\n\n\n\n\nImportant\n\n\n\n\n Hyper-parameter: parameters that one has freedom to specify before fitting the model and affect the fitting process in ways that change the outcome of the fitting.\n Parameter tuning: process that attempts to find the optimal set of hyper-parameter values.\n\n\n\nIn mgcv::gam(), the hyper-parameters are the penalty parameter \\(\\lambda\\) (specified by. sp), the number of knots (specified by k)\\(^1\\), the type of splines (specified by bs). Coefficient estimates (\\(\\alpha\\), \\(\\beta_1, \\dots, \\beta_K\\)) change when the value of sp is altered. Here is what happens when k \\(= 3\\) (less flexible than the k \\(= 39\\) case above).\n\n\n\\(^1\\) or more precisely, how many knots and where to place them\n\n#=== fit ===#\ngam_fit_wiggly <- gam(y ~ s(x, k = 3, bs = \"cr\", sp = 0), data = data)\n\n#=== assign the fitted values to a variable ===#\ndata[, y_hat := gam_fit_wiggly$fitted.values]\n\n#=== plot ===#\nggplot(data = data) +\n  geom_line(aes(y = y_hat, x = x), color = \"red\") +\n  geom_point(aes(y = y, x = x), size = 0.7) +\n  theme_bw()\n\n\n\n\n\n\n\n\nHyper-parameters can significantly influence the outcome. Since the user gets to pick any numbers, it can potentially be used to twist the results in a way that favors the outcomes they want to have. Therefore, it is important to pick the values of hyper-parameters wisely. One way of achieving the goal is cross-validation, which is a data-driven way of finding the best value of hyper-parameters. We will discuss cross-validation in ?sec-cv in detail.\nHere is the fitted curve when the optimal value of sp is picked by gam() automatically given k = 40 and bs = \"cr\" using cross-validation.\n\n\nThat is, we are not tuning k and bs here. gam() uses generalized cross-validation, which we do not cover in this book.\n\n#=== fit ===#\ngam_fit_cv <- gam(y ~ s(x, k = 40, bs = \"cr\"), data = data)\n\n#=== assign the fitted values to a variable ===#\ndata[, y_hat := gam_fit_cv$fitted.values]\n\n#=== plot ===#\nggplot(data = data) +\n  geom_line(aes(y = y_hat, x = x), color = \"red\") +\n  geom_point(aes(y = y, x = x), size = 0.7) +\n  theme_bw()\n\n\n\n\n\n\n\n\nYou can see that the tuning of sp is successful and has resulted in a much better fitted curve compared to the case where sp was forced to be 0. As you will see, hyper-parameter tuning will be critical for many of the machine learning methods we will look at later."
  },
  {
    "objectID": "B01-nonlinear.html#sec-local",
    "href": "B01-nonlinear.html#sec-local",
    "title": "1  Non-linear function estimation",
    "section": "1.3 Local Regression",
    "text": "1.3 Local Regression\nLocal regression is a non-parametric method that predicts \\(y\\) at the target value of \\(X\\) (say \\(x_0\\)) using the observations that are “close” to it (called neighbors). One of such methods is Nadaraya-Watson (NW) estimator.\nPrediction of \\(y\\) at a particular value of \\(X\\) by the NW estimator takes the following steps:\n\nFind the weights for all the observations based on the choice of Kernel function (\\(K(\\cdot)\\)) and bandwidth value (\\(h\\))\nCalculate the weighted mean of \\(y\\)\n\nMathematically, it can be written as follows:\n\\[\n\\begin{aligned}\n\\hat{f}(X = x_0) & = \\sum_{i=1}^N W_i(x_0)\\cdot Y_i \\\\\n\\mbox{where, } W_i(x_0) & = \\frac{K(\\frac{x_0 - X_i}{h})}{\\sum_{i=1}^n K(\\frac{x_0 - X_i}{h})}\n\\end{aligned}\n\\]\n\n\\(K()\\): Kernel function\n\\(h\\): bandwidth\n\nNote that \\(\\sum_{i=1}^N W_i = 1\\), and \\(W_i(x_0)\\) is considered a weight for observation \\(i\\). So, this estimator simply calculates the weighted average of observed values of the dependent variable.\nNow, let’s have a closer look at the weight. There are many types of Kernel functions. Some of them include\n\nUniform: \\(K(u) = 1/2\\cdot I{|u| < 1}\\)\nEpanechnikov: \\(K(u) = \\frac{3}{4}(1-u^2)\\cdot I{|u| < 1}\\)\nGaussian: \\(K(u) = \\frac{1}{\\sqrt{2\\pi}}\\cdot e^{-\\frac{1}{2}u^2}\\) (density function of standard normal distribution)\n\n\n\n\\(I\\{\\cdot\\}\\) is an indicator function that takes 1 if the statement inside the curly brackets is true, and 0 otherwise.\nFigure 1.1 presents graphical representations of these functions.\n\n\nCode\nu_seq <- seq(-3, 3, length = 1000)\n\nuni_data <- \n  data.table(\n    u = u_seq,\n    d = 1/2\n  ) %>% \n  .[, type := \"Uniform\"] %>% \n  .[abs(u) > 1, d := 0]\n\nepanechnikov_data <- \n  data.table(\n    u = u_seq\n  ) %>% \n  .[, d := 3 / 4 * (1-u^2)] %>% \n  .[, type := \"Epanechnikov\"] %>% \n  .[abs(u) > 1, d := 0]\n\ngaussian_data <-\n  data.table(\n    u = u_seq\n  ) %>% \n  .[, d := dnorm(u)] %>% \n  .[, type:= \"Gaussian\"]\n\nall_data <- \n  rbind(uni_data, epanechnikov_data, gaussian_data) %>% \n  .[, type := factor(type, levels = c(\"Uniform\", \"Epanechnikov\", \"Gaussian\"))]\n\nggplot(all_data) +\n  geom_line(aes(y = d, x = u), color = \"blue\") +\n  facet_grid(type ~ .) +\n  theme_bw()\n\n\n\n\n\nFigure 1.1: Kernel function examples\n\n\n\n\nClearly, the choice of Kernel function affects the weights given to each observation. Bandwidth (\\(h\\)) determines how “local” the estimator is. The lower the value of \\(h\\) is, the more “local” the estimator is. Let’s see this using the uniform Kernel with bandwidth values of \\(2\\) and \\(5\\). We use the same data generated in Section 1.2. Suppose we are interested in estimating y at x = 100.\nFigure 1.2 presents the the data points on the first row for \\(h = 2\\) (left) and \\(h=5\\) (right) and corresponding weights on the second row \\(h = 2\\) (left) and \\(h=5\\) (right).\nAs you can see, when \\(h\\) is higher, more data points will involve in estimating \\(y\\): 5 observations when \\(h=2\\) and 12 observations when \\(h=5\\).\n\n\n\n\n\nFigure 1.2: Weights under h = 2 and h = 5.\n\n\n\n\nLet’s write a code to predict y at several different values of x using data for illustration. We will use the Epanechnikov Kernel function with bandwidth value of \\(20\\). First, we work on prediction at x = 120.\n\nx0 <- 120\nh <- 20\n\nlr_data <-\n  copy(data) %>% \n  #=== calculate u ===#\n  .[, u := (x - x0)/h] %>% \n  #=== K(u) ===#\n  .[, k_of_u := 3 / 4 * (1-u^2)] %>% \n  #=== turn k_of_u to 0 if abs(u) > 1  ===#\n  .[abs(u) >= 1, k_of_u := 0] %>% \n  #=== find the weight ===#\n  .[, weight := k_of_u / sum(k_of_u)]\n\nHere are some of the data points that have non-zero weights:\n\nlr_data[weight != 0, .(x, y, k_of_u, weight)] %>% head()\n\n          x        y     k_of_u       weight\n1: 100.2256 243.1649 0.01682189 0.0005248584\n2: 100.8496 219.0103 0.06236832 0.0019459481\n3: 101.4737 250.9886 0.10645429 0.0033214707\n4: 102.0977 214.2540 0.14907983 0.0046514262\n5: 102.7218 221.5801 0.19024493 0.0059358145\n6: 103.3459 262.3424 0.22994958 0.0071746358\n\n\nWe can then simply calculate the weighted mean.\n\n(\ny_hat_x120 <- lr_data[, sum(weight * y)]\n)\n\n[1] 233.8969\n\n\nWe can do this for many values of \\(X\\) and and connects the dots to find the “fitted curve.” First, let’s define a function that predicts y at a particular value of x using the Epanechnikov Kernel function for a given value of \\(h\\).\n\nest_y_nw <- function(x0, data, h)\n{\n  y_hat <-\n    copy(data) %>% \n    #=== calculate u ===#\n    .[, u := (x - x0)/h] %>% \n    #=== K(u) ===#\n    .[, k_of_u := 3 / 4 * (1-u^2)] %>% \n    #=== turn k_of_u to 0 if abs(u) > 1  ===#\n    .[abs(u) >= 1, k_of_u := 0] %>% \n    #=== find the weight ===#\n    .[, weight := k_of_u / sum(k_of_u)] %>% \n    .[, sum(weight * y)]\n\n  return_data <-\n    data.table(\n      x = x0,\n      y_hat = y_hat\n    )\n\n  return(return_data)\n}\n\nNow, we do predictions at various values of x.\n\n#=== sequence of values of x to do predictions at ===#\nx_seq <- seq(data[, min(x)], data[, max(x)], length = 100)\n\n#=== predict ===#\ny_hat_lc <-\n  lapply(\n    x_seq,\n    function(x) est_y_nw(x, data, h)\n  ) %>% \n  rbindlist() %>% \n  .[, type := \"Local Constant\"]\n\nFigure 1.3 visualizes the prediction results.\n\n\nCode\nggplot() +\n  geom_line(\n    data = y_hat_lc,\n    aes(y = y_hat, x = x),\n    color = \"red\"\n  ) +\n  geom_point(\n    data = y_hat_lc,\n    aes(y = y_hat, x = x),\n    color = \"red\",\n    size = 0.6\n  ) +\n  geom_point(\n    data = data,\n    aes(y = y, x = x),\n    size = 0.6\n  ) +\n  theme_bw()\n\n\n\n\n\nFigure 1.3: Predictions at various values of x\n\n\n\n\nOne critical difference between local non-parametric regression and smoothing splines is that the former fits the data locally (unless \\(h\\) is very high) while smoothing splines fits the data globally (it uses all the data points to fit a single curve).\nAnother important distinction between the two approaches is that local non-parametric regression requires going through the entire process above whenever predicting \\(y\\) at a particular value of \\(X\\). This means that you need the entire train dataset every time. On the other hand, once smoothing splines regression is conducted, then you no longer need the train data any more because the fitted curve is characterized completely by the basis functions and their estimated coefficients.\nNow, let’s see how the choice of \\(h\\) affects the fitted “curve.” We will try h = 2, 10, 20, 50\n\n\nCode\nget_y_hat_given_h <- function(data, x_seq, h_val)\n{\n\n  return_data <-\n    lapply(\n      x_seq,\n      function(x) est_y_nw(x, data, h_val)\n    ) %>% \n    rbindlist() %>% \n    .[, h := h_val]\n\n  return(return_data)\n}\n\ny_hat_h_various <- \n  lapply(\n    c(2, 10, 20, 50),\n    function(x) get_y_hat_given_h(data, x_seq, x)\n  ) %>% \n  rbindlist()\n\nggplot(y_hat_h_various) +\n  geom_point(data = data, aes(y = y, x = x), size = 0.8) +\n  geom_line(aes(y = y_hat, x = x), color = \"red\") +\n  facet_wrap(h ~ .) +\n  theme_bw()\n\n\n\n\n\nFigure 1.4: Fitted curves at different values of h\n\n\n\n\nAs you can see in Figure 1.4, at \\(h = 2\\), the fitted curve is extremely wiggly and over-fitted. Increasing the value of \\(h\\) make the fitted curves smoother. Nobody uses NW estimator for any practical applications when the problem at hand is high-dimensional as it suffers from the curse of dimensionality. However, NW estimator is a great example to illustrate the importance of the choice of an arbitrary parameter (hyper-parameter). As discussed above, we will later look at cross-validation as a way to tune hyper-parameters.\n\n1.3.1 Local linear regression\nThe NW estimator is a locally constant regression. You can see this from the fact that the NW estimator comes from solving the following minimization problem:\n\\[\n\\begin{aligned}\n\\hat{f}(x = x_0) = argmin_{\\alpha} \\sum_{i=1}^N W_i(x_0)(Y_i - \\alpha)^2\n\\end{aligned}\n\\]\nBy modifying the objective function slightly, we will have local linear regression:\n\\[\n\\begin{aligned}\n\\{\\hat{\\alpha}, \\hat{\\beta}\\} = argmin_{\\alpha,\\beta} \\sum_{i=1}^N W_i(x_0)(Y_i - \\alpha - \\beta (x_i - x_0))^2\n\\end{aligned}\n\\]\nThen the fitted value is given by \\(\\hat{f}(x = x_0) = \\hat{\\alpha}\\). This is because the value of \\(x\\) is re-centered around the evaluation point. Alternatively, you can solve the following without re-centering,\n\\[\n\\begin{aligned}\n\\{\\hat{\\alpha}, \\hat{\\beta}\\} = argmin_{\\alpha,\\beta} \\sum_{i=1}^N W_i(x_0)(Y_i - \\alpha - \\beta x_i)^2\n\\end{aligned}\n\\]\n, and then calculate \\(\\hat{f}(x = x_0) = \\hat{\\alpha} + \\hat{\\beta}x_0\\). They will provide the same estimate of \\(\\hat{f}(x = x_0)\\).\nLet’s implement this as an illustration for \\(x_0 = 150\\) with the Epanechnikov kernel function with \\(h=20\\).\n\nx0 <- 150\nh <- 20\n\nlr_data <-\n  copy(data) %>% \n  #=== calculate u ===#\n  .[, u := (x - x0)/h] %>% \n  #=== K(u) ===#\n  .[, k_of_u := 3 / 4 * (1-u^2)] %>% \n  #=== turn k_of_u to 0 if abs(u) > 1  ===#\n  .[abs(u) >= 1, k_of_u := 0] %>% \n  #=== find the weight ===#\n  .[, weight := k_of_u / sum(k_of_u)] %>% \n  .[, x_diff := x - x0]\n\n#=== run linear regression with the weights ===#\nols_res <- lm(y ~ x_diff, data = lr_data, weights = weight)\n\n#=== predict ===#\n(\ny_hat_x0 <- ols_res$coefficient[\"(Intercept)\"]\n)\n\n(Intercept) \n   238.5055 \n\n\nAlternatively,\n\n#=== run linear regression with the weights no centering around x0 ===#\nols_res <- lm(y ~ x, data = lr_data, weights = weight)\n\n#=== predict ===#\n(\ny_hat_x0 <- predict(ols_res, newdata = data.table(x = x0))\n)\n\n       1 \n238.5055 \n\n\nLocal linear regression is better than local constant regression if the underlying curve is significantly non-linear instead of a flat line. Local linear regression also does better near the boundary of the support of \\(x\\). Figure 1.5 presents predicted values by local constant and linear regressions. In this case, it is hard to see the difference between the two except at the edges of \\(x\\) support.\n\n\nCode\nest_y_ll <- function(x0, data, h){\n\n  lr_data <-\n    copy(data) %>% \n    #=== calculate u ===#\n    .[, u := (x - x0)/h] %>% \n    #=== K(u) ===#\n    .[, k_of_u := 3 / 4 * (1-u^2)] %>% \n    #=== turn k_of_u to 0 if abs(u) > 1  ===#\n    .[abs(u) >= 1, k_of_u := 0] %>% \n    #=== find the weight ===#\n    .[, weight := k_of_u / sum(k_of_u)] %>% \n    .[, x_diff := x - x0]\n\n  #=== run linear regression with the weights ===#\n  ols_res <- lm(y ~ x_diff, data = lr_data, weights = weight)\n\n  return_data <-\n    data.table(\n      x = x0,\n      y_hat = ols_res$coefficient[\"(Intercept)\"]\n    )\n\n  return(return_data)\n}\n\ny_hat_ll <-\n  lapply(\n    x_seq,\n    function(x) est_y_ll(x, data, h)\n  ) %>% \n  rbindlist() %>% \n  .[, type := \"Local Linear\"]\n\n#=== combine with the local constant results ===#\ndata_all <- rbind(y_hat_lc, y_hat_ll)\n\n#=== plot ===#\nggplot() +\n  geom_line(\n    data = data_all,\n    aes(y = y_hat, x = x, color = type)\n  ) +\n  geom_point(\n    data = data_all,\n    aes(y = y_hat, x = x, color = type),\n    size = 0.6\n  ) +\n  geom_point(\n    data = data,\n    aes(y = y, x = x),\n    size = 0.6\n  ) +\n  scale_color_discrete(name = \"\") +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 1.5: Predictions at various values of x by NW and LL\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nIn Section 6.2, we will see that random forest is a special case of local constant regression\nUnderstanding local regression helps you understand how generalized random forest works in Chapter 17\n\n\n\nYou can make the function locally more flexible by including polynomials of \\(x\\). For example, this is a locally cubic regression.\n\\[\n\\begin{aligned}\nMin_{\\alpha,\\beta_1,\\beta_2,\\beta_3} \\sum_{i=1}^N W_i(x_0)(Y_i - \\alpha - \\beta (x_i-x_0) - \\beta_2 (x_i-x_0)^2 - \\beta_3 (x_i-x_0)^3)^2\n\\end{aligned}\n\\]\n\n\n\n\n\n\nUseful Resource\n\n\n\nFor those who are interested in fuller treatments of local regression, this chapter of García-Portugués (2022).\n\n\n\n\n1.3.2 K-nearest neighbor\nK-nearest neighbor (KNN) regression is a special case of local constant regression. The prediction of \\(y\\) conditional on \\(x\\) is simply the average of \\(y\\) observed for the K closest (in terms of distance to \\(x\\)) observations in the data. So, it is like the NW estimator with the uniform Kernel function. But, it is not a NW estimator because NW estimator does not fix the number of observations in the neighbor.\n\n\nPredictions at points around a more densely populated area have larger numbers of observations in their neighbors.\nThe hyper-parameter for KNN regression is k (the number of neighbors). The choice of the value of \\(k\\) has a dramatic impacts on the fitted curve just like how the choice of \\(h\\) affects the fitted curve by the NW estimator.\n\n\nCode\nplot_data <-\n  data.table(\n    k = c(1, 5, 10, 20)\n  ) %>% \n  rowwise() %>% \n  mutate(knn_fit = list(\n    knnreg(y ~ x, k = k, data = data)\n  )) %>% \n  mutate(eval_data = list(\n    data.table(x = seq(0, 250, length = 1000)) %>% \n    .[, y := predict(knn_fit, newdata = .)]\n  )) %>% \n  dplyr::select(k, eval_data) %>% \n  unnest() %>% \n  mutate(k_txt = paste0(\"k = \", k)) %>% \n  mutate(k_txt = factor(k_txt, levels = paste0(\"k = \", c(1, 5, 10, 20))))\n\nggplot() +\n  geom_point(data = data, aes(y = y, x = x), color = \"darkgray\", size = 0.7) +\n  geom_line(data = plot_data, aes(y = y, x = x), color = \"red\") +\n  facet_wrap(k_txt ~ ., nrow = 2) +\n  theme_bw()\n\n\n\n\n\nAs you can see, at k \\(= 1\\), the fitted curve fits perfectly with the observed data, and it is highly over-fitted. While increasing k to \\(5\\) alleviates the over-fitting problem, the fitted curve is still very much wiggly."
  },
  {
    "objectID": "B01-nonlinear.html#efficiency-parametric-vs-non-parametric-methods",
    "href": "B01-nonlinear.html#efficiency-parametric-vs-non-parametric-methods",
    "title": "1  Non-linear function estimation",
    "section": "1.4 Efficiency: Parametric vs Non-parametric Methods",
    "text": "1.4 Efficiency: Parametric vs Non-parametric Methods\nSemi-parametric and non-parametric approached are more flexible in representing relationships between the dependent and independent variables than linear-in-parameter models. So, why don’t we just use semi-parametric or non-parametric approaches instead for all the econometric tasks? One reason you might prefer linear-in-parameter models has something to do with efficiency.\n\n\n\n\n\n\nTip\n\n\n\n\nparametric:\n\npro: can be efficient than non-parametric approach when the underlying process is modeled well with parametric models\ncons: not robust to functional form mis-specifications\n\nsemi-, non-parametric:\n\npro: safe-guard against model mis-specification especially when you are modeling highly complex (non-linear in a way that is hard to capture with parametric models and multi-way interactions of variables)\ncons: possible loss in efficiency compared to parametric approach when the underlying model can be approximated well with parametric models\n\n\n\n\n\n1.4.1 Monte Carlo Simulation (parametric vs non-parametric)\nConsider the following data generating process:\n\\[\n\\begin{aligned}\ny = log(x) + v\n\\end{aligned}\n\\]\nYour objective is to understand the impact of treatment (increasing \\(x = 1\\) to \\(x = 2\\)). The true impact of the treatment is \\(TE[x=1 \\rightarrow x=2] = log(2) - log(1) = 0.6931472\\).\n\n\nCode\nmc_run <- function(i)\n{\n  print(i) # progress tracker\n\n  #=== set the number of observations (not really have to be inside the function..) ===#\n  N <- 1000\n\n  #=== generate the data ===#\n  x <- 3 * runif(N)\n  e <- 2 * rnorm(N)\n  y <- log(x) + e\n\n  data <-\n    data.table(\n      x = x,\n      y = y\n    )\n\n  eval_data <- data.table(x = c(1, 2))\n\n  #=== linear model with OLS ===#\n  lm_trained <- lm(y ~ log(x), data = data)\n  te_lm <- lm_trained$coefficient[\"log(x)\"] * log(2)\n\n  #=== gam ===#\n  gam_trained <- gam(y ~ s(x, k = 5), data = data)\n  y_hat_gam <- predict(gam_trained, newdata = eval_data)\n  te_gam <- y_hat_gam[2] - y_hat_gam[1]\n\n  #=== KNN ===#\n  knn_trained <- knnreg(y ~ x, k = 10, data = data)\n  y_hat_knn <- predict(knn_trained, newdata = eval_data)\n  te_knn <- y_hat_knn[2] - y_hat_knn[1]\n  \n  #=== combined the results ===#\n  return_data <-\n    data.table(\n      te = c(te_lm, te_gam, te_knn),\n      type = c(\"lm\", \"gam\", \"knn\")\n    )\n\n  return(return_data)\n}\n\nset.seed(5293)\n\nmc_results <-\n  mclapply(\n    1:500,\n    mc_run,\n    mc.cores = detectCores() * 3 / 4\n  ) %>% \n  rbindlist()\n\n# lapply(\n#   1:100,\n#   mc_run\n# )\n\n\nFigure 1.6 presents the density plots of treatment effect estimates by method. As you can see, the (correctly-specified) linear model performs better than the other methods. All the methods are unbiased, however they differ substantially in terms of efficiency. Note that there was no point in applying random forest at all as there is only a single explanatory variable and the none of the strong points of RF over linear model manifest in this simulation. Clearly, this simulation by no means is intended to claim that linear model is the best. It is merely showcasing  a scenario where (correctly-specified) linear model performs far better than the non-parametric models. This is also a reminder that no method works the best all the time. There are many cases where parametric models perform better (contextual knowledge is critical so that your parametric model can represent the true data generating process well). There are of course many cases non-parametric modeling work better than parametric modeling.\n\n\nCode\nggplot(data = mc_results) +\n  geom_density(aes(x = te, fill = type), alpha = 0.4) +\n  geom_vline(xintercept = log(2)) +\n  theme_bw()\n\n\n\n\n\nFigure 1.6: Results of the MC simulations on the efficiency of various methods"
  },
  {
    "objectID": "B01-nonlinear.html#references",
    "href": "B01-nonlinear.html#references",
    "title": "1  Non-linear function estimation",
    "section": "References",
    "text": "References\n\n\nGarcía-Portugués, E. 2022. Notes for Predictive Modeling. https://bookdown.org/egarpor/PM-UC3M/.\n\n\nWood, Simon N. 2006. Generalized Additive Models: An Introduction with r. chapman; hall/CRC."
  },
  {
    "objectID": "B02-bias-variance-tradeoff.html",
    "href": "B02-bias-variance-tradeoff.html",
    "title": "2  Bias-variance Trade-off",
    "section": "",
    "text": "Packages to load for replication\n\nlibrary(data.table)\nlibrary(tidyverse)\nlibrary(mgcv)\nlibrary(caret)\nlibrary(parallel)\n\nSuppose you have a dataset: \\(D = \\{X_1, y_1\\}, \\{X_2, y_2\\}, \\dots, \\{X_N, y_N\\}\\), where \\(X_i\\) is a collection of features and \\(y_i\\) is the dependent variable for \\(i\\)th observation. Further, suppose you use \\(D\\) for training ML models and \\(\\hat{f}()\\) is a trained model.\nA most common measure of how good a model is mean squared error (MSE) defined as below:\n\\[\nMSE = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{f}(x_i))^2\n\\]\n\\(\\hat{f}(x_i)\\) is the value of \\(y\\) predicted by the trained model \\(\\hat{f}()\\), so \\(y_i - \\hat{f}(x_i)\\) is the residual (termed error more often).\nWhen you get the MSE of a trained model for the very data that is used to train the model, then we may call it training MSE.\nHowever, we are typically interested in how the trained model performs for the data that we have not seen. Let \\(D^{test} = \\{X^{test}_1, y^{test}_1\\}, \\{X^{test}_2, y^{test}_2\\}, \\dots, \\{X^{test}_M, y^{test}_M\\}\\) denote new data set with \\(M\\) data points. Then the test MSE would be:\n\\[\nMSE_{test} = \\frac{1}{M} \\sum_{i=1}^M (y^{test}_{i} - \\hat{f}(x^{test}_{i}))^2\n\\]\nTypically, we try different ML approaches (Random Forest, Support Vector Machine, Causal Forest, Boosted Regression Forest, Neural Network, etc). We also try different values of hyper-parameters for the same approach (e.g., tree depth and minimum observations per leaf for RF). Ideally, we would like to pick the model that has the smallest test \\(MSE\\) among all the models.\nSuppose you do not have a sufficiently large dataset to split to train and test datasets (often the case). So, you used all the available observations to train a model. That means you can get only training \\(MSE\\).\n\n\n\n\n\n\nImportant\n\n\n\nIn this case, can we trust the model that has the lowest training \\(MSE\\)?\n\n\nThe quick answer is no. The problem is that the model with the lowest training \\(MSE\\) does not necessarily achieve the lowest test \\(MSE\\).\nLet’s run some simulations to see this. The data generating process is as follows:\n\\[\ny  = (x - 2.5)^3 + \\mu\n\\]\nwhere \\(\\mu\\) is the error term.\n\nset.seed(943843)\n\n# define the data generating process\n# x fixed\ngen_data <- function(x) {\n  \n  ey <- (x - 2.5)^3 # E[y|x]\n  y <- ey + 3 * rnorm(length(x)) # y = E[y|x] + u\n  return_data <- data.table(y = y, x = x, ey = ey)\n\n  return(return_data)\n}\n\n## generate train data\ntrain_data <- gen_data(x = runif(100) * 5)\n\n  \n## generate test data\n# test data is large to stabilize test MSE \ntest_data <- gen_data(x = runif(10000) * 5)\n\nVisually, here is the relationship between \\(E[y]\\) and \\(x\\):\n\n\nCode\nggplot(data = train_data) +\n  geom_line(aes(y = ey, x = x)) +\n  theme_bw()\n\n\n\n\n\nNow, let’s define a function that runs regression with different levels of flexibility using a generalized additive model from the mgcv package, predict \\(y\\) for both the train and test datasets, and find train and test MSEs. Specifically, we vary the value of k (the number of knots) in gam() while intentionally setting sp to \\(0\\) so that the wiggliness of the fitted curve is not punished.\n\n\nA very brief introduction of generalized additive mode is available in Chapter 1\n\nest_gam <- function(k, train_data, test_data) \n{\n\n  # define train and test data\n  train_data <- copy(train_data)\n  test_data <- copy(test_data)\n\n  #--- train a model ---#  \n  trained_model <- gam(y ~ s(x, k = k), sp = 0, data = train_data)\n\n  #--- predict y for the train and test datasets ---#\n  train_data[, y_hat := predict(trained_model, newdata = train_data)] \n  train_data[, type := \"Train\"]\n  test_data[, y_hat := predict(trained_model, newdata = test_data)] \n  test_data[, type := \"Test\"]\n\n  #--- combine before returning ---#\n  return_data <- \n    rbind(train_data, test_data) %>%\n    .[, num_knots := k]\n\n  return(return_data)\n}\n\nWe now loop over the number of knots (num_knots).\n\nsim_results <- \n  lapply(1:50, function(x) est_gam(x, train_data, test_data)) %>%\n  rbindlist()\n\nFigure 2.1 presents the fitted regression lines for num_knots \\(= 1, 4, 5, 15, 25\\), and \\(50\\), along with the observed data points in the train dataset.\n\n\nCode\nggplot(sim_results[num_knots  %in% c(1, 4, 15, 25, 50), ]) +\n  geom_point(data = train_data, aes(y = y, x = x), color = \"grey\") +\n  geom_line(aes(y = y_hat, x = x, color = factor(num_knots))) +\n  theme_bw()\n\n\n\n\n\nFigure 2.1: Fitted curves by gam() with different numbers of knots\n\n\n\n\nWhen the number of knots is \\(1\\), gam is not flexible enough to capture the underlying cubic function. However, once the number of knots becomes \\(4\\), it is capable of capturing the underlying non-linearity. However, when you increase the number of knots to 15, you see that the fitted curve is very wiggly (sudden and large changes in \\(y\\) when \\(x\\) is changed slighly). When num_knots \\(= 50\\), the fitted curve looks crazy and does not resemble the underlying smooth cubic curve.\nNow, let’s check how train and test MSEs change as k changes. As you can see in Figure 2.2 below, train MSE goes down as k increases (the more complex the model is, the better fit you will get for the train data). However, test MSE is the lowest when num_knots \\(= 4\\), and it goes up afterward instead of going down. As we saw earlier, when model is made too flexible, it is trained to fit the trained data too well and lose generalizability (predict well for the dataset that has not been seen). This phenomenon is called over-fitting.\n\n\nCode\n#--- get train and test MSEs by k ---#\nsummary <- \n  sim_results[, .(mse = mean((y_hat - y)^2)), by = .(num_knots, type)]\n\nggplot(summary) +\n  geom_line(aes(y = mse, x = num_knots, color = type)) +\n  geom_point(aes(y = mse, x = num_knots, color = type)) +\n  xlab(\"Number of knots\") +\n  ylab(\"MSE\") +\n  scale_color_discrete(name = \"\") +\n  theme_bw()\n\n\n\n\n\nFigure 2.2: Train and test MSEs as a function of the number of knots\n\n\n\n\nIf we were to trust train MSE in picking the model, we would pick the model with k \\(= 50\\) in this particular instance. This clearly tells us that we should  NOT  use training MSE to pick the best model."
  },
  {
    "objectID": "B02-bias-variance-tradeoff.html#bias-variance-trade-off",
    "href": "B02-bias-variance-tradeoff.html#bias-variance-trade-off",
    "title": "2  Bias-variance Trade-off",
    "section": "2.2 Bias-variance trade-off",
    "text": "2.2 Bias-variance trade-off\nExpected test MSE at \\(x = x_0\\) can be written in general (no matter what the trained model is) as follows:\n\\[\nE[(y_0 - \\hat{f}(x_0))^2] = Var(\\hat{f}(x_0)) + Bias(\\hat{x}_0)^2 + Var(\\mu)\n\\]\n\n\\(Bias(\\hat{x}_0) = E[\\hat{f}(x_0)]-y_0\\)\n\\(Var(\\hat{f}(x_0)) = E[(E[\\hat{f}(x_0)]-\\hat{f}(x_0))^2]\\)\n\nThe first term is the variance of predicted value at \\(x_0\\), the second term is the squared bias of \\(\\hat{f}(x_0)\\) (how much \\(\\hat{f}(x_0)\\) differs from \\(E[y_0]\\) on average), and \\(Var(\\mu)\\) is the variance of the error term.\nTo illustrate this trade-off, we will run Monte Carlo simulations. We repeat the following steps 500 times.\n\nstep 1: generate train and test datasets\nstep 2: train gam with different values of \\(k\\) \\((1, 5, 15, 25, 50)\\) using the train dataset\nstep 3: predict \\(y\\) using the test dataset\n\nOnce all the iterations are completed, simulation results are summarized to estimate \\(Var(\\hat{f}(x_0))\\) and \\(E[y - \\hat{f}(x_0)]^2\\) for all the values of \\(x_0\\) (all the \\(x\\) values observed in the test dataset) by \\(k\\). We then average them to find the overall \\(Var(\\hat{f}(x_0))\\) and \\(E[y - \\hat{f}(x_0)]^2\\) by \\(k\\).\n\nx_train <- runif(100) * 5\n# x_test is fixed to make it easier to get average conditonal on a given value of x later  \nx_test <- runif(100) * 5\n\n# function that performs steps 1 ~ 3 (a single iteration) \nrun_mc <- function(i, x_train, x_test)\n{\n  print(i) # track progress\n  train_data <- gen_data(x_train) # generate data\n  test_data <- gen_data(x_test) # generate data\n  # run gam for K = 1, ..., 50\n  sim_results <- \n    lapply(\n      c(1, 5, 15, 25, 50), \n      function(x) est_gam(x, train_data, test_data)\n    ) %>%\n    rbindlist()\n\n  return(sim_results)\n}\n\n# runs run_mc 500 times\nmc_results <-\n  mclapply(\n    1:500,\n    function(x) run_mc(x, x_train, x_test),\n    mc.cores = 12\n  ) %>%\n  rbindlist(idcol = \"sim_id\") \n\nFigure 2.3 shows plots fitted curves for all the 500 simulations by \\(k\\) (grey lines). The blue line is the true \\(E[y|x]\\). The red line is \\(E[\\hat{f}(x)]\\)1. Figure Figure 2.4 plots the average2 \\(Var(\\hat{f}(x))\\) (red), \\(E[y - \\hat{f}(x)]^2\\) (blue), and test MSE (darkgreen) from the test datasets for different values of \\(k\\).\n\n\nCode\nmc_results_sum <- \n    mc_results %>%\n    .[type == \"Test\", ] %>% \n    .[, .(mean_y_hat = mean(y_hat)), by = .(x, ey, num_knots)]\n\nggplot() +\n    geom_line(data = mc_results[type == \"Test\", ], aes(y = y_hat, x = x, group = sim_id), color = \"gray\") +\n    geom_line(data = mc_results_sum, aes(y = mean_y_hat, x = x), color = \"red\") +\n    geom_line(data = mc_results_sum, aes(y = ey, x= x), color = \"blue\") +\n    facet_wrap(. ~ num_knots, ncol = 5) +\n    theme_bw()\n\n\n\n\n\nFigure 2.3: Bias-variance trade-off of GAM models with differing number of knots\n\n\n\n\nAs you can see in Figure 2.3, when \\(k = 1\\), it clearly has a significant bias in estimating \\(E[y|x]\\) except for several values of \\(x\\) at which \\(E[\\hat{f}(x)]\\) only happens to be unbiased. The model is simply too restrictive and suffers significant bias. However, the variance of \\(\\hat{f}(x)\\) is the smallest as shown in Figure 2.4. As we increase the value of \\(k\\) (making the model more flexible), bias dramatically reduces. However, the variance of \\(\\hat{f}(x)\\) slightly increases. Going from \\(k = 5\\) to \\(k = 15\\) further reduces bias. That is, even though individual fitted curves may look very bad, on average they perform well (as you know that what bias measures). However, the variance of \\(\\hat{f}(x)\\) dramatically increases (this is why individual fitted curves look terrible). Moving to a even higher value of \\(k\\) does not reduce bias, but increases the variance of \\(\\hat{f}(x)\\) even further. That is, increasing \\(k\\) from 15 to a higher value of \\(k\\) increases the variance of \\(\\hat{f}(x)\\) while not reducing bias at all.\nAccording to MSE presented in Figure 2.4, \\(k = 5\\) is the best model among all the models tried in this experiment. In this experiment, we had test datasets available. However, in practice, we need to pick the best model when test datasets are not available most of the time. For such a case, we would like a clever way to estimate test MSE even when test datasets are not available. We will later talk about cross-validation as a means to do so.\n\n\nCode\nsum_stat <- \n  mc_results %>%\n  .[type == \"Test\", ] %>% \n  .[\n    , \n    .(\n      var_hat = var(y_hat), # varianc of y_hat\n      bias_sq = mean(y_hat - ey)^2, # squared bias\n      mse = mean((y - y_hat)^2)\n    ),\n    by = .(x, num_knots)\n  ] %>%\n  .[\n    ,\n    .(\n      mean_var_hat = mean(var_hat),\n      mean_bias_sq = mean(bias_sq),\n      mean_mse = mean(mse)\n    ),\n    by = .(num_knots)\n  ]\n\nggplot(data = sum_stat) +\n  geom_line(aes(y = mean_var_hat, x = num_knots, color = \"Variance\")) +\n  geom_point(aes(y = mean_var_hat, x = num_knots, color = \"Variance\")) +\n  geom_line(aes(y = mean_bias_sq, x = num_knots, color = \"Bias\")) +\n  geom_point(aes(y = mean_bias_sq, x = num_knots, color = \"Bias\")) +\n  geom_line(aes(y = mean_mse, x = num_knots, color = \"test MSE\")) +\n  geom_point(aes(y = mean_mse, x = num_knots, color = \"test MSE\")) +\n  scale_color_manual(\n    values = c(\"Variance\" = \"red\", \"Bias\" = \"blue\", \"test MSE\" = \"darkgreen\"),\n    name = \"\"\n    ) +\n  ylab(\"\") +\n  xlab(\"Number of knots\") +\n  theme_bw()\n\n\n\n\n\nFigure 2.4: Expected variance of predicted values, bias, and test ME"
  },
  {
    "objectID": "B02-bias-variance-tradeoff.html#additional-example-k-nearest-neighbor-regression",
    "href": "B02-bias-variance-tradeoff.html#additional-example-k-nearest-neighbor-regression",
    "title": "2  Bias-variance Trade-off",
    "section": "2.3 Additional Example (K-nearest neighbor regression)",
    "text": "2.3 Additional Example (K-nearest neighbor regression)\nAnother example of bias-variance trade-off is presented using KNN as the regression method. Its hyper-parameter - the number of neighbors (k) - is varied to see its effect.\n\nfit_knn <- function(k, train_data, test_data) \n{\n\n  # define train and test data\n  train_data <- copy(train_data)\n  test_data <- copy(test_data)\n\n  #--- train a model ---#  \n  trained_model <- knnreg(y ~ x, k = k, data = train_data)\n\n  #--- predict y for the train and test datasets ---#\n  train_data[, y_hat := predict(trained_model, newdata = train_data)] \n  train_data[, type := \"Train\"]\n  test_data[, y_hat := predict(trained_model, newdata = test_data)] \n  test_data[, type := \"Test\"]\n\n  #--- combine before returning ---#\n  return_data <- \n    rbind(train_data, test_data) %>%\n    .[, num_nbs := k]\n\n  return(return_data)\n}\n\nWe now loop over the number of neighbors (k).\n\n## generate train data\ntrain_data <- gen_data(x = runif(1000) * 5)\n  \n## generate test data\ntest_data <- gen_data(x = runif(1000) * 5)\n\nsim_results <- \n  lapply(1:50, function(x) fit_knn(x, train_data, test_data)) %>%\n  rbindlist()\n\nFigure 2.5 presents the fitted regression lines for \\(k = 1, 5, 15, 25\\), and \\(50\\) using knnreg(), along with the observed data points in the train dataset.\n\n\nCode\nggplot(sim_results[num_nbs  %in% c(1, 5, 15, 25, 50), ]) +\n  geom_point(data = train_data, aes(y = y, x = x), color = \"gray\") +\n  geom_line(aes(y = y_hat, x = x), color = \"red\") +\n  facet_grid(num_nbs ~ .) +\n  theme_bw()\n\n\n\n\n\nFigure 2.5: Fitted curves by knnreg() with different numbers of neighbors\n\n\n\n\n\n\nCode\n#--- get train and test MSEs by k ---#\nsummary <- \n  sim_results[, .(mse = mean((y_hat - y)^2)), by = .(num_nbs, type)]\n\nggplot(summary) +\n  geom_line(aes(y = mse, x = num_nbs, color = type)) +\n  geom_point(aes(y = mse, x = num_nbs, color = type)) +\n  xlab(\"Number of neighbors\") +\n  ylab(\"MSE\") +\n  scale_color_discrete(name = \"\") +\n  theme_bw()\n\n\n\n\n\nFigure 2.6: Train and test MSEs as a function of the number of knots"
  },
  {
    "objectID": "B02-bias-variance-tradeoff.html#references",
    "href": "B02-bias-variance-tradeoff.html#references",
    "title": "2  Bias-variance Trade-off",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "B03-cross-validation.html",
    "href": "B03-cross-validation.html",
    "title": "3  Cross-validation",
    "section": "",
    "text": "No model works the best all the time, and searching for the best modeling approach and specifications is an essential part of modeling applications.\nFor example, we may consider five approaches with varying modeling specifications for each of the approaches:\n\nRandom Forest (RF)\n\nnumber of trees (1000, 2000)\nnumber of variables used in each tree (3, 5, 8)\nand many other hyper parameters\n\nLASSO\n\npenalty parameter (1, 2, 3, etc)\n\nGAM\n\nnumber of knots\npenalty parameter\n\nBoosted Regression Forest (BRF)\n\nnumber of trees (1000, 2000)\nnumber of variables used in each tree (3, 5, 8)\nand many other hyper parameters\n\nConvolutional Neural Network (CNN)\n\nconvolution matrix dimension\nthe order of convolution\nlearning rate\nand many other hyper parameters\n\n\nOur goal here is to find the model that would performs the best when applied to the data that has not been seen yet.\nWe saw earlier that training MSE is not appropriate for that purpose as picking the model with the lowest training MSE would very much likely to lead you to an over-fitted model. In this lecture, we consider a better way of selecting a model using only train data.\n\n\n\n\n\n\nImportant\n\n\n\nWhy CV? When the amount of data is limited and you cannot afford to split the data into training and test datasets, you can use a CV to estimate test MSE (by validating a trained model as if you have test data within the training data) for the purpose of picking the right model and hyper-parameter values.\n\n\nHere is a workflow of identifying the final model and come up with the final trained model.\n\nMake a list of models (e.g., RF, BRF, NN) with various hyper-parameter values to try for each of of the models (like above)\nConduct a CV to see which model-hyper-parameter combination minimizes MSE\nTrain the best model specification to the entire training dataset, which becomes your final trained model\n\n\n\n\n\n\n\nNote\n\n\n\nNote that you do not use any of the models trained during the CV process. They are ran purely for the purpose of finding the best model and hyper-parameter values."
  },
  {
    "objectID": "B03-cross-validation.html#leave-one-out-cross-validation-loocv",
    "href": "B03-cross-validation.html#leave-one-out-cross-validation-loocv",
    "title": "3  Cross-validation",
    "section": "3.2 Leave-One-Out Cross-Validation (LOOCV)",
    "text": "3.2 Leave-One-Out Cross-Validation (LOOCV)\n\n\nPackages to load for replication\n\nlibrary(data.table)\nlibrary(tidyverse)\nlibrary(mgcv)\nlibrary(rsample)\nlibrary(parallel)\n\nConsider a dataset: \\(D = \\{X_1, y_1\\}, \\{X_2, y_2\\}, \\dots, \\{X_N, y_N\\}\\), where \\(X_i\\) is a collection of features and \\(y_i\\) is the dependent variable for \\(i\\)th observation. Further, suppose you use \\(D\\) for training ML models and \\(\\hat{f}()\\) is a trained model.\nLOOCV leaves out a single observation (say \\(i\\)), and train a model (say, GAM with the number of knots of 10) using the all the other observations (-\\(i\\)), and then find MSE for the left-out observation. This process is repeated for all the observations, and then the average of the individual MSEs is calculated.\n\n3.2.1 R demonstration using mgcv::gam()\nLet’s demonstrate this using R. Here is the dataset we use.\n\nset.seed(943843)\n\n# define the data generating process\n# x fixed\ngen_data <- function(x) {\n  \n  ey <- (x - 2.5)^3 # E[y|x]\n  y <- ey + 3 * rnorm(length(x)) # y = E[y|x] + u\n  return_data <- data.table(y = y, x = x, ey = ey)\n\n  return(return_data)\n}\n\n## generate train data\ndata <- gen_data(x = runif(100) * 5)\n\nVisually, here is the relationship between \\(E[y]\\) and \\(x\\):\n\n\nCode\nggplot(data = data) +\n  geom_line(aes(y = ey, x = x)) +\n  theme_bw()\n\n\n\n\n\nFor example, for the case where the first observation is left out for validation,\n\n# leave out the first observation\nleft_out_observation <- data[1, ]\n\n# all the rest\ntrain_data <- data[-1, ]\n\nNow we train a gam model using the train_data, predict \\(y\\) for the first observation, and find the MSE.\n\n#=== train the model ===#\nfitted <- gam(y ~ s(x, k = 10), sp = 0, data = train_data)\n\n#=== predict y for the first observation ===#\ny_fitted <- predict(fitted, newdata = left_out_observation)\n\n#=== get MSE ===#\nMSE <- (left_out_observation[, y] - y_fitted) ^ 2\n\nAs described above, LOOCV repeats this process for every single observation of the data. Now, let’s write a function that does the above process for any \\(i\\) you specify.\n\n#=== define the modeling approach ===#\ngam_k_10 <- function(train_data) \n{\n  gam(y ~ s(x, k = 30), sp = 0, data = train_data)\n}\n\n#=== define the process of getting MSE for ith observation ===#\nget_mse <- function(i, model)\n{\n  left_out_observation <- data[i, ]\n\n  # all the rest\n  train_data <- data[-i, ]\n\n  #=== train the model ===#\n  fitted <- model(train_data)\n\n  #=== predict y for the first observation ===#\n  y_fitted <- predict(fitted, newdata = left_out_observation)\n\n  #=== get MSE ===#\n  MSE <- (left_out_observation[, y] - y_fitted) ^ 2 \n\n  return(MSE)\n} \n\nFor example, this gets MSE for the 10th observation.\n\nget_mse(10, gam_k_10)\n\n       1 \n1.523446 \n\n\nLet’s now loop over \\(i = 1:100\\).\n\nmse_indiv <-\n  lapply(\n    1:100,\n    function(x) get_mse(x, gam_k_10)\n  ) %>% \n  #=== list to a vector ===#\n  unlist() \n\nHere is the distribution of MSEs.\n\nhist(mse_indiv)\n\n\n\n\nWe now get the average MSE.\n\nmean(mse_indiv)\n\n[1] 12.46016\n\n\n\n\n3.2.2 Selecting the best GAM specification: Illustration\nNow, let’s try to find the best (among the ones we try) GAM specification using LOOCV. We will try ten different GAM specifications which vary in penalization parameter. Penalization parameter can be set using the sp option for mgcv::gam(). A greater value of sp leads to a more smooth fitted curve.\n\nspecify_gam <- function(sp) {\n  function(train_data) {\n    gam(y ~ s(x, k = 30), sp = sp, data = train_data)\n  }\n}\n\nget_mse_by_sp <- function(sp)\n{\n\n  temp_gam <- specify_gam(sp)\n\n  mse_indiv <-\n    lapply(\n      1:100,\n      function(x) get_mse(x, temp_gam)\n    ) %>% \n    #=== list to a vector ===#\n    unlist() %>% \n    mean()\n\n  return_data <-\n    data.table(\n      mse = mse_indiv,\n      sp = sp\n    )\n\n  return(return_data)\n}\n\nFor example, the following code gets you the average MSE for sp \\(= 3\\).\n\nget_mse_by_sp(3)\n\n        mse sp\n1: 11.56747  3\n\n\nNow, let’s loop over ten values of sp: 0, 0.2, 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2.\n\n(\nmse_data <- \n  lapply(\n    seq(0, 2, by = 0.2),\n    function(x) get_mse_by_sp(x)\n  ) %>% \n  rbindlist()\n)\n\n          mse  sp\n 1: 12.460156 0.0\n 2:  9.909992 0.2\n 3:  9.957858 0.4\n 4: 10.049327 0.6\n 5: 10.164749 0.8\n 6: 10.293142 1.0\n 7: 10.427948 1.2\n 8: 10.565081 1.4\n 9: 10.701933 1.6\n10: 10.836829 1.8\n11: 10.968701 2.0\n\n\n\n\n\nSo, according to the LOOCV, we should pick sp \\(= 0.2\\) as the penalty parameter.\nNow, that we know sp \\(= 0.2\\) produces the lowest LOOCV MSE, we rerun gam() using the entire dataset (not leaving out any of the observations) and make it our final trained model.\n\nfinal_gam_spec <- specify_gam(sp = 1)\n\nfit_gam <- final_gam_spec(train_data)\n\nHere is what the fitted curve looks like:\n\nplot(fit_gam)\n\n\n\n\nLooks good. By the way, here are the fitted curves for some other sp values.\n\n\nCode\nfitted_curves <- \n  lapply(\n    c(0, 0.6, 1, 2),\n    function(x) {\n      temp_gam <- specify_gam(sp = x)\n      fit_gam <- temp_gam(train_data)   \n    }\n  )  \n\nfor (plot in fitted_curves) {\n  plot(plot)\n}\n\n\n\n\n\n\n\n\n(a) sp = 0\n\n\n\n\n\n\n\n(b) sp = 0.6\n\n\n\n\n\n\n\n\n\n(c) sp = 1\n\n\n\n\n\n\n\n(d) sp = 2\n\n\n\n\nFigure 3.1: Fitted curves at various penalization parameters\n\n\n\nLOOCV is perfectly general and can be applied to any statistical methods. However, it can be extremely computationally burdensome because you need to fit the same model for as many as the number of observations. So, if you have 10,000 observations, then you need to fit the model 10,000 times, which can take a long long time.\n\n\n3.2.3 Summary\n\n\n\n\n\n\nNote\n\n\n\nLOOCV is perfectly general and can be applied to any statistical methods.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nLOOCV can be highly computation-intensive when the dataset is large"
  },
  {
    "objectID": "B03-cross-validation.html#k-fold-cross-validation-kcv",
    "href": "B03-cross-validation.html#k-fold-cross-validation-kcv",
    "title": "3  Cross-validation",
    "section": "3.3 K-fold Cross-Validation (KCV)",
    "text": "3.3 K-fold Cross-Validation (KCV)\nKCV is a type of cross-validation that overcomes the LOOCV’s drawback of being computationally too intensive when the dataset is large. KCV first splits the entire dataset intro \\(K\\) folds (K groups) randomly. It then leaves out a chunk of observations that belongs to a fold (group), trains the model using the rest of the observations in the other folds, evaluate the trained model using the left-out group. It repeats this process for all the groups and average the MSEs obtained for each group.\nLet’s demonstrate this process using R.\n\nset.seed(89534)\ndata <- gen_data(x = runif(500) * 5)\n\nYou can use rsample::vfold_cv() to split the data into groups.\n\n#=== split into 5 groups ===#\n(\ndata_folds <- rsample::vfold_cv(data, v = 5)\n)\n\n#  5-fold cross-validation \n# A tibble: 5 × 2\n  splits            id   \n  <list>            <chr>\n1 <split [400/100]> Fold1\n2 <split [400/100]> Fold2\n3 <split [400/100]> Fold3\n4 <split [400/100]> Fold4\n5 <split [400/100]> Fold5\n\n\nAs you can see, rsample::vfold_cv() creates \\(v\\) (\\(=5\\) here) splits. And each split has both train and test datasets. <split [400/100]> means that \\(400\\) and \\(100\\) observations for the train and test datasets, respectively. Note that, the \\(100\\) observations in the first split (called Fold 1) are in the train datasets of the rest of the splits (Fold 2 through Fold 5).\nYou can extract the train and test datasets like below using the training() and testing() functions.\n\ntrain_data <- data_folds[1, ]$splits[[1]] %>% training()\ntest_data <- data_folds[1, ]$splits[[1]] %>% testing()\n\nNow, let’s get MSE for the first fold.\n\n#=== train the model ===#\nfitted_model <- gam(y ~ s(x, k = 30), sp = 0, data = train_data)\n\n#=== predict y for the test data ===#\ny_hat <- predict(fitted_model, test_data)\n\n#=== calculate MSE for the fold ===#\n(test_data[, y] - y_hat)^2 %>% mean()\n\n[1] 12.05604\n\n\nNow that we know how to get MSE for a single fold, let’s loop over folds and get MSE for each of the folds. We first create a function that gets us MSE for a single fold.\n\nget_mse_by_fold <- function(data, fold, model)\n{\n\n  test_data <- data_folds[fold, ]$splits[[1]] %>% testing()\n  train_data <- data_folds[fold, ]$splits[[1]] %>% training()\n\n  #=== train the model ===#\n  fitted_model <- model(train_data)\n\n  #=== predict y for the test data ===#\n  y_hat <- predict(fitted_model, test_data)\n\n  #=== calculate MSE for the fold ===#\n  mse <- (test_data[, y] - y_hat)^2 %>% mean() \n\n  return_data <- \n    data.table(\n      k = fold, \n      mse = mse\n    )\n\n  return(return_data)\n}\n\nThis will get you MSE for the third fold.\n\nget_mse_by_fold(data, 3, gam_k_10)\n\n   k      mse\n1: 3 10.65129\n\n\nNow, let’s loop over the row number of data_folds (loop over splits).\n\n(\nmse_all <-\n  lapply(\n    seq_len(nrow(data_folds)),\n    function(x) get_mse_by_fold(data, x, gam_k_10)\n  ) %>% \n  rbindlist()\n)\n\n   k       mse\n1: 1 12.056038\n2: 2  9.863496\n3: 3 10.651289\n4: 4 10.705704\n5: 5 10.166634\n\n\nBy averaging MSE values, we get\n\nmse_all[, mean(mse)]\n\n[1] 10.68863\n\n\n\n3.3.1 Selecting the best GAM specification: Illustration\nJust like we found the best gam specification (choice of penalization parameter) using LOOCV, we do the same now using KCV.\n\nget_mse_by_sp_kcv <- function(sp)\n{\n\n  temp_gam <- specify_gam(sp)\n\n  mse_by_k <-\n    lapply(\n      seq_len(nrow(data_folds)),\n      function(x) get_mse_by_fold(train_data, x, temp_gam)\n    ) %>% \n    rbindlist()\n\n  return_data <-\n    mse_by_k %>% \n    .[, sp := sp]\n\n  return(return_data[])\n}\n\nFor example, the following code gets you the MSE for all the folds for sp \\(= 3\\).\n\nget_mse_by_sp_kcv(3)\n\n   k      mse sp\n1: 1 13.56925  3\n2: 2 11.22831  3\n3: 3 11.45746  3\n4: 4 10.48461  3\n5: 5 11.29421  3\n\n\nNow, let’s loop over ten values of sp: 0, 0.2, 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2.\n\n(\nmse_results <- \n  lapply(\n    seq(0, 2, by = 0.2),\n    function(x) get_mse_by_sp_kcv(x)\n  ) %>% \n  rbindlist()\n)\n\n    k       mse  sp\n 1: 1 12.056038 0.0\n 2: 2  9.863496 0.0\n 3: 3 10.651289 0.0\n 4: 4 10.705704 0.0\n 5: 5 10.166634 0.0\n 6: 1 11.476153 0.2\n 7: 2  9.840434 0.2\n 8: 3  9.813155 0.2\n 9: 4 10.289085 0.2\n10: 5  9.895722 0.2\n11: 1 11.620416 0.4\n12: 2  9.882467 0.4\n13: 3  9.916178 0.4\n14: 4 10.239119 0.4\n15: 5  9.992636 0.4\n16: 1 11.785544 0.6\n17: 2  9.952820 0.6\n18: 3 10.033789 0.6\n19: 4 10.218321 0.6\n20: 5 10.090156 0.6\n21: 1 11.957640 0.8\n22: 2 10.040814 0.8\n23: 3 10.156511 0.8\n24: 4 10.211168 0.8\n25: 5 10.189377 0.8\n26: 1 12.130712 1.0\n27: 2 10.140277 1.0\n28: 3 10.281839 1.0\n29: 4 10.213564 1.0\n30: 5 10.290230 1.0\n31: 1 12.301318 1.2\n32: 2 10.246975 1.2\n33: 3 10.408152 1.2\n34: 4 10.223465 1.2\n35: 5 10.392306 1.2\n36: 1 12.467400 1.4\n37: 2 10.357879 1.4\n38: 3 10.534197 1.4\n39: 4 10.239448 1.4\n40: 5 10.495093 1.4\n41: 1 12.627767 1.6\n42: 2 10.470802 1.6\n43: 3 10.659006 1.6\n44: 4 10.260390 1.6\n45: 5 10.598085 1.6\n46: 1 12.781780 1.8\n47: 2 10.584159 1.8\n48: 3 10.781853 1.8\n49: 4 10.285370 1.8\n50: 5 10.700824 1.8\n51: 1 12.929162 2.0\n52: 2 10.696811 2.0\n53: 3 10.902210 2.0\n54: 4 10.313617 2.0\n55: 5 10.802917 2.0\n    k       mse  sp\n\n\nLet’s now get the average MSE by sp:\n\n(\nmean_mse_data <- mse_results[, .(mean_mse = mean(mse)), by = sp]\n)\n\n     sp mean_mse\n 1: 0.0 10.68863\n 2: 0.2 10.26291\n 3: 0.4 10.33016\n 4: 0.6 10.41613\n 5: 0.8 10.51110\n 6: 1.0 10.61132\n 7: 1.2 10.71444\n 8: 1.4 10.81880\n 9: 1.6 10.92321\n10: 1.8 11.02680\n11: 2.0 11.12894\n\n\n\n\n\nSo, according to the KCV, we should pick sp \\(= 0.2\\) as the penalty parameter.\nBy the way, here is what MSE values look like for each fold based on the value of sp by fold.\n\n\nCode\nggplot(data = mse_results) +\n  geom_line(aes(y = mse, x = sp, color = factor(k))) +\n  scale_color_discrete(name = \"Fold\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nEven though we compared different specifications of the same approach (GAM), we can compare across different models as well. For example, you can find KCV for an RF model with a particular specifications of its hyper-parameters and compare the KCV with those of the GAM model specifications and see what comes at the top."
  },
  {
    "objectID": "B03-cross-validation.html#repeated-k-fold-cross-validation-rkcv",
    "href": "B03-cross-validation.html#repeated-k-fold-cross-validation-rkcv",
    "title": "3  Cross-validation",
    "section": "3.4 Repeated K-fold Cross-Validation (RKCV)",
    "text": "3.4 Repeated K-fold Cross-Validation (RKCV)\nAs its name suggests, repeated KCV repeats the process of KCV multiple times. Each KCV iteration splits the original data into k-fold in a different way. A single KCV may not be reliable as the original data can be split into such a way that favors one parameter set of or model class over the others. However, if we repeat KCV multiple times, then we can safe-guard against this randomness in a KCV procedure. Repeated KCV is preferred over a single KCV.\nYou can use rsample::vfold_cv() to create repeated k-fold datasets by using the repeats argument.\n\n#=== split into 5 groups ===#\n(\ndata_folds <- rsample::vfold_cv(data, v = 5, repeats = 5)\n)\n\n#  5-fold cross-validation repeated 5 times \n# A tibble: 25 × 3\n   splits            id      id2  \n   <list>            <chr>   <chr>\n 1 <split [400/100]> Repeat1 Fold1\n 2 <split [400/100]> Repeat1 Fold2\n 3 <split [400/100]> Repeat1 Fold3\n 4 <split [400/100]> Repeat1 Fold4\n 5 <split [400/100]> Repeat1 Fold5\n 6 <split [400/100]> Repeat2 Fold1\n 7 <split [400/100]> Repeat2 Fold2\n 8 <split [400/100]> Repeat2 Fold3\n 9 <split [400/100]> Repeat2 Fold4\n10 <split [400/100]> Repeat2 Fold5\n# … with 15 more rows\n\n\nThe output has 5 (number of folds) times 5 (number of repeats) splits. It also has an additional column that indicates which repeat each row is in (id). You can apply get_mse_by_fold() (this function is defined above and calculate MSE) to each row (split) one by one and calculate MSE just like we did above.\n\n(\nmean_mse <-\n  lapply(\n    seq_len(nrow(data_folds)),\n    function(x) get_mse_by_fold(data, x, gam_k_10)\n  ) %>% \n  rbindlist() %>% \n  .[, mean(mse)]\n)\n\n[1] 10.65908"
  },
  {
    "objectID": "B03-cross-validation.html#how-are-loocv-kcv-and-rkcv-different",
    "href": "B03-cross-validation.html#how-are-loocv-kcv-and-rkcv-different",
    "title": "3  Cross-validation",
    "section": "3.5 How are LOOCV, KCV and RKCV different?",
    "text": "3.5 How are LOOCV, KCV and RKCV different?"
  },
  {
    "objectID": "B03-cross-validation.html#references",
    "href": "B03-cross-validation.html#references",
    "title": "3  Cross-validation",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "B04-regularization.html",
    "href": "B04-regularization.html",
    "title": "4  Regression Shrinkage Methods",
    "section": "",
    "text": "We have talked about variance-bias trade-off. When you “shrink” coefficients towards zero, you may be able to achieve lower variance of \\(\\hat{f}(x)\\) while increasing bias, which can result in a lower MSE.\nConsider the following generic linear model:\n\\[\ny = X\\beta + \\mu\n\\]\n\n\\(y\\): dependent variable\n\\(X\\): a collection of explanatory variables (\\(K\\) variables)\n\\(\\beta\\): a collection of coefficients on the explanatory variables \\(X\\)\n\\(\\mu\\): error term\n\nBorrowing from the documentation of the glmnet package(), the minimization problem shrinkage methods solve to estimate coefficients for a linear model can be written as follows:\n\\[\nMin_{\\beta} \\sum_{i=1}^N (y_i - X_i\\beta)^2 + \\lambda \\huge[\\normalsize(1-\\alpha)||\\beta||^2_2/2 + \\alpha ||\\beta||_1\\huge]\n\\tag{4.1}\\]\n\n\\(||\\beta||_1 = |\\beta_1| + |\\beta_2| + \\dots+ |\\beta_K|\\) (called L1 norm)\n\\(||\\beta||_2 = (|\\beta_1|^2 + |\\beta_2|^2 + \\dots+ |\\beta_K|^2)^{\\frac{1}{2}}\\) (called L2 norm)\n\n\\(\\lambda (> 0)\\) is the penalization parameter that governs how much coefficients shrinkage happens (more details later).\nThe shrinkage method is called Lasso when \\(\\alpha = 1\\), Ridge regression when \\(\\alpha = 0\\), and elastic net when \\(\\alpha \\in (0, 1)\\).\n\n\n\n\n\n\nNote\n\n\n\n\nLasso : \\(\\alpha = 1\\)\nRidge : \\(\\alpha = 0\\)\nElastic net : \\(0 < \\alpha < 1\\)\n\n\n\nRidge regression and elastic net are rarely used. So, we are going to cover only Lasso here."
  },
  {
    "objectID": "B04-regularization.html#lasso",
    "href": "B04-regularization.html#lasso",
    "title": "4  Regression Shrinkage Methods",
    "section": "4.2 Lasso",
    "text": "4.2 Lasso\n\n\nPackages to load for replication\n\nlibrary(data.table)\nlibrary(tidyverse)\nlibrary(glmnet)\n\nWhen there are many potential variables to include, it is hard to know which ones to include. Lasso can be used to select variables to build a more parsimonious model, which may help reducing MSE.\nAs mentioned above, Lasso is a special case of shrinkage methods where \\(\\alpha = 1\\) in Equation 4.1. So, the optimization problem of Lasso is\n\\[\nMin_{\\beta} \\sum_{i=1}^N (y_i - X_i\\beta)^2 + \\lambda \\sum_{k=1}^K |\\beta_k|\n\\tag{4.2}\\]\n, where \\(\\lambda\\) is the penalization parameter.\nAlternatively, we can also write the optimization problem as the constrained minimization problem as follows1:\n\\[\n\\begin{aligned}\nMin_{\\beta} & \\sum_{i=1}^N (y_i - X_i\\beta)^2 \\\\\n\\mbox{s.t. } & \\sum_{k=1}^K |\\beta_k| \\leq t\n\\end{aligned}\n\\tag{4.3}\\]\nA graphical representation of the minimization problem is highly illustrative on what Lasso does. Consider the following data generating process:\n\\[\ny  = 0.2 x_1 + 2 * x_2 + \\mu\n\\]\nWhen \\(t\\) is set to 1 in Equation 4.3, Lasso tries to estimate the coefficient on \\(x_1\\) and \\(x_2\\) by solving the following problem:\n\\[\n\\begin{align}\nMin_{\\beta} & \\sum_{i=1}^N (y_i - \\beta_1 x_1 - \\beta_2 x_2)^2 \\\\\n\\mbox{s.t. } & \\sum_{k=1}^K |\\beta_k| \\leq \\textcolor{red}{1}    \n\\end{align}\n\\]\nThis means that, we need to look for the combinations of \\(\\beta_1\\) and \\(\\beta_2\\) such that the sum of their absolute values is less than 1. Graphically, here is what the constraint looks like:\n\n\nCode\nggplot() +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  geom_segment(aes(x = 1, y = 0, xend = 0, yend = 1), color = \"red\", size = 1.2) +\n  geom_segment(aes(x = 1, y = 0, xend = 0, yend = -1), color = \"red\", size = 1.2) +\n  geom_segment(aes(x = -1, y = 0, xend = 0, yend = 1), color = \"red\", size = 1.2) +\n  geom_segment(aes(x = -1, y = 0, xend = 0, yend = -1), color = \"red\", size = 1.2) +\n  coord_equal() +\n  xlab(\"beta_1\") +\n  ylab(\"beta_2\") +\n  theme_minimal()\n\n\n\n\n\nNow, let’s calculate what value the objective function takes at different values of \\(\\beta_1\\) and \\(\\beta_2\\).\nWe first generate data.\n\nN <- 1000 # number of observations\nx_1 <- rnorm(N)\nx_2 <- rnorm(N)\nmu <- rnorm(N) # error term\ny <- 2 * x_1 + 0.2 * x_2 + mu\n\ndata <-\n  data.table(\n    y = y,\n    x_1 = x_1,\n    x_2 = x_2\n  )\n\nWithout the constraint, here is the combination of \\(\\beta_1\\) and \\(\\beta_2\\) that minimizes the objective function of Equation 4.3, which is the same as OLS estimates.\n\n(\nols_coefs_1 <- lm(y ~ x_1 + x_2, data = data)$coefficient\n)\n\n (Intercept)          x_1          x_2 \n-0.006005028  2.029380795  0.141220376 \n\n\nWe now calculate the value of the objective functions at different values of \\(\\beta_1\\) and \\(\\beta_2\\). Here is the set of \\(\\{\\beta_1, \\beta_2\\}\\) combinations we look at.\n\n(\nbeta_table <- \n  data.table::CJ(\n    beta_1 = seq(-2, 2, length = 50),\n    beta_2 = seq(-1, 1, length = 50) \n  )\n)\n\n      beta_1     beta_2\n   1:     -2 -1.0000000\n   2:     -2 -0.9591837\n   3:     -2 -0.9183673\n   4:     -2 -0.8775510\n   5:     -2 -0.8367347\n  ---                  \n2496:      2  0.8367347\n2497:      2  0.8775510\n2498:      2  0.9183673\n2499:      2  0.9591837\n2500:      2  1.0000000\n\n\n\n\n\n\n\n\nNote\n\n\n\ndata.table::CJ() takes more than one set of vectors and find the complete combinations the values of the vectors. Trying\n\ndata.table::CJ(x1 = c(1, 2, 3), x2 = c(4, 5, 6))\n\nwill help you understand exactly what it does.\n\n\nLoop over the row numbers of beta_table to find SSE for all the rows (all the combinations of \\(\\beta_1\\) and \\(\\beta_2\\)).\n\n#=== define the function to get SSE ===#\nget_sse <- function(i, data)\n{\n  #=== extract beta_1 and beta_2 for ith observation  ===#\n  betas <- beta_table[i, ]\n\n  #=== calculate SSE ===#\n  sse <-\n    copy(data) %>% \n    .[, y_hat := x_1 * betas[, beta_1] + x_2 * betas[, beta_2]] %>% \n    .[, se := (y - y_hat)^2] %>% \n    .[, sum(se)]\n\n  return(sse)\n}\n\n#=== calculate SSE for each row of beta_table ===#\nsse_all <-\n  lapply(\n    1:nrow(beta_table),\n    function(x) get_sse(x, data)\n  ) %>% \n  unlist()\n\n#=== assign the calculated sse values as a variable ===#\n(\nbeta_table[, sse_1 := sse_all]\n)\n\nHere is the contour map of SSE as a function of \\(\\beta_1\\) and \\(\\beta_2\\). The solution to the unconstrained problem (OLS estimates) is represented by the red point. Since Lasso needs to find a point within the red square, the solution would be \\(\\beta_1 = 1\\) and \\(\\beta_2 = 0\\) (yellow point). Lasso did not give anything to \\(\\beta_2\\) as \\(x_1\\) is a much bigger contributor of the two included variables. Lasso tends to give the coefficient of \\(0\\) to some of the variables when the constraint is harsh, effectively eliminating them from the model. For this reason, Lasso is often used as a variable selection method.\n\n\nCode\nggplot() +\n  stat_contour(\n    data = beta_table, \n    aes(x = beta_1, y = beta_2, z = sse_1, color = ..level..),\n    size = 1.2,\n    breaks = \n      round(\n        quantile(beta_table$sse_1, seq(0, 1, 0.05)),\n        0\n      )\n    ) +\n  scale_color_viridis_c(name = \"SSE\") +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  geom_segment(aes(x = 1, y = 0, xend = 0, yend = 1), color = \"red\", size = 1.2) +\n  geom_segment(aes(x = 1, y = 0, xend = 0, yend = -1), color = \"red\", size = 1.2) +\n  geom_segment(aes(x = -1, y = 0, xend = 0, yend = 1), color = \"red\", size = 1.2) +\n  geom_segment(aes(x = -1, y = 0, xend = 0, yend = -1), color = \"red\", size = 1.2) +\n  #=== OLS point estimates (solutions without the constraint) ===#\n  geom_point(\n    aes(x = ols_coefs_1[\"x_1\"], y = ols_coefs_1[\"x_2\"]),\n    color = \"red\",\n    size = 3\n  ) +\n  geom_point(\n    aes(x = 1, y = 0),\n    color = \"yellow\",\n    size = 3\n  ) +\n  coord_equal() +\n  theme_minimal()\n\n\n\n\n\nLet’s consider a different data generating process: \\(y = x_1 + x_2 + \\mu\\). Here, \\(x_1\\) and \\(x_2\\) are equally important unlike the previous case. Here is what happens:\n\n\nCode\nN <- 1000 # number of observations\nx_1 <- rnorm(N)\nx_2 <- rnorm(N)\nmu <- rnorm(N) # error term\ny <- x_1 + x_2 + mu\n\ndata <-\n  data.table(\n    y = y,\n    x_1 = x_1,\n    x_2 = x_2\n  )\n\nols_coefs_2 <- lm(y ~ x_1 + x_2, data = data)$coefficient\n\n#=== calculate sse for each row of beta_table ===#\nsse_all <-\n  lapply(\n    1:nrow(beta_table),\n    function(x) {\n      betas <- beta_table[x, ]\n      sse <-\n        copy(data) %>% \n        .[, y_hat := x_1 * betas[, beta_1] + x_2 * betas[, beta_2]] %>% \n        .[, se := (y - y_hat)^2] %>% \n        .[, sum(se)]\n      return(sse)\n    }\n  ) %>% \n  unlist()\n\n#=== assign the calculated sse values as a variable ===#\nbeta_table[, sse_2 := sse_all]\n\n#=== visualize ===#\nggplot() +\n  stat_contour(\n    data = beta_table, \n    aes(x = beta_1, y = beta_2, z = sse_2, color = ..level..),\n    size = 1.2,\n    breaks = \n      round(\n        quantile(beta_table$sse_2, seq(0, 1, 0.05)),\n        0\n      )\n    ) +\n  scale_color_viridis_c(name = \"SSE\") +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  geom_segment(aes(x = 1, y = 0, xend = 0, yend = 1), color = \"red\", size = 1.2) +\n  geom_segment(aes(x = 1, y = 0, xend = 0, yend = -1), color = \"red\", size = 1.2) +\n  geom_segment(aes(x = -1, y = 0, xend = 0, yend = 1), color = \"red\", size = 1.2) +\n  geom_segment(aes(x = -1, y = 0, xend = 0, yend = -1), color = \"red\", size = 1.2) +\n  #=== OLS point estimates (solutions without the constraint) ===#\n  geom_point(\n    aes(x = ols_coefs_2[\"x_1\"], y = ols_coefs_2[\"x_2\"]),\n    color = \"red\",\n    size = 3\n  ) +\n  geom_point(\n    aes(x = 0.5, y = 0.5),\n    color = \"yellow\",\n    size = 3\n  ) +\n  coord_equal() +\n  theme_minimal()\n\n\n\n\n\nIn this case, the solution would be (very close to) \\(\\{\\beta_1 = 0.5, \\beta_2 = 0.5\\}\\), with neither of them sent to zero. This is because \\(x_1\\) and \\(x_2\\) are equally important in explaining \\(y\\)."
  },
  {
    "objectID": "B04-regularization.html#sec-ridge-en",
    "href": "B04-regularization.html#sec-ridge-en",
    "title": "4  Regression Shrinkage Methods",
    "section": "4.3 Ridge and Elastic Net regression",
    "text": "4.3 Ridge and Elastic Net regression\nRidge regression uses L2 norm for regularization and solves the following minimization problem:\n\\[\n\\begin{aligned}\nMin_{\\beta} & \\sum_{i=1}^N (y_i - X_i\\beta)^2 \\\\\n\\mbox{s.t. } & \\sum_{k=1}^K \\beta_k^2 \\leq t\n\\end{aligned}\n\\tag{4.4}\\]\nFigure 4.1 shows the constraint when \\(t=1\\) (red circle) and the contour of SSE for the first model we considered (\\(E[y|x] = 2 \\times x_1 + 0.2 \\times x_2\\)). Unlike Lasso, the constraint is a circle (since it is two-dimensional), and you can expect that Ridge coefficient estimates do not generally become 0. Therefore, Ridge regression cannot be used for variable selection.\n\n\nCode\nggplot() +\n  stat_contour(\n    data = beta_table, \n    aes(x = beta_1, y = beta_2, z = sse_1, color = ..level..),\n    size = 1.2,\n    breaks = \n      round(\n        quantile(beta_table$sse_1, seq(0.033, 1, 0.05)),\n        0\n      )\n    ) +\n  scale_color_viridis_c(name = \"SSE\") +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  ggforce::geom_circle(aes(x0 = 0, y0 = 0, r = 1), color = \"red\") +\n  #=== OLS point estimates (solutions without the constraint) ===#\n  geom_point(\n    aes(x = ols_coefs_1[\"x_1\"], y = ols_coefs_1[\"x_2\"]),\n    color = \"red\",\n    size = 3\n  ) +\n  geom_point(\n    aes(x = 0.99, y = 0.1),\n    color = \"orange\",\n    size = 3\n  ) +\n  coord_equal() +\n  theme_minimal()\n\n\n\n\n\nFigure 4.1: Illustration of Ridge Regression\n\n\n\n\nRidge regression estimator has a nice analytical formula. Let \\(Y\\) and \\(X\\) denote the \\(N \\times 1\\) matrix of the dependent variable and \\(N \\times K\\) matrix, respectively (\\(N\\) is the number of observations and \\(K\\) is the number of covariates). Then,\n\\[\n\\begin{aligned}\n\\hat{\\beta}_{Ridge} = (X'X + \\lambda I)^{-1}X'Y\n\\end{aligned}\n\\]\nwhere \\(\\lambda\\) is the penalization parameter and \\(I\\) is the \\(K \\times K\\) identify matrix.\nElastic net is at somewhere between Lasso and Ridge with \\(0 < \\alpha < 1\\) in Equation 4.1 and solves the following minimization problem:\n\\[\n\\begin{aligned}\nMin_{\\beta} & \\sum_{i=1}^N (y_i - X_i\\beta)^2 \\\\\n\\mbox{s.t. } & \\frac{1-\\alpha}{2}\\sqrt{\\sum_{k=1}^K \\beta_k^2} + \\alpha\\sum_{k=1}^K |\\beta_k| \\leq t\n\\end{aligned}\n\\tag{4.5}\\]\nFigure 4.2 shows the constraint when \\(t=1\\) (red circle) and the contour of SSE for the first model we considered (\\(E[y|x] = 2 \\times x_1 + 0.2 \\times x_2\\)). Its constraint is a mix of that of Lasso and Ridge regression. It has four pointy points at the points where either one of \\(\\beta_1\\) and \\(\\beta_2\\) just like Lasso. But, the curves that connect those points are not straight. Elastic net can eliminate variables (setting coefficients to 0), but not as strongly as Lasso does.\n\n\nCode\nget_const <- function(x1, x2, alpha){\n  (1 - alpha) * sqrt(x1^2 + x2^2) / 2 + alpha * (abs(x1) + abs(x2))\n}\n\nalpha <- 0.5\n\nen_const_data <-\n  CJ(\n    x1 = seq(-2, 2, length = 1000),\n    x2 = seq(-2, 2, length = 1000)\n  ) %>% \n  .[, c := abs(get_const(x1, x2, alpha)-1)] %>% \n  .[, x2_sign := x2 < 0] %>% \n  .[, .SD[which.min(c), ], by = .(x1, x2_sign)] %>% \n  .[abs(x1) <= 2/(1+alpha), ]\n\nggplot() +\n  stat_contour(\n    data = beta_table, \n    aes(x = beta_1, y = beta_2, z = sse_1, color = ..level..),\n    size = 1.2,\n    breaks = \n      round(\n        quantile(beta_table$sse_1, seq(0, 1, 0.05)),\n        0\n      )\n    ) +\n  scale_color_viridis_c(name = \"SSE\") +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  #=== elastic net constraint ===#\n  geom_line(data = en_const_data[x1 > 0 & x2 > 0], aes(x=x1, y = x2), color = \"red\") +\n  geom_line(data = en_const_data[x1 < 0 & x2 > 0], aes(x=x1, y = x2), color = \"red\") +\n  geom_line(data = en_const_data[x1 > 0 & x2 < 0], aes(x=x1, y = x2), color = \"red\") +\n  geom_line(data = en_const_data[x1 < 0 & x2 < 0], aes(x=x1, y = x2), color = \"red\") +\n  #=== OLS point estimates (solutions without the constraint) ===#\n  geom_point(\n    aes(x = ols_coefs_1[\"x_1\"], y = ols_coefs_1[\"x_2\"]),\n    color = \"red\",\n    size = 3\n  ) +\n  geom_point(\n    aes(x = 1.33, y = 0),\n    color = \"orange\",\n    size = 3\n  ) +\n  coord_equal() +\n  theme_minimal()\n\n\n\n\n\nFigure 4.2: Illustration of Ridge Regression\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhile Lasso is frequently used. Ridge regression or elastic net is nowhere near as popular."
  },
  {
    "objectID": "B04-regularization.html#lasso-implementation",
    "href": "B04-regularization.html#lasso-implementation",
    "title": "4  Regression Shrinkage Methods",
    "section": "4.4 Lasso implementation",
    "text": "4.4 Lasso implementation\nYou can use the glmnet() from the glmnet package to run Lasso. For demonstration, we use the QuickStartExample data.\n\n#=== get the data ===#\ndata(QuickStartExample)\n\n#=== see the structure ===#\nstr(QuickStartExample)\n\nList of 2\n $ x: num [1:100, 1:20] 0.274 2.245 -0.125 -0.544 -1.459 ...\n $ y: num [1:100, 1] -1.275 1.843 0.459 0.564 1.873 ...\n\n\nAs you can see, QuickStartExample is a list of two elements. First one (x) is a matrix of dimension 100 by 20, which is the data of explanatory variables. Second one (y) is a matrix of dimension 100 by 1, which is the data for the dependent variable.\n\n\n\n\n\n\nNote\n\n\n\nIf you are used to running regressions in R, you should have specified a model using formula (e.g., y ~ x). However, most of the machine learning functions in R accept the dependent variable and explanatory variables in a matrix form (or data.frame). This is almost always the case for ML methods in Python as well.\n\n\nBy default, alpha parameter for glmnet() (\\(\\alpha\\) in Equation 4.1) is set to 1. So, to run Lasso, you can simply do the following:\n\n#=== extract X and y ===#\nX <- QuickStartExample$x\ny <- QuickStartExample$y\n\n#=== run Lasso ===#\nlasso <- glmnet(X, y)\n\nBy looking at the output below, you can see that glmnet() tried many different values of \\(\\lambda\\).\n\nlasso\n\n\nCall:  glmnet(x = X, y = y) \n\n   Df  %Dev  Lambda\n1   0  0.00 1.63100\n2   2  5.53 1.48600\n3   2 14.59 1.35400\n4   2 22.11 1.23400\n5   2 28.36 1.12400\n6   2 33.54 1.02400\n7   4 39.04 0.93320\n8   5 45.60 0.85030\n9   5 51.54 0.77470\n10  6 57.35 0.70590\n11  6 62.55 0.64320\n12  6 66.87 0.58610\n13  6 70.46 0.53400\n14  6 73.44 0.48660\n15  7 76.21 0.44330\n16  7 78.57 0.40400\n17  7 80.53 0.36810\n18  7 82.15 0.33540\n19  7 83.50 0.30560\n20  7 84.62 0.27840\n21  7 85.55 0.25370\n22  7 86.33 0.23120\n23  8 87.06 0.21060\n24  8 87.69 0.19190\n25  8 88.21 0.17490\n26  8 88.65 0.15930\n27  8 89.01 0.14520\n28  8 89.31 0.13230\n29  8 89.56 0.12050\n30  8 89.76 0.10980\n31  9 89.94 0.10010\n32  9 90.10 0.09117\n33  9 90.23 0.08307\n34  9 90.34 0.07569\n35 10 90.43 0.06897\n36 11 90.53 0.06284\n37 11 90.62 0.05726\n38 12 90.70 0.05217\n39 15 90.78 0.04754\n40 16 90.86 0.04331\n41 16 90.93 0.03947\n42 16 90.98 0.03596\n43 17 91.03 0.03277\n44 17 91.07 0.02985\n45 18 91.11 0.02720\n46 18 91.14 0.02479\n47 19 91.17 0.02258\n48 19 91.20 0.02058\n49 19 91.22 0.01875\n50 19 91.24 0.01708\n51 19 91.25 0.01557\n52 19 91.26 0.01418\n53 19 91.27 0.01292\n54 19 91.28 0.01178\n55 19 91.29 0.01073\n56 19 91.29 0.00978\n57 19 91.30 0.00891\n58 19 91.30 0.00812\n59 19 91.31 0.00739\n60 19 91.31 0.00674\n61 19 91.31 0.00614\n62 20 91.31 0.00559\n63 20 91.31 0.00510\n64 20 91.31 0.00464\n65 20 91.32 0.00423\n66 20 91.32 0.00386\n67 20 91.32 0.00351\n\n\nYou can access the coefficients for each value of lambda by applying coef() method to lasso.\n\n#=== get coefficient estimates ===#\ncoef_lasso <- coef(lasso)\n\n#=== check the dimension ===#\ndim(coef_lasso)\n\n[1] 21 67\n\n#=== take a look at the first and last three ===#\ncoef_lasso[, c(1:3, 65:67)]\n\n21 x 6 sparse Matrix of class \"dgCMatrix\"\n                   s0           s1         s2          s64          s65\n(Intercept) 0.6607581  0.631235043  0.5874616  0.111208836  0.111018972\nV1          .          0.139264992  0.2698292  1.378068980  1.378335220\nV2          .          .            .          0.023067319  0.023240539\nV3          .          .            .          0.762792114  0.763209604\nV4          .          .            .          0.059619334  0.060253956\nV5          .          .            .         -0.901460720 -0.901862151\nV6          .          .            .          0.613661389  0.614081490\nV7          .          .            .          0.117323876  0.117960550\nV8          .          .            .          0.396890604  0.397260052\nV9          .          .            .         -0.030538991 -0.031073136\nV10         .          .            .          0.127412702  0.128222375\nV11         .          .            .          0.246801359  0.247227761\nV12         .          .            .         -0.063941712 -0.064471794\nV13         .          .            .         -0.045935249 -0.046242852\nV14         .         -0.005878595 -0.1299063 -1.158552963 -1.159038292\nV15         .          .            .         -0.137103471 -0.138012175\nV16         .          .            .         -0.045085698 -0.045661882\nV17         .          .            .         -0.047272446 -0.048039238\nV18         .          .            .          0.051702567  0.052180547\nV19         .          .            .         -0.001791685 -0.002203174\nV20         .          .            .         -1.144262012 -1.144641845\n                     s66\n(Intercept)  0.110845721\nV1           1.378578220\nV2           0.023398270\nV3           0.763589908\nV4           0.060832496\nV5          -0.902227796\nV6           0.614464085\nV7           0.118540773\nV8           0.397596878\nV9          -0.031560145\nV10          0.128960349\nV11          0.247615990\nV12         -0.064955124\nV13         -0.046522983\nV14         -1.159480668\nV15         -0.138840304\nV16         -0.046186890\nV17         -0.048737920\nV18          0.052615915\nV19         -0.002578088\nV20         -1.144987654\n\n\nApplying plot() method gets you how the coefficient estimates change as the value of \\(\\lambda\\) changes:\n\nplot(lasso)\n\n\n\n\nA high L1 Norm is associated with a “lower” value of \\(\\lambda\\) (weaker shrinkage). You can see that as \\(\\lambda\\) increases (L1 Norm decreases), coefficients on more and more variables are set to 0.\nNow, the obvious question is which \\(\\lambda\\) should we pick? One way to select a \\(\\lambda\\) is K-fold cross-validation (KCV), which we covered in section. We can implement KCV using the cv.glmnet() function. You can set the number of folds using the nfolds option (the default is 10). Here, let’s 5-fold CV.\n\ncv_lasso <- cv.glmnet(X, y, nfolds = 5)\n\nThe results of KCV can be readily visualized by applying the plot() method:\n\nplot(cv_lasso)\n\n\n\n\nThere are two vertical dotted lines. The left one indicates the value of \\(\\lambda\\) where CV MSE is minimized (called lambda.min). The right one indicates the  highest  (most regularized) value of \\(\\lambda\\) such that the CV error is within one standard error of the minimum (called lambda.1se).\nYou can access the MSE-minimizing \\(\\lambda\\) as follows:\n\ncv_lasso$lambda.min\n\n[1] 0.03276581\n\n\nYou can access the coefficient estimates when \\(\\lambda\\) is lambda.min as follows\n\ncoef(cv_lasso, s = \"lambda.min\")\n\n21 x 1 sparse Matrix of class \"dgCMatrix\"\n                      s1\n(Intercept)  0.128660890\nV1           1.358328625\nV2           0.008183930\nV3           0.729395934\nV4           0.014458167\nV5          -0.870720166\nV6           0.582517165\nV7           0.076231973\nV8           0.367970084\nV9           .          \nV10          0.068030038\nV11          0.215812809\nV12         -0.024238268\nV13         -0.019278242\nV14         -1.124155296\nV15         -0.070162005\nV16         -0.002157957\nV17          .          \nV18          0.019755628\nV19          .          \nV20         -1.114758360\n\n\nThe following code gives you the coefficient estimates when \\(\\lambda\\) is lambda.1se\n\ncoef(cv_lasso, s = \"lambda.1se\")\n\n21 x 1 sparse Matrix of class \"dgCMatrix\"\n                     s1\n(Intercept)  0.15473991\nV1           1.29441444\nV2           .         \nV3           0.62890756\nV4           .         \nV5          -0.77785401\nV6           0.48387954\nV7           .         \nV8           0.28419264\nV9           .         \nV10          .         \nV11          0.09130386\nV12          .         \nV13          .         \nV14         -1.03241106\nV15          .         \nV16          .         \nV17          .         \nV18          .         \nV19          .         \nV20         -0.96190123\n\n\n\n\n\n\n\n\nNote\n\n\n\nglmnet() can be used to much broader class of models (e.g., Logistic regression, Poisson regression, Cox regression, etc). As the name suggests it’s elastic  net  methods for  generalized  linear  model."
  },
  {
    "objectID": "B04-regularization.html#scaling",
    "href": "B04-regularization.html#scaling",
    "title": "4  Regression Shrinkage Methods",
    "section": "4.5 Scaling",
    "text": "4.5 Scaling\nUnlike linear model estimation without shrinkage (regularization), shrinkage method is sensitive to the scaling of independent variables. Scaling of a variable has basically no consequence in linear model without regularization. It simply changes the interpretation of the scaled variable and the coefficient estimates on all the other variables remain unaffected. However, scaling of a single variable has a ripple effect to the other variables in shrinkage methods. This is because the penalization term: \\(\\lambda \\huge[\\normalsize(1-\\alpha)||\\beta||^2_2/2 + \\alpha ||\\beta||_1\\huge]\\). As you can see, \\(\\lambda\\) is applied universally to all the coefficients without any consideration of the scale of the variables.\nLet’s scale the first variable in X (this variable is influential as it survived even when \\(\\lambda\\) is very low) by 1/1000 and see what happens. Now, by default, the standardize option is set to TRUE. So, we need to set it to FALSE explicitly to see the effect.\nHere is before scaling:\n\ncv.glmnet(X, y, nfolds = 5, standardize = FALSE) %>% \n  coef(s = \"lambda.min\")\n\n21 x 1 sparse Matrix of class \"dgCMatrix\"\n                      s1\n(Intercept)  0.144972142\nV1           1.347294582\nV2           .          \nV3           0.710566556\nV4           .          \nV5          -0.843772317\nV6           0.560984722\nV7           0.040488094\nV8           0.353438701\nV9           .          \nV10          0.012575264\nV11          0.188217969\nV12          .          \nV13          .          \nV14         -1.093002349\nV15         -0.002483784\nV16          .          \nV17          .          \nV18          .          \nV19          .          \nV20         -1.062531091\n\n\nHere is after scaling:\n\n#=== scale the first variable ===#\nX_scaled <- X\nX_scaled[, 1] <- X_scaled[, 1] / 1000\n\ncv.glmnet(X_scaled, y, nfolds = 5, standardize = FALSE) %>% \n  coef(s = \"lambda.min\")\n\n21 x 1 sparse Matrix of class \"dgCMatrix\"\n                     s1\n(Intercept)  0.48468325\nV1           .         \nV2           .         \nV3           0.58495226\nV4           .         \nV5          -0.77303610\nV6           0.72015880\nV7           0.03426215\nV8           0.08734948\nV9           .         \nV10          .         \nV11          0.46374479\nV12          .         \nV13          .         \nV14         -1.02293498\nV15          .         \nV16          .         \nV17          .         \nV18          0.09113589\nV19          .         \nV20         -1.28272851\n\n\nAs you can see, the coefficient on the first variable is 0 after scaling. Setting standardize = TRUE (or not doing anything with this option) gives you very similar results whether the data is scaled or not.\n\n#=== not scaled ===#\ncv.glmnet(X, y, nfolds = 5, standardize = TRUE) %>% \n  coef(s = \"lambda.min\")\n\n21 x 1 sparse Matrix of class \"dgCMatrix\"\n                     s1\n(Intercept)  0.14867414\nV1           1.33377821\nV2           .         \nV3           0.69787701\nV4           .         \nV5          -0.83726751\nV6           0.54334327\nV7           0.02668633\nV8           0.33741131\nV9           .         \nV10          .         \nV11          0.17105029\nV12          .         \nV13          .         \nV14         -1.07552680\nV15          .         \nV16          .         \nV17          .         \nV18          .         \nV19          .         \nV20         -1.05278699\n\n#=== scaled ===#\ncv.glmnet(X_scaled, y, nfolds = 5, standardize = TRUE) %>% \n  coef(s = \"lambda.min\")\n\n21 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s1\n(Intercept)    0.14867414\nV1          1333.77821030\nV2             .         \nV3             0.69787701\nV4             .         \nV5            -0.83726751\nV6             0.54334327\nV7             0.02668633\nV8             0.33741131\nV9             .         \nV10            .         \nV11            0.17105029\nV12            .         \nV13            .         \nV14           -1.07552680\nV15            .         \nV16            .         \nV17            .         \nV18            .         \nV19            .         \nV20           -1.05278699\n\n\nWhile you do not have to worry about scaling issues as long as you are using glmnet(), this is something worth remembering."
  },
  {
    "objectID": "B04-regularization.html#references",
    "href": "B04-regularization.html#references",
    "title": "4  Regression Shrinkage Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "B05-bootstrap.html",
    "href": "B05-bootstrap.html",
    "title": "5  Bootstrap",
    "section": "",
    "text": "Bootstrap can be used to quantify the uncertainty associated with an estimator. For example, you can use it to estimate the standard error (SE) of a coefficient of a linear model. Since there are closed-form solutions for that, bootstrap is not really bringing any benefits to this case. However, the power of bootstrap comes in handy when you do NOT have a closed form solution. We will first demonstrate how bootstrap works using a linear model, and then apply it to a case where no-closed form solution is available."
  },
  {
    "objectID": "B05-bootstrap.html#how-it-works",
    "href": "B05-bootstrap.html#how-it-works",
    "title": "5  Bootstrap",
    "section": "5.2 How it works",
    "text": "5.2 How it works\n\n\nPackages to load for replication\n\nlibrary(data.table)\nlibrary(tidyverse)\nlibrary(mgcv)\nlibrary(caret)\nlibrary(parallel)\nlibrary(fixest)\nlibrary(tidymodels)\nlibrary(ranger)\n\nHere are the general steps of a bootstrap:\n\nStep 1: Sample the data with replacement (You can sample the same observations more than one times. You draw a ball and then you put it back in the box.)\nStep 2: Run a statistical analysis to estimate whatever quantity you are interested in estimating\nRepeat Steps 1 and 2 many times and store the estimates\nDerive uncertainty measures from the collection of estimates obtained above\n\nLet’s demonstrate this using a very simple linear regression example.\nHere is the data generating process:\n\nset.seed(89343)\nN <- 100\nx <- rnorm(N)\ny <- 2 * x + 2 * rnorm(N)\n\nWe would like to estimate the coefficient on \\(x\\) by applying OLS to the following model:\n\\[\ny = \\alpha + \\beta_x + \\mu\n\\]\nWe know from the econometric theory class that the SE of \\(\\hat{\\beta}_{OLS}\\) is \\(\\frac{\\sigma}{\\sqrt{SST_x}}\\), where \\(\\sigma^2\\) is the variance of the error term (\\(\\mu\\)) and \\(SST_x = \\sum_{i=1}^N (x_i - \\bar{x})^2\\) (\\(\\bar{x}\\) is the mean of \\(x\\)).\n\nmean_x <- mean(x)\nsst_x <- ((x-mean(x))^2) %>% sum()\n(\nse_bhat <- sqrt(4 / sst_x)\n)\n\n[1] 0.217933\n\n\nSo, we know that the true SE of \\(\\hat{\\beta}_{OLS}\\) is 0.217933. There is not really any point in using bootstrap in this case, but this is a good example to see if bootstrap works or not.\nLet’s implement a single iteration of the entire bootstrap steps (Steps 1 and 2).\n\n#=== set up a dataset ===#\ndata <-\n  data.table(\n    y = y,\n    x = x\n  )\n\nNow, draw observations with replacement so the resulting dataset has the same number of observations as the original dataset.\n\nnum_obs <- nrow(data)\n\n#=== draw row numbers ===#\n(\nrow_indices <- sample(seq_len(num_obs), num_obs, replace = TRUE)\n)\n\n  [1] 29 56 39 83 52 66 70 19  4 34 28 34 81 32  9 95 99 86  4 79 30 15 41 97 43\n [26] 89 60 41 16 19 66 96 34 91 86 67 75 28 74 50 71 95 74 87 58 27  9 65 80 41\n [51] 71 64 21 47 45 77 97 94 72 50 23 10 33 45 14 17 82 56 33 75 70 63 78 81 64\n [76] 16 84 90  2 17  5 46 53 37 93 85 72 63 10 35 42 20 70 49 74 32 25 73 76 32\n\n\nUse the sampled indices to create a bootstrapped dataset:\n\n\nYou could also use bootstraps() from the rsample package.\n\ntemp_data <- data[row_indices, ]\n\nNow, apply OLS to get a coefficient estimate on \\(x\\) using the bootstrapped dataset.\n\nlm(y ~ x, data = temp_data)$coefficient[\"x\"]\n\n       x \n2.040957 \n\n\nThis is the end of Steps 1 and 2. Now, let’s repeat this step 1000 times. First, we define a function that implements Steps 1 and 2.\n\nget_beta <- function(i, data)\n{\n  num_obs <- nrow(data)\n\n  #=== sample row numbers ===#\n  row_indices <- sample(seq_len(num_obs), num_obs, replace = TRUE)\n\n  #=== bootstrapped data ===#\n  temp_data <- data[row_indices, ]\n\n  #=== get coefficient ===#\n  beta_hat <- lm(y ~ x, data = temp_data)$coefficient[\"x\"]\n\n  return(beta_hat)\n}\n\nNow repeat get_beta() many times:\n\nbeta_store <-\n  lapply(\n    1:1000,\n    function(x) get_beta(x, data)\n  ) %>% \n  unlist()\n\nCalculate standard deviation of \\(\\hat{\\beta}_{OLS}\\),\n\nsd(beta_store)\n\n[1] 0.2090611\n\n\nNot, bad. What if we make the number of observations to 1000 instead of 100?\n\nset.seed(67343)\n\n#=== generate data ===#\nN <- 1000\nx <- rnorm(N)\ny <- 2 * x + 2 * rnorm(N)\n\n#=== set up a dataset ===#\ndata <-\n  data.table(\n    y = y,\n    x = x\n  )\n\n#=== true SE ===#\nmean_x <- mean(x)\nsst_x <- sum(((x-mean(x))^2))\n(\nse_bhat <- sqrt(4 / sst_x)\n)\n\n[1] 0.06243842\n\n\n\n#=== bootstrap-estimated SE ===#\nbeta_store <-\n  lapply(\n    1:1000,\n    function(x) get_beta(x, data)\n  ) %>% \n  unlist()\n\nsd(beta_store)\n\n[1] 0.06147708\n\n\nThis is just a single simulation. So, we cannot say bootstrap works better when the number of sample size is larger only from these experiments. But, it is generally true that bootstrap indeed works better when the number of sample size is larger."
  },
  {
    "objectID": "B05-bootstrap.html#sec-boot-comp",
    "href": "B05-bootstrap.html#sec-boot-comp",
    "title": "5  Bootstrap",
    "section": "5.3 A more complicated example",
    "text": "5.3 A more complicated example\nConsider a simple production function (e.g., yield response functions for agronomists):\n\\[\ny = \\beta_1 x + \\beta_2 x^2 + \\mu\n\\]\n\n\\(y\\): output\n\\(x\\): input\n\\(\\mu\\): error\n\nThe price of \\(y\\) is 5 and the cost of \\(x\\) is 2. Your objective is to identify the amount of input that maximizes profit. You do not know \\(\\beta_1\\) and \\(\\beta_2\\), and will be estimating them using the data you have collected. Letting \\(\\hat{\\beta_1}\\) and \\(\\hat{\\beta_2}\\) denote the estimates of \\(\\beta_1\\) and \\(\\beta_2\\), respectively, the mathematical expression of the optimization problem is:\n\\[\nMax_x 5(\\hat{\\beta}_1 x + \\hat{\\beta}_2 x^2) - 2 x\n\\]\nThe F.O.C is\n\\[\n5\\hat{\\beta}_1 + 10 \\hat{\\beta}_2 x - 2 = 0\n\\]\nSo, the estimated profit-maximizing input level is \\(\\hat{x}^* = \\frac{2-5\\hat{\\beta}_1}{10\\hat{\\beta}_2}\\). What we are interested in knowing is the SE of \\(x^*\\). As you can see, it is a non-linear function of the coefficients, which makes it slightly harder than simply getting the SE of \\(\\hat{\\beta_1}\\) or \\(\\hat{\\beta_2}\\). However, bootstrap can easily get us an estimate of the SE of \\(\\hat{x}^*\\). The bootstrap process will be very much the same as the first bootstrap example except that we will estimate \\(x^*\\) in each iteration instead of stopping at estimating just coefficients. Let’s work on a single iteration first.\n\n\nAlternatively, you could use the delta method, which lets you find an estimate of the SE of a statistics that is a non-linear function of estimated coefficients.\nHere is the data generating process:\n\nset.seed(894334)\n\nN <-  1000\nx <-  runif(N) * 3\ney <- 6 * x - 2 * x^2\nmu <- 2 * rnorm(N)\ny <- ey + mu\n\ndata <- \n  data.table(\n    x = x,\n    y = y,\n    ey = ey\n  )\n\nUnder the data generating process, here is what the production function looks like:\n\n\nCode\nggplot(data = data) +\n  geom_line(aes(y = ey, x = x)) +\n  theme_bw()\n\n\n\n\n\n\nnum_obs <- nrow(data)\nrow_indices <- sample(seq_len(num_obs), num_obs, replace = TRUE)\nboot_data <- data[row_indices, ]\nreg <- lm(y ~ x + I(x^2), data = boot_data)\n\nNow that we have estimated \\(\\beta_1\\) and \\(\\beta_2\\), we can easily estimate \\(x^*\\) using its analytical formula.\n\n(\nx_star <- (2 - 5 * reg$coef[\"x\"])/ (10 * reg$coef[\"I(x^2)\"])\n)\n\n      x \n1.40625 \n\n\nWe can repeat this many times to get a collection of \\(x^*\\) estimates and calculate the standard deviation.\n\nget_x_star <- function(i)\n{\n  row_indices <- sample(seq_len(num_obs), num_obs, replace = TRUE)\n  boot_data <- data[row_indices, ]\n  reg <- lm(y ~ x + I(x^2), data = boot_data)\n  x_star <- (2 - 5 * reg$coef[\"x\"])/ (10 * reg$coef[\"I(x^2)\"])\n}\n\n\nx_stars <- \n  lapply(\n    1:1000,\n    get_x_star\n  ) %>%\n  unlist()\n\nHere is the histogram:\n\n\nCode\nhist(x_stars, breaks = 30)\n\n\n\n\n\nSo, it seems to follow a normal distribution. You can get standard deviation of x_stars as an estimate of the SE of \\(\\hat{x}^*\\).\n\nsd(x_stars)\n\n[1] 0.01783721\n\n\nYou can get the 95% confidence interval (CI) like below:\n\nquantile(x_stars, prob = c(0.025, 0.975))\n\n    2.5%    97.5% \n1.364099 1.430915"
  },
  {
    "objectID": "B05-bootstrap.html#one-more-example-with-a-non-parametric-model",
    "href": "B05-bootstrap.html#one-more-example-with-a-non-parametric-model",
    "title": "5  Bootstrap",
    "section": "5.4 One more example with a non-parametric model",
    "text": "5.4 One more example with a non-parametric model\nWe now demonstrate how we can use bootstrap to get an estimate of the SE of \\(\\hat{x}^*\\) when we use random forest (RF) as our regression method instead of OLS. When RF is used, we do not have any coefficients like the OLS case above. Even then, bootstrap allows us to estimate the SE of \\(\\hat{x}^*\\).\n\n\nRF will be covered in Section 6.2. Please take a quick glance at what RF is to confirm this point. No deep understanding of RF is necessary to understand the significance of bootstrap in this example. Please also note that there is no benefit in using RF in this example because we have only one variable and semi-parametric approach like gam() will certainly do better. RF is used here only because it does not give you coefficient estimates (there are no coefficients in the first place for RF) like linear models.\nThe procedure is exactly the same except that we use RF to estimate the production function and also that we need to conduct numerical optimization as no analytical formula is available unlike the case above.\nWe first implement a single iteration.\n\n#=== get bootstrapped data ===#\nrow_indices <- sample(seq_len(num_obs), num_obs, replace = TRUE)\nboot_data <- data[row_indices, ]\n\n#=== train RF ===#\nreg_rf <- ranger(y ~ x, data = boot_data)\n\nOnce you train RF, we can predict yield at a range of values of \\(x\\), calculate profit, and then pick the value of \\(x\\) that maximizes the estimated profit.\n\n#=== create series of x values at which yield will be predicted ===#\neval_data <- data.table(x = seq(0, 3, length = 1000))\n\n#=== predict yield based on the trained RF ===#\neval_data[, y_hat := predict(reg_rf, eval_data)$predictions]\n\nHere is what the estimated production function looks like:\n\n\nCode\n#=== plot ===#\nggplot(data = eval_data) +\n  geom_line(aes(y = y_hat, x = x)) +\n  theme_bw()\n\n\n\n\n\nWell, it is very spiky (we need to tune hyper-parameters using KCV. But, more on this later. The quality of RF estimation has nothing to do with the goal of this section).\nWe can now predict profit at each value of \\(x\\).\n\n#=== calculate profit ===#\neval_data[, profit_hat := 5 * y_hat - 2 * x]\n\nhead(eval_data)\n\n             x      y_hat profit_hat\n1: 0.000000000 -0.4807528  -2.403764\n2: 0.003003003 -0.4807528  -2.409770\n3: 0.006006006 -0.4807528  -2.415776\n4: 0.009009009  1.0193781   5.078872\n5: 0.012012012  1.0193781   5.072866\n6: 0.015015015  1.2466564   6.203252\n\n\nThe only thing left for us to do is find the \\(x\\) value that maximizes profit.\n\neval_data[which.max(profit_hat), ]\n\n          x    y_hat profit_hat\n1: 1.273273 8.675033   40.82862\n\n\nOkay, so 1.2732733 is the \\(\\hat{x}^*\\) from this iteration.\nAs you might have guessed already, we can just repeat this step to get an estimate of the SE of \\(\\hat{x}^*_{RF}\\).\n\nget_x_star_rf <- function(i)\n{\n  print(i) # progress tracker\n  \n  #=== get bootstrapped data ===#\n  row_indices <- sample(seq_len(num_obs), num_obs, replace = TRUE)\n  boot_data <- data[row_indices, ]\n\n  #=== train RF ===#\n  reg_rf <- ranger(y ~ x, data = boot_data)\n\n  #=== create series of x values at which yield will be predicted ===#\n  eval_data <- data.table(x = seq(0, 3, length = 1000))\n\n  #=== predict yield based on the trained RF ===#\n  eval_data[, y_hat := predict(reg_rf, eval_data)$predictions]\n  \n  #=== calculate profit ===#\n  eval_data[, profit_hat := 5 * y_hat - 2 * x]\n  \n  #=== find x_star_hat ===#\n  x_star_hat <- eval_data[which.max(profit_hat), x]\n  \n  return(x_star_hat)\n}\n\n\nx_stars_rf <- \n  mclapply(\n    1:1000,\n    get_x_star_rf,\n    mc.cores = 12\n  ) %>%\n  unlist()\n\n#=== Windows user ===#\n# library(future.apply)\n# plan(\"multisession\", workers = detectCores() - 2)\n# x_stars_rf <- \n#   future_lapply(\n#     1:1000,\n#     get_x_star_rf\n#   ) %>%\n#   unlist()\n\nHere are the estimate of the SE of \\(\\hat{x}^*_{RF}\\) and 95% CI.\n\nsd(x_stars_rf)\n\n[1] 0.3949154\n\nquantile(x_stars_rf, prob = c(0.025, 0.975))\n\n     2.5%     97.5% \n0.5465465 2.0570571 \n\n\nAs you can see, the estimation of \\(x^*\\) is much more inaccurate than the previous OLS approach. This is likely due to the fact that we are not doing a good job of tuning the hyper-parameters of RF (but, again, more on this later).\nThis conclude the illustration of the power of using bootstrap to estimate the uncertainty of the statistics of interest (\\(x^*\\) here) when the analytical formula of the statistics is non-linear or not even known."
  },
  {
    "objectID": "B05-bootstrap.html#bag-of-little-bootstraps",
    "href": "B05-bootstrap.html#bag-of-little-bootstraps",
    "title": "5  Bootstrap",
    "section": "5.5 Bag of Little Bootstraps",
    "text": "5.5 Bag of Little Bootstraps\nBag of little bootstraps (BLB) is a variant of bootstrap that has similar desirable statistical properties as bootstrap, but is more (computer) memory friendly (Kleiner et al. 2014).\nWhen the sample size is large (say 1GB) bootstrap is computationally costly both in terms of time and memory because the data of the same size is bootstrapped for many times (say, 1000 times). BLB overcomes this problem.\nSuppose you are interested in \\(\\eta\\) (e.g., standard error, confidence interval) of the estimator \\(\\hat{\\theta}\\). Further, let \\(N\\) denoted the sample size of the training data.\nBLB works as follows:\n\nSplit the dataset into \\(S\\) sets of subsamples of size \\(B\\) so that \\(s \\times B = N\\)  without replacement.\nFor each of the subsample, bootstrap \\(N\\) ( not \\(B\\) ) samples  with replacement  \\(M\\) times, and find \\(\\eta\\) (call it \\(\\eta_s\\)) for each of the subsample sets.\nAverage \\(\\hat{\\eta}_s\\) to get \\(\\hat{eta}\\).\n\n\n5.5.1 Demonstrations\nLet’s go back to the example discussed in Section 5.3, where the goal is to identify the confidence interval (\\(\\eta\\)) of the economically optimal input rate (\\(\\theta\\)).\n\n Step 1: \n\n\nStep 1: Split the dataset into \\(S\\) sets of subsamples of size \\(B\\) so that \\(s \\times B = N\\)  without replacement\nHere, we set \\(S\\) at 5, so \\(B = 200\\).\n\nN <- nrow(data)\nS <- 5\n\n(\nsubsamples <-\n  data[sample(seq_len(N), N), ] %>%\n  .[, subgroup := rep(1:S, each = N/S)] %>% \n  split(.$subgroup)\n)\n\n$`1`\n             x           y       ey subgroup\n  1: 2.3733159  5.30861102 2.974639        1\n  2: 1.6275776  3.09436504 4.467448        1\n  3: 0.2254504 -0.00443899 1.251047        1\n  4: 1.9988901  3.51795123 4.002217        1\n  5: 2.6819931  2.65018050 1.705785        1\n ---                                        \n196: 0.6356858  1.70404729 3.005922        1\n197: 0.5894847  5.41369203 2.841924        1\n198: 1.1667444  5.82214614 4.277881        1\n199: 0.9570687  4.01145812 3.910451        1\n200: 2.6627629  5.73397413 1.795965        1\n\n$`2`\n             x           y        ey subgroup\n  1: 1.8809192  4.57934497 4.2098011        2\n  2: 0.1314569 -0.61561180 0.7541794        2\n  3: 2.5253701  1.49742398 2.3972324        2\n  4: 0.8415726  5.29003218 3.6329468        2\n  5: 0.4067221  3.22079460 2.1094867        2\n ---                                         \n196: 0.3281349  5.33737092 1.7534644        2\n197: 0.1399768  3.43767708 0.8006737        2\n198: 2.6292985 -0.06800098 1.9493698        2\n199: 2.3410270  3.73951071 3.0853472        2\n200: 2.8142845 -2.75691735 1.0453126        2\n\n$`3`\n             x          y          ey subgroup\n  1: 1.2296299  5.0020342 4.353800036        3\n  2: 0.8974602  5.2290199 3.773891666        3\n  3: 1.0734530  1.0161772 4.136115258        3\n  4: 2.8013991  2.7505559 1.112720709        3\n  5: 1.5841919  4.9322236 4.485823441        3\n ---                                          \n196: 0.1508607  1.6273472 0.859646069        3\n197: 0.6124024  1.8253166 2.924341101        3\n198: 2.9990628 -0.4053550 0.005621654        3\n199: 2.8691630 -0.9073811 0.750785549        3\n200: 1.0579288  6.2074265 4.109146183        3\n\n$`4`\n             x          y       ey subgroup\n  1: 2.3998924 -0.8807641 2.880388        4\n  2: 1.0257974  6.6735166 4.050264        4\n  3: 1.1611026  3.5404186 4.270297        4\n  4: 1.0894566  2.5421898 4.162908        4\n  5: 0.1975354  0.3857778 1.107172        4\n ---                                       \n196: 0.7921199  7.0038608 3.497812        4\n197: 2.6279086  1.9564617 1.955645        4\n198: 1.4766912  2.8731862 4.498913        4\n199: 0.6217361  2.1170408 2.957305        4\n200: 2.0381772  1.8095725 3.920731        4\n\n$`5`\n             x          y          ey subgroup\n  1: 0.5453160 10.3885051 2.677156938        5\n  2: 1.2077153  1.2218954 4.329139357        5\n  3: 1.1022868  1.8370493 4.183648383        5\n  4: 2.9998229 -0.5902619 0.001062293        5\n  5: 1.8845175  0.8490326 4.204292516        5\n ---                                          \n196: 1.5471997  4.8499174 4.495544367        5\n197: 0.5813285  3.7268747 2.812085354        5\n198: 1.5839438  7.7635746 4.485906861        5\n199: 0.2343757  1.8121762 1.296390509        5\n200: 1.9727159  1.9638589 4.053079371        5\n\n\n\n\nYou could alternatively do this:\n\nfold_data <- vfold_cv(data, v = 5)\n\nsubsamples <-\n  lapply(\n    1:5,\n    function(x) fold_data[x, ]$splits[[1]] %>% assessment()\n  )\n\n\n Steps 2: \n\n\n\nStep 2: For each of the subsample, bootstrap \\(N\\) ( not \\(B\\) ) samples  with replacement  \\(M\\) times, and find \\(\\eta\\) (call it \\(\\eta_s\\)) for each of the subsample sets.\n\nInstead of creating bootstrapped samples \\(M\\) times, only one bootstrapped sample from the first subsample is created for now, and then the estimate of \\(x^*\\) is obtained.\n\n(\nboot_data <-\n  sample(\n    seq_len(nrow(subsamples[[1]])),\n    N,\n    replace = TRUE\n  ) %>% \n  subsamples[[1]][., ]\n)\n\n               x          y          ey\n   1: 2.99982294 -0.5902619 0.001062293\n   2: 0.36841088  0.6414166 1.939012145\n   3: 0.36841088  0.6414166 1.939012145\n   4: 1.52659488  4.1079431 4.498585425\n   5: 2.68124652  1.4721212 1.709313320\n  ---                                  \n 996: 2.20473789  2.7760518 3.506689002\n 997: 0.67635816  6.1153578 3.143228227\n 998: 1.89649565  6.3148116 4.185582404\n 999: 0.03379467 -0.2925077 0.200483835\n1000: 2.51539783  1.7992266 2.437934475\n\n\nWe can now train a linear model using the bootstrapped data and calculate \\(\\hat{x}^*\\) based on it.\n\nreg <- lm(y ~ x + I(x^2), data = boot_data)\n\n(\nx_star <- (2 - 5 * reg$coef[\"x\"])/ (10 * reg$coef[\"I(x^2)\"])\n)\n\n       x \n1.367197 \n\n\nWe can loop this process over all the \\(100\\) (\\(M = 100\\)) bootstrapped data from the fist subsample set. Before that, let’s define a function that conducts a single implementation of bootstrapping and estimating \\(x^*\\) from a single subsample set.\n\nget_xstar_boot <- function(subsample)\n{\n  boot_data <-\n    sample(\n      seq_len(nrow(subsample)),\n      N,\n      replace = TRUE\n    ) %>% \n    subsample[., ]\n  reg <- lm(y ~ x + I(x^2), data = boot_data)\n\n  x_star <- (2 - 5 * reg$coef[\"x\"])/ (10 * reg$coef[\"I(x^2)\"])\n\n  return(x_star)\n}\n\nThis is a single iteration.\n\nget_xstar_boot(subsamples[[1]])\n\n       x \n1.400198 \n\n\nNow, let’s repeat this 100 (\\(M = 100\\)) times.\n\nM <- 100\n\nxstar_hats <-\n  lapply(\n    1:M,\n    function(x) get_xstar_boot(subsamples[[1]])\n  ) %>% \n  unlist()\n\nWe can now get the 95% confidence interval of \\(\\hat{x}^*\\).\n\ncf_lower <- quantile(xstar_hats, prob = 0.025) \ncf_upper <- quantile(xstar_hats, prob = 0.975) \n\nSo, [1.36848876286897,1.44320748221065] is the 95% confidence interval estimate from the first subsample set. We repeat this for the other subsample sets and average the 95% CI from them (get_CI() defined on the side).\n\n\nThis function find the 95% confidence interval (\\(\\eta\\)) from a single subsample set.\n\nget_CI <- function(subsample){\n  xstar_hats <-\n    lapply(\n      1:M,\n      function(x) get_xstar_boot(subsamples[[1]])\n    ) %>% \n    unlist() \n\n  cf_lower <- quantile(xstar_hats, prob = 0.025) \n  cf_upper <- quantile(xstar_hats, prob = 0.975) \n\n  return_data <-\n    data.table(\n      cf_lower = cf_lower,\n      cf_upper = cf_upper\n    )\n\n  return(return_data)\n}\n\n\n(\nci_data <-\n  lapply(\n    subsamples,\n    function(x) get_CI(x)\n  ) %>% \n  rbindlist()\n)\n\n   cf_lower cf_upper\n1: 1.374328 1.443085\n2: 1.368850 1.443649\n3: 1.365456 1.444492\n4: 1.368362 1.440309\n5: 1.362626 1.445573\n\n\n\n Steps 3: \nBy averaging the lower and upper bounds, we get our estimate of the lower and upper bound of the 95% CI of \\(\\hat{x}^*\\).\n\nci_data[, .(mean(cf_lower), mean(cf_upper))]\n\n         V1       V2\n1: 1.367924 1.443422\n\n\nNote the estimated CI is very much similar to that from a regular bootstrap obtained in Section 5.3.\n\n\n\n\n\n\nTip\n\n\n\n\nBag of little bootstraps (BLB) is a variant of bootstrap that has similar desirable statistical properties as bootstrap, but is more computation and memory friendly."
  },
  {
    "objectID": "B05-bootstrap.html#references",
    "href": "B05-bootstrap.html#references",
    "title": "5  Bootstrap",
    "section": "References",
    "text": "References\n\n\nKleiner, Ariel, Ameet Talwalkar, Purnamrita Sarkar, and Michael I. Jordan. 2014. “A Scalable Bootstrap for Massive Data.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 76 (4): 795–816. https://doi.org/10.1111/rssb.12050."
  },
  {
    "objectID": "P01-random-forest.html",
    "href": "P01-random-forest.html",
    "title": "6  Random Forest",
    "section": "",
    "text": "The main goal of this chapter is to learn random forest (RF) for regression. RF is first conceived by Breiman (2001), and it has been one of the most popular machine learning methods. It also has been an inspiration to many other ML methods including extreme gradient boosting (Chen and Guestrin 2016), generalized random forest (Athey, Tibshirani, and Wager 2019).\nWe first start with understanding how a regression tree is built, which is the foundation of all the forest-based ML methods. We then learn how random forest is built as an extension of a regression tree and how to train it."
  },
  {
    "objectID": "P01-random-forest.html#sec-rt",
    "href": "P01-random-forest.html#sec-rt",
    "title": "6  Random Forest",
    "section": "6.1 Regression tree",
    "text": "6.1 Regression tree\n\n\nPackages to load for replication\n\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(rpart)\nlibrary(rattle)\nlibrary(wooldridge)\nlibrary(gganimate)\n\n\n6.1.1 What is it?\nHere is an example of regression tree to explain logged salary (lsalary) using the mlb1 data from the wooldridge package.\n\n\nCode\n#=== get mlb1 data (from wooldridge) ===#\ndata(mlb1)\n\n#=== build a simple tree ===#\nsimple_tree <-\n  rpart(\n    lsalary ~ hits + runsyr, \n    data = mlb1, \n    control = rpart.control(minsplit = 200)\n  )\n\nfancyRpartPlot(simple_tree, digits = 4)\n\n\n\n\n\n\n\n\n\n\n\nTerminology alert\n\n\n\n\nnode : a group of observations represented by a green box\nroot node: first node to which all the observations belong\nleaf or  terminal node: A node that is not split into more nodes\n\n\n\nHere is how you read the figure. At the first (root) node, all the observations belong to it (\\(n=353\\)) and the estimate of lsalary is \\(13.49\\), which is simply the sample average of lsalary. Now, the whole dataset is split into two groups (nodes) based on the criteria of whether hits is less than 262 or not. If yes, then such observations will be grouped into the node with “2” on top (the leftmost node), and the estimated lsalary for all the observations in that group (\\(n = 132\\)) is \\(12.35\\), which is the sample average of lsalary of the \\(132\\) observations in the group. If no, then such observations will be grouped into the node with “3” on top, and the estimated lsalary for all the observations in that group (\\(n = 221\\)) is \\(14.17\\). This node is further split into two groups based on whether runsyr is less than \\(44\\) or not. For those observations with runsyr \\(< 44\\) (second node at the bottom), estimated lsalary is \\(13.58\\). For those with runsyr \\(>= 44\\) (rightmost node at the bottom), estimated lsalary is \\(14.56\\).\nAs illustrated in the figure above, a regression tree splits the data into groups based on the value of explanatory variables, and all the observations in the same group will be assigned the same estimate, which isthe sample average of the dependent variable of the group.\n\n\n\n\n\n\nTip\n\n\n\nThe estimated value of \\(Y\\) (the dependent variable) for \\(X = x\\) is the average of \\(Y\\) from all the observations that belong to the same terminal node (leaf) as \\(X = x\\).\n\n\nAnother way of illustrating this grouping is shown below:\n\n\nCode\nggplot(mlb1) +\n  geom_point(aes(y = hits, x = runsyr, color = lsalary)) +\n  scale_color_viridis_c() +\n  geom_hline(yintercept = 262) +\n  geom_line(\n    data = data.table(x = 44, y = seq(262, max(mlb1$hits), length = 100)), \n    aes(y = y, x = x)\n  ) +\n  annotate(\n    \"text\", \n    x = 75, y = 100, \n    label = \"Region (Node) 2\", \n    color = \"red\"\n  ) +\n  annotate(\n    \"text\", \n    x = 22, y = 1500, \n    label = \"Region (Node) 6\", \n    color = \"red\"\n  ) +\n  annotate(\n    \"text\", \n    x = 90, y = 1500, \n    label = \"Region (Node) 7\", \n    color = \"red\"\n  )\n\n\n\n\n\nAn algorithm called recursive binary splitting is used to split the predictor space like the example above. Suppose you have K explanatory variables (\\(X_1, \\dots, X_K\\)). Further, let \\(c\\) denote the threshold that splits the sample into two regions such that {\\(X|X_k < c\\)} and {\\(X|X_k \\geq c\\)}.\n\n\n{\\(X|X_k < c\\)} means observations that satisfy the condition stated right to the vertical bar (|). Here, it means all the observations for which its \\(X_k\\) value is less than \\(c\\).\n\nStep 1: For each of the covariates (\\(X_1\\) through \\(X_K\\)), find all the threshold values that result in  unique splits.\nStep 2: For each of the covariate-threshold combinations, calculate the sum of the squared residuals of its resulting split, and find the threshold value with the lowest sum of the squared residuals by covariate.\nStep 3: Among all the best splits by covariate (as many as the number of explanatory variables), pick the variable-threshold combination that leads to the lowest sum of the squared residuals.\n\nThe data is then split according to the chosen criteria and then the same process is repeated for each of the branches, ad infinitum until the user-specified stopping criteria is met. This way of splitting is called a greedy (or I would call it myopic or shortsighted) approach because the split is sought to minimize the immediate RSS without considering the implication of the split for the later splits.\nLet’s try to write this process for the first split from the beginning node using the mlb1 data as an illustration based on a simple grid search to find the optimal thresholds (Step 1).\n\n\nNote that the R codes here are deliberately inefficient as they focus on illustrating the process.\n\n#=== get data ===#\nlibrary(wooldridge)\ndata(mlb1)\n\nmlb1_dt <- \n  mlb1 %>% \n  data.table() %>% # turn into data.table \n  .[, salary := NULL] %>% # remove salary (use lsalary instead)\n  na.omit() # remove observations with NA in any of the variables\n\nLet’s work on splitting based on hruns. One way to find the all the threshold values is simply find the mean of two consecutive numbers from the ordered unique hruns values.\n\nvalue_seq <- \n  #=== order hruns values and find the unique values ===#\n  mlb1_dt[order(hruns), unique(hruns)] %>%\n  #=== get the rolling mean ===#\n  frollmean(2) %>% \n  .[-1]\n\nFigure 6.1 shows all the threshold values (blue lines) stored in value_seq that result in unique splits.\n\n\nCode\nthreshold_data <- \n  data.table(thre = value_seq) %>% \n  .[, id := 1:.N]\n\nggplot(mlb1_dt) +\n  geom_point(aes(y = lsalary, x = hruns)) +\n  geom_vline(\n    data = threshold_data, \n    aes(xintercept = thre), \n    color = \"blue\", \n    size = 0.2\n  ) +\n  geom_vline(xintercept = value_seq[[50]], color = \"red\") +\n  geom_vline(xintercept = value_seq[[70]], color = \"orange\")\n\n\n\n\n\nFigure 6.1: All the threshold values when splitting based on hruns\n\n\n\n\nFor each value in value_seq, we find RSS. For example, for the 50th value in value_seq (the red line in Figure 6.1),\n\ncopy(mlb1_dt) %>% \n  #=== find the mean of lsalary by whether hruns is less than the threshold or not ===#\n  .[, y_hat := mean(lsalary), by = (hruns < value_seq[50])] %>% \n  #=== get squared residuals ===#\n  .[, (lsalary - y_hat)^2] %>% \n  #=== get RSS ===#\n  sum()\n\n[1] 291.1279\n\n\nHow about 70th value in value_seq (the orange line in Figure 6.1)?\n\ncopy(mlb1_dt) %>% \n  #=== find the mean of lsalary by whether hruns is less than the threshold or not ===#\n  .[, y_hat := mean(lsalary), by = (hruns < value_seq[70])] %>% \n  #=== get squared residuals ===#\n  .[, (lsalary - y_hat)^2] %>% \n  #=== get RSS ===#\n  sum()\n\n[1] 322.3688\n\n\nThis means value_seq[50] (49.5) is a better threshold than value_seq[70] (79.5).\nOkay, let’s consider all the candidate threshold values now, not just 50th and 70th, and then pick the best.\n\nget_rss <- function(i, var_name, value_seq, data)\n{\n  rss <-\n    copy(data) %>% \n    setnames(var_name, \"var\") %>% \n    .[, y_hat := mean(lsalary), by = (var < value_seq[i])] %>% \n    .[, (lsalary - y_hat)^2] %>% \n    sum()\n\n  return_data <-\n    data.table(\n      rss = rss,\n      var_name = var_name,\n      var_value = value_seq[i]\n    )\n\n  return(return_data)\n}\n\nHere are RSS values at every value in value_seq.\n\nrss_value <-\n  lapply(\n    seq_len(length(value_seq)),\n    function(x) get_rss(x, \"hruns\", value_seq, mlb1_dt) \n  ) %>% \n  rbindlist()\n\nhead(rss_value)\n\n        rss var_name var_value\n1: 396.8287    hruns       0.5\n2: 383.2393    hruns       1.5\n3: 362.5779    hruns       2.5\n4: 337.0760    hruns       3.5\n5: 325.4865    hruns       4.5\n6: 309.2780    hruns       5.5\n\ntail(rss_value)\n\n        rss var_name var_value\n1: 437.6865    hruns     279.0\n2: 440.0806    hruns     291.5\n3: 441.9307    hruns     348.0\n4: 437.6143    hruns     398.5\n5: 441.0689    hruns     406.5\n6: 443.3740    hruns     423.0\n\n\nFinding the threshold value that minimizes RSS,\n\nrss_value[which.min(rss), ]\n\n        rss var_name var_value\n1: 259.7855    hruns      27.5\n\n\nOkay, so, the best threshold for hruns is 27.5\nSuppose we are considering only five explanatory variables in building a regression tree: hruns, years, rbisyr, allstar, runsyr, hits, and bavg. We do the same operations we did for hruns for all the variables.\n\nget_rss_by_var <- function(var_name, data)\n{\n\n  temp_data <- \n    copy(data) %>% \n    setnames(var_name, \"temp_var\")\n\n  #=== define a sequence of values of hruns ===#\n  value_seq <-\n    temp_data[order(temp_var), unique(temp_var)] %>%\n    #=== get the rolling mean ===#\n    frollmean(2) %>% \n    .[-1]\n\n  setnames(temp_data, \"temp_var\", var_name)\n\n  #=== get RSS ===#\n  rss_value <-\n    lapply(\n      seq_len(length(value_seq)),\n      function(x) get_rss(x, var_name, value_seq, temp_data) \n    ) %>% \n    rbindlist() %>% \n    .[which.min(rss),]\n\n  return(rss_value)\n}\n\nLooping over the set of variables,\n\n(\nmin_rss_by_var <-\n  lapply(\n    c(\"hruns\", \"years\", \"rbisyr\", \"allstar\", \"runsyr\", \"hits\", \"bavg\"),\n    function(x) get_rss_by_var(x, mlb1_dt)\n  ) %>% \n  rbindlist()\n)\n\n        rss var_name  var_value\n1: 259.7855    hruns  27.500000\n2: 249.9090    years   3.500000\n3: 261.1537   rbisyr  31.794642\n4: 277.3388  allstar   7.417582\n5: 249.5106   runsyr  37.666666\n6: 205.1488     hits 356.500000\n7: 375.0281     bavg 252.499992\n\n\nSo, the variable-threshold combination that minimizes RSS is hits - 356.5. We now have the first split. This tree is developed further by splitting nodes like this.\n\n\n6.1.2 A note on splitting algorithm\nMinimizing RSS is equivalent to  maximize  the sum of  similarity score from the two splits. Let \\(C_1(r)\\) and \\(C_2(r)\\) denote the set of observations that belong to the first and second splits, respectively given splitting rule \\(r\\). For a given split combination \\(C_1(r)\\) and \\(C_2(r)\\), the estimators for the two splits are simply the average value of \\(Y\\) in the respective splits. Denoting the means as \\(\\hat{\\mu}_1\\) and \\(\\hat{\\mu}_2\\), minimizing RSS can be written as\n\\[\n\\begin{aligned}\n\\min_r \\sum_{i\\in C_1} (\\hat{\\mu}_1 - Y_i)^2 + \\sum_{i\\in C_2} (\\hat{\\mu}_2 - Y_i)^2\n\\end{aligned}\n\\tag{6.1}\\]\n\n\n\\(\\hat{\\mu}_k = \\frac{\\sum_{i\\in C_k} Y_i}{N_k}\\), where \\(N_k\\) is the number of observations in \\(C_k\\).\nNow, in general, the following holds\n\\[\n\\begin{aligned}\n\\sum_{i\\in C_k} (\\hat{\\mu}_k - Y_i)^2 = \\sum_{i\\in C_k} Y_i^2 - \\sum_{i\\in C_k} \\hat{\\mu}_k^2\n\\end{aligned}\n\\]\nSo, Equation 6.1 can be rewritten as\n\\[\n\\begin{aligned}\n\\min_r \\sum_{i\\in C_1(r)} Y_i^2 + \\sum_{i\\in C_2(r)} Y_i^2 - (\\sum_{i\\in C_1(r)} \\hat{\\mu}_1^2 + \\sum_{i\\in C_2(r)} \\hat{\\mu}_2^2)\n\\end{aligned}\n\\]\nSince \\(\\sum_{i\\in C_1(r)} Y_i^2 + \\sum_{i\\in C_2(r)} Y_i^2\\) is always the same, it can be further reduced to\n\\[\n\\begin{aligned}\n& \\min_r - N_1 (\\frac{\\sum_{i\\in C_1(r)} Y_i}{N_1})^2 - N_2 (\\frac{\\sum_{i\\in C_2(r)} Y_i}{N_2})^2 \\\\\n\\Rightarrow & \\min_r - \\frac{(\\sum_{i\\in C_1(r)} Y_i)^2}{N_1} - \\frac{(\\sum_{i\\in C_2(r)} Y_i)^2}{N_2}\n\\end{aligned}\n\\tag{6.2}\\]\nwhere \\(N_1\\) and \\(N_2\\) are the number of observations in \\(C_1\\) and \\(C_2\\), respectively. \\(\\frac{(\\sum_{i\\in C_k} Y_i)^2}{N_k}\\) is called  similarity score . The name comes from the fact that the more similar element in \\(C_k\\) are, the higher the score is. Consider for example, a four-observation case where their values of the dependent variable are \\(\\{3, 2, -1, -2\\}\\). For the split \\(\\{3, 2\\}\\), \\(\\{-1, -2\\}\\), the similarity score is \\(17 (=25/2 + 9/2)\\). However, if the split is \\(\\{2, -2\\}\\), \\(\\{3, -1\\}\\), then the score is \\(2 (=0/2 + 4/2)\\). Since summation happens first and then the sum is squared, the more similar the elements are, the higher the score is.\nEquation 6.2 can be reformulated as the maximization problem with the objective function being the sum of the similarity scores from the two splits.\n\\[\n\\begin{aligned}\n\\textcolor{red}{\\max}_r \\frac{(\\sum_{i\\in C_1(r)} Y_i)^2}{N_1} + \\frac{(\\sum_{i\\in C_2(r)} Y_i)^2}{N_2}\n\\end{aligned}\n\\tag{6.3}\\]\nNote that it is much more computationally efficient to solve this maximization problem than the RSS minimization problem (Equation 6.1). In the RSS minimization problem, for each split, you calculate the mean, subtract it from \\(Y_i\\) for all the observations, square them and sum them. On the other hand, in the similarity score maximization problem, you can simply sum \\(Y_i\\) and then square it for each split. So, programatically, tree building is done using Equation 6.3. You can also see the use of Equation 6.3 in the extreme gradient boosting algorithm (Chapter 8) and also in the implementation of GRF (Chapter 17).\n\n\n6.1.3 Training a regression tree\nYou can fit a regression tree using rpart() from the rpart package. Its syntax is similar to that of lm() for a quick fitting.\n\nrpart(\n  formula,\n  data\n)\n\nUsing mlb1, let’s fit a regression tree where lsalary is the dependent variable and hruns, years, rbisyr, allstar, runsyr, hits, and bavg are the explanatory variables.\n\n#=== fit a tree ===#\nfitted_tree <-\n  rpart(\n    lsalary ~ hruns + years + rbisyr + allstar + runsyr + hits + bavg, \n    data = mlb1_dt\n  )\n\nHere is the visualization of the fitted tree using fancyRpartPlot() from the rattle package.\n\nfancyRpartPlot(fitted_tree)\n\n\n\n\nNow, you may wonder why rpart() is not building a tree that has as many leaves as the number of observations so that we have a perfect prediction for the train data (mlb1). If we are simply implementing recursive binary splitting, then it should not have stopped where it stopped. This is because rpart() sets parameter values that control the development of a tree by default. Those default parameters can be seen below:\n\nrpart.control()\n\n$minsplit\n[1] 20\n\n$minbucket\n[1] 7\n\n$cp\n[1] 0.01\n\n$maxcompete\n[1] 4\n\n$maxsurrogate\n[1] 5\n\n$usesurrogate\n[1] 2\n\n$surrogatestyle\n[1] 0\n\n$maxdepth\n[1] 30\n\n$xval\n[1] 10\n\n\nFor example, minsplit is the minimum number of observations that must exist in a node in order for a split to be attempted. cp refers to the complexity parameter. For a given value of cp, a tree is build to minimize the following:\n\\[\n\\sum_{t=1}^T\\sum_{x_i\\in R_t} (y_i - \\hat{y}_{R_t})^2 + cp\\cdot T\n\\]\nwhere \\(R_t\\) is the \\(t\\)th region and \\(\\hat{y_{R_t}}\\) is the estimate of \\(y\\) for all the observations that reside in \\(R_t\\). So, the first term is RSS. The objective function has a penalization term (the second term) just like shrinkage methods we saw in Section 4.1. A higher value of cp leads to a less complex tree with less leaves.\nIf you want to build a much deeper tree that has many leaves, then you can do so using the control option like below.\n\nfull_tree <-\n  rpart(\n    lsalary ~ hruns + years + rbisyr + allstar + runsyr + hits + bavg, # formula\n    data = mlb1_dt, # data\n    control = # control of the hyper parameters\n      rpart.control(\n        minsplit = 2, \n        cp = 0 # complexity parameter\n      )\n  )\n\nLet’s see how amazing this tree is by comparing the observed and fitted lsalary values.\n\n#=== get fitted values ===#\nmlb1_dt[, y_hat := predict(full_tree, newdata = mlb1_dt)]\n\n#=== visualize the fit ===#\nggplot(data = mlb1_dt) +\n  geom_point(aes(y = lsalary, x = y_hat)) +\n  geom_abline(slope = 1, color = \"red\")\n\n\n\n\nYes, perfect prediction accuracy! At least for the train data anyway. But, we all know we want nothing to do with this kind of model. It is clearly over-fitting the train data.\nIn order to find a reasonable model, we can use KCV over cp. Fortunately, when we run rpart(), it automatically builds multiple trees at different values of cp that controls the number of leaves and conduct KCV. You can visualize this using plotcp().\n\nplotcp(fitted_tree)\n\n\n\n\nMSE and cp are presented on the y- and x-axis, respectively. According to the KCV results, cp \\(= 0.018\\) provides the tree with the smallest number of leaves (the most simple) where the MSE value is within one standard deviation from the lowest MSE. You can access the tree built under cp \\(= 0.018\\) like below.\n\n#=== get the best tree ===#\nbest_tree <- prune(full_tree, cp = 0.018)\n\n#=== visualize it ===#\nfancyRpartPlot(best_tree)\n\n\n\n\nEven though how a regression tree is build in R. In practice, you never use a regression tree itself as the final model for your research as its performance is rather poor and tend to over-fit compared to other competitive methods. But, understanding how building a regression tree is important to understand its derivatives like random forest, boosted regression forest."
  },
  {
    "objectID": "P01-random-forest.html#sec-rf",
    "href": "P01-random-forest.html#sec-rf",
    "title": "6  Random Forest",
    "section": "6.2 Random Forest (RF)",
    "text": "6.2 Random Forest (RF)\nRegression tree approach is often not robust and suffers from high variance. Here, we look at the process called  bagging  and how it can be used to train RF model, which is much more robust than a regression tree.\n\n6.2.1 Bagging (Bootstrap Averaging)\nBefore talking about how RF is trained. Let’s first talk about the concept of bagging. Let \\(\\theta(X)\\) denote the statistics of interest you would like to estimate from the data. Bagging (Bootstrap averaging) works like this:\n\nBootstrap the data many times\nEstimate \\(\\theta(X)\\) for each of the bootstrapped datasets (\\(\\hat{\\theta}_1, \\dots, \\hat{\\theta}_B\\))\nAverage the estimates and use it as the final estimate of \\(\\theta(X)\\).\n\n\\[\n\\hat{\\theta}(X) = \\frac{\\hat{\\theta}_1(X) + \\dots + \\hat{\\theta}_B(X)}{B}\n\\]\nTo understand the power of bagging, we need to understand the power of averaging and when it is most effective using a very simple example of estimating the expected value of a random variable.\nConsider two random variables \\(x_1\\) and \\(x_2\\) from the identical distribution, where \\(E[x_i] = \\alpha\\) and \\(Var(x_i) = \\sigma^2\\). You are interested in estimating \\(E[x_i]\\). We have two options:\n\nOption 1: Use \\(x_1\\) as the estimate of \\(E[x_i]\\).\nOption 2: Use the mean of \\(x_1\\) and \\(x_2\\) as the estimate of \\(E[x_i]\\)\n\nThe variance of the first estimator is of course simply the variance of \\(x_1\\), so \\(\\sigma^2\\).\nFor option 2, we know the following relationship holds in general:\n\\[\n\\begin{aligned}\nVar(\\frac{x_1 + x_2}{2}) & = \\frac{Var(x_1)}{4} + \\frac{Var(x_2)}{4} + \\frac{Cov(x_1, x_2)}{2} \\\\\n& = \\frac{\\sigma^2}{2} + \\frac{Cov(x_1, x_2)}{2}\n\\end{aligned}\n\\]\nSo, how good the mean of \\(x_1\\) and \\(x_2\\) as an estimator depends on \\(Cov(x_1, x_2)\\). When they are perfectly positively correlated, then \\(Cov(x_1, x_2) = Var(x_1) = \\sigma^2\\). So, \\(Var(\\frac{x_1 + x_2}{2})\\) is \\(\\sigma^2\\), which is no better than option 1.\n\n\nThis makes sense because adding information from one more variable that is perfectly correlated with the other variable does nothing because they are the same values.\nHowever, as long as \\(x_1\\) and \\(x_2\\) are not perfectly correlated, option 2 is better. The benefit of averaging is greater when the value of \\(Cov(x_1, x_2)\\) is smaller.\nLet’s do a little experiment to see this. We consider four approaches:\n\nApproach 0: use \\(x_1\\) as the estimator\nApproach 1: use \\((x_1 + x_2)/2\\) as the estimator (\\(x_1\\) and \\(x_2\\) are independent)\nApproach 2: use \\((x_1 + x_2)/2\\) as the estimator (\\(x_1\\) and \\(x_2\\) are positively correlated)\nApproach 3: use \\((x_1 + x_2)/2\\) as the estimator (\\(x_1\\) and \\(x_2\\) are negatively correlated)\n\n\n#=== set the number of observations to 1000 ===#\nN <- 1000\n\n\n#=== first approach (no correlation) ===#\nx_1 <- rnorm(N)\nx_2 <- rnorm(N)\n\ncor(x_1, x_2)\n\n[1] -0.0272246\n\n#=== second approach (positively correlated) ===#\nx_1 <- rnorm(N)\nx_2 <- 0.5 * x_1 + sqrt(1-(0.5)^2) * rnorm(N)\n\ncor(x_1, x_2)\n\n[1] 0.5203703\n\n#=== third approach (negatively correlated) ===#\nx_1 <- rnorm(N)\nx_2 <- - 0.8 * x_1 - sqrt(1-(0.8)^2) * rnorm(N)\n\ncor(x_1, x_2)\n\n[1] -0.7989847\n\n\nThe following function runs a single iteration of estimating \\(E[x]\\) using the four approaches.\n\nget_alpha <- function(i)\n{\n  #=== approach 0 ===#\n  alpha_hat_0 <- rnorm(1)\n\n  #=== approach 1 (no correlation) ===#\n  x_1 <- rnorm(1)\n  x_2 <- rnorm(1)\n\n  alpha_hat_1 <- (x_1 + x_2) / 2\n\n  #=== approach 2 (positively correlated) ===#\n  x_1 <- rnorm(1)\n  x_2 <- 0.5 * x_1 + sqrt(1-(0.5)^2) * rnorm(1)\n\n  alpha_hat_2 <- (x_1 + x_2) / 2\n\n  #=== approach 3 (negatively correlated) ===#\n  x_1 <- rnorm(1)\n  x_2 <- - 0.8 * x_1 - sqrt(1-(0.8)^2) * rnorm(1)\n\n  alpha_hat_3 <- (x_1 + x_2) / 2\n\n  return_data <-\n    data.table(\n      alpha_hat_0 = alpha_hat_0,\n      alpha_hat_1 = alpha_hat_1,\n      alpha_hat_2 = alpha_hat_2,\n      alpha_hat_3 = alpha_hat_3\n    )\n\n  return(return_data)\n\n} \n\nHere is the results of single iteration.\n\nget_alpha(1)\n\n   alpha_hat_0 alpha_hat_1 alpha_hat_2 alpha_hat_3\n1:  -0.9378108  -0.5568843   -0.288834  -0.1875634\n\n\nRepeating this many times,\n\nset.seed(234934)\n\nsim_results <-\n  lapply(\n    1:1000,\n    get_alpha\n  ) %>% \n  rbindlist() %>% \n  melt()\n\nFigure 6.2 shows the density plot of the estimates from the four approaches.\n\nsim_results %>% \n  .[, label := gsub(\"alpha_hat_\", \"Approach \", variable)] %>% \n  ggplot(data = .) +\n    geom_density(\n      aes(x = value, fill = label), \n      alpha = 0.5\n    ) +\n  scale_fill_discrete(name = \"\") +\n  theme_bw()\n\n\n\n\nFigure 6.2: Results of simple MC simulations to estimate the expected value of x\n\n\n\n\nAs you can see, they are all pretty much unbiased. However, all the approaches that average two values (approaches 1, 2, and 3) outperformed the base approach that relied on a single value each iteration. You can see that when the random variables are negatively correlated, the power of averaging is greater compared to when they are independent or positively correlated. The independent approach (approach 1) is better than the positive correlation approach (approach 2).\nNow that we understand the power of bagging, let’s apply this to a regression problem using a tree. The statistics of interest is \\(E[y|X]\\), which is denoted as \\(f(x)\\).\n\nBootstrap the data \\(B\\) times\nTrain a regression tree to each of the bootstrapped dataset, which results in \\(B\\) distinctive trees\nTo predict \\(E[y|x]\\), average the estimate from all the trees (\\(\\hat{f}_1(x), \\dots, \\hat{f}_B(x)\\)) and use it as the final estimate.\n\n\\[\n\\hat{f}(x) = \\frac{\\hat{f}_1(X) + \\dots + \\hat{f}_B(X)}{B}\n\\]\nLet’s implement this for \\(B = 10\\) using mlb1_dt. First, define a function that bootstrap data, fit a regression tree, and then return the fitted values (a single iteration).\n\ntrain_a_tree <- function(i, data)\n{\n  #=== number of observations ===#\n  N <- nrow(data)\n\n  #=== bootstrapped data ===#\n  boot_data <- data[sample(1:N, N, replace = TRUE), ]\n\n  #=== train a regression tree ===#\n  rpart <-\n    rpart(\n      lsalary ~ hruns + years + rbisyr + allstar + runsyr + hits + bavg, \n      data = boot_data\n    )\n\n  #=== predict ===#\n  return_data <-\n    copy(data) %>% \n    .[, y_hat := predict(rpart, newdata = data)] %>% \n    .[, .(id, y_hat)] %>% \n    .[, tree := i] \n\n  return(return_data)\n}\n\nWe now repeat train_a_tree() 10 times.\n\n#=== create observation id for later group-by averaging ===#\nmlb1_dt[, id := 1:.N]\n\n(\ny_estimates <-\n  lapply(\n    1:10,\n    function(x) train_a_tree(x, mlb1_dt) \n  ) %>% \n  rbindlist() %>% \n  .[order(id),]\n)\n\n       id    y_hat tree\n   1:   1 15.15792    1\n   2:   1 15.04446    2\n   3:   1 15.21238    3\n   4:   1 15.07311    4\n   5:   1 14.92840    5\n  ---                  \n3296: 330 11.98177    6\n3297: 330 12.09743    7\n3298: 330 12.02678    8\n3299: 330 12.02287    9\n3300: 330 12.02302   10\n\n\nBy averaging \\(y\\) estimates by id, we can get bagging estimates.\n\ny_estimates[, mean(y_hat), by = id]\n\n      id       V1\n  1:   1 15.09137\n  2:   2 14.66143\n  3:   3 14.61425\n  4:   4 14.31035\n  5:   5 13.74852\n ---             \n326: 326 13.04455\n327: 327 12.79717\n328: 328 12.79717\n329: 329 13.67179\n330: 330 12.02051\n\n\nThis is bagging of many regression trees.\n\n\n6.2.2 Random Forest (RF)\nNow, let’s take a look at the individual estimates of \\(y\\) for the first observation from the bagging process we just implemented.\n\ny_estimates[id == 1, ]\n\n    id    y_hat tree\n 1:  1 15.15792    1\n 2:  1 15.04446    2\n 3:  1 15.21238    3\n 4:  1 15.07311    4\n 5:  1 14.92840    5\n 6:  1 14.98571    6\n 7:  1 15.12060    7\n 8:  1 15.14064    8\n 9:  1 15.05676    9\n10:  1 15.19375   10\n\n\nHmm, the estimates look very similar. Actually, that is not just of the observations with id == 1. This is because the trained trees are very similar for many reasons, and the trees are highly “positively” correlated with each other. From our very simple experiment above, we know that the power of bagging is not very high when that is the case.\nRF introduces additional uncertainty to the process to make trees less correlated with each other (decorrelate trees). Specifically, for any leave of any tree, they consider only a randomly select subset of the explanatory variables when deciding how to split a leave. A typical choice of the number of variables considered at each split is \\(\\sqrt{K}\\), where \\(K\\) is the number of the included explanatory variables. In the naive example above, all \\(K\\) variables are considered for all the split decisions of all the trees. Some variables are more influential than others and they get to be picked as the splitting variable at similar places, which can result in highly correlated trees. Instead, RF gives other variables a chance, which helps decorrelate the trees. This means that the tree we build in RF is not deterministic. Depending on which variables are selected for consideration in splitting, the tree will be different even if you use the same bootstrapped dataset.\nLet’s code the process of building a tree for RF. We first bootstrap a dataset.\n\nn_obs <- nrow(mlb1_dt)\n\nboot_data <- mlb1_dt[sample(1:n_obs, n_obs, replace = TRUE), ]\n\nLet’s now build a tree using boot_data. We first split the entire dataset into two.\nWe can use get_rss_by_var() we wrote earlier, which gets us RSS-minimizing threshold and the minimized RSS value for a single variable. Earlier when we build a regression tree, we looped over all the explanatory variables, which are hruns, years, rbisyr, allstar, runsyr, hits, and bavg. But, when building a tree in RF, you can choose to select just a subset of the variables. Here, let’s randomly select \\(\\sqrt{K}\\) variables. So, rounding \\(\\sqrt{7}\\), we have three.\n\nvar_list <- c(\"hruns\", \"years\", \"rbisyr\", \"allstar\", \"runsyr\", \"hits\", \"bavg\")\nK <- length(var_list)\nK_for_split <- sqrt(K) %>% round()\n\nWe randomly select three variables among the list of variables (var_list).\n\n(\nvars_for_split <- sample(var_list, K_for_split, replace = FALSE)\n)\n\n[1] \"years\"   \"bavg\"    \"allstar\"\n\n\nYou only consider these 3 variables in this splitting process.\n\n(\nmin_rss_by_var <-\n  lapply(\n    vars_for_split,\n    function(x) get_rss_by_var(x, mlb1_dt)\n  ) %>% \n  rbindlist()\n)\n\n        rss var_name  var_value\n1: 249.9090    years   3.500000\n2: 375.0281     bavg 252.499992\n3: 277.3388  allstar   7.417582\n\n\nSo, our choice of split criteria is\n\nmin_rss_by_var[which.min(rss), ]\n\n       rss var_name var_value\n1: 249.909    years       3.5\n\n\nYou will repeat this process for any splits you consider until you met the stopping criteria.\nBy the way, if we were to use all the variables instead of just these three, then we would have use the following criteria.\n\n(\n  lapply(\n    var_list,\n    function(x) get_rss_by_var(x, mlb1_dt)\n  ) %>% \n  rbindlist() %>% \n  .[which.min(rss), ]\n)\n\n        rss var_name var_value\n1: 205.1488     hits     356.5"
  },
  {
    "objectID": "P01-random-forest.html#implementation",
    "href": "P01-random-forest.html#implementation",
    "title": "6  Random Forest",
    "section": "6.3 Implementation",
    "text": "6.3 Implementation\nWe can use ranger() from the ranger package to train an RF model. Another commonly used R package for RF is the randomForest package. However, ranger() is much faster.\nThe ranger() function has many options you can specify that determine how trees are built. Here are some of the important ones (see here for the complete description of the hyper-parameters.):\n\nmtry: the number of variables considered in each split (default is the square root of the total numbers of explanatory variables rounded down.)\nnum.trees: the number of tree to be built (default is 500)\nmin.node.size: minimum number of observations in each node (default varies based on the the type of analysis)\nreplace: where sample with or without replacement when bootstrapping samples (default is TRUE)\nsample.fraction: the fraction of the entire observations that are used in each tree (default is 1 if sampling with replacement, 0.632 if sampling without replacement)\n\n\n\nBy setting replace = FALSE, you will only use a fraction (randomly selected) of the entire sample in building each tree. This is an alternative to the algorithm discussed earlier, which uses bootstrapped (sample with replacement) dataset to build each tree.\nLet’s try fitting an RF with ranger() with the default parameters.\n\n#=== load the package ===#\nlibrary(ranger)\n\n#=== fit and RF ===#\n(\nrf_fit <- \n  ranger(\n    lsalary ~ hruns + years + rbisyr + allstar + runsyr + hits + bavg, \n    data = mlb1_dt\n  )\n)\n\nRanger result\n\nCall:\n ranger(lsalary ~ hruns + years + rbisyr + allstar + runsyr +      hits + bavg, data = mlb1_dt) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      330 \nNumber of independent variables:  7 \nMtry:                             2 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       0.3640505 \nR squared (OOB):                  0.7308852 \n\n\n\n\nSince we have many trees, it is no longer possible to have a nice graphical representation of the trained RF model like we did with a regression tree.\nIn the output, you can see OOB prediction error (MSE). OOB stands for  out-of-bag. When bootstrapping, some of the train data will not be used to build a tree.\n\n#=== bootstrapped data ===#\nboot_data <- mlb1_dt[sample(1:n_obs, n_obs, replace = TRUE), ]\n\n#=== which rows (observations) from the original datasets are missing? ===#\nmlb1_dt[, id %in% unique(boot_data$id)] %>% mean()\n\n[1] 0.630303\n\n\nSo, only \\(65\\%\\) of the rows from the original data (mlb1_dt) in this bootstrapped sample (many duplicates of the original observations). The observations that are NOT included in the bootstrapped sample is called out-of-bag observations. This provides a great opportunity to estimate test MSE while training an RF model! For a given regression tree, you can apply it to the out-of-bag samples to calculate MSE. You can repeat this for all the trees and average the MSEs, effectively conducting cross-validation. When the number of trees is large enough, OOB MSE is almost equivalent to MSE from LOOCV (James et al., n.d.). This means that we can tune hyper-parameters by comparing OOB MSEs under different sets of hyper-parameter values.\nYou can use a simple grid-search to find the best hyper-parameter values. Grid-search is simply a brute-force optimization methods that goes through a set of combinations of hyper-parameters and see which combination comes at the top. The computational intensity of grid-search depends on how many hyper-parameters you want to vary and how many values you would like to look at for each of the hyper-parameters. Here, let’s tune mtry, min.node.size, and sample.fraction.\n\n#=== define set of values you want to look at ===#\nmtry_seq <- c(2, 4, 7)\nmin_node_size_seq <- c(2, 5, 10)\nsample_fraction_seq <- c(0.5, 0.75, 1)\n\n#=== create a complete combinations of the three parameters ===#\n(\nparameters <-\n  data.table::CJ(\n    mtry = mtry_seq,\n    min_node_size = min_node_size_seq,\n    sample_fraction = sample_fraction_seq\n  )\n)\n\n    mtry min_node_size sample_fraction\n 1:    2             2            0.50\n 2:    2             2            0.75\n 3:    2             2            1.00\n 4:    2             5            0.50\n 5:    2             5            0.75\n 6:    2             5            1.00\n 7:    2            10            0.50\n 8:    2            10            0.75\n 9:    2            10            1.00\n10:    4             2            0.50\n11:    4             2            0.75\n12:    4             2            1.00\n13:    4             5            0.50\n14:    4             5            0.75\n15:    4             5            1.00\n16:    4            10            0.50\n17:    4            10            0.75\n18:    4            10            1.00\n19:    7             2            0.50\n20:    7             2            0.75\n21:    7             2            1.00\n22:    7             5            0.50\n23:    7             5            0.75\n24:    7             5            1.00\n25:    7            10            0.50\n26:    7            10            0.75\n27:    7            10            1.00\n    mtry min_node_size sample_fraction\n\n\nIn total, we have 27 (\\(3 \\times 3 \\times 3\\)) cases. You can see how quickly the number of cases increases as you increase the number of parameters to tune and the values of each parameter. We can now loop over the rows of this parameter data (parameters) and get OOB MSE for each of them.\n\noob_mse_all <-\n  lapply(\n    seq_len(nrow(parameters)),\n    function(x) {\n\n      #=== Fit the mode ===#\n      rf_fit <- \n        ranger(\n          lsalary ~ hruns + years + rbisyr + allstar + runsyr + hits + bavg, \n          data = mlb1_dt,\n          num.trees = 1000,\n          mtry = parameters[x, mtry],\n          min.node.size = parameters[x, min_node_size],\n          sample.fraction = parameters[x, sample_fraction]\n        )\n\n      #=== return OOB SME ===#\n      return(rf_fit$prediction.error)\n      \n    }\n  ) %>% \n  unlist()\n\n#=== assign OOB MSE to the parameters data ===#\nparameters[, oob_mse := oob_mse_all]\n\n#=== take a look ===#\nparameters\n\n    mtry min_node_size sample_fraction   oob_mse\n 1:    2             2            0.50 0.3611884\n 2:    2             2            0.75 0.3622574\n 3:    2             2            1.00 0.3701546\n 4:    2             5            0.50 0.3601237\n 5:    2             5            0.75 0.3634168\n 6:    2             5            1.00 0.3665383\n 7:    2            10            0.50 0.3618498\n 8:    2            10            0.75 0.3621112\n 9:    2            10            1.00 0.3662482\n10:    4             2            0.50 0.3626452\n11:    4             2            0.75 0.3668244\n12:    4             2            1.00 0.3735296\n13:    4             5            0.50 0.3630609\n14:    4             5            0.75 0.3705019\n15:    4             5            1.00 0.3742656\n16:    4            10            0.50 0.3610209\n17:    4            10            0.75 0.3623941\n18:    4            10            1.00 0.3661138\n19:    7             2            0.50 0.3737602\n20:    7             2            0.75 0.3766916\n21:    7             2            1.00 0.3827657\n22:    7             5            0.50 0.3697100\n23:    7             5            0.75 0.3725411\n24:    7             5            1.00 0.3822502\n25:    7            10            0.50 0.3695385\n26:    7            10            0.75 0.3699830\n27:    7            10            1.00 0.3800858\n    mtry min_node_size sample_fraction   oob_mse\n\n\nSo, the best choice among the ones tried is:\n\nparameters[which.min(oob_mse), ]\n\n   mtry min_node_size sample_fraction   oob_mse\n1:    2             5             0.5 0.3601237\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe same cross-validation procedure can be done with less coding effort using the mlr3 framework (see Chapter 18)."
  },
  {
    "objectID": "P01-random-forest.html#references",
    "href": "P01-random-forest.html#references",
    "title": "6  Random Forest",
    "section": "References",
    "text": "References\n\n\nAthey, Susan, Julie Tibshirani, and Stefan Wager. 2019. “Generalized Random Forests.” The Annals of Statistics 47 (2): 1148–78.\n\n\nBreiman, Leo. 2001. “Random Forests.” Machine Learning 45 (1): 5–32.\n\n\nChen, Tianqi, and Carlos Guestrin. 2016. “XGBoost.” In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM. https://doi.org/10.1145/2939672.2939785.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. n.d. An Introduction to Statistical Learning. Vol. 112. Springer."
  },
  {
    "objectID": "P02-boosted-regression-forest.html",
    "href": "P02-boosted-regression-forest.html",
    "title": "7  Boosted Regression Forest",
    "section": "",
    "text": "Packages to load for replication\n\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(rpart)\nlibrary(rattle)\nlibrary(wooldridge)\n\nDataset for replication\n\n#=== get data ===#\ndata(mlb1)\n\nmlb1_dt <- \n  mlb1 %>% \n  data.table() %>% # turn into data.table \n  .[, salary := NULL] %>% # remove salary (use lsalary instead)\n  na.omit() # remove observations with NA in any of the variables\n\nIn training RF that uses the idea of bagging, the original data is used to generate many bootstrapped datasets, a regression tree is trained on each of them  independently , and then they are averaged when prediction. Boosting is similar to bagging (bootstrap aggregation) in that it trains many statistical models and then combine them. However, instead of training models independently, it trains models  sequentially  in a manner that improves prediction step by step.\nWhile there are many variants of boosting methods (see Chapter 10 of Hastie et al. (2009)), we will look at gradient boosting using trees for regression in particular (Algorithm 10.3 in Hastie et al. (2009) presents the generic gradient tree boosting algorithm), where squared error is used as the loss function.\n\n\n\n\n\n\nNote\n\n\n\n Algorithm: Gradient Boosting Regression Forest \n\nSet \\(f_0(X_i) = \\frac{\\sum_{i=1}^N y_i}{N}\\) for all \\(i = 1, \\dots, N\\)\nFor b = 1 to B,\n\n\nFor \\(i = 1, \\dots, N\\), calculate \\[\n    r_{i,b} =  (y_i - f_{b-1}(X_i))\n    \\]\nFit a regression tree to \\(r_{i, b}\\), which generates terminal regions \\(R_{j,b}\\), \\(j = 1, \\dots, J\\), and denote the predicted value of region \\(R_{j,b}\\) as \\(\\gamma_{j,b}\\).\nSet \\(f_b(X_i) = f_{b-1}(X_i) + \\lambda \\cdot \\sum_{j=1}^J\\gamma_{j, b}\\cdot I(X_i \\in R_{j,b})\\)\n\n\nFinally, \\(\\hat{f}(X_i) = f_B(X_i)\\)\n\n\n\nLet’s try to go through this algorithm a bit to have it sink in for you.\n\n Step 1 \nStep 1 finds the mean of the dependent variable. This quantity is used as the starting estimate for the dependent variable.\n\n(\nf_0 <- mean(mlb1_dt$lsalary)\n)\n\n[1] 13.51172\n\n\n\n Step 2\n\\(b = 1\\)\nNow, we get residuals:\n\nmlb1_dt[, resid_1 := lsalary - f_0]\n\nThe residuals contain information in lsalary that was left unexplained by simply using the mean of lsalary. By training a regression tree using the residuals as the dependent variable, we are finding a tree that can explain the unexplained parts of lsalary using the explanatory variables.\n\ntree_fit_b1 <- \n  rpart(\n    resid_1 ~ ., # . means all variables\n    data = mlb1_dt \n  )\n\nHere is the fitted value of the residuals (\\(\\sum_{j=1}^J\\gamma_{j, b}\\cdot I(X_i \\in R_{j,b})\\))\n\nresid_1_hat <- predict(tree_fit_b1, newdata = mlb1_dt)\nhead(resid_1_hat)\n\n         1          2          3          4          5          6 \n 1.7134881  1.7134881  1.2414996  1.2414996  0.5054178 -0.1851016 \n\n\nNow, we update our prediction according to \\(f_b(X_i) = f_{b-1}(X_i) + \\lambda \\cdot \\sum_{j=1}^J\\gamma_{j, b}\\cdot I(X_i \\in R_{j,b})\\). We set \\(\\lambda\\) to be \\(0.2\\) in this illustration.\n\nlambda <- 0.2\nf_1 <- f_0 + lambda * resid_1_hat\nhead(f_1)\n\n       1        2        3        4        5        6 \n13.85441 13.85441 13.76002 13.76002 13.61280 13.47470 \n\n\nDid we actually improve prediction accuracy? Let’s compare f_0 and f_1.\n\nsum((mlb1_dt$lsalary - f_0)^2)\n\n[1] 445.0615\n\nsum((mlb1_dt$lsalary - f_1)^2)\n\n[1] 288.3205\n\n\nGreat. Let’s move on to \\(b = 2\\).\n\n#=== get negative of the residuals ===#\nmlb1_dt[, resid_2 := lsalary - f_1]\n\n#=== fit a regression tree ===#\ntree_fit_b2 <- \n  rpart(\n    resid_2 ~ ., # . means all variables\n    data = mlb1_dt \n  )\n\n#=== get predicted values ===#\nresid_2_hat <- predict(tree_fit_b2, newdata = mlb1_dt)\n\n#=== update ===#\nf_2 <- f_1 + lambda * resid_2_hat\n\n\nsum((mlb1_dt$lsalary - f_1)^2)\n\n[1] 288.3205\n\nsum((mlb1_dt$lsalary - f_2)^2)\n\n[1] 186.9229\n\n\nWe further improved our predictions. We repeat this process until certain user-specified stopping criteria is met.\nAs you probably have noticed, there are several key parameters in the process above that controls the performance of gradient boosting forest. \\(\\lambda\\) controls the speed of learning. The lower \\(\\lambda\\) is, slower the learning speed is. \\(B\\) (the number of trees) determines how many times we want to make small improvements to the original prediction. When you increase the value of \\(\\lambda\\), you should decrease the value of \\(B\\). Too high values of \\(\\lambda\\) and \\(B\\) can lead to over-fitting.\nYou may have been wondering why this algorithm is called Gradient boosting. Gradient boosting is much more general than the one described here particularly for gradient tree boosting for regression. It can be applied to both regression and classification1. In general, Step 2.a can be written as follows:\n\\[\nr_{i,b} = - \\huge[\\normalsize\\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)}\\huge]\\normalsize_{f = f_{b-1}}\n\\]\nwhere \\(L(y_i, f(x_i))\\) is the loss function. For regression, the loss function is almost always squared error: \\((y_i - f(x_i))^2\\). For, \\(L(y_i, f(x_i)) = (y_i - f(x_i))^2\\), the negative of the derivative of the loss function with respect to \\(f(x_i)\\) is\n\\[\n- \\huge[\\normalsize\\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)}\\huge]\\normalsize_{f = f_{b-1}} = - (- 2 (y_i - f(x_i))) = 2 (y_i - f(x_i))\n\\]\nThis is why we have \\(r_{i,b} = (y_i - f_{b-1}(X_i))\\) at Step 2.a. And, as you just saw, we are using the gradient of the loss function for model updating, which is why it is called  gradient  boosting. Note that it does not really matter whether you have \\(2\\) in front of the residuals or not the fitted residuals is multiplied (scaled) by \\(\\lambda\\) to when updating the model. You can always find the same \\(\\lambda\\) that would result in the same results as when just non-scaled residuals are used.\nMost R and python packages allow you to use a fraction of the train sample that are randomly selected and/or to use a subset of the included variables in building a tree within Step 2. This generate randomness in the algorithm and they are referred to as  stochastic gradient boosting."
  },
  {
    "objectID": "P02-boosted-regression-forest.html#implementation",
    "href": "P02-boosted-regression-forest.html#implementation",
    "title": "7  Boosted Regression Forest",
    "section": "7.2 Implementation",
    "text": "7.2 Implementation\nWe can use the gbm package to train a gradient boosting regression. Just like ranger(), gbm takes formula and data like below.\n\nlibrary(gbm)\n\n#=== fit a gbm model ===#\ngbm_fit <- \n  gbm(\n    lsalary ~ hruns + years + rbisyr + allstar + runsyr + hits + bavg, \n    data = mlb1_dt \n  )\n\nDistribution not specified, assuming gaussian ...\n\n\nHere is the list of some parameters to be aware of:\n\nn.trees: Number of trees (\\(B\\)). Default is \\(100\\).\ninteraction.depth: 1 implies an additive model without interactions between included variables2, 2 implies a model with 2-way interactions. Default is 1.\nn.minobsinnode: Minimum number of observations in a terminal node (leaf).\nshrinkage: Learning rate (\\(\\lambda\\)). Default is 0.1.\nbag.fraction: The fraction of the train data observations that are select randomly in building a tree. Default is 0.5.\ncv.folds: The number of folds in conducting KCV\n\nBy specifying cv.folds, gbm() automatically conducts cross-validation for you.\n\n#=== gbm fit with CV ===#\ngbm_fit <- \n  gbm(\n    lsalary ~ hruns + years + rbisyr + allstar + runsyr + hits + bavg, # . means all variables\n    data = mlb1_dt,\n    cv.folds = 5,\n  )\n\nDistribution not specified, assuming gaussian ...\n\n#=== see the MSE history ===#  \ngbm_fit$cv.error\n\n  [1] 1.2265130 1.1183285 1.0297003 0.9449797 0.8686668 0.8118960 0.7642695\n  [8] 0.7119927 0.6737441 0.6361349 0.6074548 0.5796558 0.5553846 0.5374066\n [15] 0.5175769 0.4993261 0.4879486 0.4745424 0.4637919 0.4547543 0.4447901\n [22] 0.4363922 0.4299422 0.4253180 0.4231648 0.4166543 0.4126393 0.4079809\n [29] 0.4043625 0.4016804 0.4003579 0.3985493 0.3972166 0.3949607 0.3922563\n [36] 0.3918055 0.3907375 0.3888032 0.3879004 0.3872201 0.3863084 0.3837488\n [43] 0.3823724 0.3811102 0.3796948 0.3790407 0.3765814 0.3748108 0.3744447\n [50] 0.3737110 0.3721948 0.3722761 0.3708322 0.3703440 0.3701004 0.3695578\n [57] 0.3687270 0.3681950 0.3673914 0.3669241 0.3665696 0.3646738 0.3651054\n [64] 0.3655068 0.3649733 0.3646982 0.3646698 0.3643215 0.3649467 0.3643970\n [71] 0.3641570 0.3634895 0.3640304 0.3637571 0.3633007 0.3639285 0.3641807\n [78] 0.3643721 0.3669819 0.3674338 0.3674411 0.3671956 0.3670639 0.3669578\n [85] 0.3663330 0.3673892 0.3660270 0.3657856 0.3658539 0.3669376 0.3663275\n [92] 0.3665520 0.3676419 0.3674104 0.3672738 0.3685010 0.3687835 0.3695083\n [99] 0.3692864 0.3679122\n\n\nYou can visualize the CV results using gbm.perf().\n\ngbm.perf(gbm_fit)\n\n\n\n\n[1] 75\n\n\nNote that it will tell you what the optimal number of trees is  given  the values of the other hyper-parameters (here default values). If you want to tune other parameters as well, you need to program it yourself."
  },
  {
    "objectID": "P02-boosted-regression-forest.html#resources",
    "href": "P02-boosted-regression-forest.html#resources",
    "title": "7  Boosted Regression Forest",
    "section": "7.3 Resources",
    "text": "7.3 Resources\n\nGradient Boosting with Scikit-Learn, XGBoost, LightGBM, and CatBoost by Jason Brownlee\n\n\n\n\n\n\n\nHastie, Trevor, Robert Tibshirani, Jerome H Friedman, and Jerome H Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Vol. 2. Springer."
  },
  {
    "objectID": "P03-xgb.html",
    "href": "P03-xgb.html",
    "title": "8  Extreme Gradient Boosting",
    "section": "",
    "text": "Extreme gradient boosting (XGB) has been extremely popular due to its superb performance (Chen and Guestrin 2016). It is a variant of gradient boosting and follows the same steps as gradient boosting covered in Chapter 7. However, it has it has its own way of building a tree, which is more mindful of avoiding over-fitting trees.\nXGB can be implemented by the XGBoost package (for both R and Python). They have several different ways of build trees (Step 2.ii). While we discuss only a variant of the available algorithms in this chapter, understanding of the general idea of XGB can still be gained."
  },
  {
    "objectID": "P03-xgb.html#tree-updating-in-xgb-general-case",
    "href": "P03-xgb.html#tree-updating-in-xgb-general-case",
    "title": "8  Extreme Gradient Boosting",
    "section": "8.1 Tree updating in XGB (general case)",
    "text": "8.1 Tree updating in XGB (general case)\nLet \\(f_{i,b}(x_i)\\) be the prediction for the \\(i\\)th observation at the \\(b\\)-th iteration. Further, let \\(I_j\\) denote a set of observations that belong to leaf \\(j\\) (\\(j = 1, \\dots, J\\)) of the tree that is built at Step 2.ii. Then, at Step 2.iii, \\(f_{i,b-1}(x_i)\\) is updated to \\(f_{i,b}(x_i)\\) according to the following:\n\\[\n\\begin{aligned}\nf_{i,b}(x_i) = f_{i,b-1}(x_i) + \\eta w_j(x_i)\n\\end{aligned}\n\\]\nIdeally, it would be great to find \\(w_j(x_i)\\) is such that it minimizes the following objective:\n\\[\n\\Psi = \\sum_{i=1}^N [L(y_i, f_{i,b}(x_i) + w_j(x_i))] + \\Omega(w_j)\n\\tag{8.1}\\]\nwhere \\(L()\\) is the user-specified loss-function that is differentiable and \\(\\Omega(w_j)\\) is the regularization term. However, solving this minimization problem exactly can be too complicated depending on the loss function specification. Instead of Equation 8.1, XGB uses the second order Taylor expansion of \\(L()\\) about \\(w\\)1.\n\\[\n\\tilde{\\Psi} = \\sum_{i=1}^N [L(y_i, f_{i,b}(x_i)) + g_i w_j(x_i) + \\frac{1}{2}h_i w_j(x_i)^2] + \\Omega(w_j)\n\\tag{8.2}\\]\nwhere \\(g_i = \\frac{\\partial L(y_i, p_i)}{\\partial p_i}\\) (first-order derivative) and \\(h_i = \\frac{\\partial^2 L(y_i, p_i)}{\\partial p_i^2}\\) (second-order derivative). Since \\(L(y_i, f_{i,b}(x_i))\\) is just a constant, we can safely remove it from the objective function, which leads to\n\\[\n\\tilde{\\Psi}_t = \\sum_{i=1}^N [g_i w_j(x_i) + \\frac{1}{2}h_i w_j(x_i)^2] + \\Omega(w_j)\n\\tag{8.3}\\]\nSuppose the L2 norm is used and \\(\\Omega(w_j) = \\frac{1}{2}\\lambda\\sum_{j=1}^J w_j^2\\).\nLet \\(I_j\\) denote a set of observations that belong to leaf \\(j\\) (\\(j = 1, \\dots, J\\)). Then, Equation 8.3 is written as follows:\n\\[\n\\tilde{\\Psi}_t = \\sum_{j=1}^J\\huge[\\normalsize (\\sum_{i\\in I_j}g_i)w_j + \\frac{1}{2}(\\sum_{i\\in I_j}h_i + \\lambda)w_j^2 \\huge]\\normalsize + \\gamma J\n\\tag{8.4}\\]\n\n\nRemember that all the observations in the same leaf shares the same prediction. So, for all \\(i\\)s that belong to leaf \\(j\\), the prediction is denoted as \\(w_j\\) in Equation 8.4. That is, \\(w_t(x_i)\\) that belongs to leaf \\(j\\) is \\(w_j\\).\nFor a given tree structure (denoted as \\(q(x)\\)), the leaves can be treated independently in minimizing this objective.\nTaking the derivative of \\(\\tilde{\\Psi}_t\\) w.r.t \\(w_j\\),\n\\[\n\\begin{aligned}\n(\\sum_{i\\in I_j}g_i) + (\\sum_{i\\in I_j}h_i + \\lambda)w_j = 0 \\\\\n\\Rightarrow w_j^* = \\frac{-\\sum_{i\\in I_j}g_i}{\\sum_{i\\in I_j}h_i + \\lambda}\n\\end{aligned}\n\\tag{8.5}\\]\nThe minimized value of \\(\\tilde{\\Psi}_t\\) is then (obtained by plugging \\(w_j^*\\) into Equation 8.4),\n\\[\n\\begin{aligned}\n\\tilde{\\Psi}_t(q)^* & = \\sum_{j=1}^J\\huge[\\normalsize (\\sum_{i\\in I_j}g_i)\\frac{-\\sum_{i\\in I_j}g_i}{\\sum_{i\\in I_j}h_i + \\lambda} + \\frac{1}{2}(\\sum_{i\\in I_j}h_i + \\lambda)(\\frac{-\\sum_{i\\in I_j}g_i}{\\sum_{i\\in I_j}h_i + \\lambda})^2 \\huge]\\normalsize + \\gamma J \\\\\n& = \\sum_{j=1}^J\\huge[\\normalsize \\frac{-(\\sum_{i\\in I_j}g_i)^2}{\\sum_{i\\in I_j}h_i + \\lambda} + \\frac{1}{2}\\frac{(\\sum_{i\\in I_j}g_i)^2}{\\sum_{i\\in I_j}h_i + \\lambda} \\huge]\\normalsize + \\gamma J \\\\\n& = -\\frac{1}{2} \\sum_{j=1}^J \\huge[\\normalsize\\frac{(\\sum_{i\\in I_j}g_i)^2}{\\sum_{i\\in I_j}h_i + \\lambda}\\huge]\\normalsize + \\gamma J\n\\end{aligned}\n\\tag{8.6}\\]\nFor rotational convenience, we call \\(\\frac{(\\sum_{i\\in I_j}g_i)^2}{\\sum_{i\\in I_j}h_i + \\lambda}\\) quality score and denote it by \\(Q_j\\) ( Quality score for leaf \\(j\\)).\nWe could find the best tree structure by finding \\(w_j^*(q)\\) according to Equation 8.4 and calculate \\(\\tilde{\\Psi}_t(q)^*\\) according to Equation 8.6 for each of all the possible tree structures, and then pick the tree structure q(x) that has the lowest \\(\\tilde{\\Psi}_t(q)^*\\).\nHowever, it is impossible to consider all possible tree structures practically. So, a greedy (myopic) approach that starts from a single leaf and iteratively splits leaves is used instead.\nConsider splitting an existing leaf \\(p\\) (where in the tree it may be located) into two leaves \\(L\\) and \\(R\\) according to splitting rule \\(s\\) when there are \\(J\\) existing leaves. Then, the resulting minimized objective is\n\\[\n-\\frac{1}{2} \\huge[\\normalsize Q_L(s) + Q_R(s) + \\Gamma \\huge]\\normalsize + \\gamma(J+1)\n\\]\nwhere \\(\\Gamma\\) is the sum of quality scores for all the leaves except \\(L\\) and \\(R\\).\n\n\n\\[\n\\Gamma = \\sum_{j\\ne \\{L, R\\}}^J Q_j\n\\]\nThe minimized objective before splitting is\n\\[\n-\\frac{1}{2} \\huge[\\normalsize Q_p + \\Gamma \\huge]\\normalsize + \\gamma J\n\\]\nSo, the reduction  in loss after the split is\n\\[\nG(s,p) = \\frac{1}{2} \\huge[\\normalsize Q_L(s) + Q_R(s) - Q_p \\huge]\\normalsize - \\gamma\n\\]\nLet’s call \\(G(s, p)\\) simply a gain of split parent node \\(p\\) using splitting rule \\(s\\).\n\n\nA more positive value of gain (\\(G(s, L, R)\\)) means a more successful split.\nWe calculate the gain for all the possible splits of \\(p\\) and pick the split that has the highest gain.\n\n\nDifferent patterns of \\(I_L\\) and \\(I_R\\) arise from different variable-cutpoint combinations\nIf the highest gain is negative, then leaf \\(p\\) is not split.\nOnce the best tree is built, then we update our prediction based on \\(w^*\\) of the terminal nodes of the tree. For observation \\(i\\) that belongs to leaf \\(j\\) of the tree,\n\\[\n\\begin{aligned}\nf_{i,b} = f_{i,b-1} + \\eta \\cdot w_j^*\n\\end{aligned}\n\\tag{8.7}\\]\nwhere \\(\\eta\\) is the learning rate."
  },
  {
    "objectID": "P03-xgb.html#tree-updating-in-xgb-regression",
    "href": "P03-xgb.html#tree-updating-in-xgb-regression",
    "title": "8  Extreme Gradient Boosting",
    "section": "8.2 Tree updating in XGB (regression)",
    "text": "8.2 Tree updating in XGB (regression)\nWe now make the general tree updating algorithm specific to regression problems, where the loss function is squared error: \\(L(y_i, p_i) = \\frac{1}{2}(y_i - p_i)^2\\), where \\(p_i\\) is the predicted value for \\(i\\).\n\nFirst, let’s find \\(g_i\\) and \\(h_i\\) for \\(L(y_i, p_i) = \\frac{1}{2}(y_i - p_i)^2\\).\n\\[\n\\begin{aligned}\ng_i = \\frac{\\partial L(y_i, p_i)}{\\partial p_i}  = -(y_i - p_i)\\\\\n\\end{aligned}\n\\tag{8.8}\\]\n\\[\n\\begin{aligned}\nh_i = \\frac{\\partial^2 L(y_i, p_i)}{\\partial p_i^2} = 1 \\\\\n\\end{aligned}\n\\tag{8.9}\\]\nSo, \\(g_i\\) is simply the negative of the residual for \\(i\\).\nNow, suppose you are at iteration \\(b\\) and the predicted value for \\(i\\) is denoted as \\(f_{i,b}(x_i)\\). Further, let \\(r_{i,b}\\) denote the residual (\\(y_i - f_{i,b}(x_i)\\)).\nGiven this, here is how a tree is built at iteration \\(b\\).\n\n\n\n\n\n\nNote\n\n\n\nSteps 2.ii and 2.iii of the XGB algorithm:\n\nFor a given splitting rule (\\(s\\)) among all the possible splits (denoted as leaves \\(L\\) and \\(R\\)) of parent node \\(p\\), calculate the quality score (\\(Q\\)) and then gain (\\(G\\)):\n\n\\[\nQ_j(s) = \\frac{(\\sum_{i\\in I_j(s)}r_{i,b})^2}{N_j + \\lambda}, \\;\\; j = \\{L, R\\}\n\\]\n\\[\n\\begin{aligned}\nG(s) = \\frac{1}{2}[Q_L(s) + Q_R(s) - Q_p] - \\gamma\n\\end{aligned}\n\\]\nwhere \\(Q_p\\) is the quality score of the parent node.\n\nFind the splitting rule that produces the highest gain. If the highest gain is positive, then split the parent node according to the associated splitting rule, otherwise do not split the parent node.\nRepeat 1 and 2 until no further splits are possible.\nFor each observation, calculate \\(w^*\\). For all the observations that belong to leaf \\(j\\), \\(w^*\\) can be obtained as follows (plugging Equation 8.9 and Equation 8.8 into Equation 8.5)\n\n\\[\n\\begin{aligned}\nw_j^* & = \\frac{\\sum_{i\\in I_j}r_{i,b}}{\\sum_{i\\in I_j}1 + \\lambda} \\\\\n      & = \\frac{\\sum_{i\\in I_j}r_{i,b}}{N_j + \\lambda}\n\\end{aligned}\n\\]\n\n\nUpdate the prediction for each observation according to\n\n\\[\n\\begin{aligned}\nf_{i,b} = f_{i,b-1} + \\eta \\cdot w_j^*\n\\end{aligned}\n\\]\n\n\n\nThat is, for a given leaf \\(j\\), the optimal predicted value (\\(w_j^*\\)) is the sum of the residuals of all the observations in leaf \\(j\\) divided by the number of observations in leaf \\(j\\) plus \\(\\lambda\\). When \\(\\lambda = 0\\), the optimal predicted value (\\(w_j^*\\)) is simply the mean of the residuals."
  },
  {
    "objectID": "P03-xgb.html#illustration-of-xgb-for-regression",
    "href": "P03-xgb.html#illustration-of-xgb-for-regression",
    "title": "8  Extreme Gradient Boosting",
    "section": "8.3 Illustration of XGB for regression",
    "text": "8.3 Illustration of XGB for regression\n\n\nPackages to load for replication\n\nlibrary(tidyverse)\nlibrary(data.table)\n\nIn order to further our understanding of the entire XGB algorithm, let’s take a look at a simple regression problem as an illustration. We consider a four-observation data as follows:\n\n(\ndata <-\n  data.table(\n    y = c(-3, 7, 8, 12),\n    x = c(1, 4, 6, 8)\n  )\n)\n\n    y x\n1: -3 1\n2:  7 4\n3:  8 6\n4: 12 8\n\n\n\n\nCode\n(\ng_0 <-\n  ggplot(data) +\n  geom_point(aes(y = y, x = x)) +\n  theme_bw()\n)\n\n\n\n\n\nFirst step (\\(b = 0\\)) is to make an initial prediction. This can be any number, but let’s use the mean of y and set it as the predicted value for all the observations.\n\n(\nf_0 <- mean(data$y) # f_0: the predicted value for all the observations\n)\n\n[1] 6\n\n\nLet’s set \\(\\gamma\\), \\(\\lambda\\), and \\(\\eta\\) to \\(10\\), \\(1\\), and \\(0.3\\), respectively.\n\ngamma <- 10\nlambda <- 1\neta <- 0.3\n\nWe have a single-leaf tree at the moment. And the quality score for this leaf is\n\n\nquality score for leaf \\(j\\) is \\(\\frac{(\\sum_{i\\in I_j}r_{i,b})^2}{N_j + \\lambda}\\)\n\n#=== get residuals ===#\ndata[, resid := y - f_0]\n\n#=== get quality score ===#\n(\nq_0 <- (sum(data$resid))^2/(nrow(data) + lambda)\n)\n\n[1] 0\n\n\nQuality score of the leaf is 0.\n\n\nSince we are using the mean of \\(y\\) as the prediction, of course, the sum of the residuals is zero, which then means that the quality score is zero.\nNow, we have three potential to split patterns: {x, 2}, {x, 5}, {x, 7}.\n\n\n{x, 2} means the leaf is split into two leaves: \\({x | x <2}\\) and \\({x | x >= 2}\\). Note that any number between \\(1\\) and \\(4\\) will result in the same split results.\nLet’s consider them one by one.\n\n8.3.0.1  Split: {x, 2} \nHere is the graphical representations of the split:\n\n\nCode\ng_0 +\n  geom_vline(xintercept = 2, color = \"red\") +\n  annotate(\"text\", x = 1.25, y = 6, label = \"leaf L\", color = \"red\") +\n  annotate(\"text\", x = 5, y = 6, label = \"leaf R\", color = \"red\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nCode\nDiagrammeR::grViz(\n\"\ndigraph {\n  graph [ranksep = 0.2]\n  node [shape = box]\n    T1R [label = 'L: -9']\n    T1L [label = 'R: 1 , 2 , 6']\n    T0 [label = '-9, 1 , 2 , 6']\n  edge [minlen = 2]\n    T0->T1L\n    T0->T1R\n  { rank = same; T1R; T1L}\n}\n\"\n)\n\n\n\n\n\n\nLet’s split the data.\n\n#=== leaf L ===#\n(\ndata_L_1 <- data[x < 2, ]\n)\n\n    y x resid\n1: -3 1    -9\n\n#=== leaf R ===#\n(\ndata_R_1 <- data[x >= 2, ]\n)\n\n    y x resid\n1:  7 4     1\n2:  8 6     2\n3: 12 8     6\n\n\nUsing ?eq-w-reg,\n\n\n\\(w_j^* = \\frac{\\sum_{i\\in I_j}r_{i,b}}{N_j + \\lambda}\\)\n\nw_L <- (sum(data_L_1$resid))/(nrow(data_L_1) + lambda)\nw_R <- (sum(data_R_1$resid))/(nrow(data_R_1) + lambda)\n\n\\[\n\\begin{aligned}\nw_L^* & = -9 / (1 + 1) = -4.5 \\\\\nw_R^* & = 1 + 2 + 6 / (3 + 1) = 2.25\n\\end{aligned}\n\\]\nUsing ?eq-q-reg, the quality scores for the leaves are\n\n\n\\(Q_j = \\frac{(\\sum_{i\\in I_j}r_{i,b})^2}{N_j + \\lambda}\\)\n\nq_L <- (sum(data_L_1$resid))^2/(nrow(data_L_1) + lambda)\nq_R <- (sum(data_R_1$resid))^2/(nrow(data_R_1) + lambda)\n\n\n\n\n\nCode\nDiagrammeR::grViz(\n  paste0(\n  \"\n  digraph {\n    graph [ranksep = 0.2]\n    node [shape = box]\n      T1R [label = 'L: -9 \\n Q score = \", round(q_L, digits = 2), \"']\n      T1L [label = 'R: 1 , 2 , 6 \\n Q score = \", round(q_R, digits = 2), \"']\n      T0 [label = '-9, 1 , 2 , 6']\n    edge [minlen = 2]\n      T0->T1L\n      T0->T1R\n    { rank = same; T1R; T1L}\n  }\n  \"\n  )\n\n)\n\n\n\n\n\n\n\\[\n\\begin{aligned}\nq_L^* & = (-9)^2 / (1 + 1) = 40.5 \\\\\nq_R^* & = (1 + 2 + 6)^2 / (3 + 1) = 20.25\n\\end{aligned}\n\\]\nNotice that residuals are first summed and then squared in the denominator of the quality score (the higher, the better). This means that if the prediction is off in the same direction (meaning they are similar) among the observations within the leaf, then the quality score is higher. On the other hand, if the prediction is off in both directions (meaning they are not similar), then the residuals cancel each other out, resulting in a lower quality score. Since we would like to create leaves consisting of similar observations, a more successful split has a higher quality score.\nFinally, the gain of this split is\n\n\n\\[\nG(s, L, R) = \\frac{1}{2} \\huge[\\normalsize Q_L + Q_R - Q_s \\huge]\\normalsize - \\gamma\n\\]\nwhere \\(s\\) is the leaf before split, \\(L\\) and \\(R\\) are leaves after the split of leaf \\(s\\).\n\ngain_1 <- (q_L + q_R - q_0)/2 - gamma\n\n\\[\nG_1 = \\frac{40.5 + 20.25 - 0}{2} - 10 = 20.375\n\\]\nNow that we have gone through the process of finding update value (\\(w\\)), quality score (\\(q\\)), and gain (\\(G\\)) for a given split structure, let’s write a function that returns the values of these measures by feeding the cutpoint before moving onto the next split candidate.\n\nget_info <- function(data, cutpoint, lambda, gamma)\n{\n  q_0 <- (sum(data$resid))^2/(nrow(data) + lambda)\n\n  data_L <- data[x < cutpoint, ]\n  data_R <- data[x >= cutpoint, ]\n\n  w_L <- (sum(data_L$resid))/(nrow(data_L) + lambda)\n  w_R <- (sum(data_R$resid))/(nrow(data_R) + lambda)\n\n  q_L <- (sum(data_L$resid))^2/(nrow(data_L) + lambda)\n  q_R <- (sum(data_R$resid))^2/(nrow(data_R) + lambda)\n\n  gain <- (q_L + q_R - q_0)/2 - gamma\n\n  return(list(\n    w_L = w_L, \n    w_R = w_R, \n    q_L = q_L, \n    q_R = q_R, \n    gain = gain \n  ))\n}\n\n\n\n8.3.0.2  Split: {x, 5} \n\nmeasures_2 <- get_info(data, 5, lambda, gamma)\n\n\n\nCode\ng_0 +\n  geom_vline(xintercept = 5, color = \"red\") +\n  annotate(\"text\", x = 3, y = 6, label = \"leaf L\", color = \"red\") +\n  annotate(\"text\", x = 7, y = 6, label = \"leaf R\", color = \"red\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nCode\nDiagrammeR::grViz(\n  paste0(\n    \"\n    digraph {\n      graph [ranksep = 0.2]\n      node [shape = box]\n        T1R [label = 'L: -9, 1 \\n Q score = \", round(measures_2$q_L, digits = 2), \"']\n        T1L [label = 'R: 2 , 6 \\n Q score = \", round(measures_2$q_R, digits = 2), \"']\n        T0 [label = '-9, 1 , 2 , 6']\n      edge [minlen = 2]\n        T0->T1L\n        T0->T1R\n      { rank = same; T1R; T1L}\n    }\n    \"\n  )\n)\n\n\n\n\n\n\n\\[\n\\begin{aligned}\nq_L^* & = (-9 + 1)^2 / (2 + 1) = 21.33 \\\\\nq_R^* & = (2 + 6)^2 / (2 + 1) = 21.33\n\\end{aligned}\n\\]\n\\[\nG_2 = \\frac{21.33 + 21.33 - 0}{2} - 10 = 11.3333333\n\\]\n\n\n8.3.0.3  Split: {x, 7} \n\nmeasures_3 <- get_info(data, 7, lambda, gamma)\n\n\n\nCode\ng_0 +\n  geom_vline(xintercept = 7, color = \"red\") +\n  annotate(\"text\", x = 4, y = 6, label = \"leaf L\", color = \"red\") +\n  annotate(\"text\", x = 8, y = 6, label = \"leaf R\", color = \"red\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nCode\nDiagrammeR::grViz(\n  paste0(\n    \"\n    digraph {\n      graph [ranksep = 0.2]\n      node [shape = box]\n        T1R [label = 'L: -9, 1, 2 \\n Q score = \", round(measures_3$q_L, digits = 2), \"']\n        T1L [label = 'R: 6 \\n Q score = \", round(measures_3$q_R, digits = 2), \"']\n        T0 [label = '-9, 1 , 2 , 6']\n      edge [minlen = 2]\n        T0->T1L\n        T0->T1R\n      { rank = same; T1R; T1L}\n    }\n    \"\n  )\n)\n\n\n\n\n\n\n\\[\n\\begin{aligned}\nq_L^* & = (-9+1+2)^2 / (3 + 1) = 9 \\\\\nq_R^* & = (6)^2 / (1 + 1) = 18\n\\end{aligned}\n\\]\n\\[\nG_3 = \\frac{9 + 18 - 0}{2} - 10 = 3.5\n\\]\nAmong all the splits we considered, the first case (Split: {x, 2}) has the highest score. This is easy to confirm visually and shows picking a split based on the gain measure indeed makes sense.\nNow we consider how to split leaf R (leaf L cannot be split further as it has only one observation). We have two split candidates: {x, 5} and {x, 7}. Let’s get the gain measures using get_info().\n\n#=== first split ===#\nget_info(data_R_1, 5, lambda, gamma)$gain \n\n[1] -9.208333\n\n#=== second split ===#\nget_info(data_R_1, 7, lambda, gamma)$gain\n\n[1] -9.625\n\n\nSo, neither of the splits has a positive gain value. Therefore, we do not adopt either of the splits. For this iteration (\\(b=1\\)), this is the end of tree building.\n\n\n\n\n\n\nNote\n\n\n\nIf the value of \\(\\gamma\\) is lower (say, 0), then we would have adopted the second split.\n\nget_info(data_R_1, 5, lambda, 0)$gain # first split\n\n[1] 0.7916667\n\nget_info(data_R_1, 7, lambda, 0)$gain # second split\n\n[1] 0.375\n\n\nAs you can see, a higher value of \\(\\gamma\\) leads to a more aggressive tree pruning.\n\n\nSo, the final tree for this iteration (\\(b = 1\\)) is\n\n\nCode\nDiagrammeR::grViz(\n  paste0(\n  \"\n  digraph {\n    graph [ranksep = 0.2]\n    node [shape = box, width = 0.3, height = 0.15, fontsize = 3, fixedsize = TRUE, penwidth = 0.2]\n      T1R [label = 'L: -9 \\n w* = \", round(w_L, digits = 2), \"']\n      T1L [label = 'R: 1 , 2 , 6 \\n w* = \", round(w_R, digits = 2), \"']\n      T0 [label = '-9, 1 , 2 , 6']\n    edge [penwidth = 0.2, arrowsize = 0.3, len = 0.3]\n      T0->T1L\n      T0->T1R\n    { rank = same; T1R; T1L}\n  }\n  \"\n  )\n\n)\n\n\n\n\n\n\nWe now use \\(w^*\\) from this tree to update our prediction according to Equation 8.7.\n\n\n\\(f_{i,b} = f_{i,b-1} + \\eta \\cdot w_j^*\\)\n\nmeasures_1 <- get_info(data, 2, lambda, gamma)\n\nSince the first observation is in \\(L\\),\n\\[\nf_{i = 1,b = 1} = 6 + 0.3 \\times -4.5 = 4.65\n\\]\nSince the second, third, and fourth observations are in \\(R\\),\n\\[\n\\begin{aligned}\nf_{i = 2,b = 1} = 6 + 0.3 \\times 2.25 = 6.68 \\\\\nf_{i = 3,b = 1} = 6 + 0.3 \\times 2.25  = 6.68\\\\\nf_{i = 4,b = 1} = 6 + 0.3 \\times 2.25 = 6.68\n\\end{aligned}\n\\]\n\ndata %>% \n  .[, f_0 := f_0] %>% \n  .[1, f_1 := f_0 + measures_1$w_L * eta] %>%\n  .[2:4, f_1 := f_0 + measures_1$w_R * eta]\n\nThe prediction updates can be seen below. Though small, we made small improvements in our prediction.\n\n\nCode\nggplot(data = data) +\n  geom_point(aes(y = y, x = x, color = \"observed\")) +\n  geom_point(aes(y = f_1, x = x, color = \"after (f1)\")) +\n  geom_point(aes(y = f_0, x = x, color = \"before (f0)\")) +\n  scale_color_manual(\n    values = \n      c(\n        \"before (f0)\" = \"blue\", \n        \"after (f1)\" = \"red\",\n        \"observed\" = \"black\"\n      ), \n    name = \"\"\n  ) +\n  geom_segment(\n    aes(y = f_0, x = x, yend = f_1, xend = x), \n    color = \"blue\",\n    arrow = arrow(length = unit(0.1, \"cm\"))\n  ) +\n  theme_bw()\n\n\n\n\n\nNow, we move on to \\(b=2\\). We first update residuals:\n\ndata[, resid := y - f_1]\n\ndata\n\n    y x  resid f_0   f_1\n1: -3 1 -7.650   6 4.650\n2:  7 4  0.325   6 6.675\n3:  8 6  1.325   6 6.675\n4: 12 8  5.325   6 6.675\n\n\nJust like at \\(b=1\\), all the possible splits are {x, 2}, {x, 5}, {x, 7}. Let’s find the gain for each split.\n\nlapply(\n  c(2, 5, 7),\n  function(x) get_info(data, x, lambda, gamma)$gain\n)\n\n[[1]]\n[1] 10.66639\n\n[[2]]\n[1] 6.267458\n\n[[3]]\n[1] 1.543344\n\n\nSo, the first split is again the best split. Should we split the right leaf, which has the observations except the first one?\n\nlapply(\n  c(5, 7),\n  function(x) get_info(data[2:3, ], x, lambda, gamma)$gain\n)\n\n[[1]]\n[1] -9.988437\n\n[[2]]\n[1] -10\n\n\nAll the splits have negative gains. So, we do not split this leaf just like at \\(b=1\\).\nSo, the final tree for this iteration (\\(b = 2\\)) is\n\n\nCode\nmeasures_b2 <- get_info(data, 2, lambda, gamma)\n\n\nDiagrammeR::grViz(\n  paste0(\n  \"\n  digraph {\n    graph [ranksep = 0.2]\n    node [shape = box, width = 0.4, height = 0.15, fontsize = 3, fixedsize = TRUE, penwidth = 0.2]\n      T1R [label = 'L: -8.18 \\n w* = \", round(measures_b2$w_L, digits = 2), \"']\n      T1L [label = 'R: 0.71 , 1.71 , 5.71 \\n w* = \", round(measures_b2$w_R, digits = 2), \"']\n      T0 [label = '-8.18, 0.71 , 1.71 , 5.71']\n    edge [penwidth = 0.2, arrowsize = 0.3, len = 0.3]\n      T0->T1L\n      T0->T1R\n    { rank = same; T1R; T1L}\n  }\n  \"\n  )\n\n)\n\n\n\n\n\n\nLet’s now update our predictions.\n\ndata %>% \n  .[1, f_2 := f_1 + measures_b2$w_L * eta] %>%  \n  .[2:4, f_2 := f_1 + measures_b2$w_R * eta] \n\n\n\nCode\nggplot(data = data) +\n  geom_point(aes(y = y, x = x, color = \"observed\"), shape = 0) +\n  geom_point(aes(y = f_2, x = x, color = \"f2\")) +\n  geom_point(aes(y = f_1, x = x, color = \"f1\")) +\n  geom_point(aes(y = f_0, x = x, color = \"f0\")) +\n  scale_color_manual(\n    values = \n      c(\n        \"f0\" = \"blue\", \n        \"f1\" = \"red\",\n        \"f2\" = \"red\",\n        \"observed\" = \"black\"\n      ), \n    name = \"\"\n  ) +\n  geom_segment(\n    aes(y = f_0, x = x, yend = f_1, xend = x), \n    color = \"blue\",\n    arrow = arrow(length = unit(0.2, \"cm\"))\n  ) +\n  geom_segment(\n    aes(y = f_1, x = x, yend = f_2, xend = x), \n    color = \"blue\",\n    arrow = arrow(length = unit(0.2, \"cm\"))\n  ) +\n  theme_bw()\n\n\n\n\n\nAgain, we made small improvements in our predictions. This process continues until user-specified stopping criteria is met.\n\n\n\n\n\n\nTip\n\n\n\n\n\\(\\lambda\\):\n\nA higher value of \\(\\lambda\\) leads to a lower value of prediction updates (\\(w^*\\)).\nA higher value of \\(\\lambda\\) leads to a lower value of quality score (\\(Q\\)), thus leading to a lower value of gain (\\(G\\)), which then leads to more aggressive pruning for a given value of \\(\\gamma\\).\n\n\\(\\gamma\\):\n\nA higher value of \\(\\gamma\\) leads to more aggressive pruning.\n\n\\(\\eta\\):\n\nA higher value of \\(\\eta\\) leads to faster learning."
  },
  {
    "objectID": "P03-xgb.html#implementation",
    "href": "P03-xgb.html#implementation",
    "title": "8  Extreme Gradient Boosting",
    "section": "8.4 Implementation",
    "text": "8.4 Implementation\n\nRPython\n\n\nYou can use the xgboost package to implement XGB modeling.\n\nlibrary(xgboost)\n\nThe first task is to create a class of matrix called xgb.DMatrix using the xgb.DMatrix() function. You provide the explanatory variable data matrix to the data option and the dependent variable matrix (vector) to the label option in xgb.DMatrix() like below.\nLet’s get the mlb1 data from the wooldridge package for demonstration.\n\nlibrary(wooldridge)\ndata(mlb1)\n\nmlb1_dt <- \n  mlb1 %>% \n  data.table() %>% # turn into data.table \n  .[, salary := NULL] %>% # remove salary (use lsalary instead)\n  na.omit() # remove observations with NA in any of the variables\n\n\nmlb1_dm_X <- \n  xgb.DMatrix(\n    data = as.matrix(mlb1_dt[, .(hruns, years, rbisyr, allstar, runsyr, hits, bavg)]),\n    label = as.matrix(mlb1_dt[, lsalary])\n  )\n\nWe can then use xgb.train() to train a model using the XGB algorithm.\n\nxgb_fit <-\n  xgb.train(\n    data = mlb1_dm_X, # independent variable\n    nrounds = 100, # number of iterations (trees to add)\n    eta = 1, # learning rate\n    objective = \"reg:squarederror\" # objective function\n  )"
  },
  {
    "objectID": "P03-xgb.html#resources",
    "href": "P03-xgb.html#resources",
    "title": "8  Extreme Gradient Boosting",
    "section": "8.5 Resources",
    "text": "8.5 Resources\n\nA Gentle Introduction to XGBoost for Applied Machine Learning\n\n\n\n\n\n\n\nChen, Tianqi, and Carlos Guestrin. 2016. “Xgboost: A Scalable Tree Boosting System.” In Proceedings of the 22nd Acm Sigkdd International Conference on Knowledge Discovery and Data Mining, 785–94."
  },
  {
    "objectID": "P04-local-linear-forest.html",
    "href": "P04-local-linear-forest.html",
    "title": "9  Local Linear Forest",
    "section": "",
    "text": "Local linear forest (LLF) is an extension of random forest (RF) and also a generalized random forest (GRF) (Friedberg et al. 2020). We first see that RF is actually a special case of local constant regression. We will then see how LLF builds on RF from the view point of a local regression method."
  },
  {
    "objectID": "P04-local-linear-forest.html#theoretical-background",
    "href": "P04-local-linear-forest.html#theoretical-background",
    "title": "9  Local Linear Forest",
    "section": "9.1 Theoretical background",
    "text": "9.1 Theoretical background\nSuppose \\(T\\) tress have been built after a random forest model is trained on a dataset. Now, let \\(\\eta_{i,t}(X)\\) takes \\(1\\) if observation \\(i\\) belongs to the same leaf as \\(X\\) in tree \\(t\\), where \\(X\\) is a vector of covariates (\\(K\\) variables). Then, the RF’s predicted value of \\(y\\) conditional on a particular value of \\(X\\) (say, \\(X_0\\)) can be written as follows:\n\\[\n\\begin{aligned}\n\\hat{y}(X_0) = \\frac{1}{T} \\cdot\\sum_{t=1}^T\\sum_{i=1}^N \\frac{\\eta_{i,t}(X_0)}{\\sum_{i=1}^N\\eta_{i,t}(X_0)}\\cdot y_i\n\\end{aligned}\n\\]\nNote that \\(\\sum_{i=1}^N\\eta_{i,t}(X_0)\\) represents the number of observations in the same leaf as \\(X_0\\). Therefore, \\(\\sum_{i=1}^N \\frac{\\eta_{i,t}(X_0)}{\\sum_{i=1}^N\\eta_{i,t}(X_0)}\\cdot y_i\\) is the average value of \\(y\\) of the leaf that \\(X_0\\) belongs to. So, while looking slightly complicated, it is the average value of \\(y\\) of the tree \\(X_0\\) belongs to averaged across the trees.\nWe can switch the summations like this,\n\\[\n\\begin{aligned}\n\\hat{y}(X_0) = \\sum_{i=1}^N \\cdot\\frac{1}{T}\\sum_{t=1}^T\\frac{\\eta_{i,t}(X_0)}{\\sum_{i=1}^N\\eta_{i,t}(X_0)}\\cdot y_i\n\\end{aligned}\n\\]\nLet \\(\\alpha(X_i, X_0)\\) denote \\(\\frac{1}{T}\\sum_{t=1}^T\\frac{\\eta_{i,t}(X_0)}{\\sum_{i=1}^N\\eta_{i,t}(X_0)}\\). Then, we can rewrite the above equation as\n\\[\n\\begin{aligned}\n\\hat{y}(X_0) = \\sum_{i=1}^N \\alpha(X_i,X_0) \\cdot y_i\n\\end{aligned}\n\\]\nIt is easy to show that \\(\\hat{y}(X_0)\\) is a solution to the following minimization problem.\n\\[\n\\begin{aligned}\nMin_{\\theta} \\sum_{i=1}^N \\alpha(X_i,X_0)\\cdot[y_i -\\theta]^2\n\\end{aligned}\n\\tag{9.1}\\]\nIn this formulation of the problem, \\(\\alpha(X_i,X_0)\\) can be considered the weight given to observation \\(i\\). By definition,\n\n\\(0 \\leq \\alpha(X_i,X_0) \\leq 1\\)\n\\(\\sum_{i=1}^N \\alpha(X_i,X_0) = 1\\)\n\nYou may notice that Equation 17.1 is actually a special case of local constant regression where the individual weights are \\(\\alpha(X_i, X_0)\\). Roughly speaking, \\(\\alpha(X_i, X_0)\\) measures how often observation \\(i\\) share the same leaves as the evaluation point (\\(X_0\\)) across \\(T\\) trees. So, it measures how similar \\(X_i\\) is to \\(X_0\\) in the RF way, but not based on euclidean distance (which is subject to curse of dimensionality). So, RF is actually a local constant regression with a special way of distributing weights to the individual observations. This interpretation leads to a natural extension. Why don’t we solve local linear regression problem instead, which would be more appropriate if \\(y\\) is a smooth function of \\(X\\)?\nRewriting Equation 17.1 as a local linear regression problem.\n\\[\n\\begin{aligned}\nMin_{\\mu, \\beta} \\sum_{i=1}^N \\alpha(X_i,X_0)\\cdot[y_i -\\mu - (X_i - X_0)\\beta]^2\n\\end{aligned}\n\\tag{9.2}\\]\nwhere \\(\\mu\\) is a scalar (intercept) and \\(\\beta\\) is a vector of parameters (\\(K \\times 1\\)).\nThis approach was proposed by Bloniarz et al. (2016) and they showed modest improvement over RF. LLF by Friedberg et al. (2020) differ from this approach in two important ways.\n\n\n\n\n\n\nLLF\n\n\n\n\nModify the splitting process in a way that the resulting splitting rules (and thus weights) are more suitable to the second stage local linear regression\n\nAt the local linear regression stage, use ridge regularization\n\n\n\nLet’s look at the first modification. In RF, when deciding how to split a node (parent node), we choose a split that solves the following problem:\n\\[\n\\begin{aligned}\n\\frac{1}{N_1}\\sum_{i\\in C_1}(Y_i - \\bar{Y_1}) + \\frac{1}{N_2}\\sum_{i\\in C_2}(Y_i - \\bar{Y_2})\n\\end{aligned}\n\\]\nwhere \\(C_1\\) and \\(C_2\\) are child nodes, and \\(\\bar{Y_1}\\) and \\(\\bar{Y_2}\\) are the mean value of the outcome for \\(C_1\\) and \\(C_2\\), respectively. Instead, LLF by Friedberg et al. (2020) first regresses \\(Y\\) on \\(X\\) using ridge regression using the observations in the parent node, finds the residuals, and then uses the residuals in place of \\(Y\\) itself.\nNow, let’s look at the second modification. LLF implemented by the grf package adds the ridge penalty to avoid over-fitting and solve the following problem:\n\\[\n\\begin{aligned}\nMin_{\\mu, \\beta} \\sum_{i=1}^N \\alpha(X_i,X_0)\\cdot[y_i -\\mu - (X_i - X_0)\\beta]^2 + \\lambda||\\beta||^2_2\n\\end{aligned}\n\\tag{9.3}\\]\nwhere \\(\\lambda\\) is the regularization parameter. LLF estimator is a weighted ridge regression, and it has a nice analytical solution (just like a regular ridge regression). With the following notations,\n\n\\(A\\): the diagonal matrix where its diagonal element at \\(\\{i, i\\}\\) is the weight for observation \\(i\\), (\\(\\alpha(X_i,X_0)\\), obtained based on the trees grown using the modified splitting process.\n\\(\\Delta\\): the \\(N \\times K\\) (the intercept plus \\(K-1\\) covariates) matrix where \\(\\Delta_{i,1} = 1\\) and \\(\\Delta_{i,j} = x_{i,j} - x_{0,j}\\).\n\\(J\\): \\((K+1) \\times (K+1)\\) diagonal matrix where its diagonal elements are all \\(1\\) except \\(J_{1,1}\\), which is 0 to not penalize the intercept.\n\\(\\theta\\): \\(\\{\\mu, \\beta\\}\\)\n\n\\[\n\\begin{aligned}\n\\hat{\\theta}_{llf} = (\\Delta'A\\Delta + \\lambda J)^{-1}\\Delta'AY\n\\end{aligned}\n\\]\n\n\n\n\n\n\nTerminology alert\n\n\n\nUsing local linear regression at the time of prediction is termed  local linear correction by the authors of the grf package."
  },
  {
    "objectID": "P04-local-linear-forest.html#performance-comparison-llf-v.s.-rf",
    "href": "P04-local-linear-forest.html#performance-comparison-llf-v.s.-rf",
    "title": "9  Local Linear Forest",
    "section": "9.2 Performance comparison: LLF v.s. RF",
    "text": "9.2 Performance comparison: LLF v.s. RF\nFirst, let’s consider the following DGP that is used in Friedberg et al. (2020).\n\n9.2.1 DGP 1\n\n\n\n\n\n\nDGP 1\n\n\n\n\\[\n\\begin{aligned}\ny_i = log(1+exp(6x)) + \\varepsilon\n\\end{aligned}\n\\]\n\n\\(\\{X_1, \\dots, X_10\\} \\sim U[-1, 1]^{10}\\)\n\\(\\varepsilon \\sim N(0, 20)\\)\n\n\n\nWe consider the following four methods presented in (tab-methods?).\n\n\n\n\n\n\n  \n  \n    \n      Method\n      Split on outcome\n      Linear Correction\n    \n  \n  \n    1\nNo\nNo\n    2\nNo\nX1\n    3\nYes\nAll\n    4\nYes\nX1\n  \n  \n  \n\n\n\n\nMethod 1 is equivalent to a standard RF estimation. Method 2 grows trees like RF, but make local linear corrections on \\(X_1\\) at the time of prediction. This means that at the prediction stage, only \\(X_1\\) is used in Equation 9.2 instead of using all of \\(10\\) variables in \\(X\\). Since we know \\(X_1\\) is the only variable that affects \\(y\\) in DGP 1, this way of linear correction should perform better than using all of the \\(10\\) variables in \\(X\\). Of course, in practice, you do not get to do this. Method 3 splits on the residuals from ridge regression, and uses all of the \\(10\\) variables in \\(X\\) for linear correction. Method 4 splits on the residuals from ridge regression and uses only \\(X1\\) for local linear correction.\nFigure 9.1 presents how \\(\\hat{y}(X)\\) for the test data (black dots) changes according to the value of \\(X_1\\) and \\(E[y|X]\\) (red line). Note that heterogeneity in \\(\\hat{y}(X)\\) at the same value of \\(X_1\\) occurs dues to modeling error: the trained model attributed some of the variations in observed \\(y\\) to other variables than \\(X_1\\) even though \\(X_1\\) is the only variable that is actually affecting \\(y\\) as you can see in DGP 1. Figure 9.1 indicates that irrespective of whether you split on \\(y\\) or residuals from ridge regression, linear correction on \\(X1\\) can substantially reduce bias. Method 4 seems to perform slightly better than Method 2 in this instance. However, when all the variables in \\(X\\) (that include many irrelevant variables) are used for linear correction (Method 3), its benefit disappears, performing about the same as Method 1. Comparing Methods 2 and 4, the benefit of split on residual is not clear at least in this instance. Let’s run MC simulations to have a better picture of the relative performance of the four methods. In each iteration, we calculate RMSE of \\(y\\) prediction for each method.\n\n\nCode\n#--------------------------\n# Generate data\n#--------------------------\nset.seed(78234)\n\nK <- 10\nN <- 1000\n\nX <- matrix(runif(N*K, -1, 1), nrow = N)\ny <- log(1 + exp(6 * X[, 1])) + sqrt(20) * rnorm(N)\n\nX_test <- matrix(runif(N*K, -1, 1), nrow = N)\nx_seq <- seq(-1, 1, length = N)\nX_test[, 1] <- x_seq\ney <- log(1 + exp(6 * x_seq))\n\ntrue_data <- data.table(x = x_seq, y = ey)\n\n#--------------------------\n# Train on outcome and ridge residuals \n#--------------------------\nllf_trained <-\n  grf::ll_regression_forest(\n    X = X,\n    Y = y,\n    enable.ll.split = TRUE\n  )\n\nrf_trained <-\n  grf::regression_forest(\n    X = X,\n    Y = y\n  )\n\n#--------------------------\n# Predictions with and without linear correction\n#--------------------------\nrf_results <-\n  data.table(\n    x = x_seq,\n    y_hat = \n      predict(\n        rf_trained, \n        newdata = X_test\n      )$predictions,\n    type = \"Method 1: Split on outcome \\n without linear correction\"\n  )\n\nrf_results_lc <-\n  data.table(\n    x = x_seq,\n    y_hat = \n      predict(\n        rf_trained, \n        newdata = X_test, \n        linear.correction.variables = 1\n      )$predictions,\n    type = \"Method 2: Split on outcome \\n with linear correction on X1\"\n  )\n\nllf_results_nlc <-\n  data.table(\n    x = x_seq,\n    y_hat = \n      predict(\n        llf_trained, \n        newdata = X_test\n      )$predictions,\n    type = \"Method 3: Split on residual \\n with linear correction on all X\"\n  )\n\nllf_results_lc <-\n  data.table(\n    x = x_seq,\n    y_hat = \n      predict(\n        llf_trained, \n        newdata = X_test, \n        linear.correction.variables = 1\n      )$predictions,\n    type = \"Method 4: Split on residual \\n with linear correction on X1\"\n  )\n\n#--------------------------\n# Combine the results and plot\n#--------------------------\nall_results <- rbind(rf_results, rf_results_lc, llf_results_nlc, llf_results_lc)\n\nggplot() + \n  geom_point(data = all_results, aes(y = y_hat, x = x), size = 0.5) +\n  geom_line(data = true_data, aes(y = y, x = x), color = \"red\") +\n  facet_wrap(type ~ ., nrow = 2) +\n  ylab(\"Predicted Value\") +\n  xlab(\"X1\") +\n  theme_bw() \n\n\n\n\n\nFigure 9.1: Comparison of predicted values\n\n\n\n\nFigure 9.2 presents the distribution of RMSE for each method over \\(200\\) iterations. As you can see, Method 4 works the best under DGP 1. However, Method 2 also works quite well and the benefit of split on residual is rather small. This is actually expected based on how DGP 1 is specified. We now look at a different DGP that illustrates when split on residuals is likely to be beneficial.\n\n\nCode\n# This function runs an experiment run above with a different dataset drawn from DGP 1 and returns rmse for each method\n\nrun_llf_sim <- function(i){\n\n  print(i)\n\n  X <- matrix(runif(N*K, -1, 1), nrow = N)\n  y <- log(1 + exp(6 * X[, 1])) + sqrt(20) * rnorm(N)\n\n  X_test <- matrix(runif(N*K, -1, 1), nrow = N)\n  x_seq <- seq(-1, 1, length = N)\n  X_test[, 1] <- x_seq\n  ey <- log(1 + exp(6 * x_seq))\n\n  true_data <- data.table(x = x_seq, y = ey)\n\n  #--------------------------\n  # Train on outcome and ridge residuals \n  #--------------------------\n  llf_trained <-\n    grf::ll_regression_forest(\n      X = X,\n      Y = y,\n      enable.ll.split = TRUE\n    )\n\n  rf_trained <-\n    grf::regression_forest(\n      X = X,\n      Y = y\n    )\n\n  #--------------------------\n  # Predictions with and without linear correction\n  #--------------------------\n  rf_results <-\n    data.table(\n      x = x_seq,\n      y_hat = \n        predict(\n          rf_trained, \n          newdata = X_test\n        )$predictions,\n      ey = ey\n    ) %>% \n    .[, .(rmse = sqrt(mean((y_hat - ey)^2)))] %>% \n    .[, type := \"Method 1: Split on outcome \\n without linear correction\"] \n      \n  rf_results_lc <-\n    data.table(\n      x = x_seq,\n      y_hat = \n        predict(\n          rf_trained, \n          newdata = X_test, \n          linear.correction.variables = 1\n        )$predictions,\n      ey = ey\n    ) %>% \n    .[, .(rmse = sqrt(mean((y_hat - ey)^2)))] %>%\n    .[, type := \"Method 2: Split on outcome \\n with linear correction on X1\"]\n\n  llf_results_nlc <-\n    data.table(\n      x = x_seq,\n      y_hat = \n        predict(\n          llf_trained, \n          newdata = X_test\n        )$predictions,\n      ey = ey\n    ) %>% \n    .[, .(rmse = sqrt(mean((y_hat - ey)^2)))] %>% \n    .[, type := \"Method 3: Split on residual \\n with linear correction on all X\"]\n\n  llf_results_lc <-\n    data.table(\n      x = x_seq,\n      y_hat = \n        predict(\n          llf_trained, \n          newdata = X_test, \n          linear.correction.variables = 1\n        )$predictions,\n      ey = ey\n    ) %>% \n    .[, .(rmse = sqrt(mean((y_hat - ey)^2)))] %>% \n    .[, type := \"Method 4: Split on residual \\n with linear correction on X1\"]\n\n  #--------------------------\n  # Combine the results and plot\n  #--------------------------\n  all_results <- \n    rbind(rf_results, rf_results_lc, llf_results_nlc, llf_results_lc)\n\n  return(all_results)\n}\n\n# mc_results_1 <-\n#   lapply(\n#     1:200,\n#     function(x) run_llf_sim(x)\n#   ) %>% \n#   rbindlist()\n\nset.seed(2134)\n\nmc_results_1 <-\n  mclapply(\n    1:200,\n    function(x) run_llf_sim(x),\n    mc.cores = 12\n  ) %>% \n  rbindlist()\n\ng_mc_llf_1 <- \n  ggplot(data = mc_results_1) + \n  geom_density(aes(x = rmse, fill = type), alpha = 0.4) +\n  theme_bw() +\n  scale_fill_discrete(\n    name = \"\",\n    guide = guide_legend(\n      nrow = 2\n    )\n  ) +\n  theme(legend.position = \"bottom\")\n\ng_mc_llf_1\n\nsaveRDS(g_mc_llf_1, \"LectureNotes/Data/g_mc_llf_1.rds\")\n\n\n\n\n\n\n\nFigure 9.2: MC simulations results\n\n\n\n\n\n\n9.2.2 DGP 2\nIn this section, we work on the following DGP.\n\n\n\n\n\n\nDGP 2\n\n\n\n\\[\n\\begin{aligned}\ny_i = 10\\cdot sin(\\pi x_{i,1}x_{i,2}) + 20(x_{i,3}-0.5)^2 + 10 x_{i,4} + 5 C_{i,5} + \\varepsilon\n\\end{aligned}\n\\]\n\n\\(\\{X_1, \\dots, X_5\\} \\sim U[0, 1]^5\\)\n\\(\\varepsilon \\sim N(0, \\sigma^2)\\)\n\n\n\nThis DGP is helpful in illustrating the power of split on residuals.\nWe first generate training and test datasets.\n\nN <- 1000\nK <- 5\nsigma <- sqrt(20)\n\nmake_data <- function(N, sigma, K)\n{\n  data <-\n    matrix(runif(N * K), nrow = N) %>% \n    data.table() %>% \n    setnames(names(.), gsub(\"V\", \"x\", names(.))) %>% \n    .[, ey := 10*sin(pi*x1*x2) + 20*(x3-0.5)^2 + 10*x4 + 5*x5] %>% \n    .[, y := ey + sigma * rnorm(N)]\n\n  return(data)\n}\n\ndata_train <- make_data(N, sigma, K)\ndata_test <- make_data(N, sigma, K)\n\nLet’s train RF and LLF using data_train.\n\n#=== RF ===#\nrf_trained <-\n  grf::regression_forest(\n    X = data_train[, .(x1, x2, x3, x4, x5)],\n    Y = data_train[, y]\n  )\n\n#=== LLF ===#\nllf_traiend <-\n  grf::ll_regression_forest(\n    X = data_train[, .(x1, x2, x3, x4, x5)] %>% as.matrix(),\n    Y = data_train[, y],\n    enable.ll.split = TRUE,\n    ll.split.weight.penalty = TRUE\n  )\n\nWe now predict \\(y\\) on the test data (data_test) based on RF and LLF with linear corrections on all the variables.\n\npred_data <-\n  data_test %>% \n  .[, y_hat_rf := predict(\n    rf_trained, \n    newdata = data_test[, .(x1, x2, x3, x4, x5)],\n    linear.correction.variables = 1:K\n  )] %>% \n  .[, y_hat_llf := predict(\n    llf_traiend, \n    newdata = data_test[, .(x1, x2, x3, x4, x5)]\n  )$predictions\n  ] %>% \n  .[, .(ey, y_hat_rf, y_hat_llf)] %>% \n  melt(id.var = \"ey\") \n\n(tab-rf-llf-dgp2?) shows the RMSE values for RF and LLF. Since both methods here use linear correction at the time of prediction, the observed difference in their performance is attributable to the way splitting is done: split on outcome or residuals.\n\n\nCode\npred_data %>% \n.[, .(rmse = sqrt(sum((value - ey)^2))), by = variable] %>% \n.[, variable := c(\"RF\", \"LLF\")] %>% \nsetnames(names(.), c(\"Method\", \"RMSE\")) %>% \ngt()\n\n\n\n\n\n\n  \n  \n    \n      Method\n      RMSE\n    \n  \n  \n    RF\n50.45657\n    LLF\n46.29489\n  \n  \n  \n\n\n\n\nLooking at how splits are done is very insightful in understanding what gives the edge to splitting on residuals. ?fig-dif-splits shows what variables are used to split nodes at various depths. When split on outcome, \\(X_4\\) is used most often at the first depth because it is a highly influential variable. On the other hand, when split on residuals, \\(X_1\\) is used most often at the first depth. This is because when ridge regression is run on the parent node, much of the linear signals does not remain in the residuals (signals from \\(X_4\\) and \\(X_5\\)). So, the trees grown focus more on complicated non-linear and interactive signals. Note that signals from \\(X_4\\) and \\(X_5\\) will be caught anyway at the prediction stage because of local linear correction. So, if you are doing linear correction, then you should not “waste” trees on linear signals. This is the motivation of split on residuals.\n\n\nCode\nrf_split <-\n  split_frequencies(rf_trained) %>% \n  data.table() %>% \n  .[, Depth := 1:.N] %>% \n  melt(id.var = \"Depth\") %>% \n  .[, value := value /sum(value), by = Depth] %>% \n  .[, type := \"Split on Outcome\"]\n\nllf_split <-\n  split_frequencies(llf_traiend) %>% \n  data.table() %>% \n  .[, Depth := 1:.N] %>% \n  melt(id.var = \"Depth\") %>% \n  .[, value := value /sum(value), by = Depth] %>% \n  .[, type := \"Split on Residual\"]\n\nrbind(rf_split, llf_split) %>% \n  .[, Variable := gsub(\"V\", \"X\", variable)] %>% \n  ggplot(data = .) + \n  geom_tile(aes(y = Depth, x = Variable, fill = value)) +\n  facet_grid(type ~ .) +\n  scale_fill_viridis_c() +\n  theme_bw()\n\n\n\n\n\nFigure 9.3: Difference in splitting\n\n\n\n\nFor more extensive performance comparison via MC simulations, see Friedberg et al. (2020)."
  },
  {
    "objectID": "P04-local-linear-forest.html#extension-to-other-grf-methods",
    "href": "P04-local-linear-forest.html#extension-to-other-grf-methods",
    "title": "9  Local Linear Forest",
    "section": "9.3 Extension to other GRF methods",
    "text": "9.3 Extension to other GRF methods"
  },
  {
    "objectID": "P04-local-linear-forest.html#implementation",
    "href": "P04-local-linear-forest.html#implementation",
    "title": "9  Local Linear Forest",
    "section": "9.4 Implementation",
    "text": "9.4 Implementation\nYou can use ll_regression_forest() from the grf package to train LLF for R and GRFForestLocalLinearRegressor() from the skgrf package for Python."
  },
  {
    "objectID": "P04-local-linear-forest.html#references",
    "href": "P04-local-linear-forest.html#references",
    "title": "9  Local Linear Forest",
    "section": "References",
    "text": "References\n\n\nBloniarz, Adam, Ameet Talwalkar, Bin Yu, and Christopher Wu. 2016. “Supervised Neighborhoods for Distributed Nonparametric Regression.” In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, edited by Arthur Gretton and Christian C. Robert, 51:1450–59. Proceedings of Machine Learning Research. Cadiz, Spain: PMLR. https://proceedings.mlr.press/v51/bloniarz16.html.\n\n\nFriedberg, Rina, Julie Tibshirani, Susan Athey, and Stefan Wager. 2020. “Local Linear Forests.” Journal of Computational and Graphical Statistics 30 (2): 503–17."
  },
  {
    "objectID": "C0P-causal-ml.html",
    "href": "C0P-causal-ml.html",
    "title": "Causal Machine Learning (CML) Methods",
    "section": "",
    "text": "Unlike prediction-oriented machine learning (POML) methods, the focus of causal machine learning (CML) methods is to identify the treatment effect of a treatment (or small number of distinct treatments).\n\\[\nTE(X) = \\theta(X)\\cdot T\n\\]\n\\(\\theta(X)\\) is the impact of the treatment when \\(T\\) is binary and marginal impact of the treatment when \\(T\\) is continuous. \\(\\theta(X)\\) is a function of attributes (\\(X\\)), meaning that the impact of the treatment can vary (heterogeneous) based on the value of the attributes.\n\n\n\\(\\theta(X) = \\theta\\), where \\(\\theta\\) is a constant, is a special case where the treatment effect is not a function of any observed features.\n\n\\(T\\) may be continuous or discrete.\n\nCML considers the following model (following the documentation of the econml Python package)\n\\[\n\\begin{aligned}\nY & = \\theta(X)\\cdot T + g(X, W) + \\varepsilon \\\\\nT & = f(X, W) + \\eta\n\\end{aligned}\n\\]\n\\(W\\) are the collection of attributes that affect \\(Y\\) along with \\(X\\) (represented by \\(g(X, W)\\)), but not as drivers of the heterogeneity in the impact of the treatment. \\(X\\) not just affects \\(Y\\) as drivers of the heterogeneity in the impact of the treatment (\\(\\theta(X)\\cdot T\\)), but also directly along with \\(W\\).\nBoth \\(X\\) and \\(W\\) are potential confounders. While we do control for them (eliminating their influence) by partialing out \\(f(X, W)\\) and \\(g(X, W)\\), the sole focus is on the estimation of \\(\\theta(X)\\). This is in stark contrast to the focus of the ML methods we have seen in earlier sections, which primarily focuses on the accurate prediction of the  level of the dependent variable, rather than how the level of the dependent variable  changes  when treated like CML methods.\nIn this chapter, we first cover double-debiased machine learning (DML) method by Chernozhukov et al. (2018), which many prominent CML methods follow. We then move on to discuss R-leaner, followed by causal forest and orthogonal forest."
  },
  {
    "objectID": "C0P-causal-ml.html#some-notes-on-cml",
    "href": "C0P-causal-ml.html#some-notes-on-cml",
    "title": "Causal Machine Learning (CML) Methods",
    "section": "Some notes on CML",
    "text": "Some notes on CML\n\n\n\n\n\n\nImportant\n\n\n\n\nPoint 1. The use of causal machine learning model does not guarantee you the estimate you got is indeed a causal effect.\nPoint 2. Careful examinations of the underlying assumptions is necessary just like traditional causal inference methods.\nPoint 3. CML is not a panacea at all. How can it be better than the traditional approaches?\n\nThey may be able to capture complex non-linear interactions of variables without specifying how they interact\nThey are robust to specification errors\nThey can be less efficient to correctly specified (or well approximated) parametric models\n\n\n\n\nPoint 1 is not just for CML, but for any statistical models in general. Any statistical model is just a mathematical manipulation of numbers. Model themselves have no ability to identify causal effects.\n More on this later…. \n\n\n\n\n\n\nChernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. 2018. “Double/debiased machine learning for treatment and structural parameters.” The Econometrics Journal 21 (1): C1–68. https://doi.org/10.1111/ectj.12097."
  },
  {
    "objectID": "C00-why-not-this.html",
    "href": "C00-why-not-this.html",
    "title": "10  Why can’t we just do this?",
    "section": "",
    "text": "\\[\n\\begin{aligned}\ny = f(T, X) +  \\mu \\\\\n\\end{aligned}\n\\]\nA very natural approach to estimate treatment effect seems to simply train ML models (e.g., random forest) to estimate \\(f(T, X)\\) by regressing \\(y\\) on \\(T\\) and \\(X\\) (treating \\(T\\) as just one of the covariates along with \\(X\\)), and then derive the effect of \\(T\\) bsed on the trained model. Let \\(\\hat{f}(T, X)\\) denote the trained model using any ML method. If \\(T\\) is binary, then we can simply estimate its treatment effect conditional on \\(X\\) by the following:\n\\[\n\\begin{aligned}\n\\theta(X) = \\hat{f}(T=1, X) - \\hat{f}(T=0, X)\n\\end{aligned}\n\\]\nwhere \\(\\theta(X)\\) denotes the impact of \\(T\\) conditional on \\(X\\). If \\(T\\) is continuous, then the impact of changing the vale of \\(T\\) from \\(a\\) to \\(b\\) is simply\n\\[\n\\begin{aligned}\n\\theta(X)_{a \\rightarrow b} = \\hat{f}(T=b, X) - \\hat{f}(T=a, X)\n\\end{aligned}\n\\]\nThis approach actually has a name and it is called S-leaner.  Indeed, lots of empirical studies in many scientific fields (not so much in economics) have used (and have been using) this approach for causal inference. So, can’t we just use this approach for every single causal inference task? The idea of S-learner seems solid.\n\n\nMany of these studies actually are not aware of the distinctions between prediction and causal inference.\nUnfortunately, S-leaner is inappropriate in many circumstances. Let’s see the performance of S-leaner using simple simulations. As a competitor, we also run causal forest (CF), one of the CML methods we will learn later (Chapter 13). You do not need to understand how CF works here. You just need to know that it is a method that can estimate heterogeneous treatment effects at this point.\nAs an example, consider the following data generating process:\n\\[\n\\begin{aligned}\ny = \\theta(X)\\cdot T + g(X) + \\mu\n\\end{aligned}\n\\]\n, where\n\\[\n\\begin{aligned}\n\\theta(X) & = 2 + x_1^2 + \\sqrt{x_1+x_2 + 1} - max(x_1 \\times x_3, 1) \\\\\ng(X) & = \\alpha \\cdot [x_1^2 + log(x_2+1) + 2\\cdot x_1*x_2]\n\\end{aligned}\n\\]\nIn this example, \\(\\alpha = 2\\). Finally, \\(\\mu\\) is independent of all the variables. So, the treatment and \\(X\\) are all unconfounded.\n\n\nPackages to load for replication\n\nlibrary(data.table)\nlibrary(DoubleML)\nlibrary(tidyverse)\nlibrary(mlr3verse)\nlibrary(patchwork)\nlibrary(parallel)\nlibrary(ranger)\n\nLet’s first generate a dataset according to the data generating process.\n\nset.seed(13934)\nN <- 1000 # number of observations\n\ndata <- \n  data.table(\n    x1 = 2 * runif(N),\n    x2 = 2 * runif(N),\n    x3 = rnorm(N, mean = 1, sd = 1),\n    mu = rnorm(N),\n    T = runif(N) > 0.5\n  ) %>% \n  #=== theta(X) ===#\n  .[, theta_x := 2 + x1^2 + sqrt(x1 + x2 + 2) - pmax(x1 * x3, 1)] %>%\n  #=== g(X) ===#\n  .[, g_x := x1^2 + log(x2 + 1) + 2 * x1 * x2] %>% \n  .[, y := theta_x * T + 2 * g_x + mu]\n\n# data[, .(theta_x, g_x)] %>% cor\n\nWe implement S-learner and then causal forest.\n\n#--------------------------\n# S-learner with RF\n#--------------------------\nrf_trained <-\n  ranger(\n    y ~ T + x1 + x2 + x3,\n    data = data\n  )\n\n#=== calculate treatment effect ===#\nte_data <-\n  copy(data) %>% \n  .[, T := FALSE] %>% \n  .[, y_hat_T0 := predict(rf_trained, data = .)$predictions] %>% \n  .[, T := TRUE] %>% \n  .[, y_hat_T1 := predict(rf_trained, data = .)$predictions] %>% \n  .[, theta_hat_s := y_hat_T1 - y_hat_T0]\n\n#--------------------------\n# Causal forest\n#--------------------------\ncf_trained <-\n  grf::causal_forest(\n    X = data[, .(x1, x2, x3)],\n    Y = data[, y],\n    W = data[, T]\n  )\n\n#=== predict treatment effect ===#\nte_data <-\n  te_data %>% \n  .[, theta_hat_cf := predict(cf_trained)$predictions]\n\nFigure 10.1 presents\n\ntop panel: the scatter plot of true (y-axis) and estimated (x-axis) treatment effects at the observation level by S-leaner (left panel) and causal forest (right panel)\nbottom panel: histogram of the ratio of estimated treatment effects to true treatment effects by S-leaner (left panel) and causal forest (right panel)\n\n\n\nCode\n#--------------------------\n# Performance visualization\n#--------------------------\nplot_data <-\n  te_data[, .(theta_x, theta_hat_cf, theta_hat_s)] %>% \n  melt(id = \"theta_x\") %>% \n  .[, variable := fcase(\n    variable == \"theta_hat_cf\", \"Causal Forest\",\n    variable == \"theta_hat_s\", \"S-learner\"\n  )] %>% \n  .[, variable := factor(variable, levels = c(\"S-learner\", \"Causal Forest\"))]\n\ng_sp <-\n  ggplot(data = plot_data) +\n  geom_point(aes(y = theta_x, x = value), size = 0.6) +\n  geom_abline(slope = 1, color = \"red\") +\n  facet_grid(. ~ variable) +\n  xlab(\"Estimated Treatment Effect\") +\n  ylab(\"True Treatment Effect\") +\n  theme_bw() +\n  coord_equal()\n\ng_hist <-\n  ggplot(data = plot_data) +\n  geom_histogram(\n    aes(x = value/theta_x), \n    fill = \"white\", color = \"blue\"\n  ) +\n  facet_grid(. ~ variable) +\n  xlab(\"Estimated treatment effect divided by the true one\") +\n  theme_bw()\n\ng_sp / g_hist\n\n\n\n\n\nFigure 10.1: Scatter plot of estimated and true treatment effects at the observation level by S-learner and causal forest\n\n\n\n\nAs you can see, S-learner is okay. It looks like the estimated treatment effects are unbiased (of course, we cannot really tell since we just run a single simulation). So, is CF. However, the biggest difference between the two is the accuracy of treatment effect estimation. CF is clearly much more efficient than S-learner in this particular case.\nNow, let’s change the data generating process a bit. Specifically, we will use \\(\\alpha = 10\\) instead of \\(\\alpha = 2\\) so that \\(g(X) = \\textcolor{red}{10} [x_1^2 + log(x_2+1) + 2\\cdot x_1 x_2]\\). This means that the nuisance function \\(g(X)\\) has a much larger share of the variation in \\(y\\) compared to the previous case. The simulation results for this data generating process are presented in Figure 10.2.\n\n\n\\(g(X)\\) is nuisance in the sense that we do not care about estimating \\(g(X)\\) itself. Rather, we just want to control for it.\n\n\nCode\n#=== update y using 5 as the multiplier of g(X) ===#\ndata[, y := theta_x * T + 10 * g_x + mu]\n\n#--------------------------\n# S-learner\n#--------------------------\nrf_trained <-\n  ranger(\n    y ~ T + x1 + x2 + x3,\n    data = data\n  )\n\nte_data <-\n  copy(data) %>% \n  .[, T := FALSE] %>% \n  .[, y_hat_T0 := predict(rf_trained, data = .)$predictions] %>% \n  .[, T := TRUE] %>% \n  .[, y_hat_T1 := predict(rf_trained, data = .)$predictions] %>% \n  .[, theta_hat_s := y_hat_T1 - y_hat_T0]\n\n# rf_trained <-\n#     regression_forest(\n#       X = data[, .(T, x1, x2, x3)] %>% as.matrix(), \n#       Y = data$y,\n#       tune.parameters = \"all\",\n#       tune.num.trees = 500\n#     )\n\n# te_data_s_wo <-\n#   copy(data) %>% \n#   .[, T := FALSE] %>% \n#   .[, y_hat_T0 := \n#     predict(\n#       rf_trained, \n#       newdata = .[, .(T, x1, x2, x3)] %>% as.matrix()\n#     )$predictions\n#   ] %>% \n#   .[, T := TRUE] %>% \n#   .[, y_hat_T1 := \n#     predict(\n#       rf_trained, \n#       newdata = .[, .(T, x1, x2, x3)] %>% as.matrix()\n#     )$predictions\n#   ] %>% \n#   .[, theta_hat := y_hat_T1 - y_hat_T0] %>% \n#   .[, learner := \"S without interactions\"] %>% \n#   .[, .(learner, theta_x, theta_hat)] \n\n#--------------------------\n# Causal forest\n#--------------------------\ncf_trained <-\n  grf::causal_forest(\n    X = data[, .(x1, x2, x3)],\n    Y = data[, y],\n    W = data[, T]\n  )\n\nte_data <-\n  te_data %>% \n  .[, theta_hat_cf := predict(cf_trained)$predictions]\n\n#--------------------------\n# Performance visualization\n#--------------------------\nplot_data <-\n  te_data[, .(theta_x, theta_hat_cf, theta_hat_s)] %>% \n  melt(id = \"theta_x\") %>% \n  .[, variable := fcase(\n    variable == \"theta_hat_cf\", \"Causal Forest\",\n    variable == \"theta_hat_s\", \"S-learner\"\n  )] %>% \n  .[, variable := factor(variable, levels = c(\"S-learner\", \"Causal Forest\"))]\n\ng_sp <-\n  ggplot(data = plot_data) +\n  geom_point(aes(y = theta_x, x = value), size = 0.6) +\n  geom_abline(slope = 1, color = \"red\") +\n  facet_grid(. ~ variable) +\n  xlab(\"Estimated Treatment Effect\") +\n  ylab(\"True Treatment Effect\") +\n  theme_bw() +\n  coord_equal()\n\ng_hist <-\n  ggplot(data = plot_data) +\n  geom_histogram(\n    aes(x = value/theta_x), \n    fill = \"white\", color = \"blue\"\n  ) +\n  facet_grid(. ~ variable) +\n  xlab(\"Estimated treatment effect divided by the true one\") +\n  theme_bw()\n\ng_sp / g_hist\n\n\n\n\n\nFigure 10.2: Scatter plot of estimated and true treatment effects at the observation level by S-learner and causal forest under more influential nuisance function g(X)\n\n\n\n\nNow, this is very interesting. S-leaner is clearly biased (Repeat this many times to confirm this trend is indeed consistent. It is consistent.). S-leaner heavily underestimates the treatment effects. However, causal forest is virtually unaffected. It is almost as if RF does not really care about what \\(\\theta(X)\\) looks like. Remember that RF is trained to minimize sum of squared residuals. So, it is only natural that they will try to build trees that explain \\(g(X)\\) better by “paying much more attention” to \\(g(X)\\) as \\(g(X)\\) is much more important in explaining the variations in \\(y\\) compared to the first case (Künzel et al. 2019). On the other hand, CF estimate \\(g(X)\\) using random forest, but then it removes \\(g(X)\\) out of the equation and focus on estimating \\(\\theta(X)\\) (very very roughly speaking. see Chapter 13 fore more details). This is an example where the differences in their algorithms can make a critical difference in the ability to estimate treatment effects.\n\n\nCausal forest implemented by the grf package uses random forest for the first-stage estimations of \\(E[T|X]\\) and \\(E[Y|X]\\).\nI hope this illustrates why we want to learn causal ML methods, not just prediction-oriented ML methods. With this motivation in mind, let’s begin our journey of learning CML starting from the next chapter.\n\n\n\n\n\n\nTip\n\n\n\nS-learner is just one of  Meta-learners  along with T-, X-, U-, and domain adaptation learners. Unlike CML, which estimate \\(\\theta(X)\\) explicitly, meta-learners do not. Performance of meta-learners will be further discussed later.\n\n\n\n\n\n\n\n\nKünzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. 2019. “Metalearners for Estimating Heterogeneous Treatment Effects Using Machine Learning.” Proceedings of the National Academy of Sciences 116 (10): 4156–65. https://doi.org/10.1073/pnas.1804597116."
  },
  {
    "objectID": "C01-dml.html",
    "href": "C01-dml.html",
    "title": "11  Double Machine Learning",
    "section": "",
    "text": "One of the most important ideas of the recent development of causal machine learning (CML) methods originate from Chernozhukov et al. (2018), which proposed Double/Debiased ML methods. In this section, we go over those key ideas that are at the heart of many other important CML methods we will learn later. We then learn various models you can estimate using the R and Python DoubleML package (P. Bach et al. 2021; Philipp Bach et al. 2022)."
  },
  {
    "objectID": "C01-dml.html#dml-the-basic-idea",
    "href": "C01-dml.html#dml-the-basic-idea",
    "title": "11  Double Machine Learning",
    "section": "11.1 DML: the basic idea",
    "text": "11.1 DML: the basic idea\n\n11.1.1 Problem Setting\nThroughout this section, we are interested in estimating the following econometric model , which is called a partially linear regression model (PLR).\n\n\nWe try to follow the notations of Chernozhukov et al. (2018) as much as possible.\n\\[\n\\begin{aligned}\ny = \\theta d + g_0(X) + \\mu \\\\\nd = m_0(X) + \\eta\n\\end{aligned}\n\\]\n\n\nPartially linear regression model is a class of models where some of the variables are linear in parameter (here, \\(\\theta d\\)) and the rest of the variables are modeled non-parametrically (here, \\(g_0(X)\\)).\nYour sole interest is in estimating \\(\\theta\\): the impact of the treatment (\\(d\\)). \\(g_0(X)\\) is the impact of a collection of variables \\(X\\). \\(m_0(X)\\) expresses how \\(X\\) affects the treatment status, \\(d\\). \\(d\\) may be binary or continuous. $\n\n\n\n\n\n\nAssumptions on the error terms\n\n\n\n\n\\(E[\\mu|D,X] = 0\\)\n\\(E[\\eta|X] = 0\\)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe treatment effect is assumed to be constant irrespective of the value of \\(X\\). So, the treatment effect is not heterogeneous. We will cover heterogeneous treatment effect estimation later.\n\n\n\n\n\n\n\n\nTerminology alert\n\n\n\n\\(g_0(X)\\) and \\(m_0(X)\\) are called  nuisance functions because knowing them is not the ultimate goal. We are only interested in controlling for them to estimate \\(\\theta\\) accurately.\n\n\n\n\n11.1.2 An intuitive, yet naive approach\n\n\nPackages to load for replication\n\nlibrary(data.table)\nlibrary(magick)\nlibrary(fixest)\nlibrary(DoubleML)\nlibrary(tidyverse)\nlibrary(mlr3)\nlibrary(parallel)\nlibrary(mlr3learners)\nlibrary(ggbrace)\nlibrary(rsample)\nlibrary(MASS)\nlibrary(ranger)\n\nConsider the estimating equation of interest below.\n\\[\n\\begin{aligned}\ny = \\theta d + g_0(X) + \\mu\n\\end{aligned}\n\\tag{11.1}\\]\nSubtracting \\(g_0(x)\\) from both sides,\n\\[\n\\begin{aligned}\ny - g_0(x) = \\theta d + \\mu\n\\end{aligned}\n\\]\nSo, if we know \\(g_0(X)\\), then we can simply regress \\(y - g_0(x)\\) on \\(d\\). Of course, we do not know \\(g_0(X)\\), so we need to estimate \\(g_0(x)\\). Let \\(\\hat{g}_0(x)\\) denote \\(g_0(x)\\) estimated by any appropriate machine learning method. Then, we can regress \\(y - \\hat{g}_0(x)\\) on \\(d\\) to estimate \\(\\theta\\) using OLS. Mathematically, it can be written as follows:\n\\[\n\\begin{aligned}\n\\hat{\\theta} = (\\frac{1}{n}\\sum_{i=1}^N d_i d_i)^{-1}\\frac{1}{n}\\sum_{i=1}^N d_i (y_i - \\hat{g}_0(X_i))\n\\end{aligned}\n\\]\nNow, subtracting \\(\\hat{g}_0(x)\\) from both sides of Equation 11.1,\n\\[\n\\begin{aligned}\ny - \\hat{g}_0(x) = \\theta d + (g_0(x) - \\hat{g}_0(x) + \\mu)\n\\end{aligned}\n\\]\nSo, as long as \\(d\\) is not correlated with \\(g_0(x) - \\hat{g}_0(x) + \\mu\\), then the regression of \\(y - \\hat{g}_0(x)\\) on \\(d\\) should work. Unfortunately, this approach turns out to be naive and suffer from bias in general (Chernozhukov et al. 2018).\nAs a way to implement this naive approach, consider the following procedures.\n\nStep 1: Estimate \\(g_0(X)\\) and then subtract the fitted value of \\(g_0(X)\\) from \\(y\\).\n\nStep 1.1: Regress \\(y\\) on \\(X\\) to estimate \\(E[y|X]\\) and call it \\(\\hat{l}_0(x)\\)\nStep 1.2: Regress \\(d\\) on \\(X\\) to estimate \\(E[d|X]\\) (\\(m_0(X)\\)), call it \\(\\hat{m}_0(x)\\), and calculate \\(\\tilde{d} = d - \\hat{m}_0(X)\\).\nStep 1.3: Get an initial estimate of \\(\\theta\\) using \\[\n\\begin{aligned}\n\\hat{\\theta}_{init} = (\\frac{1}{n}\\sum_{i=1}^N \\tilde{d}_i \\tilde{d}_i)^{-1}\\frac{1}{n}\\sum_{i=1}^N \\tilde{d}_i (y_i - \\hat{l}_0(X_i))\n\\end{aligned}\n\\tag{11.2}\\]\nStep 1.4: Regress \\(y_i - \\hat{\\theta}_{init}d\\) on \\(X\\) to estimate \\(g_0(X)\\) and call it \\(\\hat{g}_0(X)\\).\n\nStep 2: Regress \\(y - \\hat{g}_0(X)\\) on \\(d\\) using OLS. Or equivalently, use the following formula \\[\n\\begin{aligned}\n  \\hat{\\theta} = (\\frac{1}{n}\\sum_{i=1}^N d_i d_i)^{-1}\\frac{1}{n}\\sum_{i=1}^N d_i (y_i - \\hat{g}_0(X_i))\n\\end{aligned}\n\\tag{11.3}\\]\n\nTo demonstrate the bias problem, we work on the following data generating process used in the user guide for the DoubleML package.\n\\[\n\\begin{aligned}\ny_i = 0.5 d_i  + \\frac{exp(x_{i,1})}{1 + exp(x_{i,1})} + \\frac{1}{4}\\cdot x_{i,3} + \\mu_i \\\\\nd_i = x_{i,1} + \\frac{1}{4}\\cdot\\frac{exp(x_{i,3})}{1 + exp(x_{i,3})} \\eta_i\n\\end{aligned}\n\\tag{11.4}\\]\nwhere \\(\\mu_i \\sim N(0, 1)\\) and \\(\\eta_i \\sim N(0, 1)\\). The error terms (\\(\\mu_i\\) and \\(\\eta_i\\)) are independent. In this data generating process, \\(d\\) is continuous (not binary) and its effect on \\(y\\) is assumed to be linear.\nWe use the gen_data() function (defined on the right), which is a slightly generalized version of the make_plr_CCDDHNR2018() function from the DoubleML package.\n\n\ngen_data() allows you to specify \\(g_0(X)\\) and \\(m_0(X)\\) unlike make_plr_CCDDHNR2018().\n\n# g(x) = E[y-\\theta\\cdot d|X]\n# m(x) = E[d|X]\n# f(x) = E[z|X]\n\ngen_data <- function(\n  g_formula = formula(~ I(exp(x1)/(1+exp(x1))) + I(x3/4)), # formula that defines m(x)\n  m_formula = formula(~ x1 + I(exp(x3)/(1+exp(x3))/4)), # formula that defines g(x)\n  te_formula = formula(~ I(0.5*d)), # formula that defines theta(x) * t\n  n_obs = 500, \n  n_vars = 20, \n  mu_x = 0, \n  vcov_x = NULL,\n  sigma = 1 # sd of the error term in the y equation\n)\n{\n\n  if (is.null(vcov_x)) {\n    vcov_x <- matrix(rep(0, n_vars^2), nrow = n_vars)\n    for (i in seq_len(n_vars)) {\n      vcov_x[i, ] <- 0.7^abs(i - seq_len(n_vars)) \n    }\n  }\n\n  #=== draw from multivariate normal ===#\n  data <- \n    mvrnorm(n_obs, mu = rep(0, n_vars), Sigma = vcov_x) %>% \n    data.table() %>% \n    setnames(names(.), paste0(\"x\", 1:n_vars))  \n\n  #=== generate d ===#\n  if (m_formula == \"independent\") {\n    data[, d := rnorm(n_obs)]\n  } else {\n    data[, d := model.frame(m_formula, data = data) %>% rowSums() + rnorm(n_obs)]\n  }\n\n  #=== generate y ===#\n  data[, g := model.frame(g_formula, data = data) %>% rowSums()]\n\n  #=== generate treatment effect ===#\n  data[, te := model.frame(te_formula, data = data) %>% rowSums()]\n\n  #=== generate y ===#\n  data[, y := te + g + rnorm(n_obs, sd = sigma)]\n\n  return(data[])\n\n}\n\n\nset.seed(782394)\n\ntraining_data <- gen_data()\n\nIt has 20 x variables (for \\(X\\)) along with d (treatment) and y (dependent variable). Only x1 and x3 are the relevant variables. The rest of \\(X\\) do not play any role in explaining either \\(Y\\) or \\(d\\). However, they are correlated with x1 and x3 and interfere with estimating the nuisance functions.\n\nstr(training_data)\n\nClasses 'data.table' and 'data.frame':  500 obs. of  24 variables:\n $ x1 : num  -0.364 1.115 0.733 1.882 0.479 ...\n $ x2 : num  -0.5296 0.1056 -0.0411 1.0817 0.6851 ...\n $ x3 : num  -0.6156 1.0599 0.0799 0.4634 0.1374 ...\n $ x4 : num  -0.352 0.987 0.805 1.24 1.404 ...\n $ x5 : num  -0.723 0.547 1.85 0.574 0.501 ...\n $ x6 : num  -1.366 1.275 2.695 0.526 1.395 ...\n $ x7 : num  -0.9908 0.7295 1.1006 0.0523 0.3603 ...\n $ x8 : num  -0.3203 0.4991 0.7414 -0.0727 -0.4202 ...\n $ x9 : num  0.313 0.347 0.796 -1.268 -0.128 ...\n $ x10: num  0.2812 -0.6702 0.9771 0.0751 0.2713 ...\n $ x11: num  0.0656 -0.6435 1.5061 -0.8429 1.4138 ...\n $ x12: num  -0.383 0.825 2.299 -0.642 0.859 ...\n $ x13: num  0.7852 -0.0489 0.3969 0.1613 0.299 ...\n $ x14: num  -0.869 -0.27 0.264 0.787 -0.44 ...\n $ x15: num  -0.92172 0.00569 0.04343 0.7545 0.20734 ...\n $ x16: num  -2.062 -0.898 0.655 0.995 1.561 ...\n $ x17: num  -2.325 0.187 -0.114 0.89 0.947 ...\n $ x18: num  -1.1131 -0.3046 -0.0297 0.1187 0.2166 ...\n $ x19: num  -0.83 -0.7797 -0.2592 0.9764 -0.0221 ...\n $ x20: num  -0.577 -1.162 1.126 0.693 1.119 ...\n $ d  : num  -0.683 0.622 -1.909 -0.517 0.816 ...\n $ g  : num  0.256 1.018 0.695 0.984 0.652 ...\n $ te : num  -0.342 0.311 -0.955 -0.259 0.408 ...\n $ y  : num  -0.0511 0.4342 0.7377 1.5015 1.5488 ...\n - attr(*, \".internal.selfref\")=<externalptr> \n\n\n\n11.1.2.1 Step 1\nLet’s now work on Step 1. We estimate \\(g_0(X)\\) using random forest (RF). As described above, this is a four-step process.\n\n\nIt does not have to be RF. Indeed, you can use any statistical methods in this step.\nStep 1.1: Estimate \\(l_0(X)\\) by regressing \\(y\\) on \\(X\\).\n\n#--------------------------\n# Step 1.1\n#--------------------------\nrf_fitted_l0 <-\n  ranger(\n    y ~ .,\n    data = dplyr::select(training_data, c(\"y\", starts_with(\"x\"))),\n    mtry = 5,\n    num.trees = 132,\n    max.depth = 5,\n    min.node.size = 1\n  )\n\n#=== fitted values ===#\nl0_hat <- predict(rf_fitted_l0, data = training_data)$predictions\n\n#=== create y - l0_hat ===#\ntraining_data[, y_less_l := y - l0_hat]\n\nStep 1.2: Estimate \\(m_0(X)\\) by regressing \\(d\\) on \\(X\\).\n\n#--------------------------\n# Step 1.2\n#--------------------------\nrf_fitted_m0 <-\n  ranger(\n    d ~ .,\n    data = dplyr::select(training_data, c(\"d\", starts_with(\"x\"))),\n    mtry = 5,\n    num.trees = 378,\n    max.depth = 3,\n    min.node.size = 6\n  )\n\n#=== fitted values ===#\nm0_hat <- predict(rf_fitted_m0, data = training_data)$predictions\n\n#=== create y - m0_hat ===#\ntraining_data[, d_less_m := d - m0_hat]\n\n\n\nFigure of d (treatment variable) plotted against m0_hat (\\(\\hat{m}_0(X)\\)).\n\n\nCode\nggplot(training_data) +\n  geom_point(aes(y = d, x = m0_hat)) +\n  geom_abline(slope = 1, color = \"red\") +\n  theme_bw()\n\n\n\n\n\nStep 1.3: Get an initial estimate of \\(\\theta\\) using Equation 11.2.\n\n#--------------------------\n# Step 1.2\n#--------------------------\ntheta_init <- training_data[, sum(d_less_m * y_less_l) / sum(d_less_m * d_less_m) ]\n\nStep 1.4: Regress \\(y - \\theta_{init}d\\) on \\(X\\) to fit \\(g_0(X)\\).\n\n#--------------------------\n# Step 1.3\n#--------------------------\n#=== define y - treatment effect ===#\ntraining_data[, y_less_te := y - theta_init * d]\n\n#=== fit rf ===#\nrf_fitted_g0 <-\n  ranger(\n    y_less_te ~ .,\n    data = dplyr::select(training_data, c(\"y_less_te\", starts_with(\"x\"))),\n    mtry = 5,\n    num.trees = 132,\n    max.depth = 5,\n    min.node.size = 1\n  )\n\n#=== fitted values ===#\ng0_hat <- predict(rf_fitted_g0, data = training_data)$predictions\n\n#=== create y - g0 ===#\ntraining_data[, y_less_g := y - g0_hat]\n\nFigure 11.1 plots true \\(g_0(X)\\) (g) against \\(\\hat{g}_0(X)\\) (g0_hat). As you can see, \\(\\hat{g}_0(X)\\) is a bit biased.\n\n\nCode\nggplot(training_data) +\n  geom_point(aes(y = g, x = g0_hat)) +\n  geom_abline(slope = 1, color = \"red\") +\n  theme_bw() +\n  coord_equal()\n\n\n\n\n\nFigure 11.1: ?(caption)\n\n\n\n\n\n\n11.1.2.2 Step 2\nFinally, we regress \\(y - \\hat{g}_0(X)\\) on \\(d\\) (or equivalently using Equation 11.3).\n\n(\ntheta_hat <- lm(y_less_g ~ d, data = training_data)$coefficient[\"d\"]\n)\n\n        d \n0.5372165 \n\n\nSo, in this instance, we get an estimate of \\(\\theta\\) that is a bit lower than the true value of \\(\\theta\\). Let’s repeat this process many times to see how this procedure performs on average.\n\n\nCode\nfit_m0 <- function(training_data, mtry = 10) {\n\n  rf_fitted_m0 <-\n    ranger(\n      d ~ .,\n      data = dplyr::select(training_data, c(\"d\", starts_with(\"x\"))),\n      # mtry = 5,\n      mtry = mtry,\n      # num.trees = 378,\n      num.trees = 500,\n      max.depth = 3,\n      # min.node.size = 6\n      min.node.size = 10\n    )\n\n  return(rf_fitted_m0)\n\n}\n\nfit_l0 <- function(training_data, mtry = 12)\n{\n  rf_fitted_l0 <-\n    ranger(\n      y ~ .,\n      data = dplyr::select(training_data, c(\"y\", starts_with(\"x\"))),\n      mtry = mtry,\n      # num.trees = 132,\n      num.trees = 500,\n      max.depth = 5,\n      # min.node.size = 1\n      min.node.size = 10\n    )\n\n  return(rf_fitted_l0)\n}\n\n#===================================\n# Define a function that will get you g0_hat\n#===================================\n# this function will be used later\n\nfit_g0 <- function(training_data, rf_fitted_m0, mtry_l = 12, mtry_g = 12) {\n  #--------------------------\n  # Step 1.1\n  #--------------------------\n  rf_fitted_l0 <- fit_l0(training_data, mtry_l)\n\n  #=== fitted values ===#\n  l0_hat <- predict(rf_fitted_l0, data = training_data)$predictions\n\n  #=== create y - l0_hat ===#\n  training_data[, y_less_l := y - l0_hat]\n\n  #--------------------------\n  # Step 1.2\n  #--------------------------\n  #=== fitted values ===#\n  m0_hat <- predict(rf_fitted_m0, data = training_data)$predictions\n\n  #=== create y - m0_hat ===#\n  training_data[, d_less_m := d - m0_hat]\n\n  #--------------------------\n  # Step 1.2\n  #--------------------------\n  theta_init <- training_data[, sum(d_less_m * y_less_l) / sum(d_less_m * d_less_m)]\n\n  #--------------------------\n  # Step 1.3\n  #--------------------------\n  #=== define y - treatment effect ===#\n  training_data[, y_less_te := y - theta_init * d]\n\n  #=== fit rf ===#\n  rf_fitted_g0 <-\n    ranger(\n      y_less_te ~ .,\n      data = dplyr::select(training_data, c(\"y_less_te\", starts_with(\"x\"))),\n      mtry = mtry_g,\n      # num.trees = 132,\n      num.trees = 500,\n      max.depth = 5,\n      # min.node.size = 1\n      min.node.size = 10\n    )\n\n  return(rf_fitted_g0)\n\n}\n\n#===================================\n# Define a function that runs a single simulation and gets you theta_hat\n#===================================\nrun_sim_naive <- function(i){\n\n  # training_data <- data[[i]] %>% data.table()\n  training_data <- gen_data()\n\n  rf_fitted_m0 <- fit_m0(training_data)\n  rf_fitted_g0 <- fit_g0(training_data, rf_fitted_m0)\n  g0_hat <- predict(rf_fitted_g0, data = training_data)$predictions\n\n  #=== create y - g0 ===#\n  training_data[, y_less_g := y - g0_hat]\n\n  theta_hat <- lm(y_less_g ~ d, data = training_data)$coefficient[\"d\"]\n\n  # theta_hat <- training_data[, sum(d * y_less_g) / sum(d * d)]\n\n  return(theta_hat)\n\n}\n\n#===================================\n# Repeat MC simulations 500 times\n#===================================\ntheta_hats_apr1 <-\n  mclapply(\n    1:500,\n    run_sim_naive,\n    mc.cores = detectCores() / 4 * 3\n  ) %>% \n  unlist() %>% \n  data.table(theta = .)\n\n#===================================\n# Plot the results\n#===================================\nggplot(theta_hats_apr1) +\n  geom_histogram(aes(x = theta), color = \"grey\", fill = \"white\") +\n  theme_bw() + \n  geom_vline(aes(xintercept = 0.5,color = \"True Value\")) +\n  geom_vline(aes(xintercept = theta_hats_apr1[, mean(theta)], color = \"Mean of the Estimates\")) +\n  scale_color_manual(\n    values = c(\"True Value\" = \"red\", \"Mean of the Estimates\" = \"blue\"),\n    name = \"\"\n  ) +\n  guides(color = guide_legend(nrow = 1, byrow = TRUE)) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 11.2: Simulation results of the naive procedure\n\n\n\n\nFigure 11.2 shows the histogram of \\(\\hat{\\theta}\\) from 500 simulations. You can see that this procedure has led to consistent underestimation of the treatment effect (mean value of \\(\\hat{\\theta}\\) is 0.468). There are two sources of bias in this approach: regularization and over-fitting bias.\n\n\n\n\n\n\nNote\n\n\n\n\nRegularization bias: the bias coming from bias in estimating \\(g_0(X)\\)\nOver-fitting bias: the bias coming from over-fitting \\(g_0(X)\\) and \\(m_0(X)\\) due to the fact that the same sample is used for \\(g_0(X)\\) and \\(m_0(X)\\) estimation and \\(\\theta\\) estimation\n\n\n\nRegularization bias is termed so because bias in estimating \\(g_0(X)\\) can occur when some form of regularization is implemented (e.g., lasso).\n\n\n\n\n\n\n\n\n11.1.3 Overcoming the regularization bias\nRegularization bias can be overcome by double-debiasing (orthogonalizing both \\(d\\) and \\(y\\)). Specifically,\n\nStep 1: Estimate \\(g_0(X)\\) and then subtract the fitted value of \\(g_0(X)\\) from \\(y\\)\nStep 2: Subtract \\(\\hat{m}_0(X)\\) from \\(d\\) (\\(\\tilde{d} = d - \\hat{m}_0(x)\\))\nStep 3: Calculate \\(\\hat{\\theta}\\) based on the following formula\n\n\\[\n\\begin{aligned}\n\\hat{\\theta} = (\\frac{1}{n}\\sum_{i=1}^N \\tilde{d}_i d_i)^{-1}\\frac{1}{n}\\sum_{i=1}^N \\tilde{d}_i (y_i - \\hat{g}_0(X_i))\n\\end{aligned}\n\\]\nThe key difference from the previous approach is that this approach uses IV-like formula, where \\(\\tilde{d}\\) is acting like an instrument.\n\n\nFor \\(y = X\\beta + \\mu\\) with instruments \\(Z\\), the IV estimator is \\[\n\\begin{aligned}\n\\hat{\\beta} = (Z'X)^{-1}Z'y\n\\end{aligned}\n\\]\nWe have done Steps 1 and 2 already in the previous approach. So,\n\n#--------------------------\n# Step 3\n#--------------------------\n(\ntheta_hat <- training_data[, sum(d_less_m * y_less_g) / sum(d_less_m * d)]\n)\n\n[1] 0.5325107\n\n\nNow, let’s repeat this 500 times.\n\n\nCode\nrun_sim_dereg <- function(i)\n{\n  training_data <- gen_data()\n  # training_data <- data[[i]] %>% data.table\n\n  rf_fitted_m0 <- fit_m0(training_data)\n  m0_hat <- predict(rf_fitted_m0, data = training_data)$predictions\n\n  #=== create d - m0_hat ===#\n  training_data[, d_less_m := d - m0_hat]\n\n  #=== get g0_hat ===#\n  rf_fitted_g0 <- fit_g0(training_data, rf_fitted_m0)\n  g0_hat <- predict(rf_fitted_g0, data = training_data)$predictions\n\n  #=== create y - g0 ===#\n  training_data[, y_less_g := y - g0_hat]\n\n  theta_hat <- training_data[, sum(d_less_m * y_less_g) / sum(d_less_m * d)]\n\n  return(theta_hat)\n}\n\ntheta_hats_apr2 <-\n  mclapply(\n    1:500,\n    function(x) run_sim_dereg(x),\n    mc.cores = detectCores() / 4 * 3\n  ) %>% \n  unlist() %>% \n  data.table(theta = .)\n\nggplot(theta_hats_apr2) +\n  geom_histogram(aes(x = theta), color = \"grey\", fill = \"white\") +\n  theme_bw() + \n  geom_vline(aes(xintercept = 0.5,color = \"True Value\")) +\n  geom_vline(aes(xintercept = theta_hats_apr2[, mean(theta)], color = \"Mean of the Estimates\")) +\n  scale_color_manual(\n    values = c(\"True Value\" = \"red\", \"Mean of the Estimates\" = \"blue\"),\n    name = \"\"\n  ) +\n  guides(color = guide_legend(nrow = 1, byrow = TRUE)) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 11.3: Simulation results of double-debiased approach\n\n\n\n\nFigure 11.3 shows the distribution of \\(\\hat{\\theta}\\), which is centered about \\(0.478\\). The current approach still suffers from the so-called over-fitting bias(Chernozhukov et al. 2018). Let’s look at how we can overcome this bias next.\n\n\n11.1.4 Overcoming the over-fitting bias\nOver-fitting bias can be overcome by cross-fitting. First, the training data is split into \\(K\\)-folds just like K-fold cross-validation. Let’s denote them as \\(I_1, \\dots, I_k\\). For example, for \\(I_1\\), the following steps are taken (Figure 11.4 provides a visual illustration):\n\nStep 1: Estimate \\(\\hat{g}_0(x)\\) and \\(\\hat{m}_0(x)\\) using the data from the other folds (\\(I_2, \\dots, I_K\\)).\nStep 2: Estimate \\(\\hat{g}_0(x_i)\\) and \\(\\hat{m}_0(x_i)\\) for each \\(i \\in I_1\\) and calculate \\(\\tilde{y}_i = y_i - \\hat{g}_0(x_i)\\) and \\(\\tilde{d}_i = d_i - \\hat{m}_0(x_i)\\).\nStep 3: Use the following formula to obtain \\(\\hat{\\theta}\\).\n\n\\[\n\\begin{aligned}\n\\hat{\\theta} = (\\frac{1}{n}\\sum_{i=1}^N \\tilde{d}_i d_i)^{-1}\\frac{1}{n}\\sum_{i=1}^N \\tilde{d}_i (y_i - \\hat{g}_0(X_i))\n\\end{aligned}\n\\tag{11.5}\\]\nThis process is repeated for all the \\(K\\) folds, and then the the final estimate of \\(\\hat{\\theta}\\) is obtained as the average of \\(\\hat{\\theta}\\)s.\n\n\nYou can implement repeated K-fold cross-fitting using the DoublML package, which is not demonstrated here as it is very much similar in concept to repeated K-fold CV explained in Chapter 3.\n talk about algorithm_2 \n\n\nCode\nggplot() +\n  #=== fold 1 ===#\n  geom_rect(\n    aes(xmin = 0, xmax = 10, ymin = 0, ymax = 2),\n    fill = \"grey\",\n    color = \"black\",\n    size = 1.2\n  ) +\n  geom_rect(\n    aes(xmin = 0, xmax = 2.5, ymin = 0, ymax = 2),\n    fill = \"black\"\n  ) +\n  #=== fold 2 ===#\n  geom_rect(\n    aes(xmin = 0, xmax = 10, ymin = 2.2, ymax = 4.2),\n    fill = \"grey\",\n    color = \"black\",\n    size = 1.2\n  ) +\n  geom_rect(\n    aes(xmin = 2.5, xmax = 5, ymin = 2.2, ymax = 4.2),\n    fill = \"black\"\n  ) +\n  #=== fold 3 ===#\n  geom_rect(\n    aes(xmin = 0, xmax = 10, ymin = 4.4, ymax = 6.4),\n    fill = \"grey\",\n    color = \"black\",\n    size = 1.2\n  ) +\n  geom_rect(\n    aes(xmin = 5, xmax = 7.5, ymin = 4.4, ymax = 6.4),\n    fill = \"black\"\n  ) +\n  #=== fold 4 ===#\n  geom_rect(\n    aes(xmin = 0, xmax = 10, ymin = 6.6, ymax = 8.6),\n    fill = \"grey\",\n    color = \"black\",\n    size = 1.2\n  ) +\n  geom_rect(\n    aes(xmin = 7.5, xmax = 10, ymin = 6.6, ymax = 8.6),\n    fill = \"black\"\n  ) +\n  geom_brace(aes(c(0, 7.4), c(8.7, 9.2)), inherit.data=F) +\n  annotate(\n    \"text\", x = 3.5, y = 9.7, parse = TRUE,\n    label = \"'Find ' * hat(g)[0](x) * ' and ' * hat(m)[0](x) * ' from this data.'\",\n    size = 6\n  ) +\n  geom_curve(\n    aes(x = 2.2, xend = 8.5, y = 10.3, yend = 8.8),\n    arrow = arrow(length = unit(0.03, \"npc\")),\n    curvature = -0.3\n  ) +\n  geom_curve(\n    aes(x = 3.4, xend = 8, y = 10.3, yend = 8.8),\n    arrow = arrow(length = unit(0.03, \"npc\")),\n    curvature = -0.3\n  ) +\n  ylim(NA, 11) +\n  # coord_equal() +\n  theme_void()\n\n\n\n\n\nFigure 11.4: Illustration of cross-fitting\n\n\n\n\nLet’s code the cross-fitting procedure. We first split the data into 2 folds (\\(K = 2\\)).\n\n\n\\(K\\) does not have to be 2.\n\n(\ndata_folds <- rsample::vfold_cv(training_data, v = 2)\n)\n\n#  2-fold cross-validation \n# A tibble: 2 × 2\n  splits            id   \n  <list>            <chr>\n1 <split [250/250]> Fold1\n2 <split [250/250]> Fold2\n\n\nLet’s cross-fit for fold 1.\n\nsplit_1 <- data_folds[1, ]\n\n#=== data for estimating g_0  and m_0 ===#\ndata_train <- analysis(split_1$splits[[1]]) \n\n#=== data for which g_0(x_i) and m_0(x_i) are calculated ===#\ndata_target <- assessment(split_1$splits[[1]]) \n\nFirst, we fit \\(\\hat{g}_0(x)\\) and \\(\\hat{m}_0(x)\\) using the data from the other folds.\n\n#=== m0 ===#\nm_rf_fit <- fit_m0(data_train)\n\n#=== g0 ===#\ng_rf_fit <- fit_g0(data_train, m_rf_fit)\n\nNext, we predict \\(\\hat{g}_0(x_i)\\) and \\(\\hat{m}_0(x_i)\\) for each \\(i\\) of fold 1 (the target dataset) and calculate \\(\\tilde{y}_i = y_i - \\hat{g}_0(x_i)\\) and \\(\\tilde{d}_i = d_i - \\hat{m}_0(x_i)\\).\n\ndata_orth <-\n  data_target %>% \n  #=== prediction of g_0(x_i) ===#\n  .[, g_0_hat := predict(g_rf_fit, data = .)$predictions] %>% \n  #=== orthogonalize y ===#\n  .[, y_tilde := y - g_0_hat] %>% \n  #=== prediction of m_0(x_i) ===#\n  .[, m_0_hat := predict(m_rf_fit, data = .)$predictions] %>% \n  #=== orthogonalize d ===#\n  .[, d_tilde := d - m_0_hat]\n\nThen, \\(\\hat{\\theta}\\) is obtained for this fold using Equation 11.5.\n\n(\ntheta_hat <- data_orth[, sum(d_tilde * y_tilde) / sum(d_tilde * d)]\n)\n\n[1] 0.5486855\n\n\nWe can repeat this for all the folds (cross_fit() which finds \\(\\hat{\\theta}\\) for a particular fold is defined on the side).\n\n\n\ncross_fit <- function(i, data_folds, mtry_l = 12, mtry_m = 10, mtry_g = 12)\n{\n\n  #--------------------------\n  # Prepare data\n  #--------------------------\n  #=== ith split ===#\n  working_split <- data_folds[i, ]\n\n  #=== data for estimating g_0  and m_0 ===#\n  data_train <- analysis(working_split$splits[[1]]) \n\n  #=== data for which g_0(x_i) and m_0(x_i) are calculated ===#\n  data_target <- assessment(working_split$splits[[1]]) \n\n  #--------------------------\n  # Fit g0 and m0\n  #--------------------------\n  #=== m0 ===#\n  m_rf_fit <- fit_m0(data_train, mtry_m)\n\n  #=== g0 ===#\n  g_rf_fit <- fit_g0(data_train, m_rf_fit, mtry_l, mtry_g)\n\n  #--------------------------\n  # Get y_tilde and d_tilde\n  #--------------------------\n  data_orth <-\n    data_target %>% \n    #=== prediction of g_0(x_i) ===#\n    .[, g_0_hat := predict(g_rf_fit, data = .)$predictions] %>% \n    #=== orthogonalize y ===#\n    .[, y_tilde := y - g_0_hat] %>% \n    #=== prediction of m_0(x_i) ===#\n    .[, m_0_hat := predict(m_rf_fit, data = .)$predictions] %>% \n    #=== orthogonalize d ===#\n    .[, d_tilde := d - m_0_hat] %>% \n    .[, .(y_tilde, d_tilde, d)]\n\n  theta_cf <- data_orth[, sum(d_tilde * y_tilde) / sum(d_tilde * d)]\n\n  return(theta_cf)\n}\n\n\n(\ntheta_hat <- \n  lapply(\n    seq_len(nrow(data_folds)), # loop over folds\n    function(x) cross_fit(x, data_folds) # get theta_hat\n  ) %>% \n  unlist() %>%\n  mean() # average them\n)\n\n[1] 0.5247844\n\n\nOkay, now that we understand the steps of this approach, let’s repeat this many times (get_theta_cf() that finds \\(\\hat{\\theta}\\) by cross-fitting is defined on the side).\n\n\n\nget_theta_cf <- function(data_folds, mtry_l = 12, mtry_m = 10, mtry_g = 12){\n\n  theta_hat <- \n    lapply(\n      seq_len(nrow(data_folds)),\n      function(x) cross_fit(x, data_folds, mtry_l, mtry_m, mtry_g)\n    ) %>% \n    unlist() %>% \n    mean()\n\n  return(theta_hat)\n}\n\n\n\nCode\ntheta_hats_cf <-\n  mclapply(\n    1:500,\n    function(x) {\n      print(x)\n      training_data <- gen_data()\n      data_folds <- rsample::vfold_cv(training_data, v = 2)\n      theta_hat <-  get_theta_cf(data_folds)\n      return(theta_hat)\n    },\n    mc.cores = detectCores() * 3 / 4\n  ) %>% \n  unlist()\n\n#=== visualize the results ===#\nggplot() +\n  geom_histogram(aes(x = theta_hats_cf), color = \"grey\", fill = \"white\") +\n  theme_bw() + \n  geom_vline(aes(xintercept = 0.5,color = \"True Value\")) +\n  geom_vline(aes(xintercept = mean(theta_hats_cf), color = \"Mean of the Estimates\")) +\n  scale_color_manual(\n    values = c(\"True Value\" = \"red\", \"Mean of the Estimates\" = \"blue\"),\n    name = \"\"\n  ) +\n  guides(color = guide_legend(nrow = 1, byrow = TRUE)) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 11.5: The distribution of treatment effect estimated by the double-debiased approach with cross-fitting\n\n\n\n\nFigure 11.5 shows the distribution of \\(\\hat{\\theta}\\) with double-debiasing and cross-fitting. It is slightly biased in this instance (mean is 0.502), but the average \\(\\hat{\\theta}\\) is very close to the true parameter.\n\n\n\n\n\n\nImportant\n\n\n\n\nDouble/debiased (double orthogonalization) can help overcome the bias in \\(\\hat{\\theta}\\) that comes from the bias in estimating \\(g_0(X)\\).\nCross-fitting can help overcome the bias from estimating \\(g_0(X)\\), \\(m_0(X)\\), and \\(\\theta\\) using the same data."
  },
  {
    "objectID": "C01-dml.html#models-and-implementation-by-doubleml",
    "href": "C01-dml.html#models-and-implementation-by-doubleml",
    "title": "11  Double Machine Learning",
    "section": "11.2 Models and implementation by DoubleML",
    "text": "11.2 Models and implementation by DoubleML\nIn R, DoubleML is built on top of mlr3, which uses R6 classes provided by the R6 package. Since R6 implements encapsulated object-oriented programming like Python, DoubleML in R and Python work in a very similar manner. Here, we will use R for demonstrations.\n\n11.2.1 Partially linear model\n\\[\n\\begin{aligned}\ny = \\theta d + g(X) + \\mu \\\\\nd = m(X) + \\eta\n\\end{aligned}\n\\tag{11.6}\\]\n\n\n\n\n\n\nNote\n\n\n\nAssumptions on the error terms\n\n\\(E[\\mu|D,X] = 0\\)\n\\(E[\\eta|X] = 0\\)\n\\(E[\\eta\\cdot \\mu|D, X] = 0\\)\n\n\n\n\n\nCode\nDiagrammeR::grViz(\n  paste0(\n  \"\n  digraph {\n    graph [layout = circo, ranksep = 0.5]\n\n    node [shape = box]\n      Y [label = 'Y']\n      u [label = '\\U03BC']\n\n    node [shape = box]\n      X [label = 'X']\n      T [label = 'T']\n      v [label = '\\U03B7']\n\n    edge [minlen = 2]\n      {v, X}->T\n      {X, T}->Y\n      u->Y\n  }\n  \"\n  )\n)\n\n\n\n\n\n\nThere are two ways to estimate \\(\\theta\\) using DoublMLPLR(). They differ in how their score functions are defined. You pick either one of the two:\n\npartialling-out: \\([Y - l(X) - \\theta(D-m(X))][D-m(X)]\\)\nIV-type: \\([Y - \\theta D - g(X)][D-m(X)]\\)\n\nSince \\(g(X)\\) and \\(m(X)\\) themselves appear in the data generating process (Equation 11.7), it is clear what they are. However, it is not immediately clear what \\(l(X)\\) represents. \\(l(X)\\) refers to \\(E[Y|X]\\).\n\\[\n\\begin{aligned}\nl(X) = E[Y|X] & = E[\\theta d + g(X) + \\mu] \\\\\n              & = \\theta E[d|X] + E[g(X)] + E[\\mu|X] \\\\\n              & = \\theta m(X) + g(X) \\;\\; \\mbox{(by the assumptions on the error terms)}\\\\\n\\end{aligned}\n\\]\nGiven this, the scores functions can be rewritten as:\n\npartialling-out: \\(\\mu\\cdot \\eta\\)\nIV-type: \\(\\mu\\cdot \\eta\\)\n\nTherefore, the two score functions are actually identical in meaning, but represented by different terms, which result in different equations to calculate \\(\\hat{\\theta}\\). Here are how \\(\\theta\\) is identified for the two scores functions:\npartialling-out\n\\[\n\\begin{aligned}\nE\\large(\\normalsize[Y - l(X) - \\theta(D-m(X))][D-m(X)]\\large)\\normalsize = 0 \\\\\n\\theta = \\frac{E[(Y - l(X))(D-m(X))]}{E[(D-m(X))(D-m(X))]}\n\\end{aligned}\n\\]\nThe empirical analog of this moment condition is\n\\[\n\\begin{aligned}\n\\hat{\\theta} = \\frac{\\sum_{i=1}^N (Y_i - l(X_i))(D_i-m(X_i))}{\\sum_{i=1}^N (D_i-m(X_i))(D_i-m(X_i))}\n\\end{aligned}\n\\]\nIV-type\n\\[\n\\begin{aligned}\n\\hat{\\theta} = \\frac{\\sum_{i=1}^N (Y_i - g(X_i))(D_i-m(X_i))}{\\sum_{i=1}^N D_i(D_i-m(X_i))}\n\\end{aligned}\n\\]\n\n\n\\(D_i-m(X_i)\\) is acting like an instrument.\nThe choice of the score function results in different steps to estimate \\(\\theta\\) and what you need to supply to DoublMLPLR(). \\(g_0(X)\\) is estimated in several steps (see Section 11.1.2) as it needs to remove the effect of \\(\\theta D\\) (though imperfectly) before estimating only \\(g_0(X)\\). \\(l_0(X)\\), however, is estimated by simply regressing \\(Y\\) on \\(X\\). In running DoublMLPLR(), there are three key options:\n\nml_l: ML method to use to estimate \\(l(X)\\)\nml_m: ML method to use to estimate \\(m(X)\\)\nml_g: ML method to use to estimate \\(g(X)\\)\n\nWhen partialling-out is selected, you need to specify ml_l and ml_m. However, you do not need to specify the ml_g. When IV-type is used you need to specify all three. This is because estimating g(X) involves first estimating l(X) (see Section 11.1.2). So, the partialling-out option requires a smaller number of steps and easier to specify for the user.\nLet’s implement DoublMLPLR() using a synthetic dataset that follows the DGP represented by Equation 11.4. We can use make_plr_CCDDHNR2018() to create such a dataset.\n\n(\ndata <- \n  make_plr_CCDDHNR2018(\n    alpha = 0.5, \n    n_obs = 500, \n    dim_x = 20, \n    return_type = 'data.table'\n  )\n)\n\n             X1          X2          X3         X4           X5         X6\n  1:  1.2440476  1.64639297  0.71131792  0.4783044  0.008239645  0.4533324\n  2:  0.9023502  0.15267901  0.01064853  0.2088653 -0.305263506  0.7158397\n  3: -1.9637574 -1.87906651 -0.67356583 -0.5550050  0.029957814  0.5359109\n  4: -1.9718906 -0.82930335 -1.11817257 -1.6561757 -1.853304959 -0.5511951\n  5: -1.0273249 -0.69520796 -1.62652575 -0.8325634 -0.769455086  0.7121183\n ---                                                                      \n496: -0.5828880  0.03691634 -0.09796938 -0.2660888  0.256374614  0.8409559\n497:  0.1310249 -0.04537018 -0.63309501 -1.9887410 -0.511560500 -0.6410372\n498:  0.6988148  1.18801352  1.40573962  1.7227378  0.135727751 -0.9089638\n499:  0.9334815  0.53835718  0.63176578  0.6217877  0.949987407  1.3972804\n500: -1.8301636 -0.48985250  0.28928829  0.2081408  0.289517934  0.1162203\n             X7          X8         X9         X10        X11        X12\n  1:  0.2998599  0.08169994 -0.3034012 -0.41817280 -0.1684357 -0.5818317\n  2:  0.4535326  0.39097316 -0.1131795 -0.32836997 -0.4328069  0.3964901\n  3:  1.9876844  0.32706905 -0.3247179 -0.95800870 -0.6582622 -0.8564451\n  4: -1.1271986  0.01316699 -0.6730823 -0.27714289 -0.4080058  0.2292704\n  5:  0.9164861  0.25645093  0.8497441  1.43501126  1.0167186 -0.1490919\n ---                                                                    \n496:  1.9224220 -0.36454764  0.5100894  0.56865912  1.2143600  1.4910141\n497: -0.3887281  0.19720732 -0.2633226 -0.36455774 -0.6915640  0.1428711\n498: -0.4677247  0.09132417  1.3396831  0.12893152  0.5207629  0.3854993\n499:  1.4887376  1.83162991  1.0472136  1.52179830  1.8261050  2.2777564\n500:  1.2296010  0.99825727  1.2843710 -0.08255473 -0.2282313 -0.4273543\n            X13        X14        X15        X16        X17        X18\n  1:  0.6359233  0.6519038  0.8225963  1.4501328  1.3638703  2.3590094\n  2:  0.4119444  0.8873855  0.4601086  0.8426136  1.1745062  1.0474801\n  3: -1.7953205 -2.7288900 -1.8070682 -2.3540323 -1.0996324 -0.4669641\n  4:  0.6332562 -1.5151797 -0.5069169 -0.3316435 -0.5820351 -1.1698090\n  5:  0.3762819  0.2109936  1.5015694  0.7507966  0.4815073  0.4143088\n ---                                                                  \n496:  0.7537537  0.5242330  1.0652588  2.2025996  1.3431624  0.4493184\n497: -0.5253672  0.1851662  0.1890410 -1.1936561 -2.0104705 -1.6484655\n498:  0.3943351 -1.0036494  0.2384225 -0.1725888  0.8901573  0.3837909\n499:  3.3185716  1.9115865  1.6692008  0.3438597 -0.1976231  0.3944083\n500:  0.5623116  0.8880912  1.5738437  1.2236440  1.8337886  2.8762545\n             X19        X20          y          d\n  1:  2.15034400  1.9143797  1.8599237  1.2501768\n  2:  0.94456253 -0.4795718  2.3923080  2.1841305\n  3: -0.06033539  0.5126436 -0.9123374 -1.4574737\n  4: -1.12264600 -0.3507258 -1.7675132 -2.9801900\n  5:  0.08113000 -1.6986236 -1.7201920 -0.8987789\n ---                                             \n496: -0.17389979  0.7992646  0.2055072 -1.3320553\n497: -2.00535182 -0.9286602 -0.2724589 -1.6061591\n498:  1.25943944  0.5382165  0.4598890  0.4722447\n499: -0.67025460 -0.4495312  0.4208797  0.4633706\n500:  1.44951329  0.3391455 -1.7206897 -3.5137542\n\n\nNote that the treatment variable is continuous in this dataset.\nWe first need to create a DoubleMLData object from the train data using DoubleMLData$new(). Specify the dependent variable using the y_col option and the treatment variable(s) by d_cols. The rest of the variables in the dataset provided will be regarded as covariates.\n\n(\nobj_dml_data <- \n  DoubleMLData$new(\n    data, \n    y_col = \"y\", \n    d_cols = \"d\"\n  )\n)\n\n================= DoubleMLData Object ==================\n\n\n------------------ Data summary      ------------------\nOutcome variable: y\nTreatment variable(s): d\nCovariates: X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12, X13, X14, X15, X16, X17, X18, X19, X20\nInstrument(s): \nNo. Observations: 500\n\n\nNow, let’s specify ml_l, ml_m, and ml_g. The DoubleML follows mlr3 (see Chapter 18 for how to use mlr3).\n\n(\nml_l <- \n  lrn(\n    \"regr.ranger\",\n    num.trees = 500, \n    mtry = 15, \n    min.node.size = 5\n  )\n)\n\n<LearnerRegrRanger:regr.ranger>\n* Model: -\n* Parameters: num.threads=1, num.trees=500, mtry=15, min.node.size=5\n* Packages: mlr3, mlr3learners, ranger\n* Predict Type: response\n* Feature types: logical, integer, numeric, character, factor, ordered\n* Properties: hotstart_backward, importance, oob_error, weights\n\n\nLet’s use the same ML method for ml_m and ml_g (you can use any appropriate ML methods). We can simply use the clone() method to replicate ml_l.\n\nml_m <- ml_l$clone()\nml_g <- ml_l$clone()\n\nWe now set up the DoubleMLPLR estimator by providing a DoubleMLData and ML methods. By default, score is set to \"partialling out\", so we do not need to provide ml_g. The apply_cross_fitting option is set to TRUE by default.\n\ndml_plr_obj <- DoubleMLPLR$new(obj_dml_data, ml_l, ml_m)\n\nWe can fit the model by invoking fit() on dml_plr_obj.\n\ndml_plr_obj$fit()\n\nINFO  [15:22:20.923] [mlr3] Applying learner 'regr.ranger' on task 'nuis_l' (iter 5/5) \nINFO  [15:22:21.394] [mlr3] Applying learner 'regr.ranger' on task 'nuis_l' (iter 3/5) \nINFO  [15:22:21.855] [mlr3] Applying learner 'regr.ranger' on task 'nuis_l' (iter 1/5) \nINFO  [15:22:22.301] [mlr3] Applying learner 'regr.ranger' on task 'nuis_l' (iter 2/5) \nINFO  [15:22:22.754] [mlr3] Applying learner 'regr.ranger' on task 'nuis_l' (iter 4/5) \nINFO  [15:22:23.483] [mlr3] Applying learner 'regr.ranger' on task 'nuis_m' (iter 1/5) \nINFO  [15:22:23.938] [mlr3] Applying learner 'regr.ranger' on task 'nuis_m' (iter 4/5) \nINFO  [15:22:24.388] [mlr3] Applying learner 'regr.ranger' on task 'nuis_m' (iter 3/5) \nINFO  [15:22:24.836] [mlr3] Applying learner 'regr.ranger' on task 'nuis_m' (iter 5/5) \nINFO  [15:22:25.290] [mlr3] Applying learner 'regr.ranger' on task 'nuis_m' (iter 2/5) \n\n\nSee the results with print(). You can see \\(\\hat{\\theta}\\) at the bottom.\n\nprint(dml_plr_obj)\n\n================= DoubleMLPLR Object ==================\n\n\n------------------ Data summary      ------------------\nOutcome variable: y\nTreatment variable(s): d\nCovariates: X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12, X13, X14, X15, X16, X17, X18, X19, X20\nInstrument(s): \nNo. Observations: 500\n\n------------------ Score & algorithm ------------------\nScore function: partialling out\nDML algorithm: dml2\n\n------------------ Machine learner   ------------------\nml_l: regr.ranger\nml_m: regr.ranger\n\n------------------ Resampling        ------------------\nNo. folds: 5\nNo. repeated sample splits: 1\nApply cross-fitting: TRUE\n\n------------------ Fit summary       ------------------\n Estimates and significance testing of the effect of target variables\n  Estimate. Std. Error t value Pr(>|t|)    \nd   0.52331    0.04536   11.54   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIf you are using IV-type, then you need to provide ml_g as well like below.\n\n#=== set up ===#\ndml_plr_obj_iv <- \n  DoubleMLPLR$new(\n    obj_dml_data, \n    ml_l, \n    ml_m, \n    ml_g,\n    score = \"IV-type\"\n  )\n\n#=== fit ===#\ndml_plr_obj_iv$fit()\n\nINFO  [15:22:25.915] [mlr3] Applying learner 'regr.ranger' on task 'nuis_l' (iter 2/5) \nINFO  [15:22:26.385] [mlr3] Applying learner 'regr.ranger' on task 'nuis_l' (iter 4/5) \nINFO  [15:22:26.860] [mlr3] Applying learner 'regr.ranger' on task 'nuis_l' (iter 1/5) \nINFO  [15:22:27.319] [mlr3] Applying learner 'regr.ranger' on task 'nuis_l' (iter 3/5) \nINFO  [15:22:27.791] [mlr3] Applying learner 'regr.ranger' on task 'nuis_l' (iter 5/5) \nINFO  [15:22:28.435] [mlr3] Applying learner 'regr.ranger' on task 'nuis_m' (iter 1/5) \nINFO  [15:22:28.886] [mlr3] Applying learner 'regr.ranger' on task 'nuis_m' (iter 5/5) \nINFO  [15:22:29.353] [mlr3] Applying learner 'regr.ranger' on task 'nuis_m' (iter 4/5) \nINFO  [15:22:29.822] [mlr3] Applying learner 'regr.ranger' on task 'nuis_m' (iter 2/5) \nINFO  [15:22:30.273] [mlr3] Applying learner 'regr.ranger' on task 'nuis_m' (iter 3/5) \nINFO  [15:22:30.772] [mlr3] Applying learner 'regr.ranger' on task 'nuis_g' (iter 5/5) \nINFO  [15:22:31.366] [mlr3] Applying learner 'regr.ranger' on task 'nuis_g' (iter 4/5) \nINFO  [15:22:31.836] [mlr3] Applying learner 'regr.ranger' on task 'nuis_g' (iter 3/5) \nINFO  [15:22:32.306] [mlr3] Applying learner 'regr.ranger' on task 'nuis_g' (iter 2/5) \nINFO  [15:22:32.773] [mlr3] Applying learner 'regr.ranger' on task 'nuis_g' (iter 1/5) \n\n#=== print the results ===#\nprint(dml_plr_obj_iv)\n\n================= DoubleMLPLR Object ==================\n\n\n------------------ Data summary      ------------------\nOutcome variable: y\nTreatment variable(s): d\nCovariates: X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12, X13, X14, X15, X16, X17, X18, X19, X20\nInstrument(s): \nNo. Observations: 500\n\n------------------ Score & algorithm ------------------\nScore function: IV-type\nDML algorithm: dml2\n\n------------------ Machine learner   ------------------\nml_l: regr.ranger\nml_m: regr.ranger\nml_g: regr.ranger\n\n------------------ Resampling        ------------------\nNo. folds: 5\nNo. repeated sample splits: 1\nApply cross-fitting: TRUE\n\n------------------ Fit summary       ------------------\n Estimates and significance testing of the effect of target variables\n  Estimate. Std. Error t value Pr(>|t|)    \nd   0.54946    0.04565   12.04   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n11.2.2 Partially linear IV model\nWhen using observational data, it is rarely the case that conditional unconfoundedness of the treatment is satisfied. In such a case, you may want to consider a DML-IV approach implemented by DoubleMLPLIV(). Just like IV for linear models, finding the right instrument is critical for the DML-IV approach to be consistent.\n\\[\n\\begin{aligned}\ny = \\theta d + g(X) + \\mu \\\\\nZ = m(X) + \\varepsilon \\\\\nd = r(X) + \\eta\n\\end{aligned}\n\\tag{11.7}\\]\nWhen \\(\\eta\\) and \\(\\mu\\) are correlated, DoubleMLPLR() is inconsistent. However, as long as the following assumptions are satisfied, DoubleMLPLIV() is consistent.\n\n\n\n\n\n\nNote\n\n\n\nAssumptions on the error terms\n\n\\(E[\\mu|Z,X] = 0\\)\n\\(E[\\varepsilon|X] = 0\\)\n\\(E[\\varepsilon\\cdot \\mu|Z, X] = 0\\)\n\n\n\nHere is the causal diagram for the partially linear model of interest. As you can see, the treatment variable \\(T\\) is confounded as \\(\\mu\\) affects both \\(Y\\) and \\(T\\) (through \\(\\eta\\)).\n\n\nCode\nDiagrammeR::grViz(\n  paste0(\n  \"\n  digraph {\n    graph [ranksep = 0.6]\n    node [shape = box]\n      Y [label = 'Y']\n      X [label = 'X']\n      T [label = 'T']\n      Z [label = 'Z']\n      varep [label = '\\U03B5']\n      mu [label = '\\U03BC']\n      eta [label = '\\U03B7']\n    edge [minlen = 2]\n      X->Y\n      T->Y\n      Z->T\n      X->T\n      X->Z\n      varep->Z\n      mu->{Y, eta}\n      eta->{T, mu}\n    { rank = same; Y; Z; varep}\n    { rank = same; X; T; mu}\n  }\n  \"\n  )\n)\n\n\n\n\n\n\nJust like DoublMLPLR(), there are two ways to estimate \\(\\theta\\) using DoublMLPLIV(). The two score functions are:\n\npartialling-out: \\([Y - l(X) - \\theta(D-r(X))][Z-m(X)]\\)\nIV-type: \\([Y - \\theta D - g(X)][Z-m(X)]\\)\n\n, where \\(r(X) = E[D|X]\\).\n\n\n\n\n\n\nConfusing notations\n\n\n\n\\(r(X)\\) is the same as \\(m(X)\\) in the partially line model case. \\(m(X)\\) represents \\(E[Z|X]\\) in the IV model.\n\n\nLet \\(\\tilde{Z}\\), \\(\\tilde{D}\\), \\(\\tilde{Y}_l\\), and \\(\\tilde{Y}_g\\) denote \\(Z-m(X)\\), \\(D-r(X)\\), \\(Y-l(X)\\), and \\(Y-g(X)\\), respectively. Then,\n\npartialling-out: \\(\\theta = (\\tilde{Z}'\\tilde{D})^{-1}\\tilde{Z}'\\tilde{Y}_l\\)\nIV-type: \\(\\theta = (\\tilde{Z}'D)^{-1}\\tilde{Z}'\\tilde{Y}_g\\)\n\n\nWe now implement DoublMLPLIV() using a synthetic dataset generated from the following DGP:\n\\[\n\\begin{aligned}\ny_i & = 0.5 d_i  + \\frac{exp(x_{i,1})}{1 + exp(x_{i,1})} + \\frac{1}{4}\\cdot x_{i,3} + \\mu_i \\\\\nz_i & = x_{i,1} + x_{i,2} + \\varepsilon_i\\\\\nd_i & = x_{i,1} + \\frac{1}{4}\\cdot\\frac{exp(x_{i,3})}{1 + exp(x_{i,3})} + z_i + \\eta_i\n\\end{aligned}\n\\tag{11.8}\\]\nIn this DGP, \\(\\mu_i\\) and \\(\\eta_i\\) are correlated, and using DoublMLPLR() would result in biased estimation of the coefficient on \\(d\\).\n\n\nCode\ngen_data_iv <- function(\n  g_formula = formula(~ I(exp(x1)/(1+exp(x1))) + I(x3/4)), # formula that defines g(x)\n  m_formula = formula(~ x1 + I(exp(x3)/(1+exp(x3))/4) + z), # formula that defines m(x)\n  f_formula = formula(~ x1 + x2), # formula that defines f(x)\n  te_formula = formula(~ I(0.5*d)), # formula that defines theta(x) * t\n  n_obs = 500, \n  Kx = 20, \n  vcov_x = NULL,\n  sigma = 1 # sd of the error term in the y equation\n)\n{\n\n  #=== generate X (with correlation between them)  ===#\n  if (is.null(vcov_x)) {\n    vcov_x <- matrix(rep(0, Kx^2), nrow = Kx)\n    for (i in seq_len(Kx)) {\n      vcov_x[i, ] <- 0.7^abs(i - seq_len(Kx)) \n    }\n  }\n\n  #=== draw from multivariate normal ===#\n  data <- \n    mvrnorm(n_obs, mu = rep(0, Kx), Sigma = vcov_x) %>% \n    data.table() %>% \n    setnames(names(.), paste0(\"x\", 1:Kx))  \n\n  #=== generate z ===#\n  data[, z := model.frame(f_formula, data = data) %>% rowSums() + rnorm(n_obs)]\n\n  #=== generate d ===#\n  if (m_formula == \"independent\") {\n    data[, d := rnorm(n_obs)]\n  } else {\n    data[, d := model.frame(m_formula, data = data) %>% rowSums() + rnorm(n_obs)]\n  }\n\n  mu_eta_shared <- 2 * rnorm(n_obs)\n  data[, d := d + mu_eta_shared]\n\n  #=== generate g ===#\n  data[, g := model.frame(g_formula, data = data) %>% rowSums()]\n\n  #=== generate treatment effect ===#\n  data[, te := model.frame(te_formula, data = data) %>% rowSums()]\n\n  #=== generate y ===#\n  data[, mu := rnorm(n_obs, sd = sigma) + mu_eta_shared ] \n  data[, y := te + g + mu]\n\n  return(data[])\n\n}\n\n\nWe use the gen_data_iv() function (unfold the code block above to see the code of the function).\n\nset.seed(54723)\n\n#=== generate data ===#\n(\ndata <- gen_data_iv(n_obs = 1000, Kx = 5) \n)\n\n              x1           x2           x3         x4         x5          z\n   1:  0.4342727 -0.745980013 -0.704451415 -0.3262466 -0.2965651  0.8824660\n   2:  1.1221999  0.440041841 -0.149247123 -0.3154535  0.9614910  1.5824434\n   3:  0.6144291  0.211276289  0.940231471  1.0861388  0.4304919 -0.1262146\n   4:  0.7403732 -0.007733362  0.675543112  0.2610650  0.2550941  2.7051984\n   5:  1.1458767  1.609769721  1.346189077  2.2836731  0.9752921  3.0936889\n  ---                                                                      \n 996:  0.9907100  0.479463194  1.330201588  0.8966727  1.4158782 -1.1358031\n 997:  0.3205677  0.393600079  0.005436036  0.2999711 -1.2597707  2.4024529\n 998: -0.8804082 -1.413008324 -1.103570335 -1.2363371  0.5292358 -1.8307569\n 999: -0.3638923 -0.051591926  0.102583336  0.1909432  1.2612405 -0.4458673\n1000:  2.1410822  2.186894688  1.985896026  3.0639276  2.5900096  3.7491882\n               d          g         te         mu          y\n   1: -0.2983235 0.43078062 -0.1491618 -1.2165815 -0.9349626\n   2:  0.5436652 0.71708477  0.2718326 -1.4279135 -0.4389961\n   3:  4.6018486 0.88400835  2.3009243  5.2507943  8.4357270\n   4:  1.5940576 0.84596324  0.7970288  0.5512981  2.1942901\n   5:  2.9352110 1.09530424  1.4676055  0.1264622  2.6893719\n  ---                                                       \n 996: -2.1250033 1.06177854 -1.0625016 -1.2311256 -1.2318486\n 997:  1.3813705 0.58082160  0.6906852 -2.4822287 -1.2107219\n 998: -3.9802637 0.01720062 -1.9901318 -1.9907950 -3.9637262\n 999: -4.2886512 0.43566352 -2.1443256 -3.1408462 -4.8495083\n1000:  7.9860106 1.39130650  3.9930053  2.2313960  7.6157078\n\n\nColumn named z is the excluded instrument in the dataset.\nJust like the case with DoublMLPLR(), we first need to create a DoubleMLData object. Unlike the DoublMLPLR() case, we need to specify which column is the excluded instrument(s) with the z_cols option.\n\n(\nobj_dml_data <- \n  DoubleMLData$new(\n    dplyr::select(data, - g, -te, -mu),\n    y_col=\"y\",\n    d_col = \"d\",\n    z_cols= \"z\"\n  )\n)\n\n================= DoubleMLData Object ==================\n\n\n------------------ Data summary      ------------------\nOutcome variable: y\nTreatment variable(s): d\nCovariates: x1, x2, x3, x4, x5\nInstrument(s): z\nNo. Observations: 1000\n\n\nWe set up learners for \\(l(X)\\), \\(m(X)\\), and \\(r(X)\\) as we are using the default score, which is \"partialling out\" here. We just use the same learner for all three.\n\nml_l <- \n  lrn(\n    \"regr.ranger\",\n    num.trees = 500, \n    mtry = 5, \n    min.node.size = 5\n  )\nml_r <- ml_l$clone()\nml_m <- ml_l$clone()\n\nWe now set up DML IV estimation by invoking the new() method on DoubleMLPLIV.\n\ndml_pliv_obj <- \n  DoubleMLPLIV$new(\n    obj_dml_data, \n    ml_l,\n    ml_m,\n    ml_r\n  )\n\nThen, fit the model.\n\ndml_pliv_obj$fit()\n\nINFO  [15:22:33.375] [mlr3] Applying learner 'regr.ranger' on task 'nuis_l' (iter 2/5) \nINFO  [15:22:33.851] [mlr3] Applying learner 'regr.ranger' on task 'nuis_l' (iter 3/5) \nINFO  [15:22:34.328] [mlr3] Applying learner 'regr.ranger' on task 'nuis_l' (iter 4/5) \nINFO  [15:22:34.800] [mlr3] Applying learner 'regr.ranger' on task 'nuis_l' (iter 5/5) \nINFO  [15:22:35.264] [mlr3] Applying learner 'regr.ranger' on task 'nuis_l' (iter 1/5) \nINFO  [15:22:35.896] [mlr3] Applying learner 'regr.ranger' on task 'nuis_r' (iter 3/5) \nINFO  [15:22:36.354] [mlr3] Applying learner 'regr.ranger' on task 'nuis_r' (iter 5/5) \nINFO  [15:22:36.811] [mlr3] Applying learner 'regr.ranger' on task 'nuis_r' (iter 2/5) \nINFO  [15:22:37.392] [mlr3] Applying learner 'regr.ranger' on task 'nuis_r' (iter 1/5) \nINFO  [15:22:37.852] [mlr3] Applying learner 'regr.ranger' on task 'nuis_r' (iter 4/5) \nINFO  [15:22:38.355] [mlr3] Applying learner 'regr.ranger' on task 'nuis_m' (iter 5/5) \nINFO  [15:22:38.808] [mlr3] Applying learner 'regr.ranger' on task 'nuis_m' (iter 3/5) \nINFO  [15:22:39.266] [mlr3] Applying learner 'regr.ranger' on task 'nuis_m' (iter 4/5) \nINFO  [15:22:39.720] [mlr3] Applying learner 'regr.ranger' on task 'nuis_m' (iter 2/5) \nINFO  [15:22:40.169] [mlr3] Applying learner 'regr.ranger' on task 'nuis_m' (iter 1/5) \n\nprint(dml_pliv_obj)\n\n================= DoubleMLPLIV Object ==================\n\n\n------------------ Data summary      ------------------\nOutcome variable: y\nTreatment variable(s): d\nCovariates: x1, x2, x3, x4, x5\nInstrument(s): z\nNo. Observations: 1000\n\n------------------ Score & algorithm ------------------\nScore function: partialling out\nDML algorithm: dml2\n\n------------------ Machine learner   ------------------\nml_l: regr.ranger\nml_m: regr.ranger\nml_r: regr.ranger\n\n------------------ Resampling        ------------------\nNo. folds: 5\nNo. repeated sample splits: 1\nApply cross-fitting: TRUE\n\n------------------ Fit summary       ------------------\n Estimates and significance testing of the effect of target variables\n  Estimate. Std. Error t value Pr(>|t|)    \nd   0.45841    0.07558   6.065 1.32e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe estimate is close to the true value.\n\n\nRun MC simulations to check if DoubleMLPLIV() is indeed consistent under the DGP.\nNow, let’s use DoublMLPLR() to see what we get.\n\nobj_dml_data <- \n  DoubleMLData$new(\n    dplyr::select(data, - g, -te, -mu),\n    y_col=\"y\",\n    d_col = \"d\"\n  )\n\n#=== set up MLPLR ===#\ndml_pl_obj <- \n  DoubleMLPLR$new(\n    obj_dml_data, \n    ml_l,\n    ml_m\n  )\n\n#=== fit ===#\ndml_pl_obj$fit()\n\nINFO  [15:22:40.800] [mlr3] Applying learner 'regr.ranger' on task 'nuis_l' (iter 3/5) \nINFO  [15:22:41.267] [mlr3] Applying learner 'regr.ranger' on task 'nuis_l' (iter 1/5) \nINFO  [15:22:41.742] [mlr3] Applying learner 'regr.ranger' on task 'nuis_l' (iter 2/5) \nINFO  [15:22:42.207] [mlr3] Applying learner 'regr.ranger' on task 'nuis_l' (iter 4/5) \nINFO  [15:22:42.674] [mlr3] Applying learner 'regr.ranger' on task 'nuis_l' (iter 5/5) \nINFO  [15:22:43.315] [mlr3] Applying learner 'regr.ranger' on task 'nuis_m' (iter 1/5) \nINFO  [15:22:43.774] [mlr3] Applying learner 'regr.ranger' on task 'nuis_m' (iter 2/5) \nINFO  [15:22:44.230] [mlr3] Applying learner 'regr.ranger' on task 'nuis_m' (iter 3/5) \nINFO  [15:22:44.692] [mlr3] Applying learner 'regr.ranger' on task 'nuis_m' (iter 5/5) \nINFO  [15:22:45.158] [mlr3] Applying learner 'regr.ranger' on task 'nuis_m' (iter 4/5) \n\nprint(dml_pl_obj)\n\n================= DoubleMLPLR Object ==================\n\n\n------------------ Data summary      ------------------\nOutcome variable: y\nTreatment variable(s): d\nCovariates: x1, x2, x3, x4, x5, z\nInstrument(s): \nNo. Observations: 1000\n\n------------------ Score & algorithm ------------------\nScore function: partialling out\nDML algorithm: dml2\n\n------------------ Machine learner   ------------------\nml_l: regr.ranger\nml_m: regr.ranger\n\n------------------ Resampling        ------------------\nNo. folds: 5\nNo. repeated sample splits: 1\nApply cross-fitting: TRUE\n\n------------------ Fit summary       ------------------\n Estimates and significance testing of the effect of target variables\n  Estimate. Std. Error t value Pr(>|t|)    \nd   1.29805    0.02109   61.54   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAs you can see, \\(\\hat{\\theta}\\) is quite large. This is the expected direction of bias as \\(d\\) and \\(\\mu\\) are positively correlated."
  },
  {
    "objectID": "C01-dml.html#suggested-exercises",
    "href": "C01-dml.html#suggested-exercises",
    "title": "11  Double Machine Learning",
    "section": "11.3 Suggested Exercises",
    "text": "11.3 Suggested Exercises"
  },
  {
    "objectID": "C01-dml.html#references",
    "href": "C01-dml.html#references",
    "title": "11  Double Machine Learning",
    "section": "References",
    "text": "References\n\n\nBach, P., V. Chernozhukov, M. S. Kurz, and M. Spindler. 2021. “DoubleML – An Object-Oriented Implementation of Double Machine Learning in R.” https://arxiv.org/abs/2103.09603.\n\n\nBach, Philipp, Victor Chernozhukov, Malte S. Kurz, and Martin Spindler. 2022. “DoubleML – An Object-Oriented Implementation of Double Machine Learning in Python.” Journal of Machine Learning Research 23 (53): 1–6. http://jmlr.org/papers/v23/21-0862.html.\n\n\nChernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. 2018. “Double/debiased machine learning for treatment and structural parameters.” The Econometrics Journal 21 (1): C1–68. https://doi.org/10.1111/ectj.12097."
  },
  {
    "objectID": "C02-xstr-learner.html",
    "href": "C02-xstr-learner.html",
    "title": "12  S-, X-, T-, and R-learner",
    "section": "",
    "text": "In this section, we look at the S-, X-, T-, and R-learner, which are method that estimate heterogeneous treatment effects when the treatment is binary. While X-learner and T-learner cannot be extended to continuous treatment cases, S-learner and R-learner can be."
  },
  {
    "objectID": "C02-xstr-learner.html#motivation",
    "href": "C02-xstr-learner.html#motivation",
    "title": "12  S-, X-, T-, and R-learner",
    "section": "12.1 Motivation",
    "text": "12.1 Motivation\nIn Chapter 11, the basic idea of double machine learning (DML) methods was introduced when the treatment effect is homogeneous. We now turn our focus to the task of estimating heterogeneous treatment effects: the impact of a treatment varies based on observed attributes of the subjects. Heterogeneous treatment effect is also referred to as  conditional  average treatment effect (CATE).\n\n\n Conditional  on observed attributes.\nUnderstanding how treatment effects vary can be highly valuable in many circumstances.\n Example 1:  If we come to know a particular drug is effective on elderly people but detrimental to kids, then doctors can make a smart decision of prescribing the drug to elderly people, but not to kids.\n\n\nIn this example, the heterogeneity driver is age.\n Example 2:  If we come to know that fertilizer is more effective in increasing corn yield in soil type A than B, then farmers can apply more fertilizer on the parts of the field where soil type is A but less on where soil type is B.\n\n\nIn this example, the heterogeneity driver is soil type.\nAs you can see in these examples, knowledge on the heterogeneity of the treatment effect and its drivers can help decision makers smart-target treatments and policies."
  },
  {
    "objectID": "C02-xstr-learner.html#modeling-framework",
    "href": "C02-xstr-learner.html#modeling-framework",
    "title": "12  S-, X-, T-, and R-learner",
    "section": "12.2 Modeling Framework",
    "text": "12.2 Modeling Framework\nThe model of interest in general form is as follows:\n\\[\n\\begin{aligned}\nY_i & = \\theta(X_i)\\cdot T_i + g(X_i) + \\varepsilon_i \\\\\nT_i & = f(X_i) + \\eta_i\n\\end{aligned}\n\\tag{12.1}\\]\n\n\\(Y\\): dependent variable\n\\(T\\): treatment variable\n\\(X\\): features\n\nHere are the assumptions:\n\n\\(E[\\varepsilon|T, X] = 0\\)\n\\(E[\\eta|X] = 0\\)\n\\(E[\\eta\\cdot\\varepsilon|T, X] = 0\\)\n\nFor the notational convenicence, let \\(\\mu_1(X)\\) and \\(\\mu_0(X)\\) denote the expected value of the potential conditional outcomes:\n\\[\n\\begin{align}\n\\mu_1(X) & = E[Y|T=1, X] = \\theta(X) + g(X)\\\\\n\\mu_0(X) & = E[Y|T=0, X] = g(X)\n\\end{align}\n\\]"
  },
  {
    "objectID": "C02-xstr-learner.html#sec-stx",
    "href": "C02-xstr-learner.html#sec-stx",
    "title": "12  S-, X-, T-, and R-learner",
    "section": "12.3 S-, T-, and X-Learner",
    "text": "12.3 S-, T-, and X-Learner\nIn this section, S-, T-, and X-Learner are introduced, accompanied by a simple R code demonstrations. For demonstrations, a synthetic dataset that follows the DGP below is used.\n\n\n\n\n\n\nDGP 1\n\n\n\n\\[\n\\begin{aligned}\nY_i & = (x_{i,1} + x_{i,2}^2)\\cdot T_i + \\sqrt{x_{i,3}} + \\mu_i \\\\\nT_i|X_i & = Bernouli((1+x_{i,1})/2) \\\\\n\\mu_i|X_i & = N(0,1)\n\\end{aligned}\n\\]\n\n\nHere is the dataset according to the DGP.\n\nset.seed(58734)\n\nN <- 1000\n\n(\ndata <-\n  data.table(\n    x1 = runif(N),\n    x2 = runif(N),\n    x3 = runif(N),\n    mu = rnorm(N)\n  ) %>% \n  .[, T := runif(N) < ((0.5+x1)/2)] %>% \n  .[, Y := (x1 + x2^2) * T + sqrt(x3) + mu] %>% \n  .[, id := 1:.N]\n) \n\n\n12.3.1 S-learner\nS-learner estimates CATE by taking the following steps:\n\nRegress \\(Y\\) on \\(T\\) and \\(X\\) to estimate \\(E[Y|T, X]\\) using any appropriate ML regression methods and call it \\(\\hat{\\mu}(T,X)\\).\nEstimate \\(\\hat{\\theta}(X)\\) as \\(\\hat{\\mu}(T=1,X)-\\hat{\\mu}(T=0,X)\\)\n\nIn this approach, no special treatment is given to \\(T\\). It is just a covariate along with others (\\(X\\)). This approach is named S-learner by Künzel et al. (2019) because it involves estimating a single response function.\nHere is a quick demonstration of how S-learner works (no cross-validation conducted in estimating \\(E[Y|T, X]\\) in this example).\n\n#--------------------------\n# step 1\n#--------------------------\n# RF used here, but any appropriate method is acceptable\nrf_trained <-\n  ranger(\n    Y ~ T + x1 + x2 + x3,\n    data = data\n  )\n\n#--------------------------\n# step 2\n#--------------------------\n# Estimate treatment effect at X_0 = {x1 = 0.5, x2 = 0.5, x3 = 0.5}\n\n#=== data for treated (1) and control (0) ===#\neval_data_1 <- data.table(T = TRUE, x1 = 0.5, x2 = 0.5, x3 = 0.5)\neval_data_0 <- data.table(T = FALSE, x1 = 0.5, x2 = 0.5, x3 = 0.5)\n\n#=== predicted value of Y conditional on Y and X ===#\nmu_hat_1 <- predict(rf_trained, data = eval_data_1)$predictions\nmu_hat_0 <- predict(rf_trained, data = eval_data_0)$predictions\n\n#=== theta_hat(X) ===#\n(\ntheta_hat <- mu_hat_1 - mu_hat_0\n)\n\n[1] 1.132512\n\n\n\n\n12.3.2 T-learner\n\nRegress \\(Y\\) on \\(X\\) using the treated observations to estimate \\(\\mu_1(X)\\) using any appropriate ML regression methods.\nRegress \\(Y\\) on \\(X\\) using the control observations to estimate \\(\\mu_0(X)\\) using any appropriate ML regression methods.\nEstimate \\(\\hat{\\theta}(X)\\) as \\(\\hat{\\mu}_1(X)-\\hat{\\mu}_0(X)\\)\n\nThis approach is named T-learner by Künzel et al. (2019) because it involves estimating two functions.\nHere is a quick demonstration of how T-learner works (no cross-validation conducted in estimating \\(E[Y|T=1, X]\\) and \\(E[Y|T=0, X]\\) in this example).\n\n#--------------------------\n# step 1\n#--------------------------\n# RF used here, but any appropriate method is acceptable\nrf_trained_1 <-\n  ranger(\n    Y ~ x1 + x2 + x3,\n    data = data[T == TRUE, ]\n  )\n\n#--------------------------\n# step 2\n#--------------------------\n# RF used here, but any appropriate method is acceptable\nrf_trained_0 <-\n  ranger(\n    Y ~ x1 + x2 + x3,\n    data = data[T == FALSE, ]\n  )\n\n#--------------------------\n# step 3\n#--------------------------\n# Estimate treatment effect at X_0 = {x1 = 0.5, x2 = 0.5, x3 = 0.5}\n\n#=== data for treated (1) and control (0) ===#\neval_data <- data.table(x1 = 0.5, x2 = 0.5, x3 = 0.5)\n\n#=== predicted value of Y conditional on Y and X ===#\nmu_hat_1 <- predict(rf_trained_1, data = eval_data)$predictions\nmu_hat_0 <- predict(rf_trained_0, data = eval_data)$predictions\n\n#=== theta_hat(X) ===#\n(\ntheta_hat <- mu_hat_1 - mu_hat_0\n)\n\n[1] 1.185506\n\n\n\n\n12.3.3 X-learner\n\nEstimate \\(\\mu_1(X)\\) and \\(\\mu_0(X)\\) using any appropriate ML regression methods. (Steps 1 and 2 of the T-learner)\nImpute individual treatment effect for the treated and control groups as follows\n\n\\[\n\\begin{align}\n\\tilde{D}_i^1(X_i) = Y^1_i - \\hat{\\mu}_0(X_i)\\\\\n\\tilde{D}_i^0(X_i) =  \\hat{\\mu}_1(X_i) - Y^0_i\n\\end{align}\n\\]\n\n\nThis is similar to cross-fitting we saw in Chapter 11, where the folds are the treated and control groups.\n\n\n\n\nRegress \\(\\tilde{D}_i^1(X_i)\\) on \\(X\\) using the observations in the treated group and denote the predicted value as \\(\\hat{\\theta}_1(X)\\)\nRegress \\(\\tilde{D}_i^0(X_i)\\) on \\(X\\) using the observations in the control group and denote the predicted value as \\(\\hat{\\theta}_0(X)\\)\n\n\nCalculate \\(\\hat{\\theta}(X)\\) as their weighted average\n\n\\[\n\\begin{align}\n\\hat{\\theta}(X) = h(X)\\cdot\\hat{\\theta}_0(X) + [1-h(X)]\\cdot\\hat{\\theta}_1(X)\n\\end{align}\n\\tag{12.2}\\]\nAny value of \\(h(X)\\) is acceptable. One option of \\(h(X)\\) may be the estimated propensity score \\(E[W|X]\\).\nHere is a quick demonstration of how X-learner works (no cross-validation conducted in estimating \\(E[Y|T=1, X]\\) and \\(E[Y|T=0, X]\\) in this example).\n\n#--------------------------\n# Step 1\n#--------------------------\n# RF used here, but any appropriate method is acceptable\ntreated_data <- data[T == TRUE, ]\n\nrf_trained_1 <-\n  ranger(\n    Y ~ x1 + x2 + x3,\n    data = treated_data\n  )\n\n# RF used here, but any appropriate method is acceptable\ncontrol_data <- data[T == FALSE, ]\n\nrf_trained_0 <-\n  ranger(\n    Y ~ x1 + x2 + x3,\n    data = control_data\n  )\n\n#--------------------------\n# step 2 (imputed individual treatment effect)\n#--------------------------\n#=== treated samples ===#\ntreated_data[, mu_hat_0 := predict(rf_trained_0, data = treated_data)$predictions]\ntreated_data[, D_tilde_1 := Y - mu_hat_0]\n\n#=== control samples ===#\ncontrol_data[, mu_hat_1 := predict(rf_trained_1, data = control_data)$predictions]\ncontrol_data[, D_tilde_0 := mu_hat_1 - Y]\n\n#--------------------------\n# step 3 (regress D on X)\n#--------------------------\n#=== treated ===#\nrf_trained_D1 <-\n  ranger(\n    D_tilde_1 ~ x1 + x2 + x3,\n    data = treated_data\n  )\n\n#=== control ===#\nrf_trained_D0 <-\n  ranger(\n    D_tilde_0 ~ x1 + x2 + x3,\n    data = control_data\n  )\n\n#--------------------------\n# step 4\n#--------------------------\n# Estimate treatment effect at X_0 = {x1 = 0.5, x2 = 0.5, x3 = 0.5}\neval_data <- data.table(x1 = 0.5, x2 = 0.5, x3 = 0.5)\n\n#=== predicted value of Y conditional on Y and X ===#\ntheta_hat_1 <- predict(rf_trained_D1, data = eval_data)$predictions\ntheta_hat_0 <- predict(rf_trained_D0, data = eval_data)$predictions\n\n#=== regress T on X ===#\nrf_trained_T <-\n  ranger(\n    T ~ x1 + x2 + x3,\n    data = data,\n    probability = TRUE\n  )\n\n#=== propensity score estimate ===#\np_score <- predict(rf_trained_T, data = eval_data)$predictions[, 2]\n\n#=== weighted average of theta_hat_1 and theta_hat_0 with propensity score ===#\n(\ntheta_hat <- p_score * theta_hat_0 + (1-p_score) * theta_hat_1\n)\n\n[1] 0.9973382"
  },
  {
    "objectID": "C02-xstr-learner.html#sec-r-learner",
    "href": "C02-xstr-learner.html#sec-r-learner",
    "title": "12  S-, X-, T-, and R-learner",
    "section": "12.4 R-learner",
    "text": "12.4 R-learner\n\n12.4.1 Theoretical background\nUnder the assumptions,\n\\[\n\\begin{aligned}\nE[Y|X] = \\theta(X)\\cdot f(X) + g(X)\n\\end{aligned}\n\\tag{12.3}\\]\n\n\n\\(f(X) = E[T|X]\\)\nLet, \\(l(X)\\) denote \\(E[Y|X]\\). Taking the difference of Equation 12.1 and Equation 12.3 on both sides,\n\\[\n\\begin{aligned}\nY_i \\textcolor{red}{-l(X_i)} & = \\theta(X_i)\\cdot T_i + g(X_i) + \\varepsilon_i \\textcolor{red}{-[\\theta(X_i)\\cdot f(X_i) + g(X_i)]} \\\\\n\\Rightarrow Y_i - l(X_i) & = \\theta(X_i)\\cdot (T_i -f(X_i)) + \\varepsilon_i \\\\\n\\end{aligned}\n\\]\n\n\nThis is akin to residualization/orthogonalization seen in the DML approach in Chapter 11.\nSo, the problem of identifying \\(\\theta(X)\\) reduces to estimating the following model:\n\\[\n\\begin{aligned}\nY_i - l(X_i) & = \\theta(X_i)\\cdot (T_i -f(X_i)) + \\varepsilon_i\n\\end{aligned}\n\\]\nSince \\(E[(T_i -f(X_i))\\cdot\\varepsilon_i|X] = E[\\eta_i\\cdot\\varepsilon_i|X] = 0\\) by assumption, we can regress \\(Y_i - l(X_i)\\) on \\(X_i\\) and \\(T_i -f(X_i)\\) to estimate \\(\\theta(X)\\). Specifically, we can minimize the following objective function:\n\\[\n\\begin{aligned}\nMin_{\\theta(X)}\\sum_{i=1}^N \\large(\\normalsize[Y_i - l(X_i)] - [\\theta(X_i)\\cdot (T_i -f(X_i))]\\large)^2\n\\end{aligned}\n\\tag{12.4}\\]\n\n\n12.4.2 Estimation steps\nIn practice, we of course do not observe \\(l(X)\\) and \\(f(X)\\). So, we first need to estimate them using the data at hand and then subtract them from \\(Y_i\\) and \\(T_i\\), respectively. You can use any suitable statistical methods to estimate \\(l(X)\\) and \\(f(X)\\). Some machine learning methods allow you to estimate them without assuming any functional form or structural assumptions. If you believe they are linear (in parameter) functions of \\(X\\), you could alternatively use lasso or other linear models. Nie and Wager (2021) proposes that the estimation of \\(l(X)\\) and \\(f(X)\\) is done by cross-fitting (see Section 11.1.4) to avoid over-fitting bias. Let \\(I_{-i}\\) denote all the observations that belong to the folds that \\(i\\) does  not  belong to. Further, let \\(\\hat{l}(X_i)^{I_{-i}}\\) and \\(\\hat{f}(X_i)^{I_{-i}}\\) denote \\(l(X_i)\\) and \\(f(X_i)\\) estimated using \\(I_{-i}\\).\n\n\nJust like the DML approach discussed in Chapter 11, both \\(Y\\) and \\(T\\) are orthogonalized.\nThen the quality of fit (explaining the heterogeneity in the impact of treatment) can be expressed as follows, which is the empirical version of Equation 12.4:\n\\[\n\\begin{aligned}\n\\sum_{i=1}^N [Y_i - \\hat{l}(X_i)^{I_{-i}} - \\theta(X)\\cdot (T_i - \\hat{f}(X_i)^{I_{-i}})]^2\n\\end{aligned}\n\\]\nThis is called  R-score, and it can be used for causal model selection, which will be covered later.\nLet \\(\\tilde{Y}_i\\) and \\(\\tilde{T}_i\\) denote \\(Y_i - \\hat{l}(X_i)^{I_{-i}}\\) and \\(T_i - \\hat{f}(X_i)^{I_{-i}}\\), respectively. The final stage of the R-learner is to estimate \\(\\theta(X)\\) by minimizing the R-score plus the regularization term (if desirable).\n\\[\n\\begin{aligned}\n\\hat{\\theta}(X) = argmin_{\\theta(X)}\\;\\;\\sum_{i=1}^N [\\tilde{Y}_i - \\theta(X)\\cdot\\tilde{T}_i]^2 + \\Lambda(\\theta(X))\n\\end{aligned}\n\\tag{12.5}\\]\nwhere \\(\\Lambda(\\theta(X))\\) is the penalty on the complexity of \\(\\theta(X)\\). For example, if you choose to use lasso, then \\(\\Lambda(\\theta(X))\\) is the L1 norm. You have lots of freedom as to what model you use in the final stage. The econml package offers several off-the-shelf choices of R-learner (DML) approaches that differ in the model used at the final stage, including causal forest, lasso, etc.\n\n\n\n\n\n\nSummary of differences among the learners\n\n\n\n\nR-learner is the only learner that takes into account how the treatment status is determined\nS- and R-learner can be be readily applicable to continuous treatment cases, while T- and X-learner cannot.\n\n\n\n\n\n12.4.3 R-learner by hand\nThis section goes through R codes to implement the estimation steps provided above to further our understanding of how R-learner works using the same synthetic dataset as the one used in Section 12.3.\n\nWe first cross-fit \\(E[Y|X]\\) and \\(E[T|X]\\) using random forest for both cases. We will use repeated (3 times) 5-fold cross-fitting. Resampling the data,\n\n(\ndata_folds <- rsample::vfold_cv(data, v = 5, repeats = 3)\n)\n\n#  5-fold cross-validation repeated 3 times \n# A tibble: 15 × 3\n   splits            id      id2  \n   <list>            <chr>   <chr>\n 1 <split [800/200]> Repeat1 Fold1\n 2 <split [800/200]> Repeat1 Fold2\n 3 <split [800/200]> Repeat1 Fold3\n 4 <split [800/200]> Repeat1 Fold4\n 5 <split [800/200]> Repeat1 Fold5\n 6 <split [800/200]> Repeat2 Fold1\n 7 <split [800/200]> Repeat2 Fold2\n 8 <split [800/200]> Repeat2 Fold3\n 9 <split [800/200]> Repeat2 Fold4\n10 <split [800/200]> Repeat2 Fold5\n11 <split [800/200]> Repeat3 Fold1\n12 <split [800/200]> Repeat3 Fold2\n13 <split [800/200]> Repeat3 Fold3\n14 <split [800/200]> Repeat3 Fold4\n15 <split [800/200]> Repeat3 Fold5\n\n\nThe following function takes a row number (n) and cross-fits \\(E[Y|X]\\) and \\(E[T|X]\\) using the training and test data stored in the nth row of data_folds.\n\ncross_fit <- function(n, data_folds) \n{\n  training_data <- analysis(data_folds[n, ]$splits[[1]])\n  eval_data <- assessment(data_folds[n, ]$splits[[1]])\n\n  #--------------------------\n  # E[Y|X]\n  #--------------------------\n  #=== train ===#\n  rf_trained_y <-\n    ranger(\n      Y ~ x1 + x2 + x3,\n      data = training_data\n    )\n\n  #=== fit ===#\n  eval_data[, y_hat := predict(rf_trained_y, data = eval_data)$predictions]\n\n  #--------------------------\n  # E[T|X]\n  #--------------------------\n  rf_trained_t <-\n    ranger(\n      T ~ x1 + x2 + x3,\n      data = training_data,\n      probability = TRUE\n    )\n\n  eval_data[, t_hat := predict(rf_trained_t, data = eval_data)$predictions[, 2]]\n\n  return(eval_data[, .(id, y_hat, t_hat)])\n}\n\nHere is what the output of the function for the first split looks like.\n\ncross_fit(1, data_folds)\n\n      id      y_hat     t_hat\n  1:   8 1.12894746 0.3900865\n  2:  10 1.20619521 0.8137849\n  3:  16 2.11055584 0.6956071\n  4:  19 1.66612748 0.7430183\n  5:  24 0.97711495 0.2783762\n ---                         \n196: 976 1.24745823 0.3559905\n197: 986 1.90226459 0.7848286\n198: 988 0.09856452 0.3313690\n199: 994 1.15828871 0.3815524\n200: 999 1.23109488 0.4655611\n\n\nRepeating this for all the splits,\n\n(\ncross_fitted_data_rp <-\n  lapply(\n    seq_len(nrow(data_folds)),\n    function(x) cross_fit(x, data_folds)\n  ) %>% \n  rbindlist()\n)\n\n       id     y_hat     t_hat\n   1:   8 1.1265820 0.3838746\n   2:  10 1.2710531 0.7914683\n   3:  16 2.0934446 0.6716373\n   4:  19 1.5689207 0.7728349\n   5:  24 0.9738104 0.2713413\n  ---                        \n2996: 984 1.4447333 0.6111246\n2997: 988 0.2112768 0.3394246\n2998: 989 1.2142513 0.5499603\n2999: 994 1.0262291 0.3408294\n3000: 996 0.9049447 0.5382365\n\n\nWe finally take the mean of the cross-fits by id as each id has tree estimates.\n\n(\ncross_fitted_data <- \n  cross_fitted_data_rp[, .(\n    t_hat = mean(t_hat),\n    y_hat = mean(y_hat)\n  ), by = id]\n)\n\n        id     t_hat     y_hat\n   1:    8 0.4532550 1.1761568\n   2:   10 0.7740161 1.1470253\n   3:   16 0.5242056 1.9554895\n   4:   19 0.6223349 1.6357163\n   5:   24 0.3034603 1.0096122\n  ---                         \n 996:  980 0.4608352 1.5986389\n 997:  981 0.1200376 0.8747466\n 998:  982 0.6291434 1.0229563\n 999:  996 0.5064177 0.8796906\n1000: 1000 0.3033786 1.3630618\n\n\nWe then merge the data to the original data, and define \\(\\tilde{Y}\\) and \\(\\tilde{T}\\).\n\n(\ndata_2nd <- \n  cross_fitted_data[data, on = \"id\"] %>% \n  .[, `:=`(\n    y_tilde = Y - y_hat,\n    t_tilde = T - t_hat\n  )] %>% \n  .[, .(y_tilde, t_tilde, x1, x2, x3)]\n)\n\n          y_tilde    t_tilde          x1         x2        x3\n   1:  0.02185133 -0.5942087 0.260608266 0.06164198 0.4589749\n   2: -1.03373018 -0.5714296 0.766984113 0.54764941 0.2009673\n   3:  1.60365721  0.4938529 0.826228121 0.87144558 0.9021309\n   4:  1.32224214  0.5060730 0.640185426 0.60915635 0.9121869\n   5:  0.29076658  0.5654463 0.513102608 0.98349865 0.2889536\n  ---                                                        \n 996: -1.92603402 -0.5064177 0.503932977 0.98448894 0.4080396\n 997: -0.94039653 -0.3125415 0.004402003 0.08928705 0.7901242\n 998:  0.11977427  0.5844593 0.543469334 0.67529131 0.8898840\n 999:  1.29398249 -0.4388722 0.927181725 0.58389265 0.4166628\n1000: -0.69038873 -0.3033786 0.037774180 0.66973547 0.8149703\n\n\n\nThe first order condition of Equation 12.5 without \\(\\Lambda(\\theta(X))\\) is\n\\[\n\\begin{aligned}\n\\sum_{i=1}^N (\\tilde{Y}_i - \\theta(X)\\cdot\\tilde{T}_i)\\cdot \\tilde{T}_i = 0\n\\end{aligned}\n\\]\nThis can be rewritten as\n\\[\n\\begin{aligned}\n\\sum_{i=1}^N \\tilde{T}_i^2(\\frac{\\tilde{Y}_i}{\\tilde{T}_i} - \\theta(X)) = 0\n\\end{aligned}\n\\]\nSo, this problem can be considered the problem of estimating \\(\\theta(X)\\) when the dependent variable is \\(\\frac{\\tilde{Y}_i}{\\tilde{T}_i}\\) with individual weights of \\(\\tilde{T}_i^2\\).\n\ndata_2nd[, `:=`(\n  weight = t_tilde^2,\n  y_to_t = y_tilde / t_tilde\n)]\n\nLet’s use xgboost() for a non-parametric estimation of \\(\\theta(X)\\).\n\n#=== set up the data with weights ===#\ndata_2nd_xgb <- \n  xgb.DMatrix(\n    data = data_2nd[, .(x1, x2, x3)] %>% as.matrix(),\n    label = data_2nd[, y_to_t],\n    weight = data_2nd[, weight]\n  )\n\n#=== train ===#\nxgb_trained_2nd <-\n  xgboost(\n    data = data_2nd_xgb,\n    nrounds = 200,\n    objective = \"reg:squarederror\"\n  )\n\nYou can now predict \\(\\theta(X)\\) at particular values of \\(X\\). Let’s estimate \\(\\theta(X)\\) at \\(X_0 = \\{x_1 = 0.5, x_2 = 0.5, x_3 = 0.5\\}\\).\n\neval_data <- \n  data.table(x1 = 0.5, x2 = 0.5, x3 = 0.5) %>% \n  as.matrix() %>% \n  xgb.DMatrix(data = .)\n\n(\ntheta_hat <- predict(xgb_trained_2nd, eval_data)\n)\n\n[1] 2.164435\n\n\nYou could alternatively estimate \\(\\theta(X)\\) parametrically using OLS. Suppose we somehow know that \\(\\theta(X)\\) takes the following form \\(\\beta_1 x_1 + \\beta_2 x_2^2 + \\beta_3 x_3\\). Then, the second stage estimation would be regressing \\(\\tilde{Y}\\) on \\(x_1\\times T\\), \\(x_2^2\\times T\\), and \\(x_2\\times T\\).\n\n#=== train ===#\nols_2nd_stage <- lm(y_tilde ~ I(x1*t_tilde) + I(x2^2*t_tilde) + I(x3*t_tilde), data = data_2nd)\n\n#=== summary ===#\nsummary(ols_2nd_stage)\n\n\nCall:\nlm(formula = y_tilde ~ I(x1 * t_tilde) + I(x2^2 * t_tilde) + \n    I(x3 * t_tilde), data = data_2nd)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3354 -0.7314  0.0390  0.6332  3.3914 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       -0.01052    0.03239  -0.325    0.745    \nI(x1 * t_tilde)    0.80643    0.18888   4.269 2.15e-05 ***\nI(x2^2 * t_tilde)  1.10509    0.19827   5.574 3.21e-08 ***\nI(x3 * t_tilde)    0.28246    0.18708   1.510    0.131    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.022 on 996 degrees of freedom\nMultiple R-squared:  0.2038,    Adjusted R-squared:  0.2015 \nF-statistic: 85.01 on 3 and 996 DF,  p-value: < 2.2e-16\n\n\nThe results look pretty good mostly because we are cheating and using the correct functional form. Of course, in practice, you would not know the correct functional form of \\(\\theta(X)\\). Finally, note that you should not use the codes here since they are just for demonstration to enhance our understanding of how R-learner works."
  },
  {
    "objectID": "C02-xstr-learner.html#sec-comp-learners",
    "href": "C02-xstr-learner.html#sec-comp-learners",
    "title": "12  S-, X-, T-, and R-learner",
    "section": "12.5 Comparing the learners",
    "text": "12.5 Comparing the learners\nIn this section, we compare the performance of the learners under two DGPs, which fall under the following general model:\n\\[\n\\begin{aligned}\nY_i & =\\theta(X_i)\\cdot T + \\alpha\\cdot g(X_i) + \\mu_i \\\\\nT_i & = Bernouli(f(X_i))\n\\end{aligned}\n\\]\nwhere \\(\\alpha\\) is a constant that changes the share of the nuisance function \\(g(X)\\) in \\(Y\\)’s variation (larger \\(\\alpha\\) in magnitude, larger share of \\(g(X)\\)).\n\n\n\n\n\n\nCaveats\n\n\n\nWe cannot draw any generalizable conclusions about the performance of the learners because we only consider a specific case of the learners where the extreme gradient boost is used for all the estimation tasks and also because we consider only several DGPs.\n\n\nPerformance comparisons under more DGPs can be found in Nie and Wager (2021). MC simulations here focus more on the role of the magnitude of the nuisance part in \\(Y\\), which Nie and Wager (2021) does not look at.\n\nWe first work on the following DGP (named DGP A).\n\n\n\n\n\n\nDGP A\n\n\n\n\\[\n\\begin{aligned}\ng(X_i) & = sin(\\pi X_{i,1}X_{i,2}) + 2(X_{i,3}-0.5)^2 + X_{i,4} + 0.5 X_{i,5}\\\\\nf(X_i) & = max(0.1, min(sin(\\pi X_{i,1}X_{i,2}), 0.9)) \\\\\n\\theta(X_i) & = (X_{i,1} + X_{i,2}) / 2 \\\\\nX_i & \\sim Uni(0,1)^5\n\\end{aligned}\n\\]\n\n\n\n\nDGP A with \\(\\alpha =1\\) is almost the same as Set-up A of Nie and Wager (2021) except that they use \\(Y_i =\\theta(X_i)\\cdot (T-0.5) + \\alpha\\cdot g(X_i) + \\mu_i\\), so that \\(-0.5*\\theta(X)\\) is actually a part of the nuisance for \\(Y\\).\nThe following code generate data according to DGP A.\n\ngen_data_A <- function(N, alpha){\n  data <-\n    data.table(\n      x1 = runif(N),\n      x2 = runif(N),\n      x3 = runif(N),\n      x4 = runif(N),\n      x5 = runif(N),\n      u = rnorm(N)\n    ) %>% \n    .[, `:=`(\n      g_x = alpha * (sin(pi * x1*x2) + 2*(x3-0.5)^2 + x4 + 0.5*x5),\n      f_x = pmax(0.1, pmin(sin(pi * x1*x2), 0.9)),\n      theta_x = (x1+x2)/2\n    )] %>% \n    .[, t := as.numeric(runif(N) < f_x)] %>% \n    .[, y := theta_x * t + g_x + u] %>% \n    .[, id := 1:.N]\n\n  return(data)\n}\n\nWe use the rlearner package (Nie, Schuler, and Wager 2022) to implement S-, T-, X-, and R-learner. In particular, we will use the *boost() functions (e.g., rboost() for R-learner), which use xgboost() for all the estimation tasks. Parameter tuning is done internally (see here for the hyper parameter search space). S-learner implemented by sboost() is different from the S-learner described above in that it include the interactions of the treatment variable and feature variables: that is, it regresses \\(Y\\) on \\(T\\), \\(X\\), and \\(T*X\\).\n\nThe following code implements S-, T-, X-, and R-learner for a single iteration and calculate MSE of estimating \\(\\theta(X)\\) on the test data.\n\n\nThe rlearner package also offers *lasso() and *kern() series. The package is not designed to be flexible as there are fixed combinations of learners and you cannot specify estimation methods yourself.\n\nN <- 1000\n\nget_mse <- function(i, gen_data, alpha) {\n\n  print(i)\n\n  #--------------------------\n  # Prepare data\n  #--------------------------\n  train_data <- gen_data_A(N, alpha)\n  test_data <- gen_data_A(N, alpha)\n  test_X <- test_data[, .(x1, x2, x3, x4, x5)] %>% as.matrix()\n\n  # cor(train_data[, .(theta_x, g_x)])\n\n  #--------------------------\n  # Train and predict \n  #--------------------------\n  learner_ls <- list(rboost, sboost, xboost, tboost)\n\n  results <-\n    lapply(\n      learner_ls,\n      function(learner) {\n        trained_learner <-\n          learner(\n            train_data[, .(x1, x2, x3, x4, x5)] %>% as.matrix(), \n            train_data$t, \n            train_data$y\n          )\n\n        theta_data <-\n          data.table(\n            theta_true = test_data$theta_x,\n            theta_hat = predict(trained_learner, test_X)\n          )\n        \n        return(theta_data)\n      }\n    ) %>% \n    rbindlist(idcol = \"learner\") %>% \n    .[, learner := fcase(\n      learner == 1, \"R\",\n      learner == 2, \"S\",\n      learner == 3, \"X\",\n      learner == 4, \"T\"\n    )]\n\n  return(results)\n}\n\nRepeating experiments 100 times for \\(\\alpha = 1\\),\n\nalpha <- 1\nmc_results_1 <-\n  lapply(\n    seq_len(100),\n    function(i) get_mse(i, gen_data_A, alpha)\n  ) %>% \n  rbindlist(idcol = \"sim\")\n\n\n\n\n\n\n\nFigure 12.1 shows the histogram of MSE by learner.\n\n\nCode\nmse_data_1 <-\n  mc_results_1 %>% \n  .[, .(mse = mean((theta_true - theta_hat)^2)), by = .(learner, sim)] %>%\n  .[, type := \"alpha = 1\"] %>% \n  .[, learner := factor(learner, levels = c(\"S\", \"T\", \"X\", \"R\"))]\n\nggplot(data = mse_data_1) + \n  geom_histogram(aes(x = mse)) +\n  facet_grid(learner ~ .) +\n  theme_bw()\n\n\n\n\n\nFigure 12.1: Density of Log MSE of estimating CATE by method\n\n\n\n\nAs you can see, R-learner performs the best, followed closely by X-learner, then by S-learner, and T-learner.\nNow, we change the value of \\(\\alpha\\) to 10 from 1 to make the nuisance part have a much larger share in \\(Y\\)’s variation.\n\nalpha <- 10\nmc_results <-\n  lapply(\n    seq_len(100),\n    function(i) get_mse(i, gen_data_A, alpha)\n  ) %>% \n  rbindlist(idcol = \"sim\")\n\n\n\n\n\n\n\nFigure 12.2 shows the histogram of MSE by learner for \\(\\alpha=1\\) and \\(\\alpha=10\\). All the methods are negatively affected by the increase in the influence of the nuisance function, \\(g(X)\\). However, some learners are affected more than others. R-learner is. much less affected by the change than the other methods, and R-learner is clearly the best performing learner at \\(\\alpha = 10\\). All the other learners performed considerably poorer compared to the case with \\(\\alpha =1\\). This shows that R-learner shines particularly when the treatment effect is only the small portion of the total variation in \\(Y\\). This is a very important property because it is often the case for many scientific fields. For example, consider estimating the impact of a vocational training program on income. Such a program is unlikely to drastically change participants income level. Other factors that have nothing to do with the program (nuisance part) are likely to have much bigger role in determining income.\n\n\nCode\nmse_data_10 <-\n  mc_results_10 %>% \n  .[, .(mse = mean((theta_true - theta_hat)^2)), by = .(learner, sim)] %>%\n  .[, type := \"alpha = 10\"]\n\nmse_data_all <- \n  rbind(mse_data_1, mse_data_10) %>% \n  .[, learner := factor(learner, levels = c(\"S\", \"T\", \"X\", \"R\"))]\n\nggplot(data = mse_data_all) + \n  geom_histogram(\n    aes(x = mse, fill = type), \n    position = \"identity\", \n    alpha = 0.5,\n    bins = 75\n  ) +\n  facet_grid(learner ~ .) +\n  scale_fill_discrete(name = \"\") +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 12.2: Density of Log MSE of estimating CATE by method\n\n\n\n\nFigure 12.3 plots the true (y-axis) and estimated (x-axis) treatment effect by learner for a single iteration at \\(\\alpha = 10\\), which gives us insights into the decomposition of MSE (variance and bias).\n\n\nCode\nggplot(data = mc_results_10[sim == 1, ]) + \n  geom_point(aes(y = theta_true, x = theta_hat)) +\n  geom_abline(slope = 1, color = \"red\") +\n  facet_grid(learner ~ .) +\n  theme_bw() +\n  xlab(\"Estimated Treatment Effect\") +\n  ylab(\"True Treatment Effect\")\n\n\n\n\n\nFigure 12.3: True v.s. estimated treatment effects by learner for a single iteration (\\(\\alpha\\) = 10)\n\n\n\n\nAccording to the figure, all of them seem to suffer from positive bias. Here is the average of the true treatment effects less the estimated treatment effects by learner. So, indeed, they all suffer from bias. T-learner suffers from the most severe bias, and R-learner suffers from the smallest bias.\n\nmc_results_10[, .(bias = mean(theta_true - theta_hat)), by = learner]\n\n   learner       bias\n1:       R -0.1312763\n2:       S -0.5830240\n3:       X -0.2645536\n4:       T -0.7078119\n\n\nT-learner has the highest variance of treatment effect estimates, followed by S-learner, X-learner, and then R-learner. Here is the average (over iterations) standard deviation of treatment effect estimates by learner.\n\nmc_results_10[, .(sd = sd(theta_true - theta_hat)), by = .(learner, sim)] %>% \n  .[, .(sd = mean(sd)), by = learner]\n\n   learner        sd\n1:       R 0.2839190\n2:       S 0.8823216\n3:       X 0.6298141\n4:       T 1.2539635\n\n\nThe problem with high variance in CATE estimation is that, the effect of treatment “looks” much more heterogeneous than it truly is. This leads to over-estimation of the benefit of targeted treatment (e.g., policy, medical treatment) assignment.\n\nLet’s look at another DGP.\n\n\n\n\n\n\nDGP B (randomized trial)\n\n\n\n\\[\n\\begin{aligned}\ng(X_i) & = max(X_{i,1} + X_{i,2}, X_{i,3}, 0) + max(X_{i,4}+ X_{i,5},0)\\\\\ne(X_i) & = 1/2 \\\\\n\\theta(X_i) & = X_{i,1} + log(1+exp(X_{i,2})) \\\\\nX_i & \\sim N(0,I_5)\n\\end{aligned}\n\\]\n\n\nHere is the code to generate data according to DGP B.\n\n\nCode\ngen_data_B <- function(N, alpha){\n  data <-\n    data.table(\n      x1 = rnorm(N),\n      x2 = rnorm(N),\n      x3 = rnorm(N),\n      x4 = rnorm(N),\n      x5 = rnorm(N),\n      u = rnorm(N)\n    ) %>% \n    .[, `:=`(\n      g_x = alpha * (pmax(x1 + x2, x3) + pmax(x4 + x5, 0)),\n      e_x = 1/2,\n      theta_x = x1+log(1 + exp(x2))\n    )] %>% \n    .[, t := as.numeric(runif(N) < e_x)] %>% \n    .[, y := theta_x * t + g_x + u]\n\n  return(data)\n}\n\n\n\n\n\n\n\n\nHere is the code to run MC simulations for \\(\\alpha = 1\\) and \\(\\alpha = 10\\).\n\nmc_results_1 <-\n  future_lapply(\n    seq_len(100),\n    function(i) get_mse(i, gen_data_B, alpha = 1)\n  ) %>% \n  rbindlist(idcol = \"sim\")\n\nmc_results_10 <-\n  future_lapply(\n    seq_len(100),\n    function(i) get_mse(i, gen_data_B, alpha = 10)\n  ) %>% \n  rbindlist(idcol = \"sim\")\n\nmse_data_1 <-\n  mc_results_1 %>% \n  .[, .(mse = mean((theta_true - theta_hat)^2)), by = .(learner, sim)] %>%\n  .[, type := \"alpha = 1\"]\n\nmse_data_10 <-\n  mc_results_10 %>% \n  .[, .(mse = mean((theta_true - theta_hat)^2)), by = .(learner, sim)] %>%\n  .[, type := \"alpha = 10\"]\n\nmse_data_all <- rbind(mse_data_1, mse_data_10) \n\nFigure 12.4 presents the results. Compared to DGP A, S- and T-learner performs much better at \\(\\alpha = 1\\) almost matching that of X- and R-learner. However, once the role of nuisance function is greater at \\(\\alpha = 10\\), then the performance of S-, T-, and X-learner deteriorate substantially (especially T-learner).\n\n\nCode\nggplot(data = mse_data_all) + \n  geom_histogram(\n    aes(x = mse, fill = type), \n    position = \"identity\", \n    alpha = 0.5,\n    bins = 75\n  ) +\n  facet_grid(learner ~ .) +\n  scale_fill_discrete(name = \"\") +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 12.4: Density of Log MSE of estimating CATE by method\n\n\n\n\nFigure 12.5 presents the scatter plot of the true and estimated treatment effects by learner for a single iteration. None of them seem to be biased, but there is a clear difference in variance of CATE estimates.\n\n\nCode\nggplot(data = mc_results_10[sim == 1, ]) + \n  geom_point(aes(y = theta_true, x = theta_hat)) +\n  geom_abline(slope = 1, color = \"red\") +\n  facet_grid(learner ~ .) +\n  theme_bw() +\n  xlab(\"Estimated Treatment Effect\") +\n  ylab(\"True Treatment Effect\")\n\n\n\n\n\nFigure 12.5: True v.s. estimated treatment effects by learner for a single iteration (\\(\\alpha\\) = 10)"
  },
  {
    "objectID": "C02-xstr-learner.html#t-learner-v.s.-x-learner-optional-and-not-that-important",
    "href": "C02-xstr-learner.html#t-learner-v.s.-x-learner-optional-and-not-that-important",
    "title": "12  S-, X-, T-, and R-learner",
    "section": "12.6 T-learner v.s. X-learner (Optional, and not that important)",
    "text": "12.6 T-learner v.s. X-learner (Optional, and not that important)\nHere, an advantage of X-learner over T-learner is demonstrated. This example also serves as an illustration of how these learners are implemented. Specifically, X-learner can be advantageous when the control-treatment assignments in the sample are unbalanced. For example, it is often the case that there are plenty of observations in the control group, while there are not many treated observations. For the purpose of illustration, consider a rather extreme case where there are only 10 observations in the treated group, while there are 300 observations in the control group. We use the following toy data generating process:\n\\[\n\\begin{align}\ny = \\tau W + |x| + v\n\\end{align}\n\\]\nwhere \\(\\tau = 1\\). So, the treatment effect is not heterogeneous. For the purpose of illustrating the advantage of X-learner over T-learner, it is convenient if the underlying model is simpler.\n\nset.seed(4345)\n\nN_trt <- 20\nN_ctrl <- 300\nN <- N_trt + N_ctrl\n\ndata <- \n  data.table(\n    W = c(rep(1,N_trt), rep(0, N_ctrl)),\n    type = c(rep(\"Treated\", N_trt), rep(\"Control\", N_ctrl)),\n    x = 2 * runif(N)-1,\n    v = rnorm(N) / 2\n  ) %>% \n  .[, y := W + abs(x) + v]\n\n\nggplot(data = data) +\n  geom_point(aes(y = y, x = x, color = type)) +\n  theme_bw()\n\n\n\n\nLet’s first estimate \\(\\mu_1(X)\\) and \\(\\mu_0(X)\\) (Step 1). Since we have only \\(20\\) observations in the treated group, we will use a linear regression to avoid over-fitting (following the example in Künzel et al. (2019)).\n\nmu_1_trained <- lm(y ~ x, data = data[type == \"Treated\", ])\n\nmu_0_trained <- gam(y ~ s(x, k = 4), data = data[type == \"Control\", ])\n\nNow that \\(\\mu_1(X)\\) and \\(\\mu_0(X)\\) are estimated, we can estimate \\(\\hat{\\theta}(X)\\) by T-learner.\n\nx_seq <- data.table(x = seq(-1, 1, length = 100))\n#=== T-learner ===#\ntau_hat_data <- \n  x_seq %>%\n  .[, mu_1_hat := predict(mu_1_trained, newdata = x_seq)] %>%\n  .[, mu_0_hat := predict(mu_0_trained, newdata = x_seq)] %>%\n  .[, tau_hat_T := mu_1_hat - mu_0_hat]\n\nAs you can see, T-learner is heavily biased. This is because of the unreliable estimation of \\(\\mu_1(X)\\) due to lack of observations in the treated group.\n\n\nCode\nggplot(data = tau_hat_data) +\n  geom_line(aes(y = tau_hat_T, x = x)) +\n  theme_bw()\n\n\n\n\n\nNow, let’s move on to X-learner. We impute individual treatment effects (Step 2).\n\n#=== mu (treated) ===#\nmu_hat_1 <- predict(mu_0_trained, newdata = data[type == \"Treated\", ])\n\n#=== mu (control) ===#\nmu_hat_0 <- predict(mu_1_trained, newdata = data[type == \"Control\", ])\n\n#=== assign the values ===#\ndata[type == \"Treated\", mu_hat := mu_hat_1]\ndata[type == \"Control\", mu_hat := mu_hat_0]\n\n#=== find individual TE ===#\ndata[, D := ifelse(type == \"Treated\", y - mu_hat, mu_hat - y)]\n\nWe can now regress \\(D\\) on \\(X\\) (Step 3),\n\n#--------------------------\n# tau (treated)\n#--------------------------\ntau_1_trained <- lm(D ~ x, data = data[type == \"Treated\", ])\n\n#=== estimate tau_1 ===#\ntau_hat_data[, tau_hat_1 := predict(tau_1_trained, newdata = tau_hat_data)]\n\n#--------------------------\n# tau (control)\n#--------------------------\ntau_0_trained <- gam(D ~ s(x, k = 4), data = data[type == \"Control\", ])\n\n#=== estimate tau_1 ===#\ntau_hat_data[, tau_hat_0 := predict(tau_0_trained, newdata = tau_hat_data)]\n\n\n\nCode\nggplot(data = tau_hat_data) +\n  geom_line(aes(y = tau_hat_1, x = x, color = \"Treated\")) +\n  geom_line(aes(y = tau_hat_0, x = x, color = \"Control\")) +\n  scale_color_manual(values = c(\"Treated\" = \"blue\", \"Control\" = \"red\")) +\n  theme_bw()\n\n\n\n\n\nLet’s use propensity score as \\(g(X)\\) in Step 4.\n\nw_gam_trained <- \n  gam(\n    W ~ s(x, k = 4), \n    data = data, \n    family = binomial(link = \"probit\")\n  )\n\nLet’s predict \\(E[W|X]\\) at each value of \\(X\\) at which we are estiamting \\(\\tau\\).\n\ntau_hat_data[, g_x := predict(w_gam_trained, newdata = tau_hat_data, type = \"response\")]\n\nAs you can see below, the mean value of \\(g(x)\\) is small because the treatment probability is very low (it is only \\(20\\) out of \\(320\\)).\n\nmean(tau_hat_data[, g_x])\n\n[1] 0.06451538\n\n\nThis number is basically \\(20/320\\). So, in this example, we could have just used the proportion of the treated observations. Notice that \\(g(X)\\) is multiplied to \\(\\hat{\\theta}_0(X)\\) in Equation 12.2. So, we are giving a lower weight to \\(\\hat{\\theta}_0(X)\\). This is because \\(\\hat{\\theta}_0(X)\\) is less reliable because \\(\\hat{\\mu}_1(X)\\) is less reliable due to the lack of samples in the treated group.\n\ntau_hat_data[, tau_hat_X := g_x * tau_hat_0 + (1-g_x) * tau_hat_1]\n\nAs you can see, X-learner outperforms T-learner in this particular instance at least in terms of point estimates of \\(\\tau(X)\\).\n\n\nCode\nggplot(data = tau_hat_data) +\n  geom_line(aes(y = tau_hat_T, x = x, color = \"T-learner\")) +\n  geom_line(aes(y = tau_hat_X, x = x, color = \"X-learner\")) +\n  geom_hline(yintercept = 1, aes(color = \"True Treatment Effect\")) +\n  scale_color_manual(\n    values = c(\n        \"T-learner\" = \"red\", \n        \"X-learner\" = \"blue\", \n        \"True Treatment Effect\" = \"black\"\n      ),\n    name = \"\"\n    ) +\n  ylab(\"Treatment Effect\") +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. 2018. “Double/debiased machine learning for treatment and structural parameters.” The Econometrics Journal 21 (1): C1–68. https://doi.org/10.1111/ectj.12097.\n\n\nKünzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. 2019. “Metalearners for Estimating Heterogeneous Treatment Effects Using Machine Learning.” Proceedings of the National Academy of Sciences 116 (10): 4156–65. https://doi.org/10.1073/pnas.1804597116.\n\n\nNie, Xinkun, Alejandro Schuler, and Stefan Wager. 2022. Rlearner: R-Learner for Heterogeneous Treatment Effect Estimation.\n\n\nNie, Xinkun, and Stefan Wager. 2021. “Quasi-Oracle Estimation of Heterogeneous Treatment Effects.” Biometrika 108 (2): 299–319. https://doi.org/10.1093/biomet/asaa076."
  },
  {
    "objectID": "C03-cf-orf.html",
    "href": "C03-cf-orf.html",
    "title": "13  Forest-based CATE Estimators",
    "section": "",
    "text": "In Chapter 12, we saw some special cases of R-learner/DML where the final model is estimate by parametrically using a linear-in-parameter model and non-parametrically using extreme gradient boosting. Here, we learn two methods that estimate CATE non-parametrically: causal forest (Athey, Tibshirani, and Wager 2019) and orthogonal forest (Oprescu, Syrgkanis, and Wu 2019).\nLeft to be added + how variance is estimated + balance + ATE + linear correction with locally weighted linear regression"
  },
  {
    "objectID": "C03-cf-orf.html#model",
    "href": "C03-cf-orf.html#model",
    "title": "13  Forest-based CATE Estimators",
    "section": "13.1 Model",
    "text": "13.1 Model\nThe heterogeneous treatment effect model of interest in this chapter is the same as the one in Chapter 12.\n\\[\n\\begin{aligned}\nY & = \\theta(X)\\cdot T + g(X) + \\varepsilon \\\\\nT & = f(X) + \\eta\n\\end{aligned}\n\\tag{13.1}\\]\n\n\\(Y\\): dependent variable\n\\(T\\): treatment variable (can be either binary dummy or continuous)\n\\(X\\): features\n\nCausal forest and orthogonal random forest is consistent only if the following conditions fold.\n\n\\(E[\\varepsilon|X] = 0\\)\n\\(E[\\eta|X] = 0\\)\n\\(E[\\eta\\cdot\\varepsilon|X] = 0\\)"
  },
  {
    "objectID": "C03-cf-orf.html#causal-forest",
    "href": "C03-cf-orf.html#causal-forest",
    "title": "13  Forest-based CATE Estimators",
    "section": "13.2 Causal Forest",
    "text": "13.2 Causal Forest\n\n13.2.1 Brief description of how CF works\nThis section provides a brief and cursory description of how CF works. A much more detailed treatment of how CF (and also GRF) works along with the explanations of hyper-parameters is provided in Section 17.5.\nCausal Forest (CF) (as implemented by the R grf package or python econml package) is a special type of R-learner (also a DML) and also a special case of generalized random forest (GRF).\n\n\nCF can be implemented using the R grf package or python econml package. Both of them implements CF as an R-learner. However, the original causal forest proposed in Wager and Athey (2018) does not follow an R-learner procedure.\nLet \\(\\hat{f}(X_i)\\) and \\(\\hat{g}(X_i)\\) denote the estimation of \\(E[Y|X]\\) and \\(E[T|X]\\), respectively. Further, let \\(\\hat{\\tilde{Y_i}}\\) and \\(\\hat{\\tilde{T_i}}\\) denote \\(Y_i - \\hat{f}(X_i)\\) and \\(T_i - \\hat{g}(X_i)\\), respectively.\nThen, CF estimates \\(\\theta(X)\\) at \\(X = X_0\\) by solving the following equation:\n\\[\n\\begin{aligned}\n\\hat{\\theta}(X_0) = argmin_{\\theta}\\;\\;\\sum_{i=1}^N \\alpha_i(X_i, X_0)[\\hat{\\tilde{Y_i}} - \\theta\\cdot \\hat{\\tilde{T_i}}]^2\n\\end{aligned}\n\\tag{13.2}\\]\nwhere \\(\\alpha_i(X_i, X_0)\\) is the weight given to each \\(i\\). The F.O.C is\n\\[\n\\begin{aligned}\n2 \\cdot \\sum_{i=1}^N \\alpha_i(X_i, X_0)[\\hat{\\tilde{Y_i}} - \\theta\\cdot \\hat{\\tilde{T_i}}]\\hat{\\tilde{T_i}} = 0\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\hat{\\theta}(X_0) = \\frac{\\sum_{i=1}^N\\alpha_i(X_i, X_0)\\cdot\\hat{\\tilde{Y}}_i\\cdot \\hat{\\tilde{T}}_i}{\\sum_{i=1}^N\\alpha_i(X_i, X_0)\\cdot\\hat{\\tilde{T}}_i\\cdot \\hat{\\tilde{T}}_i}\n\\end{aligned}\n\\tag{13.3}\\]\nUnlike the DML approaches we saw in Chapter 12 that uses a linear model as the final model, CF does not assume any functional form for how \\(X\\) affects \\(\\theta\\) as you can see from the above minimization problem. As mentioned earlier, CF is a special case of GRF (discussed in Chapter 17), so CF is also a local constant regression (see Section 1.3.1 for a brief discussion on local regression).\n\\(\\alpha_i(X_i, X_0)\\) is determined based on the trees trained based on the pseudo outcomes that are defined specifically for causal forest estimation. Suppose \\(T\\) trees have been built and let \\(\\eta_{i,t}(X_i, X_0)\\) be 1 if observation \\(i\\) belongs to the same leaf as \\(X_0\\) in tree \\(t\\). Then,\n\\[\n\\begin{aligned}\n\\alpha_i(X_i, X_0) = \\frac{1}{T}\\sum_{t=1}^T\\frac{\\eta_{i,t}(X_i, X_0)}{\\sum_{i=1}^N\\eta_{i,t}(X_i, X_0)}\n\\end{aligned}\n\\tag{13.4}\\]\nSo, the weight given to observation \\(i\\) is higher if observation \\(i\\) belongs to the same leaf as the evaluation point \\(X_0\\) in more trees.\n\n\n13.2.2 Training a causal forest\nWe can use the causal_forest() function from the grf package to train a CF model in R. In Python, you can use CausalForestDML() from the econml package or GRFForestRegressor from the skgrf package.\nAs of now, there are some notable differences between grf and econml.\n\n\n\n\n\n\nSome differences between the grf and econml implementation of causal forest\n\n\n\n\nWhile grf support clustering, econml does not.\nIt is easy to modify the first stage estimations in econml. grf uses random forest by default for them. If you would like to try other ML methods, then you need to write a code to predict \\(Y - E[Y|X, W]\\) and \\(T - E[T|X, W]\\) with your choice of ML methods yourself (see Section 18.6 for an example code to do this).\ngrf does not cross-fit in the first-stage estimation. Rather, it uses out-of-bag prediction from the trained random forest models, which avoids over-fitting (Note that the motivation behind cross-fitting in DML is to avoid over-fitting bias in the second stage estimation. See Chapter 11).\ngrf does not make distinctions between \\(X\\) and \\(W\\).\n\n\n\n\n#=== load the Treatment dataset ===#\ndata(\"Treatment\", package = \"Ecdat\")\n\n#=== convert to a data.table ===#\n(\ndata <- \n  data.table(Treatment) %>% \n  #=== create an id variable ===#\n  .[, id := 1:.N]\n)\n\nHere are the variables in this dataset that we use.\n\nre78 (\\(Y\\)): real annual earnings in 1978 (after the treatment)\ntreat (\\(T\\)): TRUE if a person had gone through a training, FALSE otherwise.\n\n\\(X\\) includes\n\nre74: real annual earnings in 1978 (after the treatment)\nage: age\neduc: education in years\nethn: one of “other”, “black”, “hispanic”\nmarried: married or not\n\ngrf::causal_forest() takes only numeric values for \\(X\\). So, we will one-hot encode ethn, which is a factor variable at the moment.\n\n(\ndata_trt <- mltools::one_hot(data)\n)\n\n      treat age educ ethn_other ethn_black ethn_hispanic married    re74\n   1:  TRUE  37   11          0          1             0    TRUE     0.0\n   2:  TRUE  30   12          0          1             0   FALSE     0.0\n   3:  TRUE  27   11          0          1             0   FALSE     0.0\n   4:  TRUE  33    8          0          1             0   FALSE     0.0\n   5:  TRUE  22    9          0          1             0   FALSE     0.0\n  ---                                                                   \n2671: FALSE  47    8          1          0             0    TRUE 44667.4\n2672: FALSE  32    8          1          0             0    TRUE 47022.4\n2673: FALSE  47   10          1          0             0    TRUE 48198.0\n2674: FALSE  54    0          0          0             1    TRUE 49228.5\n2675: FALSE  40    8          1          0             0    TRUE 50940.9\n         re75     re78   u74   u75   id\n   1:     0.0  9930.05  TRUE  TRUE    1\n   2:     0.0 24909.50  TRUE  TRUE    2\n   3:     0.0  7506.15  TRUE  TRUE    3\n   4:     0.0   289.79  TRUE  TRUE    4\n   5:     0.0  4056.49  TRUE  TRUE    5\n  ---                                  \n2671: 33837.1 38568.70 FALSE FALSE 2671\n2672: 67137.1 59109.10 FALSE FALSE 2672\n2673: 47968.1 55710.30 FALSE FALSE 2673\n2674: 44221.0 20540.40 FALSE FALSE 2674\n2675: 55500.0 53198.20 FALSE FALSE 2675\n\n\nWe now have ethn_black, ethn_hispanic, and ethn_other from ethn. The model we are estimating is as follows:\n\\[\n\\begin{aligned}\nre78 & = \\theta(age, re74, educ, ethn\\_hipanic, ethn\\_black, married)\\cdot treat + g(age, re74, educ, ethn\\_hipanic, ethn\\_black, married) + \\varepsilon \\\\\ntreat & = f(age, re74, educ, ethn\\_hipanic, ethn\\_black, married) + \\eta\n\\end{aligned}\n\\]\n\n\n\n\n\n\nWarning\n\n\n\nNote that we are paying no attention to the potential endogeneity problem here. This is just a demonstration and the results are likely to be biased. We will look at instrumental forest later in ?sec-cf-extension, which is consistent under confoundedness as long as you can find appropriate external instruments.\n\n\nIn running causal_forest(), there are many hyper-parameters and options that we need to be aware of.\nSince CF is a GRF and GRF uses random forest algorithm on appropriate pseudo outcome, it is natural that some of the CF hyper-parameters are the same as the ones for RF (by ranger()).\n\nnum.trees: number of trees\nmtry: number of variables tried in each split (default is \\(\\sqrt{K}\\))\nmin.node.size: minimum number of observations in each leaf (default is 5)\n\nA hyper-parameter that certainly affects tree building process is sample.fraction, which we saw earlier.\n\nsample.fraction: fraction of the data used to build each tree (default is 0.5)\n\nA higher value of sample.fraction means that the trees are more correlated as they share more of the same observations.\nThere are three honesty-related options.\n\nhonesty: TRUE if using honest-sampling, 0 otherwise (default is TRUE)\nhonesty.fraction: fraction of the data (after sample.fraction is applied) that is grouped into \\(J_1\\), which is used to determine splitting rules\nhonesty.prune.leaves: TRUE if the leaves with no samples are pruned (default is TRUE)\n\nLet’s now train CF using data_trt.\n\ncf_trained <-\n  grf::causal_forest(\n    X = data_trt[, .(age, re74, educ, ethn_hispanic, ethn_black, married)] %>% as.matrix(),\n    Y = data_trt[, re78],\n    W = data_trt[, treat]\n  )\n\nHere is what cf_trained has as its attributes.\n\nnames(cf_trained)\n\n [1] \"_ci_group_size\"           \"_num_variables\"          \n [3] \"_num_trees\"               \"_root_nodes\"             \n [5] \"_child_nodes\"             \"_leaf_samples\"           \n [7] \"_split_vars\"              \"_split_values\"           \n [9] \"_drawn_samples\"           \"_send_missing_left\"      \n[11] \"_pv_values\"               \"_pv_num_types\"           \n[13] \"predictions\"              \"variance.estimates\"      \n[15] \"debiased.error\"           \"excess.error\"            \n[17] \"seed\"                     \"ci.group.size\"           \n[19] \"X.orig\"                   \"Y.orig\"                  \n[21] \"W.orig\"                   \"Y.hat\"                   \n[23] \"W.hat\"                    \"clusters\"                \n[25] \"equalize.cluster.weights\" \"tunable.params\"          \n[27] \"has.missing.values\"      \n\n\nFor example, you can get \\(\\theta(X_i)\\) by accessing the predictions attribute.\n\ncf_trained$predictions %>% head()\n\n          [,1]\n[1,]  687.1393\n[2,]  946.6872\n[3,]  187.3482\n[4,] 1569.0517\n[5,]  629.9950\n[6,] 1143.3927\n\n\n\n\n13.2.3 Predict and interpret CATE\nBefore looking at how to predict \\(\\theta(X)\\), let’s look at which variables are used to split tree nodes. You can get such information using split_frequencies() on a trained causal forest.\n\nsplit_frequencies(cf_trained)\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]  237 1514  101   45   32   67\n[2,] 1395 1064  679   32  152  272\n[3,] 1936  524 1097   25  196  201\n[4,] 1379  324  766    5   97   82\n\n\nIn this table, rows represent the depth of the nodes and columns represent covariates. For example, the second variable (educ) was used to do the first split 1514 times and split a node at the second depth 1064 times. A variable with higher numbers of splits at earlier stages is more influential in driving treatment effect heterogeneity. variable_importance() returns a measure of how important each variable is in explaining treatment effect heterogeneity based on the split information.\n\nvariable_importance(cf_trained)\n\n           [,1]\n[1,] 0.21236320\n[2,] 0.60044103\n[3,] 0.10291529\n[4,] 0.01797325\n[5,] 0.02413831\n[6,] 0.04216891\n\n\nSo, according to this measure, the second variable (educ) is the most important variable. While variable of importance measure is informative, it does not tell us how  the variables are affecting treatment effects. For that, we need to look at \\(\\theta(X)\\) at different values of \\(X\\).\nYou can use predict() to predict the treatment effect at \\(X\\). For example, consider the following evaluation point.\n\nX_0 <- \n  data.table(\n    age = 30,\n    re74 = 40000, \n    educ = 10,\n    ethn_hispanic = 0,\n    ethn_black = 1,\n    married = TRUE\n  )\n\nNote that the order of columns of the evaluation data must be the same as that of the X in causal_forest() when you trained a CF model. You can set estimate.variance to TRUE to get \\(var(\\hat{\\theta}(X))\\) along with the point estimate.\n\npredict(\n  cf_trained, \n  newdata = X_0, \n  estimate.variance = TRUE\n)\n\n  predictions variance.estimates\n1   -7512.412            2194242\n\n\nUnlike linear-in-parameter model, there are no coefficients that can immediately tell us how influential each of \\(X\\) is in driving the treatment effect heterogeneity. One way to see the impact of a variable is to change its value while the value of the rest of \\(X\\) is fixed. For example, for the given observed value of \\(X\\) except educ, you can vary the value of educ to see how educ affects the treatment effect. We can do this for all the observations and then can get a good picture of how the treatment effect varies across individuals at different values of educ.\nLet’s first create a sequence of educ values at which \\(\\hat{\\theta}\\) is predicted.\n\nage_seq <-\n  data.table(\n    age = data_trt[, seq(min(age), max(age), length = 30)]\n  )\n\nWe then create a dataset where every single individual (observation) in the original data data_trt to have all the age values in age_seq while the value of the rest of \\(X\\) fixed at their own values.\n\n\nConfirm what reshape::expand.grid.df does with this simple example.\n\nreshape::expand.grid.df(\n  data.table(a = c(1, 2, 3)), # first data set\n  data.table(b = c(1, 2), c = c(\"a\", \"b\")) # second data set\n)\n\n  a b c\n1 1 1 a\n2 2 1 a\n3 3 1 a\n4 1 2 b\n5 2 2 b\n6 3 2 b\n\n\n\ndata_te <- \n  reshape::expand.grid.df(\n    age_seq, \n    data_trt[, .(re74, educ, ethn_hispanic, ethn_black, married, id)]\n  ) %>% \n  data.table()\n\nLet’s now predict \\(\\hat{\\theta}\\) with their standard error estimates.\n\n(\ntheta_hat_with_se <- \n  predict(cf_trained, newdata = dplyr::select(data_te, -id), estimate.variance = TRUE) %>% \n  data.table() %>% \n  .[, se := sqrt(variance.estimates)] %>% \n  setnames(\"predictions\", \"theta_hat\") %>% \n  .[, .(theta_hat, se)]\n)\n\n        theta_hat        se\n    1:   454.7407 1014.1734\n    2:   472.2442  977.1713\n    3:   506.3854 1007.0151\n    4:   544.9027 1068.8522\n    5:   540.4566 1432.5713\n   ---                     \n80246: -5807.1101 2050.0903\n80247: -5795.1478 2180.4811\n80248: -5771.9151 2130.5489\n80249: -5770.8895 2154.5895\n80250: -5770.5312 2152.1874\n\n\nFigure 13.1 shows the impact of age on treatment effect for the first three individuals of data_trt. For example, if an individual that has the identical values for \\(X\\) except age and also this person is 40 years old, then the treatment effect of the training program would be about $1,000. Standard errors are fairly large and treatment effects are not statistically significantly different from 0 at any value of age for all three individuals. The impact of age seems to be very similar for all the individuals. However, you can see shifts in \\(\\hat{\\theta}\\) among them. Those shifts are due to the differences in other covariates.\n\nplot_data <- cbind(data_te, theta_hat_with_se)\n\nggplot(plot_data[id %in% 1:3, ]) +\n  geom_line(aes(y = theta_hat, x = age)) +\n  geom_ribbon(\n    aes(\n      ymin = theta_hat - 1.96 * se, \n      ymax = theta_hat + 1.96 * se, \n      x = age\n    ),\n    fill = \"blue\",\n    alpha = 0.4\n  ) +\n  facet_grid(. ~ id) +\n  theme_bw()\n\n\n\n\nFigure 13.1: Estimated treatment effect for the first three individuals at different values of the age variable\n\n\n\n\nFigure 13.2 shows the box-plot of treatment effects for all the individuals. Note that variations observed at each age value is due to heterogeneity in treatment effect driven by covariates other than age. It looks like the three individuals looked at are exceptions. For the majority of individuals, the estimated treatment effects are negative at any value of age.\n\nggplot(plot_data) +\n  geom_boxplot(aes(y = theta_hat, x = factor(round(age, digits = 2)))) +\n  theme_bw() +\n  xlab(\"Age\") +\n  ylab(\"Estimated treatment effect\") +\n  theme_bw() +\n  theme(\n    axis.text.x = element_text(angle = 90)\n  )\n\n\n\n\nFigure 13.2: Box-plot of the estimated treatment effects from all the individuals at different values of the age variable\n\n\n\n\nYou can easily repeat this analysis for other covariates to see their impacts as well.\n\n\n13.2.4 Hyper-parameter tuning"
  },
  {
    "objectID": "C03-cf-orf.html#orthogonal-random-forest",
    "href": "C03-cf-orf.html#orthogonal-random-forest",
    "title": "13  Forest-based CATE Estimators",
    "section": "13.3 Orthogonal Random Forest",
    "text": "13.3 Orthogonal Random Forest"
  },
  {
    "objectID": "C03-cf-orf.html#references",
    "href": "C03-cf-orf.html#references",
    "title": "13  Forest-based CATE Estimators",
    "section": "References",
    "text": "References\n\n\nAthey, Susan, Julie Tibshirani, and Stefan Wager. 2019. “Generalized Random Forests.” The Annals of Statistics 47 (2): 1148–78.\n\n\nOprescu, Miruna, Vasilis Syrgkanis, and Zhiwei Steven Wu. 2019. “Orthogonal Random Forest for Causal Inference.” In International Conference on Machine Learning, 4932–41. PMLR.\n\n\nWager, Stefan, and Susan Athey. 2018. “Estimation and Inference of Heterogeneous Treatment Effects Using Random Forests.” Journal of the American Statistical Association 113 (523): 1228–42. https://doi.org/10.1080/01621459.2017.1319839."
  },
  {
    "objectID": "C04-cf-extension.html#instrumental-forest",
    "href": "C04-cf-extension.html#instrumental-forest",
    "title": "14  Extensions of Causal Forest",
    "section": "14.2 Instrumental Forest",
    "text": "14.2 Instrumental Forest"
  },
  {
    "objectID": "C04-cf-extension.html#causal-forest-with-fixed-effects-panel-data",
    "href": "C04-cf-extension.html#causal-forest-with-fixed-effects-panel-data",
    "title": "14  Extensions of Causal Forest",
    "section": "14.3 Causal Forest with fixed effects (Panel Data)",
    "text": "14.3 Causal Forest with fixed effects (Panel Data)"
  },
  {
    "objectID": "C05-causal-model-selection.html",
    "href": "C05-causal-model-selection.html",
    "title": "15  Causal Model Selection",
    "section": "",
    "text": "What you will learn\n\n\n\n\nHow to tune hyper-parameters of a CATE model\nHow to select a CATE model\nModel selection can be done via cross-validated MSE of the outcome as the criteria when your goal is prediction. However, when your interest is in finding the best causal ML model, MSE of the outcome is clearly not an appropriate measure. Instead, Nie and Wager (2021) suggested the use of R-score. Let \\(\\tilde{Y}_i\\) and \\(\\tilde{T}_i\\) denote \\(Y_i - \\hat{f}(X_i)\\) and \\(T_i - \\hat{g}(X_i)\\), respectively, where \\(\\hat{f}(X_i)\\) and \\(\\hat{f}(X_i)\\) are the predicted values (preferably based on cross-fitting or out-of-bad predictions if forest-based estimation is used) of \\(Y_i\\) and \\(T_i\\) based on any appropriate machine learning methods in the first stage of DML. Further, let \\(\\hat{\\theta}(X)\\) denote CATE estimates by a CATE estimator (e.g., causal forest, X-learner)\nR-score is written as follows:\n\\[\n\\begin{aligned}\n\\sum_{i=1}^N [\\tilde{Y}_i - \\hat{\\theta}(X)\\cdot \\tilde{T}_i]^2\n\\end{aligned}\n\\]\nSo, this is just the objective function of the second stage estimation of R-learner (DML) without the regularlization term with CATE estimates plugged in. Nie and Wager (2021) suggested using cross-validated R-score to select the model for CATE estimation.\nDepending on whether you are selecting a model within the same model class (hyper-parameter tuning) or selecting a model among different classes of models (hyper-parameter tuning and model selection), procedures you follow are different."
  },
  {
    "objectID": "C05-causal-model-selection.html#hyper-parameter-tuning",
    "href": "C05-causal-model-selection.html#hyper-parameter-tuning",
    "title": "15  Causal Model Selection",
    "section": "15.1 Hyper-parameter tuning",
    "text": "15.1 Hyper-parameter tuning\n\n\n\n\n\n\nCausal model selection steps\n\n\n\nFor a given class of R-learner model you have chosen to use,\n\nImplement first stage estimations with cross-validation and calculate \\(\\tilde{Y}_i\\) and \\(\\tilde{X}_i\\).\nCreate a list of models with different hyper-parameter values under the same model class\nFor each of the models, find cross-validated R-score\nSelect the model with the lowest R-score\nTrain the model on the whole data with the hyper-parameter value set chosen above\n\n\n\nLet’s go thorough these processes using a simple example. We use the following DGP:\n\n\nThis is the same DGP as DGP A in Section 12.5.\n\\[\n\\begin{aligned}\nY_i & =\\theta(X_i)\\cdot T + \\alpha\\cdot g(X_i) + \\mu_i \\\\\nT_i & = Bernouli(f(X_i))\n\\end{aligned}\n\\]\n, where\n\\[\n\\begin{aligned}\ng(X_i) & = sin(\\pi X_{i,1}X_{i,2}) + 2(X_{i,3}-0.5)^2 + X_{i,4} + 0.5 X_{i,5}\\\\\nf(X_i) & = max(0.1, min(sin(\\pi X_{i,1}X_{i,2}), 0.9)) \\\\\n\\theta(X_i) & = (X_{i,1} + X_{i,2}) / 2 \\\\\nX_i & \\sim Uni(0,1)^5\n\\end{aligned}\n\\]\n\n\nCode\ngen_data_A <- function(N, alpha){\n  data <-\n    data.table(\n      x1 = runif(N),\n      x2 = runif(N),\n      x3 = runif(N),\n      x4 = runif(N),\n      x5 = runif(N),\n      u = rnorm(N)\n    ) %>% \n    .[, `:=`(\n      g_x = alpha * (sin(pi * x1*x2) + 2*(x3-0.5)^2 + x4 + 0.5*x5),\n      f_x = pmax(0.1, pmin(sin(pi * x1*x2), 0.9)),\n      theta_x = (x1+x2)/2\n    )] %>% \n    .[, t := as.numeric(runif(N) < f_x)] %>% \n    .[, y_det := theta_x * t + g_x] %>% \n    .[, y := y_det + u] %>% \n    .[, id := 1:.N]\n\n  return(data[])\n}\n\n\nLet’s create a dataset according to the DGP (unfold the Code chunk above to see how gen_data_A() is defined).\n\nset.seed(78243)\n\n(\ndata <- gen_data_A(N = 3000, alpha = 1)\n)\n\n              x1        x2        x3          x4        x5           u\n   1: 0.05588738 0.5024586 0.3188849 0.141067147 0.5890057  0.35256999\n   2: 0.58004369 0.1415465 0.8959332 0.659352141 0.1904962  1.43250418\n   3: 0.41380275 0.1742095 0.6409846 0.006009018 0.1884889 -0.29099820\n   4: 0.45570715 0.4409627 0.8932784 0.734628277 0.6819363 -0.97072123\n   5: 0.06816778 0.6043697 0.3343660 0.272298360 0.4449831 -1.11073251\n  ---                                                                 \n2996: 0.36169880 0.6029558 0.6076527 0.597417750 0.1641424 -0.15304283\n2997: 0.44764874 0.5301874 0.5056252 0.410082290 0.4279445 -0.01657616\n2998: 0.85780794 0.1803200 0.1772361 0.390234048 0.3423721 -0.11284575\n2999: 0.33223800 0.6996797 0.9862959 0.034219362 0.9366175 -0.27970867\n3000: 0.22820071 0.0450673 0.2772332 0.188920311 0.7176262 -0.76964159\n            g_x       f_x   theta_x t     y_det           y   id\n   1: 0.5892803 0.1000000 0.2791730 0 0.5892803  0.94185028    1\n   2: 1.3232106 0.2550841 0.3607951 1 1.6840057  3.11650986    2\n   3: 0.3645481 0.2245413 0.2940061 0 0.3645481  0.07354994    3\n   4: 1.9751290 0.5901968 0.4483349 1 2.4234640  1.45274272    4\n   5: 0.6787271 0.1290680 0.3362688 0 0.6787271 -0.43200537    5\n  ---                                                           \n2996: 1.3354524 0.6327852 0.4823273 0 1.3354524  1.18240955 2996\n2997: 1.3025441 0.6784263 0.4889181 1 1.7914622  1.77488607 2997\n2998: 1.2368142 0.4670410 0.5190640 0 1.2368142  1.12396848 2998\n2999: 1.6425852 0.6670896 0.5159589 0 1.6425852  1.36287652 2999\n3000: 0.6792872 0.1000000 0.1366340 0 0.6792872 -0.09035435 3000\n\n\n\n Step 1 :\nWe use random forest implemented by regression_forest() and probability_forest() by the grf package to estimate \\(E[Y|X]\\) and \\(E[T|X]\\), respectively. For the sake of space and simplicity, we will not conduct cross-validation to tune hyper-parameters for these models in this example (hyper-parameter tuning via cross-validation of the first-stage estimation is covered in Chapter 20).\n\nEstimate \\(E[Y|X]\\) and calculate \\(\\tilde{Y}\\)\n\n\nrf_trained_y <-\n  regression_forest(\n    X = data[, .(x1, x2, x3, x4, x5)],\n    Y = data[, y]\n  )\n\n#=== out-of-bag prediction of Y ===#\ndata[, y_hat := rf_trained_y$predictions]\n\n#=== calculate y_hat ===#\ndata[, y_tilde := y - y_hat]\n\n\n\n\n\nEstimate \\(E[T|X]\\) and calcualte \\(\\tilde{T}\\)\n\n\nrf_trained_t <-\n  probability_forest(\n    X = data[, .(x1, x2, x2, x3, x4, x5)],\n    Y = data[, factor(t)],\n  )\n\n#=== out-of-bag prediction of T ===#\ndata[, t_hat := rf_trained_t$predictions[, 2]]\n\n#=== calculate t_hat ===#\ndata[, t_tilde := t - t_hat]\n\n\n\n\n\n Steps 2 and 3:\nSuppose we have determined that we use causal forest for the second stage CATE estimation. Here is the list of hyper-parameter value sets we will examine in this example.\n\n(\npar_data <-\n  expand.grid(\n    mtry = c(2, 5),\n    min.node.size = c(5, 10, 20),\n    sample.fraction = c(0.4, 0.5)\n  ) %>% \n  data.table()\n)\n\n    mtry min.node.size sample.fraction\n 1:    2             5             0.4\n 2:    5             5             0.4\n 3:    2            10             0.4\n 4:    5            10             0.4\n 5:    2            20             0.4\n 6:    5            20             0.4\n 7:    2             5             0.5\n 8:    5             5             0.5\n 9:    2            10             0.5\n10:    5            10             0.5\n11:    2            20             0.5\n12:    5            20             0.5\n\n\nFor each of the parameter sets, we will find a cross-validated R-score. We use 5-fold cross validation repeated 3 times.\n\n(\ndata_folds <- vfold_cv(data, v = 5, repeats = 3) \n)\n\n#  5-fold cross-validation repeated 3 times \n# A tibble: 15 × 3\n   splits             id      id2  \n   <list>             <chr>   <chr>\n 1 <split [2400/600]> Repeat1 Fold1\n 2 <split [2400/600]> Repeat1 Fold2\n 3 <split [2400/600]> Repeat1 Fold3\n 4 <split [2400/600]> Repeat1 Fold4\n 5 <split [2400/600]> Repeat1 Fold5\n 6 <split [2400/600]> Repeat2 Fold1\n 7 <split [2400/600]> Repeat2 Fold2\n 8 <split [2400/600]> Repeat2 Fold3\n 9 <split [2400/600]> Repeat2 Fold4\n10 <split [2400/600]> Repeat2 Fold5\n11 <split [2400/600]> Repeat3 Fold1\n12 <split [2400/600]> Repeat3 Fold2\n13 <split [2400/600]> Repeat3 Fold3\n14 <split [2400/600]> Repeat3 Fold4\n15 <split [2400/600]> Repeat3 Fold5\n\n\nThe following function find R-score for a given fold and parameter set.\n\nget_cv_rscore_np <- function(n, parameters) {\n\n  training_data <- analysis(data_folds[n, ]$splits[[1]])\n  eval_data <- assessment(data_folds[n, ]$splits[[1]])\n\n  #=== train a CF model on training data ===#\n  cf_trained <-\n    causal_forest(\n      X = training_data[, .(x1, x2, x3, x4, x5)],\n      Y = training_data[, y],\n      W = training_data[, t],\n      Y.hat = training_data[, y_hat],\n      W.hat = training_data[, t_hat],\n      mtry = parameters[, mtry],\n      min.node.size = parameters[, min.node.size],\n      sample.fraction = parameters[, sample.fraction]\n    )\n\n  theta_hat <- predict(cf_trained, eval_data[, .(x1, x2, x3, x4, x5)])\n\n  rscore <- eval_data[, sum((y_tilde - theta_hat * t_tilde)^2)]\n\n  return_data <-\n    data.table(\n      rscore = rscore,\n      fold = n\n    ) %>% \n    cbind(., parameters)\n\n  return(return_data)\n}\n\nThe following function calculates R-score for all the folds for a given parameter set.\n\nget_cv_rscore <- function(parameters) {\n  lapply(\n    seq_len(nrow(data_folds)),\n    function(n) get_cv_rscore_np(n, parameters)\n  ) %>% \n  rbindlist()\n}\n\nFor example, for the parameter set at the first row of par_data,\n\nget_cv_rscore(par_data[1, ])\n\n      rscore fold mtry min.node.size sample.fraction\n 1: 609.5867    1    2             5             0.4\n 2: 625.8628    2    2             5             0.4\n 3: 642.8026    3    2             5             0.4\n 4: 592.2378    4    2             5             0.4\n 5: 623.8983    5    2             5             0.4\n 6: 658.4454    6    2             5             0.4\n 7: 629.8173    7    2             5             0.4\n 8: 603.9405    8    2             5             0.4\n 9: 595.9709    9    2             5             0.4\n10: 606.0262   10    2             5             0.4\n11: 609.2626   11    2             5             0.4\n12: 637.8631   12    2             5             0.4\n13: 651.3631   13    2             5             0.4\n14: 582.6500   14    2             5             0.4\n15: 619.5787   15    2             5             0.4\n\n\nRepeat this for all the rows of par_data,\n\n(\ncv_rscore <-\n  lapply(\n    seq_len(nrow(par_data)),\n    function(x) get_cv_rscore(par_data[x, ])\n  ) %>% \n  rbindlist()\n)\n\n       rscore fold mtry min.node.size sample.fraction\n  1: 609.3074    1    2             5             0.4\n  2: 625.9558    2    2             5             0.4\n  3: 642.9224    3    2             5             0.4\n  4: 591.9167    4    2             5             0.4\n  5: 623.8610    5    2             5             0.4\n ---                                                 \n176: 609.3327   11    5            20             0.5\n177: 637.5821   12    5            20             0.5\n178: 652.7371   13    5            20             0.5\n179: 582.7777   14    5            20             0.5\n180: 620.8192   15    5            20             0.5\n\n\nTaking the mean of R-score by parameter set,\n\n(\nrscore <- cv_rscore[, .(rscore = mean(rscore)), by = .(mtry, min.node.size, sample.fraction)]\n)\n\n    mtry min.node.size sample.fraction   rscore\n 1:    2             5             0.4 619.2907\n 2:    5             5             0.4 619.2949\n 3:    2            10             0.4 619.4912\n 4:    5            10             0.4 619.5872\n 5:    2            20             0.4 619.8484\n 6:    5            20             0.4 619.9799\n 7:    2             5             0.5 619.3572\n 8:    5             5             0.5 619.5302\n 9:    2            10             0.5 619.4132\n10:    5            10             0.5 619.5597\n11:    2            20             0.5 619.7925\n12:    5            20             0.5 619.9107\n\n\n\n Steps 4 and 5 :\nAnd, the best parameter set is\n\n(\nbest_par_cf <- rscore[which.min(rscore), ]\n)\n\n   mtry min.node.size sample.fraction   rscore\n1:    2             5             0.4 619.2907\n\n\nWe now train a CF on the entire dataset.\n\ncf_trained <-\n  causal_forest(\n    X = data[, .(x1, x2, x3, x4, x5)],\n    Y = data[, y],\n    W = data[, t],\n    Y.hat = data[, y_hat],\n    W.hat = data[, t_hat],\n    mtry = best_par_cf[, mtry],\n    min.node.size = best_par_cf[, min.node.size],\n    sample.fraction = best_par_cf[, sample.fraction]\n  )\n\nWe now use this trained model to predict \\(\\theta(X)\\).\n\n\n\n\n\n\nTip\n\n\n\nNote that causal_forest() lets you tune hyper-parameters using out-of-bag R-score internally, so you do not need to follow the process here in practice. See Section 13.2.4 for how to specify tuning options for causal_forest().\n\n\nThough, not necessary, we can find an \\(R^2\\)-like score by contrasting the R-score of the trained model against the R-score based on a constant (non-heterogeneous) treatment effect estimate. Constant treatment effect can be obtained by solving the following equation with respect to \\(\\theta\\).\n\\[\n\\begin{aligned}\n\\sum_{i=1}^N \\tilde{Y}_i - \\theta\\cdot \\tilde{T}_i = 0\n\\end{aligned}\n\\]\n\ntheta_c <- data[, sum(y_tilde)/sum(t_tilde)]\n\nR-score associated with theta_c is\n\n(\nrscore_base <- data[, sum((y_tilde - theta_c * t_tilde)^2)]\n)\n\n[1] 3764.594\n\n\nR-score for the trained CF model is\n\n(\nrscore_cf <- data[, sum((y_tilde - cf_trained$predictions * t_tilde)^2)]\n)\n\n[1] 3095.919\n\n\nThe score of how well a CATE model performs in capturing the heterogeneity of treatment effect can be calculated as\n\n1 - rscore_cf/rscore_base\n\n[1] 0.1776221\n\n\nAs you will see in Section 23.2, this is the score econml.score.RScorer returns when evaluating CATE models.\nBy the way, Figure 15.1 shows the quality of CATE estimates by the trained CF model.\n\n\nCode\nggplot() +\n  geom_point(aes(y = data[, theta_x], x = cf_trained$predictions), size = 0.6) +\n  geom_abline(slope = 1, color = \"red\") +\n  theme_bw() +\n  ylab(\"True Treatmente Effect\") +\n  xlab(\"Estimated Treatmente Effect\")\n\n\n\n\n\nFigure 15.1: Quality of CATE estimates by CF"
  },
  {
    "objectID": "C05-causal-model-selection.html#selecting-from-multiple-classes-of-models",
    "href": "C05-causal-model-selection.html#selecting-from-multiple-classes-of-models",
    "title": "15  Causal Model Selection",
    "section": "15.2 Selecting from multiple classes of models",
    "text": "15.2 Selecting from multiple classes of models\nIf you are comparing models when they are all under the R-learner class, selecting a model is rather straight forward. You can simply repeat the hyper-parameter tuning procedures presented above for each model, find the best model (hyper-parameter values) for each class, and then find the best model among all the best (within each class) models.\nLet’s say you are also considering an R-learner in addition to CF presented above, where \\(\\theta(X)\\) is assumed to be a linear-in-parameter model of \\(X\\).\n\\[\n\\begin{aligned}\n\\theta(X) = \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_4 + \\beta_5 x_5\n\\end{aligned}\n\\]\nYou are considering using LASSO for the second stage estimation. So, you are minimizing the following objective function.\n\\[\n\\begin{aligned}\nMin_{\\beta_1,\\dots, \\beta_5}\\sum_{i=1}^N [\\tilde{Y}_i - (\\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\beta_3 x_{i,3} + \\beta_4 x_{i,4} + \\beta_5 x_{i,5})\\cdot \\tilde{T}_i]^2 + \\lambda |\\beta|\n\\end{aligned}\n\\]\nwhere \\(\\lambda\\) is the penalization parameter and \\(|\\beta|\\) is the L1 norm.\nHere is the list of \\(\\lambda\\) values at which we evaluate LASSO as the second stage model.\n\nlambda_ls <- c(0.001, 0.01, 0.02, 0.05, 0.1, 0.25, 0.5, 1)\n\nWe now conduct cross-validation to find the R-score for each value of lambda_ls. It is important to use the same \\(\\tilde{Y}\\) and \\(\\tilde{T}\\) data as the one used for CF. For that, we can simply use data_folds.\nThe following function trains a LASSO for a given value of \\(\\lambda\\) and return cross-validated R-score.\n\nget_cv_rscore_np_lasso <- function(n, lambda) {\n\n  training_data <- analysis(data_folds[n, ]$splits[[1]])\n  eval_data <- assessment(data_folds[n, ]$splits[[1]])\n\n  eval_data_mat <- \n    eval_data[, .(x1, x2, x3, x4, x5)] %>%\n    as.matrix()\n\n  #=== train a CF model on training data ===#\n  lasso_trained <-\n    glmnet(\n      x = training_data[, .(x1 * t_tilde, x2 * t_tilde, x3 * t_tilde, x4 * t_tilde, x5* t_tilde)],\n      y = training_data[, y_tilde],\n      lambda = lambda\n    )\n\n  #=== predict theta(X) ===#\n  # note, you need to predict on X, not X *\\ tilde{T} as you are interested in theta(X), not \n  # theta(X) * \\tilde{T}\n  theta_hat <- \n    predict(\n      lasso_trained, \n      newx = eval_data_mat\n    )\n\n  rscore <- eval_data[, sum((y_tilde - theta_hat * t_tilde)^2)]\n\n  return_data <-\n    data.table(\n      rscore = rscore,\n      fold = n,\n      lambda = lambda\n    ) \n\n  return(return_data)\n}\n\nThe following function calculates R-score for all the folds for a given value of \\(\\lambda\\).\n\nget_cv_rscore_lasso <- function(lambda) {\n  lapply(\n    seq_len(nrow(data_folds)),\n    function(n) get_cv_rscore_np_lasso(n, lambda)\n  ) %>% \n  rbindlist()\n}\n\nFor example, for the first value of \\(\\lambda\\) in lambda_ls,\n\nget_cv_rscore_lasso(lambda_ls[1])\n\n      rscore fold lambda\n 1: 604.9262    1  0.001\n 2: 629.5414    2  0.001\n 3: 641.6540    3  0.001\n 4: 593.4603    4  0.001\n 5: 623.4632    5  0.001\n 6: 656.1911    6  0.001\n 7: 630.6406    7  0.001\n 8: 604.0292    8  0.001\n 9: 592.2011    9  0.001\n10: 608.1991   10  0.001\n11: 610.2925   11  0.001\n12: 635.8894   12  0.001\n13: 647.7679   13  0.001\n14: 581.4791   14  0.001\n15: 621.1256   15  0.001\n\n\nRepeat this for all the rows of par_data, find the mean R-score by \\(\\lambda\\), and find the best \\(\\lambda\\) value.\n\n(\ncv_rscore_lasso <-\n  lapply(\n    lambda_ls,\n    function(x) get_cv_rscore_lasso(x)\n  ) %>% \n  rbindlist() %>%\n  .[, .(rscore = mean(rscore)), by = lambda] %>% \n  .[which.min(rscore), ]\n)\n\n   lambda   rscore\n1:   0.01 618.3748\n\n\nRemember the R-score associated with the best hyper-parameter values for CF is\n\nbest_par_cf\n\n   mtry min.node.size sample.fraction   rscore\n1:    2             5             0.4 619.2907\n\n\nSo, in this case, we should go for LASSO with \\(\\lambda = 0.01\\).\nLet’s train LASSO with \\(\\lambda = 0.01\\) using the whole dataset.\n\nlasso_trained <-\n  glmnet(\n    x = data[, .(x1 * t_tilde, x2 * t_tilde, x3 * t_tilde, x4 * t_tilde, x5* t_tilde)],\n    y = data[, y_tilde],\n    lambda = cv_rscore_lasso[, lambda]\n  )\n\ncoef(lasso_trained)\n\n6 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s0\n(Intercept) -0.0008596708\nV1           0.3834599495\nV2           0.5372735528\nV3           .           \nV4           0.1326269912\nV5           ."
  },
  {
    "objectID": "C05-causal-model-selection.html#references",
    "href": "C05-causal-model-selection.html#references",
    "title": "15  Causal Model Selection",
    "section": "References",
    "text": "References\n\n\nNie, Xinkun, and Stefan Wager. 2021. “Quasi-Oracle Estimation of Heterogeneous Treatment Effects.” Biometrika 108 (2): 299–319. https://doi.org/10.1093/biomet/asaa076."
  },
  {
    "objectID": "E01-spatial-cv.html",
    "href": "E01-spatial-cv.html",
    "title": "16  Spatial Cross-validation",
    "section": "",
    "text": "K-fold and repeated K-fold cross-validation methods can under-estimate test MSE (that is how good the trained model is in predicting \\(y\\) in a new dataset) when the observations in the train data are not independent with each other. A typical example would be spatial correlation of the error term. In many applications including environmental and agricultural events, spatially correlated error is commonly observed. In order to combat the problem of test MSE underestimation by regular K-fold cross-validation (hereafter, simply KCV), you can use spatial K-fold cross-validation (SKCV) instead.\nFor this section, we will consider the following data generating process.\n\\[\n\\begin{aligned}\ny = \\alpha + \\beta_1 x + \\beta_2 x^2 + u\n\\end{aligned}\n\\]\nwhere \\(x\\) and \\(u\\) are spatially correlated (which makes \\(y\\) spatially correlated).\nHere is the data we are going to work with (see the side note for the code to define gen_data, which takes a seed value and generate a spatial dataset):\nWe have three main variables, y (dependent variable), x (explanatory variable), and e (error). Figure 16.1 shows how they are spatially distributed. It also shows that all of them are spatially positively correlated.\nWe are going to use gam() with k \\(= 30\\) and sp \\(= 0\\) as the model in conducting KCV and spatial KCV. Let’s first create folds for KCV and SKCV. First, here is KCV folds.\nFigure 16.2 is the visualization of the spatial distribution of training and test datasets for each of the five folds for KCV.\nNow, let’s create a five spatially clustered folds using the spatialsample package for SKCV.\nFigure 16.3 presents the spatial distribution of training and test datasets for each of the five folds for SKCV.\nLet’s now implement KCV and SKCV. Since we observe the true generating process, we can calculate how good the fitted curve is compared to true \\(E[y|X]\\) in addition to observed \\(y\\) for the left-out samples in each fold.\nYou can see that MSE values (mse_obs) are mostly greater and also more variable for SKCV. By averaging MSE over folds by CV type,\nSo, indeed KCV provides lower estimate of test MSE than SKCV. Now, it is important to recognize the fundamental difference in what is measured by KCV and SKCV. KCV measures the accuracy of the trained model applied to the new data points that are located inside the area where the train data covers geographically. SKCV, on the other hand, measures the accuracy of the trained model applied to the new data points that are  outside  the area where the train data covers geographically. In other words, it measures the modeling accuracy when the trained model is applied to a new region. So, KCV does overstate the accuracy of the model if your interest is applying the model to a new region.\nLet’s look into the SKCV results a bit more. Looking at MSE values by fold for KCV and SKCV, you can notice that MSE is much more variable for SKCV. Figure 16.4 plots data points in the test data and the curve fitted on the training data by cross-validation type and fold (Note that Fold1 for KCV and SKCV have nothing to do with each other). Since KCV uses randomly split data, all the KCV test datasets are scattered all across the area and they look similar to each other. On the other hand, SKCV test datasets look quite different from each other as they are sampled in a spatially clustered manner. If the original data has no spatial correlation, then SKCV and KCV would have resulted in very simple test datasets. However, since the original data is spatially positively correlated, one packet of a field may look very different from another pocket of the field. By looking at Figure 16.4, you can easily see that FOLD2 and FOLD5 have very high MSE for SKCV. This observation is confirmed with the MSE value by fold presented above.\nThis is primarily because of the data imbalance between the training and test datasets, in particular the error term in this case. Figure 16.5 compares the distribution of the error term for the training and test datasets by fold for SKCV. As you can see, their distributions are very different for FOLD2 and FOLD5. In FOLD2, the average of the error term for the training data is much higher than that for the test data. The opposite is true for FOLD5. These differences of course result in large differences in the average value of the dependent variable. This, then, further results in consistent under or over-estimation of the level of the dependent variable. Since squared error is defined as \\([y_i -\\hat{f}(x_i)]^2\\), the consistent bias (you can consider this as bias in intercept in the linear-in-parameter modeling setting) will naturally lead to a higher MSE. That is exactly what is observed for FOLD2 and FOLD5.\nWhile MSE values for FOLD2 and FOLD5 are high in SKCV, notice that their fitted curves are very much similar to the other folds and trace the true \\(E[y|x]\\) quite well. The culprit of their high MSEs is the consistent difference in the average value of \\(y\\) between the training and test datasets. This means that if your interest is in understanding the causal impact of \\(x\\) on \\(y\\), then the fitted curves for FOLD2 and FOLD5 are just as good as those for the other folds despite the fact that their MSE values are much higher than the rest. This illustrates well that MSE is not really a good criteria to rely on if you ultimate interest is in estimating the causal impact of a treatment even though it is a good measure if your ultimate interest in prediction (predicting the  level  of the dependent variable).\nFinally, whether KCV is over-optimistic in estimating test MSE than SKCV or not is of little importance for those who are interested in causal inference because how well the trained model predicts the  level  of the dependent variable (which MSE measures) is irrelevant. Rather, more critical question is whether the use of KCV leads us to choose sub-optimal hyper parameter values compared to SKCV. This could have a real consequence when we select a model and its hyper-parameter values for the first-stage ML applications in Double Machine Learning methods."
  },
  {
    "objectID": "E01-spatial-cv.html#hyper-parameter-tuning",
    "href": "E01-spatial-cv.html#hyper-parameter-tuning",
    "title": "16  Spatial Cross-validation",
    "section": "16.1 Hyper-parameter tuning",
    "text": "16.1 Hyper-parameter tuning\nNow, let’s see whether KCV and SKCV lead to different tuning results using gam(). Here, we fix k at \\(50\\) and vary sp to find the best sp value. The following function takes an sp value and return MSE values from both KCV and SKCV.\n\nget_mse <- function(sp, kcv_folds, skcv_folds) {\n  cv_results <-\n    rbind(kcv_folds, skcv_folds) %>% \n    #=== make it possible to apply function row by row ===#\n    rowwise() %>% \n    #=== get mse ===#\n    mutate(mse_data = list(\n      assessment(splits) %>% \n      data.table() %>% \n      .[, y_hat := predict( # predict y based on the fitted model\n        #=== train the model ===#\n        gam(y ~ s(x, k = 50, sp = sp), data = analysis(splits)), \n        newdata = .)\n      ] %>% \n      .[, .(\n        mse_obs = mean((y - y_hat)^2)\n      )]\n    )) %>% \n    dplyr::select(id, type, mse_data) %>% \n    unnest() %>% \n    data.table() %>% \n    .[, sp_v := sp] %>%\n    .[]\n\n  return(cv_results)\n\n}\n\nHere is an example at sp \\(= 0.5\\).\n\nget_mse(0.5, kcv_folds, skcv_folds)\n\n       id type   mse_obs sp_v\n 1: Fold1  KCV  771.1552  0.5\n 2: Fold2  KCV  757.1101  0.5\n 3: Fold3  KCV  699.6830  0.5\n 4: Fold4  KCV  749.1340  0.5\n 5: Fold5  KCV  904.7470  0.5\n 6: Fold6  KCV  865.9606  0.5\n 7: Fold1 SKCV  324.7865  0.5\n 8: Fold2 SKCV 2149.3093  0.5\n 9: Fold3 SKCV  646.5064  0.5\n10: Fold4 SKCV  462.3724  0.5\n11: Fold5 SKCV 2350.4966  0.5\n12: Fold6 SKCV  257.4204  0.5\n\n\nLet’s try sp values ranging from 0 to 5.\n\nsp_seq <- c(0.05, seq(0.5, 5, by = 0.25))\n\ncv_results <-\n  mclapply(\n    sp_seq,\n    function(x) get_mse(x, kcv_folds, skcv_folds),\n    mc.cores = detectCores() / 4 * 3\n  ) %>% \n  rbindlist() %>% \n  .[, .(mse = mean(mse_obs)), by = .(type, sp_v)]\n\nNow, let’s see what value of sp would have been the best to estimate \\(E[y|x]\\) (this is what we really want to know, but can’t in practice because you do not observe the true data generating process unlike the simulation we are running here).\nWe will fit gam() at the same sequence of sp values used just above using the entire training dataset and check what sp value minimizes the prediction error from true \\(E[y|x]\\).\n\n\nNote that since we are testing against the true \\(E[y|x]\\), we can simply use the training dataset instead of using independent test datasets. Of course, you cannot do this in practice.\n\n(\nmse_e <-\n  data.table(\n    sp_v = sp_seq\n  ) %>% \n  rowwise() %>% \n  mutate(fitted_values = list(\n    gam(y ~ s(x, k = 50), sp = sp_v, data = train_data)$fitted.value\n  )) %>% \n  mutate(mse = mean((train_data$y_det - fitted_values)^2)) %>%\n  data.table() %>% \n  .[, .(sp_v, mse)] %>% \n  .[, type := \"Test\"]\n)\n\nLet’s combine all the results and see how they are different.\n\n\nCode\nall_mse <- rbind(mse_e, cv_results)\n\nggplot() +\n  geom_line(data = all_mse, aes(y = mse, x = sp_v)) +\n  geom_point(data = all_mse, aes(y = mse, x = sp_v)) +\n  geom_point(\n    data = all_mse[, .SD[which.min(mse), ], by = type], \n    aes(y = mse, x = sp_v),\n    color = \"red\",\n    size = 2\n  ) +\n  facet_grid(type ~ ., scale = \"free_y\") +\n  theme_bw()\n\n\n\n\n\nFigure 16.6: Cross-validation results\n\n\n\n\nSo, the sp value that would produce the fitted curve that is closest to the true \\(E[y|x]\\) is 0.5. SKCV suggests that sp value of 2 is optimal. Finally, KCV suggests that sp value of 2.5 is optimal. But, of course this is just a single instance. Let’s run more of the same simulations to see if our guess is correct or not. Figure 16.7 presents the optimal sp values estimated by KCV and SKCV plotted against the actual optimal sp value from 100 simulations. Points around the red one-to-one line indicate good predictions by KCV and SKCV.\n\n\nCode\nrun_cv_sim <- function(i)\n{\n\n  print(i)\n\n  #=== create the training data ===#\n  training_data  <- gen_data(seed = runif(1) * 1000)\n\n  #--------------------------\n  # prepare data for CV\n  #--------------------------\n  kcv_folds <- \n    rsample::vfold_cv(training_data, v = 6) %>% \n    mutate(type := \"KCV\")\n\n  skcv_folds <- \n    spatial_clustering_cv(training_data, v = 6) %>% \n    mutate(type := \"SKCV\")\n\n  #--------------------------\n  # run KCV and SKCV\n  #--------------------------  \n  cv_results <-\n    lapply(\n      sp_seq,\n      function(x) get_mse(x, kcv_folds, skcv_folds)\n    ) %>% \n    rbindlist() %>% \n    .[, .(mse = mean(mse_obs)), by = .(type, sp_v)] %>% \n    .[, .SD[which.min(mse)], by = .(type)]\n\n  #--------------------------\n  # gam fits on the entire train dataset\n  #--------------------------\n  gam_fits <-\n    data.table(\n      sp_v = sp_seq\n    ) %>% \n    rowwise() %>% \n    mutate(gam_fit = list(\n      gam(y ~ s(x, k = 50), sp = sp_v, data = training_data)\n    )) %>% \n    mutate(mse = mean((training_data$y_det - gam_fit$fitted.value)^2)) \n\n  #--------------------------\n  #  Level 2 comment out\n  #--------------------------\n  opt_x_data <-\n    left_join(cv_results, gam_fits, by = \"sp_v\") %>% \n    rowwise() %>% \n    mutate(opt_x = \n      data.table(\n        x = seq(min(training_data$x), max(training_data$x), length = 100)\n      ) %>% \n      .[, y_hat := predict(gam_fit, newdata = .)] %>% \n      .[, pi_hat := y_hat - 8 * x] %>% \n      .[which.max(pi_hat), x]\n    ) %>% \n    data.table() %>% \n    .[, .(type, opt_x)]\n\n  #--------------------------\n  # best sp\n  #--------------------------\n  best_sp <-\n    gam_fits %>% \n    data.table() %>% \n    .[which.min(mse), sp_v]\n\n  return_results <- \n    cv_results %>% \n    .[, sp_best := best_sp] %>% \n    opt_x_data[., on = \"type\"]\n\n  return(return_results)\n}\n\nsim_results <-\n  mclapply(\n    1:100,\n    function(x) run_cv_sim(x),\n    mc.cores = detectCores() * 3 / 4\n  ) %>% \n  rbindlist(idcol = \"sim\")\n\nggplot(data = sim_results[, .N, by = .(sp_v, type, sp_best)]) +\n  geom_point(aes(x = sp_v, y = sp_best, size = N)) +\n  geom_abline(slope = 1, color = \"red\") +\n  facet_grid(type ~ .) +\n  theme_bw()\n\n\n\n\n\nFigure 16.7: Optimal penalization parameter estimated by KCV and SKCV compared against the actual optimal parameter.\n\n\n\n\nKCV seems to underestimate the optimal value of sp. While SKCV seems to be less biased, it is not doing a great job just like KCV.\nThis was an interesting experiment. However, how consequential is it in terms of treatment effect estimation? When the actual optimal sp value is 5 and KCV is suggesting sp value of 0.05 (in many cases), using sp = 0 as the final model leads to a model that is terrible at treatment effect estimation? To see this, let’s consider a simple economic problem. Suppose \\(y\\) and \\(x\\) are an output and input, respectively. Further suppose that the price of \\(y\\) is 1 and the cost of \\(x\\) is 8. We would like to solve the following profit maximization problem:\n\\[\n\\begin{aligned}\n& Max_{x} 4 \\times y - 2 \\times x \\\\\n\\Rightarrow & Max_{x} 1 \\times (10 + 48 \\times x - 4 \\times x^2) - 8 \\times x\n\\end{aligned}\n\\]\nHere, \\(1 \\times (10 + 48 \\times x - 4 \\times x^2)\\) is the revenue and \\(8 \\times x\\) is the cost (the difference is the profit).\nTaking the derivative of the objective function with respect \\(x\\),\n\\[\n1 \\times 48 - 1 \\times 4 \\times 2 \\times x - 8 = 0\n\\tag{16.1}\\]\nHere, \\(1 \\times 48 - 1 \\times 4 \\times 2 \\times x\\) represents the marginal treatment effect of \\(x\\). Now, note that the intercept (\\(10\\)) goes away and it plays no role in determining the optimal value of the input (\\(x\\)). That is, it is perfectly fine to get the intercept wrong as long as we are estimating the marginal treatment effect of \\(x\\) accurately (as we discussed earlier, MSE is not a good measure for this as it punishes a bias in intercept).\nAccording to Equation 16.1, optimal \\(x\\) is \\(5\\). Figure 16.8 shows the density plot of estimated optimal \\(x\\) by KCV and SKCV. It seems like KCV and SKCV provide similar results and it does not really matter which one to use for sp parameter tuning at least in this particular instance. It is important to note that this result clearly does not generalize. We looked at a very simply toy example. This section was not intended to give you a general advise on which KCV or SKCV you should use. It would be interesting to examine the difference between KCV and SKCV under more complex data generating process using other ML methods.\n\n\nCode\nggplot(sim_results) +\n  geom_density(aes(x = opt_x, fill = type), alpha = 0.4) +\n  scale_fill_discrete(name = \"\") +\n  theme_bw()\n\n\n\n\n\nFigure 16.8: Accuracy of estimating optimal value of the input (x) by CV type"
  },
  {
    "objectID": "E02-grf.html",
    "href": "E02-grf.html",
    "title": "17  Generalized Random Forest",
    "section": "",
    "text": "What you will learn\n\n\n\n\nWhat GRF is\nHow GRF was motivated\nRandom forest is a special case of GRF\nHonesty rule"
  },
  {
    "objectID": "E02-grf.html#random-forest-as-a-local-constant-regression",
    "href": "E02-grf.html#random-forest-as-a-local-constant-regression",
    "title": "17  Generalized Random Forest",
    "section": "17.1 Random forest as a local constant regression",
    "text": "17.1 Random forest as a local constant regression\nSuppose you are interested in estimating \\(E[y|X]\\) using a dataset and you have trained a random forest model with \\(T\\) tress. Now, let \\(\\eta_{i,t}(X)\\) takes \\(1\\) if observation \\(i\\) belongs to the same leaf as \\(X\\) in tree \\(t\\), where \\(X\\) is a vector of covariates (\\(K\\) variables). Then, the RF’s predicted value of \\(y\\) conditional on a particular value of \\(X\\) (say, \\(X_0\\)) can be written as follows:\n\\[\n\\begin{aligned}\n\\hat{y}(X_0) = \\frac{1}{T} \\cdot\\sum_{t=1}^T\\sum_{i=1}^N \\frac{\\eta_{i,t}(X_0)}{\\sum_{i=1}^N\\eta_{i,t}(X_0)}\\cdot y_i\n\\end{aligned}\n\\]\nNote that \\(\\sum_{i=1}^N\\eta_{i,t}(X_0)\\) represents the number of observations in the same leaf as \\(X_0\\). Therefore, \\(\\sum_{i=1}^N \\frac{\\eta_{i,t}(X_0)}{\\sum_{i=1}^N\\eta_{i,t}(X_0)}\\cdot y_i\\) is the average value of \\(y\\) of the leaf that \\(X_0\\) belongs to. So, while looking slightly complicated, it is the average value of \\(y\\) of the tree \\(X_0\\) belongs to averaged across the trees, which we know is how RF predicts \\(y\\) at \\(X_0\\).\nWe can switch the summations like this,\n\\[\n\\begin{aligned}\n\\hat{y}(X_0) = \\sum_{i=1}^N \\cdot\\frac{1}{T}\\sum_{t=1}^T\\frac{\\eta_{i,t}(X_0)}{\\sum_{i=1}^N\\eta_{i,t}(X_0)}\\cdot y_i\n\\end{aligned}\n\\]\nLet \\(\\alpha(X_i, X_0)\\) denote \\(\\frac{1}{T}\\sum_{t=1}^T\\frac{\\eta_{i,t}(X_0)}{\\sum_{i=1}^N\\eta_{i,t}(X_0)}\\). Then, we can rewrite the above equation as\n\\[\n\\begin{aligned}\n\\hat{y}(X_0) = \\sum_{i=1}^N \\alpha(X_i,X_0) \\cdot y_i\n\\end{aligned}\n\\]\nNow, it is easy to show that \\(\\hat{y}(X_0)\\) is a solution to the following minimization problem.\n\\[\n\\begin{aligned}\nMin_{\\theta} \\sum_{i=1}^N \\alpha(X_i,X_0)\\cdot[y_i -\\theta]^2\n\\end{aligned}\n\\tag{17.1}\\]\nIn this formulation of the problem, \\(\\alpha(X_i,X_0)\\) can be considered the weight given to observation \\(i\\).\nBy definition,\n\n\\(0 \\leq \\alpha(X_i,X_0) \\leq 1\\)\n\\(\\sum_{i=1}^N \\alpha(X_i,X_0) = 1\\)\n\nYou may notice that Equation 17.1 is actually a special case of local constant regression where the individual weights are \\(\\alpha(X_i, X_0)\\). Roughly speaking, \\(\\alpha(X_i, X_0)\\) measures how often observation \\(i\\) share the same leaves as the evaluation point (\\(X_0\\)) across \\(T\\) trees. So, it measures how similar \\(X_i\\) is to \\(X_0\\) in the RF way, but not based on euclidean distance (which is subject to curse of dimensionality). So, RF is actually a local constant regression with a special way of distributing weights to the individual observations."
  },
  {
    "objectID": "E02-grf.html#grf",
    "href": "E02-grf.html#grf",
    "title": "17  Generalized Random Forest",
    "section": "17.2 GRF",
    "text": "17.2 GRF\nInterpretation of RF as a local regression led Athey, Tibshirani, and Wager (2019) to conceive GRF, under which various statistics (e.g., conditional average treatment effect, conditional quantile) can be estimated under the unified framework.\nYou can consider prediction of \\(y\\) at \\(X = X_0\\) using RF as a three-step process.\n\n\n\n\n\n\nPredicting \\(y\\) at \\(X=X_0\\) using RF\n\n\n\n\nGrow trees (find splitting rules for each tree)\nFor \\(X = X_0\\), find the weights for individual observations based on the trees grown\n\nSolve Equation 17.1 based on the weights\n\n\n\nStep 1 is done only once. Every time you make a prediction at a different value of \\(X\\), you go over steps 2 and 3.\nGRF follows exactly the same steps except that it adjusts the way trees are grown in step 1 and adjusts the way the local minimization problem is solved at step 3 (they are actually interrelated) depending on what you would like to estimate.\nHere, we follow the notations used in Athey, Tibshirani, and Wager (2019) as much as possible. Here are the list of notations:\n\n\\(O_i\\): data for observation \\(i\\)\n\\(\\theta\\): the statistics of interest (e.g., CATE for causal forest, conditional quantile for for quantile forest)\n\\(\\nu\\): nuisance statistics (you are not interested in estimating this).\n\\(\\Psi_{\\theta, \\nu}(O)\\): score function\n\\(\\alpha_i(x)\\): weight given to observation \\(i\\) when predicting at \\(X=x\\).\n\nIn general, GRF solves the following problem to find the estimate of \\(\\theta\\) conditional on \\(X_i= x\\):\n\\[\n\\begin{aligned}\n\\sum_{i=1}^n \\alpha_i(x)\\Psi_{\\theta, \\nu}(O_i) = 0\n\\end{aligned}\n\\tag{17.2}\\]\nAs we saw earlier, GRF is RF when \\(\\Psi_{\\theta, \\nu}(O_i) = Y_i-\\theta\\). There is no nuisance statistics, \\(\\nu(x)\\), in the RF case. By changing how the score function (\\(\\Psi_{\\theta, \\nu}(O_i)\\)) is defined, you can estimate  a wide range of statistics using different approaches under the  same estimation framework (this is why it is called generalized random forest). Here are some of the statistics and estimation approaches that are under the GRF framework.\n\nConditional expectation (\\(E[Y|X]\\))\n\nRegression forest (Random forest for regression)\nLocal linear forest\nBoosted regression forest\n\nConditional average treatment effect (CATE)\n\nCausal forest\nInstrumental forest\n\nConditional quantile\n\nQuantile forest\n\n\n\n\n\n\n\n\nScore function examples\n\n\n\n\nRandom forest: \\(\\Psi_{\\theta, \\nu}(O_i) = Y_i - \\theta\\)\nCausal forest: \\(\\Psi_{\\theta, \\nu}(O_i) = (\\tilde{Y}_i- \\theta\\tilde{T}_i)\\cdot\\tilde{T}_i\\), where \\(\\tilde{v_i} = v_i - E[v_i|X_i]\\)\nQuantile forest: \\(\\Psi_{\\theta, \\nu}(O_i) = qI\\{Y_i > \\theta\\} - (1-q)I\\{Y_i \\leq \\theta\\}\\)\n\n\n\n\n\n\\(I\\{\\}\\) is an indicator function that takes 1 if the condition inside the curly brackets and 0 otherwise.\n\nSo far, we have only talked about score functions so far, but not the weights. Do all the approaches listed above use the weights derived from the trees grown by the traditional RF in solving Equation 17.2? You could (you are free to use any weights), but that would not be wise. As mentioned earlier, GRF adjusts the way trees are grown (thus weights) as well according to the score function so that weights are optimized for your objective. This makes sense. Right  neighbors should be different based on what you are interesting in estimating.\nSpecifically, GRF uses the random forest algorithm to grow trees based on  pseudo outcome (\\(\\rho_i\\))  derived from the score function that is specific to the type of regression you are running, but not on \\(Y\\). Basically, you are using exactly the same algorithm as the traditional RF we saw in Section 6.2 except that \\(Y\\) is swapped with the pseudo outcome.\n\n\nSee Athey, Tibshirani, and Wager (2019) for how the pseudo outcome is derived from a score function in general.\n\n\n\n\n\n\nExample pseudo outcomes\n\n\n\nRandom forest:\n\\[\n\\begin{aligned}\n\\rho_i = Y_i - \\hat{\\theta}_P\n\\end{aligned}\n\\]\n Causal forest:\n\\[\n\\begin{aligned}\n\\rho_i = (\\tilde{Y}_i- \\hat{\\theta}_P \\tilde{T}_i)\\cdot\\tilde{T}_i\n\\end{aligned}\n\\]\n Quantile forest:\n\\[\n\\begin{aligned}\n\\rho_i = I\\{Y_i > \\hat{\\theta}_P\\}\n\\end{aligned}\n\\]\n\n\nNote that \\(\\hat{\\theta}_P\\) in the pseudo outcomes presented above is the solution to Equation 17.2 with their respective score functions using the data in the parent node. For example, \\(\\hat{\\theta}_P = \\bar{Y}_p\\) for RF, which is the average value of \\(Y\\) in the parent node. In quantile forest, \\(\\hat{\\theta}_P\\) is the \\(q\\)th quantile of the parent node if you are estimating the \\(q\\)the conditional quantile.\n\nFinally, here are the conceptual steps of GRF:\n\n\n\n\n\n\nGRF: conceptual steps\n\n\n\n\nTraining (Forest Growing) Step:\n\nDetermine what statistics you are interested in (\\(\\theta(X)\\), e.g., CATE, conditional quantile)\nDefine the appropriate score function according to the statistics of interest\nDerive the pseudo outcome based on the score function\nGrow trees using the RF algorithm based on the pseudo outcome\n\nPrediction Step:\n\nDetermine what value of \\(X\\) you would like to predict \\(\\theta(X)\\) (call it \\(X_0\\))\nFind the individual weights \\(\\alpha(X_i, X_0)\\) according to \\[\n\\begin{aligned}\n\\alpha(X_i, X_0) = \\frac{1}{T}\\sum_{t=1}^T\\frac{\\eta_{i,t}(X_0)}{\\sum_{i=1}^N\\eta_{i,t}(X_0)}\n\\end{aligned}\n\\] , which measures how often observation \\(i\\) belongs to the same node as the evaluation point \\(X_0\\).\nSolve Equation 17.2 with the weights obtained above and the score function specified earlier\n\n\n\n\nThe training step is done only once (trees are built only once). But, whenever you predict \\(\\theta(X)\\) at different values of \\(X\\), you go through the prediction step.\n\n\nOrthogonal random forest (a forest-based heterogeneous treatment effect estimator like causal forest), on the other hand, build trees every time when predicting treatment effect \\(\\theta(X)\\) at particular values of \\(X\\), which is why orthogonal random forest takes a very long time especially when there are many evaluation points. Orthogonal random forest is covered in Chapter 13."
  },
  {
    "objectID": "E02-grf.html#examples-of-grf",
    "href": "E02-grf.html#examples-of-grf",
    "title": "17  Generalized Random Forest",
    "section": "17.3 Examples of GRF",
    "text": "17.3 Examples of GRF\n\n17.3.1 Random forest as a GRF\nHere, we take a look at RF as a GRF as an illustration to understand the general GRF procedure better. When \\(\\Psi_{\\theta, \\nu}(Y_i)\\) is set to \\(Y_i - \\theta\\), then GRF is equivalent to the traditional RF. By plugging \\(Y_i - \\theta\\) into Equation 17.2, the estimate of \\(E[Y|X=X_0]\\), \\(\\theta(X_0)\\), is identified by solving\n\\[\n\\begin{aligned}\n\\sum_{i=1}^n \\alpha_i(X_0)(Y_i - \\theta) = 0\n\\end{aligned}\n\\tag{17.3}\\]\nThe weights \\(\\alpha_i(X_i, X_0)\\) are defined as follows:\n\\[\n\\begin{aligned}\n\\alpha_i(X_i, X_0) = \\frac{1}{T}\\cdot\\sum_{t=1}^T\\frac{\\eta_{i,t}(X_0)}{\\sum_{i=1}^N\\eta_{i,t}(X_0)}\n\\end{aligned}\n\\]\n, where \\(\\eta_{i,t}(X_0)\\) is 1 if observation \\(i\\) which has feature values \\(X_i\\) belongs to the same leaf as \\(X_0\\).\n\n Step 1: Grow trees:\nNow, let’s consider growing trees to find \\(\\alpha_i(X_0)\\) in Equation 17.3. For a given sample and set of variables randomly selected, GRF starts with solving the unweighted version of Equation 17.3.\n\\[\n\\begin{aligned}\n\\sum_{i=1}^n Y_i - \\theta = 0\n\\end{aligned}\n\\tag{17.4}\\]\nThe solution to this problem is simply the mean of \\(Y\\), which will be denoted as \\(\\bar{Y}_P\\), where \\(P\\) represents the parent node. Here, the parent node include all the data points as this is the first split.\nThen the pseudo outcome (\\(\\rho_i\\)) that is used in splitting is\n\\[\n\\begin{aligned}\n\\rho_i = Y_i - \\bar{Y}_P\n\\end{aligned}\n\\]\nNow, a standard CART regression split is applied on the pseudo outcomes. That is, the variable-threshold combination that maximizes the following criteria is found:\n\\[\n\\begin{aligned}\n\\tilde{\\Delta}(C_1, C_2) = \\frac{(\\sum_{i \\in C_1} \\rho_i)^2}{N_{C_1}} + \\frac{(\\sum_{i \\in C_2} \\rho_i)^2}{N_{C_2}}\n\\end{aligned}\n\\tag{17.5}\\]\nwhere \\(C_1\\) and \\(C_2\\) represent two child node candidates for a given split. Since \\(\\bar{Y}_P\\) is just a constant, it is equivalent to splitting on \\(Y_i\\). So, this is exactly the same as how the traditional RF builds trees (see Section 6.1.2 for the rationale behind maximizing the criteria presented in Equation 17.5).\nOnce the best split is identified, each of the new child nodes is split following the exactly the same procedure. Splitting continues until one of the user-specified condition prevent a further splitting. Many trees from bootstrapped samples are created (just like the regular random forest) and they form a forest. This shows that the GRF with \\(\\Psi_{\\theta, \\nu}(Y_i, X_i) = Y_i - \\theta\\) grows trees in the same manner as the traditional RF. RF in GRF is implemented by regression_forest(). But, note that running ranger() and regression_forest() will not result in the same forest because of the randomness involved in resampling and random selection of variables. Only their algorithms are equivalent\n\n Step 2: Predict:\nTo predict \\(E[Y|X=X_0]\\), Equation 17.3 is solved\n\\[\n\\begin{aligned}\n\\sum_{i=1}^N \\alpha_i(X_i, X_0)(Y_i-\\theta) = 0\n\\end{aligned}\n\\]\nSo,\n\\[\n\\begin{aligned}\n\\theta(X_0) & = \\frac{\\sum_{i=1}^N \\alpha_i(X_0)Y_i}{\\sum_{i=1}^N \\alpha_i(X_0)}\\\\\n& = \\sum_{i=1}^N \\alpha_i(X_0)Y_i \\;\\; \\mbox{(since } \\sum_{i=1}^N \\alpha_i(X_0) = 1\\mbox{)} \\\\\n& = \\sum_{i=1}^N \\huge[\\normalsize \\frac{1}{T}\\cdot\\sum_{t=1}^T\\frac{\\eta_{i,t}(X_0)}{\\sum_{i=1}^N\\eta_{i,t}(X_0)}\\cdot y_i\\huge]\\\\\n& = \\frac{1}{T}  \\cdot\\sum_{t=1}^T\\sum_{i=1}^N \\frac{\\eta_{i,t}(X_0)}{\\sum_{i=1}^N\\eta_{i,t}(X_0)}\\cdot y_i \\;\\; \\mbox{(changing the order of the summations)} \\\\\n& = \\frac{1}{T} \\cdot\\sum_{t=1}^T \\bar{Y}_t\n\\end{aligned}\n\\]\nSo, \\(\\theta(X_0)\\) from GRF is the average of tree-specific predictions for \\(X_0\\), which is exactly how RF predicts \\(E[Y|X=X_0]\\) as well.\nSo, it has been shown that GRF with \\(\\Psi_{\\theta, \\nu}(Y_i)=Y_i - \\theta\\) grows trees in the same manner as the traditional RF and also that GRF predicts \\(E[Y|X=X_0]\\) in the same manner as the traditional RF. So, RF is a special case of GRF, where \\(\\Psi_{\\theta, \\nu}(Y_i)=Y_i - \\theta\\).\n\n\n17.3.2 Causal forest as a GRF\nCausal forest (with a single treatment variable) as an R-learner is a GRF with\n\\[\n\\begin{aligned}\n\\Psi_{\\theta, \\nu}(O_i) = [(Y_i - E[Y|X_i])- \\theta(X_i)(T_i - E[T|X_i])](T_i - E[T|X_i])\n\\end{aligned}\n\\]\nIn practice \\(E[Y|X_i]\\) and \\(E[T|X_i]\\) are first estimated using appropriate machine learning methods (e.g., lasso, random forest, gradient boosted forest) in a cross-fitting manner and then the estimation of \\(Y_i - E[Y|X_i]\\) and \\(T_i - E[T|X_i]\\) are constructed. Let’s denote them by \\(\\tilde{Y}_i\\) and \\(\\tilde{T}_i\\). Then the score function is written as\n\\[\n\\begin{aligned}\n\\Psi_{\\theta} = (\\tilde{Y}_i- \\theta\\tilde{T}_i)\\tilde{T}_i\n\\end{aligned}\n\\tag{17.6}\\]\nSo, the conditional treatment effect at \\(X=X_0\\), \\(\\theta(X_0)\\), is estimated by solving the following problem:\n\\[\n\\begin{aligned}\n\\hat{\\theta}(X_0) = \\sum_{i=1}^N \\alpha_i(x)(\\tilde{Y_i} - \\theta\\cdot \\tilde{T_i})\\tilde{T_i} = 0\n\\end{aligned}\n\\tag{17.7}\\]\nwhere \\(\\alpha_i(x)\\) is the weight obtained from the trees built using random forest on the pseudo outcomes that are derived from the score function (Equation 17.6).\nFor a given parent node \\(p\\), the pseudo outcomes are defined as\n\\[\n\\begin{aligned}\n\\rho_i = (\\tilde{Y}_i- \\hat{\\theta}_P\\tilde{T}_i)\\tilde{T}_i\n\\end{aligned}\n\\]\nwhere \\(\\hat{\\theta}_P\\) is the solution of the following problem using the observations in the parent node:\n\\[\n\\begin{aligned}\n\\sum_{i=1}^N \\tilde{T_i}(\\tilde{Y_i}-\\theta \\tilde{T_i}) = 0\n\\end{aligned}\n\\]\n, which is the unweighted version of Equation 17.7.\nThe standard CART splitting algorithm is applied on the pseudo outcomes to build trees."
  },
  {
    "objectID": "E02-grf.html#sec-grf-honest",
    "href": "E02-grf.html#sec-grf-honest",
    "title": "17  Generalized Random Forest",
    "section": "17.4 Honesty",
    "text": "17.4 Honesty\nGRF applies  honesty when it trains forests. Specifically, when building a tree, the subsample for the tree randomly selected without replacement is first split into two groups: subsample for  splitting and prediction. Then, the a tree is trained on the subsample for splitting and then find the splitting rules using the standard CART algorithm. However, when predicting (say \\(E[Y|X]\\) at \\(X=x\\)), the value of \\(Y\\) from the subsample for splitting are NOT used. Rather, only the splitting rules are taken from the trained tree and then they are applied to the subsample for prediction (Figure 17.1 illustrates this process).\n\n\nCode\nDiagrammeR::grViz(\n\"\ndigraph {\n  graph [ranksep = 0.2, fontsize = 4]\n  node [shape = box]\n    SS [label = 'Subsample for splitting']\n    SP [label = 'Subsample for predicting']\n    BD [label = 'Sample for the tree']\n    TT [label = 'Trained tree']\n    PV [label = 'Predicted value']\n  edge [minlen = 2]\n    BD->SP\n    BD->SS\n    SS->TT\n    SP->PV\n    TT->SP [label='apply the splitting rules']\n  { rank = same; SS; SP}\n  { rank = same; TT}\n  { rank = same; PV}\n}\n\"\n)\n\n\n\n\n\nFigure 17.1: Illustration of an honest tree\n\n\n\nLet’s demonstrate this using a very simple regression tree with two terminal nodes using the mlb data from the wooldridge package.\n\ndata(mlb1, package = \"wooldridge\")\nmlb1_dt <- data.table(mlb1)\n\nWe would like to train a RF using this data where the dependent variable is logged salary (lsalary). We will illustrate the honesty rule by working on building a single tree within the process of building a forest.\nWe first sample the data without replacement. Here, we draw half of the entire sample randomly without replacement following the default value of 0.5 for the sample.fraction parameter.\n\nset.seed(358349)\n\nnum_obs <- nrow(mlb1_dt)\nrow_indices <- sample(seq_len(num_obs), num_obs / 2, replace = FALSE)\nboot_mlb1_dt <- mlb1_dt[row_indices, ]\n\nboot_mlb1_dt is the data that are involved in this tree. The rest of the samples in mlb1_dt will have nothing to do with this tree whatsoever.\nWe now split the sampled data into two groups: for splitting and prediction.\n\nnum_obs_sub <- nrow(boot_mlb1_dt) \nrows_split <- sample(seq_len(num_obs_sub), num_obs_sub / 2, replace = FALSE)\n\n#=== data for splitting ===#\nsplit_data <- boot_mlb1_dt[rows_split, ]\n\n#=== data for prediction ===#\neval_data <- boot_mlb1_dt[-rows_split, ]\n\nWe then train a tree using the data for splitting (split_data):\n\n#=== build a simple tree ===#\ntree_trained <-\n  rpart(\n    lsalary ~ hits + runsyr, \n    data = split_data, \n    control = rpart.control(minsplit = 60)\n  )\n\nfancyRpartPlot(tree_trained, digits = 4)\n\n\n\n\nFigure 17.2: A simple regression tree using the subsamples for splitting\n\n\n\n\nSo the splitting rule is hits < 338 as shown in Figure 17.2. At the terminal nodes, you see the prediction of lsalary: \\(12.41\\) for the left and \\(14.36\\) for the right. These predictions are NOT honest. They are obtained from the observed values of lsalary within the node using the splitting data (the data the tree is trained for). Instead of using these prediction values, an honest prediction applied the splitting rules (hits < 338) to the data reserved for prediction.\n\n(\nhonest_pred <- eval_data[, mean(lsalary), by = hits < 338]\n)\n\n    hits       V1\n1:  TRUE 12.51945\n2: FALSE 14.09740\n\n\nSo, instead of \\(12.41\\) and \\(14.36\\), the predicted values from the honest tree are \\(12.52\\) and \\(14.1\\) for the left and right nodes, respectively. Trees are built in this manner many times to form a forest.\nMore generally, in GRF, honesty is applied by using the evaluation data to solve Equation 17.2 based on the weight \\(\\alpha_i(x)\\) derived from the trained forest using the splitting data. Honesty is required for the GRF estimator to be consistent and asymptotically normal (Athey, Tibshirani, and Wager 2019). However, the application of honesty can do more damage than help when the sample size is small."
  },
  {
    "objectID": "E02-grf.html#sec-understand-grf-example",
    "href": "E02-grf.html#sec-understand-grf-example",
    "title": "17  Generalized Random Forest",
    "section": "17.5 Understanding GRF better by example",
    "text": "17.5 Understanding GRF better by example\nThis section goes through CF model training and CATE prediction step by step with codes to enhance our understanding of how GRF (and CF) works. In the process, we also learn the role of many of the hyper-parameters that are common across models that fall under GRF. We also learn the role of hyper-parameter that are specific to CF as well.\nWhile the process explained here is for CF, the vast majority of the process is exactly the same for other models under GRF. Here are some differences:\n\nOrthogonalization of the data at the beginning for CF\nThe definition of the pseudo outcome (but, the way they are used in the algorithm is identical once they are defined)\nThe role of min.node.size, alpha, and imbalance.penalty used for safeguarding against extreme splits\n\n\n\nObviously, the codes here are only for illustration and should not be used in practice.\nWe will use a synthetic data generated from the following DGP:\n\\[\n\\begin{aligned}\ny = \\theta(X)\\cdot T + g(X) + \\mu\n\\end{aligned}\n\\]\n, where\n\\[\n\\begin{aligned}\n\\theta(X) & = 2 + 3 \\times x_1^2 - \\sqrt{3 \\times  x_2^3 + 1}\\\\\ng(X) & = 10 \\cdot [log(x_2+1) + 2\\cdot x_3\\times x_2]\n\\end{aligned}\n\\]\nWe have 10 feature variables \\(x_1, \\dots, x_{10}\\) and only \\(x_1\\) through \\(x_3\\) play a role.\n\n\\(x_1, x_2 \\sim U[0, 3]^2\\)\n\\(x_3 \\sim N(1, 1)\\)\n\\(x_4, \\dots, x_{10} \\sim U[0, 1]^{7}\\)\n\nThe error term and the treatment variable is defined as follows.\n\n\\(\\mu \\sim N(0, 1)\\)\n\\(T \\sim Ber(0.5)\\)\n\n\nset.seed(73843)\nN <- 1000 # number of observations\n\nx4_x10 <- \n  matrix(runif(N * 7), nrow = N) %>% \n  data.table() %>% \n  setnames(names(.), paste0(\"x\", 4:10))\n\n(\ndata <- \n  data.table(\n    x1 = 3 * runif(N),\n    x2 = 3 * runif(N),\n    x3 = rnorm(N, mean = 1, sd = 1),\n    mu = rnorm(N),\n    T = runif(N) > 0.5\n  ) %>% \n  #=== theta(X) ===#\n  .[, theta_x := 2 + 3 * x1^2 - sqrt(3* x2^3 + 2)] %>%\n  #=== g(X) ===#\n  .[, g_x := 10 * (log(x2 + 1) + 2 * x3 * x2)] %>% \n  .[, y := theta_x * T + g_x + mu] %>% \n  .[, .(y, T, x1, x2, x3)] %>% \n  cbind(., x4_x10)\n)\n\n               y     T        x1        x2          x3        x4        x5\n   1:  31.239440 FALSE 2.6951250 1.1965331  0.96678638 0.7210003 0.6207226\n   2:  10.144462 FALSE 0.7919764 0.2722983  1.34510173 0.6267043 0.8869069\n   3:  48.939026  TRUE 2.7143095 0.9040185  1.19934789 0.5620508 0.5455276\n   4:  36.405483  TRUE 1.0628966 1.0659064  1.20667390 0.3638505 0.3201202\n   5:  11.893620 FALSE 2.8824810 1.8198348  0.03356061 0.2570614 0.1447951\n  ---                                                                     \n 996:  74.029985  TRUE 1.6485987 2.8588233  1.04216169 0.6835214 0.7266654\n 997:  21.176748  TRUE 2.0208231 0.4248353  0.49803263 0.2851495 0.8076055\n 998:   3.388729 FALSE 1.9203044 0.5051511 -0.02137629 0.9961128 0.8446825\n 999:   9.459730 FALSE 1.7274390 0.3468632  0.85507892 0.9669933 0.4943592\n1000: 109.282950 FALSE 1.7030476 2.1927368  2.21194227 0.1442201 0.6783044\n              x6         x7         x8         x9       x10\n   1: 0.78698773 0.88919853 0.25543604 0.05640156 0.3560356\n   2: 0.44413365 0.27657781 0.08810472 0.20397022 0.7982468\n   3: 0.45238337 0.41956478 0.69912161 0.05602666 0.5905471\n   4: 0.71060284 0.11400965 0.43547480 0.57807247 0.5707400\n   5: 0.46912302 0.31966840 0.90711440 0.15150039 0.9426457\n  ---                                                      \n 996: 0.05036928 0.18803515 0.40981100 0.04716956 0.6443493\n 997: 0.14171969 0.08375323 0.78676094 0.27132948 0.6570423\n 998: 0.36225572 0.47555166 0.40332714 0.28135063 0.9657502\n 999: 0.43818445 0.59426511 0.53555370 0.91860942 0.3339725\n1000: 0.23674030 0.25005706 0.05512990 0.24352242 0.7227397\n\n\n\n17.5.1 Orthogonalization of \\(Y\\) and \\(T\\)\nCausal forest (and instrumental forest) first orthogonalizes \\(Y\\) and \\(T\\). This is not the case for the other GRF methods. By default, causal_forest() uses random forest to estimate \\(E[Y|X]\\) and \\(E[T|X]\\), and then use out-of-bag predictions. Since the treatment assignment is randomized, we could use \\(0.5\\) for as the estimate for \\(E[T|X]\\) for all the observations. But, we will estimate both in this example.\n\n#--------------------------\n# E[Y|X]\n#--------------------------\n#=== train an RF on y ===#\ny_rf_trained <-\n  regression_forest(\n    X = data[, .(x1, x2, x3)],\n    Y = data[, y]\n  )\n\n#=== OOB estimates of E[Y|X] ===#\ney_x_hat <- y_rf_trained$predictions\n\n#--------------------------\n# E[T|X]\n#--------------------------\n#=== train an RF on T ===#\nt_rf_trained <-\n  probability_forest(\n    X = data[, .(x1, x2, x3)],\n    Y = data[, factor(T)]\n  )\n\n#=== OOB estimates of E[T|X] ===#\net_x_hat <- t_rf_trained$predictions[, 2] # get the probability of T = 1\n\nLet’s now orthogonalize \\(Y\\) and \\(T\\),\n\ndata[, `:=`(y_tilde = y - ey_x_hat, t_tilde = T - et_x_hat)]\n\nHere is what the data looks like:\n\nhead(data)\n\n          y     T        x1        x2         x3        x4        x5        x6\n1: 31.23944 FALSE 2.6951250 1.1965331 0.96678638 0.7210003 0.6207226 0.7869877\n2: 10.14446 FALSE 0.7919764 0.2722983 1.34510173 0.6267043 0.8869069 0.4441337\n3: 48.93903  TRUE 2.7143095 0.9040185 1.19934789 0.5620508 0.5455276 0.4523834\n4: 36.40548  TRUE 1.0628966 1.0659064 1.20667390 0.3638505 0.3201202 0.7106028\n5: 11.89362 FALSE 2.8824810 1.8198348 0.03356061 0.2570614 0.1447951 0.4691230\n6: 78.17796  TRUE 1.3823141 1.6858092 1.91198906 0.6044051 0.8637295 0.8509355\n          x7         x8         x9       x10    y_tilde    t_tilde\n1: 0.8891985 0.25543604 0.05640156 0.3560356 -5.6636071 -0.5296217\n2: 0.2765778 0.08810472 0.20397022 0.7982468 -4.3210587 -0.4788832\n3: 0.4195648 0.69912161 0.05602666 0.5905471 11.9768866  0.4703248\n4: 0.1140096 0.43547480 0.57807247 0.5707400  0.2253922  0.5534958\n5: 0.3196684 0.90711440 0.15150039 0.9426457 -8.4757974 -0.5011004\n6: 0.7302472 0.91788817 0.76401947 0.9238519 -0.6532169  0.5113809\n\n\n\n\n17.5.2 Building trees\n\n Preparing Data:\nWhen building a tree, GRF use sampling without replacement instead of sampling with replacement as a default for RF implemented by ranger(). sample.fraction parameter determines the fraction of the entire sample drawn for each tree. Let’s use the default value, which is \\(0.5\\).\n\n(\ndata_for_the_first_tree <- data[sample(1:N, 0.5 * N, replace = FALSE), ] \n)\n\n              y     T         x1        x2         x3         x4         x5\n  1: 24.9025560  TRUE 2.33286846 0.2477951  0.8980258 0.50396757 0.08485642\n  2: 34.4617113 FALSE 0.36784858 0.9011077  1.5373583 0.83508147 0.10087835\n  3:  4.8131199 FALSE 0.35464284 0.1992028  0.9367359 0.84134117 0.24929184\n  4:  9.7958485 FALSE 2.54230941 0.7135930  0.3197887 0.05861231 0.34530906\n  5: -0.2399451 FALSE 0.88094406 0.1036436 -0.4209629 0.63817225 0.24831807\n ---                                                                       \n496: 81.0907489 FALSE 0.03481615 2.9859855  1.1341612 0.93868895 0.10845090\n497: 45.7373362  TRUE 1.11687411 2.1342024  0.8219353 0.18140031 0.52911682\n498: 81.7731653  TRUE 0.50482529 1.3572611  2.7416683 0.18128877 0.12207408\n499: -9.1361410 FALSE 2.68976646 2.2431148 -0.4614975 0.27532629 0.87256993\n500: 21.7882929  TRUE 0.41004493 2.6174698  0.3060092 0.43608964 0.80218867\n            x6        x7         x8         x9       x10    y_tilde    t_tilde\n  1: 0.5330777 0.0207677 0.75254547 0.28037254 0.5548497  10.348401  0.5165125\n  2: 0.6637159 0.8061779 0.34381605 0.74622506 0.3308037  -3.174286 -0.4424109\n  3: 0.8667869 0.2351434 0.26094758 0.48312612 0.3273708  -4.899194 -0.4703968\n  4: 0.7823649 0.8942680 0.73156533 0.55466979 0.6760430 -13.150179 -0.5710382\n  5: 0.8369564 0.9980671 0.02740824 0.36841730 0.6603217  -2.098577 -0.5943349\n ---                                                                          \n496: 0.6255471 0.4409065 0.73793156 0.16597397 0.2782975   6.647750 -0.4746825\n497: 0.7703280 0.8028466 0.79829377 0.72936803 0.3953351  -1.998624  0.4751614\n498: 0.1248504 0.7362142 0.28138516 0.45800679 0.1410164  -3.481113  0.5902025\n499: 0.9890033 0.6309494 0.11897912 0.05518469 0.2093569  -7.215089 -0.5353184\n500: 0.7045508 0.3854207 0.34308977 0.88430430 0.5601304  -5.074021  0.4222428\n\n\nBy default, honest splitting is implemented (honesty = TRUE). The honesty.fraction parameter determines the fraction of the randomly sampled data (data_for_the_first_tree), which will be used for determining splitting decisions. We will use the default value of honesty.fraction, which is \\(0.5\\).\n\n#=== number of observations ===#\nN_d <- nrow(data_for_the_first_tree)\n\n#=== indices ===#\nJ1_index <- sample(1:N_d, 0.5 * N_d, replace = FALSE)\n\n#=== data for determining splitting rules ===#\n(\ndata_J1 <- data_for_the_first_tree[J1_index, ]\n)\n\n             y     T        x1        x2          x3        x4         x5\n  1: 32.168156  TRUE 1.3202547 1.1420641  0.91158029 0.1686512 0.79225674\n  2: 63.700906 FALSE 0.5488392 1.5795742  1.65239047 0.3851233 0.64512684\n  3: 41.849736  TRUE 2.7792941 0.8210709  0.79316984 0.9278186 0.49904726\n  4: 30.063201 FALSE 0.9203382 1.2250174  0.87263820 0.6032529 0.54814557\n  5: 74.092869  TRUE 1.3308917 1.7572783  1.75875073 0.1445248 0.19666780\n ---                                                                     \n246: 59.044593 FALSE 2.9685008 1.6885064  1.49969895 0.5313985 0.15660748\n247: 79.666353  TRUE 2.3577697 1.1953114  2.31067728 0.4455486 0.01321071\n248: 37.203768 FALSE 2.1998783 1.6630233  0.78424732 0.1420179 0.95015816\n249: 14.141558  TRUE 1.1786313 1.1804383  0.05553601 0.8050410 0.10626896\n250:  5.156938 FALSE 0.5296012 1.1397422 -0.09301202 0.5284173 0.92249024\n             x6        x7        x8          x9       x10     y_tilde\n  1: 0.70056216 0.3013820 0.7349177 0.434629694 0.3931467   1.1037999\n  2: 0.94440025 0.7271771 0.4302308 0.009344572 0.2357119  -3.6279591\n  3: 0.02594909 0.2632990 0.7471883 0.913540913 0.4693372  12.4304959\n  4: 0.79733568 0.1367518 0.9528761 0.445233749 0.8557955  -2.0274153\n  5: 0.50274839 0.7840356 0.2542098 0.023763730 0.9715826  -3.1022041\n ---                                                                 \n246: 0.52848998 0.7282229 0.5323203 0.261717018 0.2347180 -13.0882951\n247: 0.64246528 0.4421114 0.8446926 0.637843209 0.5676528   4.2071069\n248: 0.20126400 0.1563281 0.2404751 0.415536469 0.7778129  -3.7512625\n249: 0.16361912 0.7145285 0.7101117 0.600831013 0.5476061  -0.2123989\n250: 0.61208680 0.1649016 0.9978272 0.497071444 0.5043290  -1.0637976\n        t_tilde\n  1:  0.4440064\n  2: -0.4874906\n  3:  0.4410276\n  4: -0.5482126\n  5:  0.5162645\n ---           \n246: -0.5135525\n247:  0.5212281\n248: -0.3979431\n249:  0.4469103\n250: -0.5387802\n\n\nAs you can see, we are just using a quarter of the original dataset (sample.fraction \\(\\times\\) honesty.fraction = \\(0.5 \\times 0.5\\)). Here is the data for repopulating the tree once the tree is built.\n\n#=== data for repopulate the tree ===#\n(\ndata_J2 <- data_for_the_first_tree[-J1_index, ]\n)\n\n               y     T        x1        x2          x3         x4         x5\n  1:  34.4617113 FALSE 0.3678486 0.9011077  1.53735835 0.83508147 0.10087835\n  2:   9.7958485 FALSE 2.5423094 0.7135930  0.31978875 0.05861231 0.34530906\n  3:  -0.2399451 FALSE 0.8809441 0.1036436 -0.42096291 0.63817225 0.24831807\n  4:  33.8904668  TRUE 2.7083542 0.7223860  0.28685672 0.47576373 0.60371551\n  5:  42.0187752 FALSE 1.6180228 1.3468851  1.20854200 0.98942551 0.32931981\n ---                                                                        \n246: 112.6993157 FALSE 1.8652859 2.4059719  2.08180730 0.34569836 0.76473310\n247:  -1.1378434  TRUE 0.4855630 1.9038798 -0.28249533 0.67252894 0.21756645\n248:  48.9883794  TRUE 2.2455929 1.4029347  1.00599231 0.30995650 0.47503141\n249:  21.5579803  TRUE 1.9840075 2.8772854  0.05766394 0.95584669 0.02832178\n250:  45.7373362  TRUE 1.1168741 2.1342024  0.82193525 0.18140031 0.52911682\n            x6        x7         x8        x9       x10    y_tilde    t_tilde\n  1: 0.6637159 0.8061779 0.34381605 0.7462251 0.3308037  -3.174286 -0.4424109\n  2: 0.7823649 0.8942680 0.73156533 0.5546698 0.6760430 -13.150179 -0.5710382\n  3: 0.8369564 0.9980671 0.02740824 0.3684173 0.6603217  -2.098577 -0.5943349\n  4: 0.4801798 0.4673316 0.63830962 0.5097753 0.5330969  10.822273  0.4342292\n  5: 0.5914005 0.7734454 0.18005650 0.1522792 0.8920292  -5.334219 -0.5456630\n ---                                                                         \n246: 0.3229140 0.7352488 0.41670762 0.7729425 0.9237995  -3.439394 -0.4548755\n247: 0.8127880 0.2702545 0.58958613 0.2344333 0.7674877  -2.027607  0.4568964\n248: 0.3251172 0.1759780 0.42293907 0.3705066 0.1681080   9.109288  0.5642331\n249: 0.5679956 0.6444142 0.53908269 0.2941420 0.9354850   1.957145  0.5125393\n250: 0.7703280 0.8028466 0.79829377 0.7293680 0.3953351  -1.998624  0.4751614\n\n\n\n Determining splitting rules:\nNow let’s find the splitting rules using data_J1 (Remember, data_J2 is not used).\nWe are at the root node to which all the observations belong. Let’s first calculate the pseudo outcome. Finding pseudo outcomes starts from solving the unweighted version of Equation 17.2 using the samples in the parent node (root node here) in general. For CF, it is\n\\[\n\\begin{aligned}\n\\sum_{i=1}^n (\\tilde{Y}_i - \\theta\\tilde{T}_i)\\cdot \\tilde{T}_i = 0\n\\end{aligned}\n\\tag{17.8}\\]\nbecause the score function for CF is \\((\\tilde{Y}_i - \\theta\\tilde{T}_i)\\cdot \\tilde{T}_i\\). So,\n\\[\n\\begin{aligned}\n\\theta_P = \\frac{\\sum_{i=1}^n \\tilde{Y}_i\\cdot \\tilde{T}_i}{\\sum_{i=1}^n\\tilde{T}_i\\cdot \\tilde{T}_i}\n\\end{aligned}\n\\]\n\n(\ntheta_p <- data_J1[, sum(y_tilde * t_tilde)/sum(t_tilde * t_tilde)]\n)\n\n[1] 7.212476\n\n\nThe pseudo outcome for CF is \\((\\tilde{Y}_i - \\theta_P\\tilde{T}_i)\\cdot \\tilde{T}_i\\). Let’s define that in data_J1.\n\ndata_J1[, rho := (y_tilde - theta_p * t_tilde) * t_tilde]\n\nNow, the number of variables to use for splitting is determined by the mtry parameter. Its default is min(ceiling(sqrt(ncol(X))+20), ncol(X)), where X is the feature matrix. Here ncol(X) is 10. So, mtry is set to 10. Following this, we will use all three variables for splitting here. Let \\(C_j\\) (\\(j = 1, 2\\)) and \\(N_j\\) denote the child node \\(j\\) and the number of observations in child node \\(j\\), respectively. We will write a function that works on a single feature variable to find all the threshold values that would result in unique splits, and then return the following information:\n\n\\((\\sum_{i\\in C_j} \\rho_i)^2\\)\n\\(N_j\\)\n\\(\\sum_{i=1}^{N_j} (T_i - \\bar{T})^2\\) (we will call this info_size)\nthe number of treated and control units\n\nThe output allows us to calculate the heterogeneity score of the split defined as\n\\[\n\\begin{aligned}\n\\sum_{j=1}^2 \\frac{(\\sum_{i\\in C_j} \\rho_i)^2}{N_j}.\n\\end{aligned}\n\\]\nIt also allows us to eliminate some of the splits based on mtry, alpha, and imbalance.penalty parameters.\n\nget_ss_by_var <- function(feature_var, outcome_var, parent_data)\n{\n  temp_data <- \n    copy(parent_data) %>% \n    setnames(feature_var, \"temp_var\") %>% \n    setnames(outcome_var, \"outcome\")  \n\n  #=== define a sequence of values of hruns ===#\n  thr_seq <-\n    temp_data[order(temp_var), unique(temp_var)] %>%\n    #=== get the rolling mean ===#\n    frollmean(2) %>% \n    .[-1]\n\n  #=== get RSS ===#\n  ss_value <-\n    lapply(\n      thr_seq,\n      function(x){\n\n        return_data <- \n          temp_data[, .(\n            het_score = sum(outcome)^2/.N, \n            nobs = .N,\n            info_size = sum((T - mean(T))^2),\n            num_treated = sum(T),\n            num_ctrl = sum(!T)\n          ), by = temp_var < x] %>% \n          setnames(\"temp_var\", \"child\") %>% \n          .[, child := ifelse(child == TRUE, \"Child 1\", \"Child 2\")] %>% \n          .[, thr := x]\n        \n        return(return_data)\n      } \n    ) %>% \n    rbindlist() %>% \n    .[, var := feature_var] %>% \n    relocate(var, thr, child)\n\n  return(ss_value)\n}\n\nFor example, here is the output for x1.\n\nget_ss_by_var(\"x1\", \"rho\", data_J1)[]\n\n     var        thr   child   het_score nobs  info_size num_treated num_ctrl\n  1:  x1 0.00332332 Child 2  0.04640943  249 62.1285141         130      119\n  2:  x1 0.00332332 Child 1 11.55594717    1  0.0000000           1        0\n  3:  x1 0.02031884 Child 2  0.03481370  248 61.8548387         130      118\n  4:  x1 0.02031884 Child 1  4.31689906    2  0.5000000           1        1\n  5:  x1 0.03951186 Child 2  0.24122929  247 61.5789474         130      117\n ---                                                                        \n494:  x1 2.92655569 Child 2 76.66320951    3  0.6666667           1        2\n495:  x1 2.95306339 Child 1  0.30401476  248 61.8548387         130      118\n496:  x1 2.95306339 Child 2 37.69783018    2  0.5000000           1        1\n497:  x1 2.96828632 Child 1  0.09327709  249 62.0803213         131      118\n498:  x1 2.96828632 Child 2 23.22599563    1  0.0000000           0        1\n\n\nRepeating this for all the feature variables,\n\n(\nthr_score_data <-\n  lapply(\n    paste0(\"x\", 1:10),\n    function(x) get_ss_by_var(x, \"rho\", data_J1)\n  ) %>% \n  rbindlist()\n)\n\n      var        thr   child    het_score nobs  info_size num_treated num_ctrl\n   1:  x1 0.00332332 Child 2 4.640943e-02  249 62.1285141         130      119\n   2:  x1 0.00332332 Child 1 1.155595e+01    1  0.0000000           1        0\n   3:  x1 0.02031884 Child 2 3.481370e-02  248 61.8548387         130      118\n   4:  x1 0.02031884 Child 1 4.316899e+00    2  0.5000000           1        1\n   5:  x1 0.03951186 Child 2 2.412293e-01  247 61.5789474         130      117\n  ---                                                                         \n4976: x10 0.98350685 Child 2 4.706587e-01    3  0.6666667           2        1\n4977: x10 0.98856960 Child 1 7.289755e-04  248 61.8991935         129      119\n4978: x10 0.98856960 Child 2 9.039296e-02    2  0.0000000           2        0\n4979: x10 0.99656474 Child 1 3.553642e-02  249 62.1285141         130      119\n4980: x10 0.99656474 Child 2 8.848568e+00    1  0.0000000           1        0\n\n\nNow, we ensure that we have at least as many min.node.size numbers of treated and control units in both child nodes. This provides a safeguard against having a very inaccurate treatment effect estimation. The default value of min.node.size is 5.\n\nmin.node.size <- 5 # referred to as mns\n\n#=== check if both child nodes have at least mns control and treatment units  ===#\nthr_score_data[, mns_met_grup := all(num_ctrl >= min.node.size & num_treated >= min.node.size), by = .(var, thr)]\n\n(\nmsn_met_data <- thr_score_data[mns_met_grup == TRUE, ]\n)\n\n      var       thr   child   het_score nobs info_size num_treated num_ctrl\n   1:  x1 0.2647367 Child 2   4.5435708  238 59.294118         126      112\n   2:  x1 0.2647367 Child 1  90.1141537   12  2.916667           5        7\n   3:  x1 0.2957597 Child 2   5.5810225  237 59.071730         125      112\n   4:  x1 0.2957597 Child 1 101.7463325   13  3.230769           6        7\n   5:  x1 0.3148280 Child 2   5.0451340  236 58.792373         125      111\n  ---                                                                      \n4510: x10 0.9262760 Child 2   0.4888869   18  3.611111          13        5\n4511: x10 0.9331452 Child 1   0.1739244  233 58.223176         119      114\n4512: x10 0.9331452 Child 2   2.3837878   17  3.529412          12        5\n4513: x10 0.9403202 Child 1   0.1370827  234 58.461538         120      114\n4514: x10 0.9403202 Child 2   2.0048350   16  3.437500          11        5\n      mns_met_grup\n   1:         TRUE\n   2:         TRUE\n   3:         TRUE\n   4:         TRUE\n   5:         TRUE\n  ---             \n4510:         TRUE\n4511:         TRUE\n4512:         TRUE\n4513:         TRUE\n4514:         TRUE\n\n\nNow, we consider how alpha and imbalance.penalty affect the potential pool of feature-threshold combinations. Each child node needs to have at least as large info_size as alpha \\(\\times\\) info_size of the parent node. The info_size of the parent node is\n\n(\ninfo_size_p <- data_J1[, sum((T - mean(T))^2)]\n)\n\n[1] 62.356\n\n\nBy default, alpha is set to 0.05. We use this number.\n\nalpha <- 0.05\n\nmsn_met_data[, info_size_met := all(info_size > (alpha * info_size_p)), by = .(var, thr)]\n\nmsn_ifs_met <- msn_met_data[info_size_met == TRUE, ]\n\nimbalance.penalty is used to further punish splits that introduce imbalance. For example, for feature x1 and thr of 0.2957597,\n\n\n   var       thr   child het_score nobs info_size num_treated num_ctrl\n1:  x1 0.2647367 Child 2  4.543571  238 59.294118         126      112\n2:  x1 0.2647367 Child 1 90.114154   12  2.916667           5        7\n   mns_met_grup info_size_met\n1:         TRUE         FALSE\n2:         TRUE         FALSE\n\n\nthe unpunished heterogeneity score is 94.6577245. With a non-zero value of imbalance.penalty, the following penalty will be subtracted from the unpunished heterogeneity score: imbalance.penalty * (1/info_size_1 + 1/info_size_2). By default, imbalance.penalty is 0. But, let’s use imbalance.penalty \\(= 1\\) here.\n\nimbalance.penalty <- 1\nmsn_ifs_met[, imb_penalty := imbalance.penalty * (1/info_size)]\n\nNow, we calculate the heterogeneity score for each feature-threshold with imbalance penalty.\n\n(\nhet_score_data <- msn_ifs_met[, .(het_with_imbp = sum(het_score - imb_penalty)), by = .(var, thr)]\n)\n\n      var       thr het_with_imbp\n   1:  x1 0.2957597   107.0009025\n   2:  x1 0.3148280    89.7830030\n   3:  x1 0.3405788    90.7423400\n   4:  x1 0.3557459    82.0711971\n   5:  x1 0.3599312    88.7968930\n  ---                            \n2226: x10 0.9199764     0.9424508\n2227: x10 0.9231653     0.1361964\n2228: x10 0.9262760     0.2326482\n2229: x10 0.9331452     2.2572036\n2230: x10 0.9403202     1.8339034\n\n\nWe now find the feature-threshold combination that maximizes the score,\n\n(\nbest_split <- het_score_data[which.max(het_with_imbp), ]\n)\n\n   var      thr het_with_imbp\n1:  x1 1.930929      953.3717\n\n\nSo, we are splitting the root node using x1 with threshold of 1.9309. Here is a visualization of the split,\n\n\nCode\nggplot(data_J1) +\n  geom_point(aes(y = rho, x = x1, color = (x1 < best_split[, thr]))) +\n  scale_color_discrete(\"Less than the threshold\") +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nNow, let’s look at the pseudo outcome and elaborate more on what we are seeing here.\n\\[\n\\begin{aligned}\n\\rho_i = (\\tilde{Y}_i- \\hat{\\theta}_P\\tilde{T}_i)\\tilde{T}_i\n\\end{aligned}\n\\]\nNote that \\(\\hat{\\theta}_P\\) is just a constant and represents the average treatment effect for the parent node. So, \\(\\rho_i\\) tends to be higher for the observations whose CATE is higher (above-average treatment effect). So, splitting on \\(\\rho\\) (instead of \\(Y\\)) made it possible to pick the right feature variable x1. This is because x1 is influential in determining CATE and higher values of x1 lead to more positive CATE.\nNote that the checks implemented with mtry, alpha, and imbalance.penalty is more important further down a tree. But, the examples above should have illustrated how they are used.\nOkay, now let’s split the root node into two child nodes according to the best split on \\(\\rho\\) we found earlier.\n\ndepth_2_node_1 <- data_J1[x1 < best_split[, thr], ]\ndepth_2_node_2 <- data_J1[x1 >= best_split[, thr], ]\n\nLet’s consider splitting the second node, depth_2_node_1. We basically follow exactly the same process the we just did. First, find \\(\\hat{\\theta}_P\\) for this (parent) node using Equation 17.8 and define the pseudo outcomes.\n\n(\ntheta_p <- depth_2_node_1[, sum(y_tilde * t_tilde)/sum(t_tilde * t_tilde)]\n)\n\n[1] 0.8311055\n\ndepth_2_node_1[, rho := (y_tilde - theta_p * t_tilde) * t_tilde]\n\nNow, we find the best split (here we ignore various checks),\n\n(\nbest_split_d2_n1 <-\n  lapply(\n    paste0(\"x\", 1:10),\n    function(x) get_ss_by_var(x, \"rho\", depth_2_node_1)\n  ) %>% \n  rbindlist() %>% \n  .[, .(het_score = sum(het_score)), by = .(var, thr)] %>% \n  .[which.max(het_score), ]\n)\n\n   var      thr het_score\n1:  x3 3.827172  610.0837\n\n\nWe keep splitting nodes until no splits is possible any more based on the value of min.node.size, alpha, and imbalance.penalty.\n\nTo see the role of honesty.prune.leaves, let’s suppose we stopped splitting after the first split based on x with threshold of 1.9309. So, we just two terminal nodes. As stated in Section 17.4, when we do prediction, data_J1 (which is used for determining the splitting rule) is not used. Rather, data_J2 (the data that was set aside) is used. When honesty.prune.leaves = TRUE (default), the tree is pruned so that no leaves are empty. Yes, we made sure that we have certain number of observations in each node with min.node.size, however, that is only for data_J1, not data_J2.\n\ndata_J2[, .N, by = x1 < best_split[, thr]]\n\n      x1   N\n1:  TRUE 172\n2: FALSE  78\n\n\nOkay, so, each child has at least one observation. So, we are not going to prune the tree. If either of the child nodes was empty, then we would have removed the leaves and had the root node as the tree.\nAs you can see, while data_J1 plays a central role in the tree building process, the other half, data_J2, plays a very small role. However, data_J2 plays the central role in prediction, which will be seen in the next section.\n\n\n17.5.3 Prediction\nSuppose you have built \\(T\\) trees already. In general for GRF, statistics of interest, \\(\\theta(X)\\), at \\(X = X_0\\) is found by solving the following equation:\n\\[\n\\begin{aligned}\n\\sum_{i=1}^N \\alpha_i(X_i, X_0)\\psi_{\\theta,\\nu}(O_i) = 0\n\\end{aligned}\n\\]\nwhere \\(\\alpha_i(X_i, X_0)\\) is the weight given to each \\(i\\) calculated based on the trees that have been built. Let \\(\\eta_{i,t}(X_i, X_0)\\) be 1 if observation \\(i\\) belongs to the same leaf as \\(X_0\\) in tree \\(t\\). Then,\n\\[\n\\begin{aligned}\n\\alpha_i(X_i, X_0) = \\frac{1}{T}\\sum_{t=1}^T\\frac{\\eta_{i,t}(X_i, X_0)}{\\sum_{i=1}^N\\eta_{i,t}(X_i, X_0)}\n\\end{aligned}\n\\tag{17.9}\\]\nSo, the weight given to observation \\(i\\) is higher if observation \\(i\\) belongs to the same leaf as the evaluation point \\(X_0\\) in more trees.\nFor CF, since the score function is defined as \\(\\psi_{\\theta,\\nu}(O_i) = \\tilde{Y}_i - \\theta\\tilde{T}_i)\\cdot \\tilde{T}_i\\), the equation can be written as\n\\[\n\\begin{aligned}\n\\sum_{i=1}^N \\alpha_i(X_i, X_0)[\\tilde{Y_i} - \\theta\\cdot \\tilde{T_i}]\\hat{\\tilde{T_i}} = 0\n\\end{aligned}\n\\]\nSo,\n\\[\n\\begin{aligned}\n\\theta_P = \\frac{\\sum_{i=1}^n \\alpha_i(X_i, X_0)\\tilde{Y}_i\\cdot \\tilde{T}_i}{\\sum_{i=1}^n\\alpha_i(X_i, X_0)\\tilde{T}_i\\cdot \\tilde{T}_i}\n\\end{aligned}\n\\]\n\nWe now illustrate how predictions are done once trees have been built. Let’s first build trees using causal_forest(). min.node.size is set deliberately high so we can work with very simple trees.\n\n\nYou do not need to know how causal_forest() works. You only need to know that causal_forest() build trees for CATE estimation.\n\nset.seed(443784)\n\ncf_trained <-\n  causal_forest(\n    X = select(data, starts_with(\"x\")),\n    Y = data[, y],\n    W = data[, T],\n    min.node.size = 50\n  )\n\nAfter training a causal forest model, we have trees like the one shown in Figure 17.3, which is the first of the \\(2000\\) trees.\n\n\n\n\n\nFigure 17.3: Example trees built in causal forest estimation\n\n\n\nYou probably noticed that the total number of samples in the leaves is only \\(250\\) instead of \\(1000\\), which is the total number of observations in data. When causal forest was trained on this dataset, only half of the entire sample are randomly selected for building each tree (due to the default setting of sample.fraction = 0.5). The halved sample is further split into two groups, each containing \\(250\\) observations (due to the default setting of honesty = TRUE and honest.fraction = 0.5). Let’s call them \\(J_1\\) and \\(J_2\\). Then, \\(J_1\\) is used to train a tree to find the splitting rules (e.g., \\(x_1 \\leq 1.44\\) for the first tree). See Section 17.5.2 for how only a subset of the drawn samples was used to determine the splitting rules. Once the splitting rules are determined (tree building process is complete), then \\(J_1\\) “vacates” the tree. Then, \\(J_2\\) repopulates the tree. In other words, the splitting rule associated with the tree is applied to each of the samples in \\(J_2\\) and place it in the node they belong to. So, the size value in each of the terminal nodes represents how many samples in \\(J_2\\) are in each node. The statistics for each terminal node are from the samples in \\(J_2\\) within the node. This will become clearer when we talk about finding individual weights.\nLet’s take a look at a tree to see what happened. We can use get_tree() to access individual trees from cf_trained.\n\n#=== get the first tree ===#\na_tree <- get_tree(cf_trained, 1)\n\ndrawn_samples attribute of the tree contains row indices that are selected randomly for this tree.\n\nhead(a_tree$drawn_samples)\n\n[1] 788 968 693  47 673 584\n\nlength(a_tree$drawn_samples)\n\n[1] 500\n\n\nAs you can see, there are 500 samples (due to sample.fraction = 0.5). The rest of the observations were not used for this tree at all. Accessing nodes attribute will give you the splitting rules for the tree built and which samples are in what node.\n\n(\nnodes <- a_tree$nodes\n)\n\n[[1]]\n[[1]]$is_leaf\n[1] FALSE\n\n[[1]]$split_variable\n[1] 1\n\n[[1]]$split_value\n[1] 1.438102\n\n[[1]]$send_missing_left\n[1] TRUE\n\n[[1]]$left_child\n[1] 2\n\n[[1]]$right_child\n[1] 3\n\n\n[[2]]\n[[2]]$is_leaf\n[1] TRUE\n\n[[2]]$samples\n  [1] 539  75 927 812 489 264 344 479 567 116 750 744 295 363 580 754 210 601\n [19] 246  68 237 528 751 782 422 842 801 517 916 319 308   7 164 449 485  37\n [37] 576 235 431 154  22  14 867 354 940 911 373 123 104 798 133 587 506 219\n [55] 531 577 562 693 298 654 895 991 974 566 176 475  21 184 380 706  59 522\n [73] 288 936 792 117 141  89 783 252 663 513 417 756 605 427 126 401 242 818\n [91] 708 983 271 512 737 817 753 811 658 494 142 932 969  69  70 370 837 124\n[109] 265 128 470 860 550 611 200 950 721 584 293  81 262\n\n[[2]]$leaf_stats\navg_Y avg_W \n38.60  0.55 \n\n\n[[3]]\n[[3]]$is_leaf\n[1] TRUE\n\n[[3]]$samples\n  [1] 411 478  38 307  39 997 869 268 276 480 425 633   8 645 999 544 984 579\n [19] 855 690 592 673 776 253   3 909 939 423 490 409 607 337 103 274 734 174\n [37] 342 339 526 878 634 503 407 429 161 788 486 699 937 330 361 406 435 809\n [55] 948 163 747 724 416 665 329  52 714 864 483 957 946 970 204 458   5 543\n [73] 624 333 698 358  34 903 791 378 581  41 146 107 735 109 741 858 666 495\n [91] 955 438 106 125 606 840  44 151  28 334 891  49 498 583 879 518 291 908\n[109] 275 460 310 767 968 959 722  10 873 487 619 178 393 474 352 364 914 136\n[127] 770 757 976\n\n[[3]]$leaf_stats\navg_Y avg_W \n 42.1   0.5 \n\n\nnodes is a list of three elements (one root node and two terminal nodes here). The samples attribute gives you row indices of the samples that belong to the terminal nodes.\n\n#=== left node ===#  \nnodes[[2]]$samples\n\n  [1] 539  75 927 812 489 264 344 479 567 116 750 744 295 363 580 754 210 601\n [19] 246  68 237 528 751 782 422 842 801 517 916 319 308   7 164 449 485  37\n [37] 576 235 431 154  22  14 867 354 940 911 373 123 104 798 133 587 506 219\n [55] 531 577 562 693 298 654 895 991 974 566 176 475  21 184 380 706  59 522\n [73] 288 936 792 117 141  89 783 252 663 513 417 756 605 427 126 401 242 818\n [91] 708 983 271 512 737 817 753 811 658 494 142 932 969  69  70 370 837 124\n[109] 265 128 470 860 550 611 200 950 721 584 293  81 262\n\n#=== right node ===#  \nnodes[[3]]$samples\n\n  [1] 411 478  38 307  39 997 869 268 276 480 425 633   8 645 999 544 984 579\n [19] 855 690 592 673 776 253   3 909 939 423 490 409 607 337 103 274 734 174\n [37] 342 339 526 878 634 503 407 429 161 788 486 699 937 330 361 406 435 809\n [55] 948 163 747 724 416 665 329  52 714 864 483 957 946 970 204 458   5 543\n [73] 624 333 698 358  34 903 791 378 581  41 146 107 735 109 741 858 666 495\n [91] 955 438 106 125 606 840  44 151  28 334 891  49 498 583 879 518 291 908\n[109] 275 460 310 767 968 959 722  10 873 487 619 178 393 474 352 364 914 136\n[127] 770 757 976\n\n\nIt is important to keep in mind that these observations with these row indices belong to \\(J_2\\). These observations were NOT used in determining the splitting rule of \\(x_1 \\leq 1.44\\). They were populating the terminal nodes by simply following the splitting rule, which is determined using the data in \\(J_1\\) following the process described in Section 17.5.2. The difference in a_tree$drawn_samples and the combination of nodes[[2]]$samples and nodes[[3]]$samples is \\(J_1\\).\n\nJ2_rows <- c(nodes[[2]]$samples, nodes[[3]]$samples)\nJ1_J2_rows <- a_tree$drawn_samples\n\n(\nJ1_rows <- J1_J2_rows[J1_J2_rows %in% J2_rows]\n)\n\n  [1] 788 968 693 673 584  10 706 378 879 506 576 344 809 690 867 818 393 480\n [19] 665 107 908 486   7 352 513 109 339 744 298   8 747 983 767 174 291  22\n [37]  49 754 125 478 539 531 724 310 204 801 253 409 624 550 361 423 106  70\n [55] 406 543 526 293 237 490 663 528 991 136 869 911 518 811 946 722 916  39\n [73] 487 370 333 425 658 184 579  21 337 698 937 219 708 566 782 936 737 178\n [91]  89 334 721  28 592 128 970 435 798 271 753 151  59 116 460 103 948  38\n[109] 154 431 817 751 133 909 489 633 878 308 276 999 950 997 619 939  14 873\n[127] 562 252 605 458   3 163 714 246 858 164 319  68  81 984 959 123 581 438\n[145] 666 416 645 757 380 756 634 407   5 587 770 265 483 611 927 401 274 268\n[163] 974  34 567 842 429  75 577 475 812 734 914 517 449 494 141 606 161 932\n[181] 474 699 330 792 601 275 583 895 860 295 783 544 503 117 235 855 654 891\n[199] 126 864 969 411 176 485 840 142 735 976 512 373 262 417 364 498 422 903\n[217] 342 607 522 580 307 358 791  52 288 210 354 146 200 124 427  44 837 104\n[235]  37 495 940 329 470 479 955 264 242 776 741 363 957  69 750  41\n\n\nAs you can see, there are 250 samples in \\(J_1\\) as well. They were used to find the splitting rule.\nFor all the trees, the information we went through here is stored and can be used for predicting \\(\\theta(X)\\).\n\nSuppose you are interested in predicting \\(\\hat{\\theta}\\) (CATE, here) at \\(X_0 = \\{x_1 = 2, x_2,\\dots, x_{10} = 1\\}\\). We can simply use predict() function in practice. But, we look into what is happening in prediction() behind the scene here.\nFor a given tree, we give 1 to the observations in \\(J_2\\) that belong to the same leaf as \\(X_0\\). For example, for the first tree, \\(X_0\\) belongs to the right leaf because \\(x1 = 2 > 1.44\\) for \\(X_0\\). We can tell which node \\(X_0\\) belongs to by supplying \\(X_0\\) to get_leaf_node().\n\nX_0 <- matrix(c(2, rep(1, 9)), nrow = 1)\n\n(\nwhich_tree_is_X0_in <- get_leaf_node(a_tree, X_0)\n)\n\n[1] 3\n\n\nSo, we give \\(1/N_t(X_0)\\) to all those in the right leaf (the third node in nodes) and 0 to those in the left leaf, where \\(N_t(X_0)\\) is the number of observations that belong to the same leaf as \\(X_0\\). All the other observations are assigned 0.\n\n#=== which row numbers in the same leaf as X_0? ===#\nrows_1 <- nodes[[which_tree_is_X0_in]]$samples\n\n#=== define eta for tree 1  ===#\ndata[, eta_t1 := 0] # first set eta to 0 for all the observations\ndata[rows_1, eta_t1 := 1 / length(rows_1)] # replace eta with 1/N_t(X_0) if in the right node\n\n#=== see the data ===#\ndata\n\n               y     T        x1        x2          x3        x4        x5\n   1:  31.239440 FALSE 2.6951250 1.1965331  0.96678638 0.7210003 0.6207226\n   2:  10.144462 FALSE 0.7919764 0.2722983  1.34510173 0.6267043 0.8869069\n   3:  48.939026  TRUE 2.7143095 0.9040185  1.19934789 0.5620508 0.5455276\n   4:  36.405483  TRUE 1.0628966 1.0659064  1.20667390 0.3638505 0.3201202\n   5:  11.893620 FALSE 2.8824810 1.8198348  0.03356061 0.2570614 0.1447951\n  ---                                                                     \n 996:  74.029985  TRUE 1.6485987 2.8588233  1.04216169 0.6835214 0.7266654\n 997:  21.176748  TRUE 2.0208231 0.4248353  0.49803263 0.2851495 0.8076055\n 998:   3.388729 FALSE 1.9203044 0.5051511 -0.02137629 0.9961128 0.8446825\n 999:   9.459730 FALSE 1.7274390 0.3468632  0.85507892 0.9669933 0.4943592\n1000: 109.282950 FALSE 1.7030476 2.1927368  2.21194227 0.1442201 0.6783044\n              x6         x7         x8         x9       x10    y_tilde\n   1: 0.78698773 0.88919853 0.25543604 0.05640156 0.3560356 -5.6636071\n   2: 0.44413365 0.27657781 0.08810472 0.20397022 0.7982468 -4.3210587\n   3: 0.45238337 0.41956478 0.69912161 0.05602666 0.5905471 11.9768866\n   4: 0.71060284 0.11400965 0.43547480 0.57807247 0.5707400  0.2253922\n   5: 0.46912302 0.31966840 0.90711440 0.15150039 0.9426457 -8.4757974\n  ---                                                                 \n 996: 0.05036928 0.18803515 0.40981100 0.04716956 0.6443493  6.1784463\n 997: 0.14171969 0.08375323 0.78676094 0.27132948 0.6570423  6.4855740\n 998: 0.36225572 0.47555166 0.40332714 0.28135063 0.9657502 -8.0385906\n 999: 0.43818445 0.59426511 0.53555370 0.91860942 0.3339725 -3.2516710\n1000: 0.23674030 0.25005706 0.05512990 0.24352242 0.7227397 -1.5570047\n         t_tilde      eta_t1\n   1: -0.5296217 0.000000000\n   2: -0.4788832 0.000000000\n   3:  0.4703248 0.007751938\n   4:  0.5534958 0.000000000\n   5: -0.5011004 0.007751938\n  ---                       \n 996:  0.5504383 0.000000000\n 997:  0.5684978 0.007751938\n 998: -0.5107851 0.000000000\n 999: -0.5187684 0.007751938\n1000: -0.3917276 0.000000000\n\n\nWe repeat this for all the trees and use Equation 17.9 to calculate the weights for the individual observations. The following function gets \\(\\eta_{i,t}(X_i, X_0)\\) for a given tree for all the observations.\n\nget_eta <- function(t, X_0) {\n\n  w_tree <- get_tree(cf_trained, t)\n  which_tree_is_X0_in <- get_leaf_node(w_tree, X_0)\n  rows <- w_tree$nodes[[which_tree_is_X0_in]]$samples\n  eta_data <- \n    data.table(\n      row_id = seq_len(nrow(data)),\n      eta = rep(0, nrow(data))\n    ) %>% \n    .[rows, eta := 1 / length(rows)]\n\n  return(eta_data)\n}\n\nWe apply get_eta() to each of the 2000 trees.\n\n(\neta_all <-\n  lapply(\n    1:2000,\n    function(x) get_eta(x, X_0)\n  ) %>% \n  rbindlist(idcol = \"t\")\n)\n\n            t row_id         eta\n      1:    1      1 0.000000000\n      2:    1      2 0.000000000\n      3:    1      3 0.007751938\n      4:    1      4 0.000000000\n      5:    1      5 0.007751938\n     ---                        \n1999996: 2000    996 0.006622517\n1999997: 2000    997 0.000000000\n1999998: 2000    998 0.000000000\n1999999: 2000    999 0.000000000\n2000000: 2000   1000 0.000000000\n\n\nCalculate the mean of \\(\\eta_{i,t}\\) by row_id (observation) to calculate \\(\\alpha(X_i, X_0)\\).\n\n(\nweights <- \n  eta_all %>% \n  .[, .(weight = mean(eta)), by = row_id]\n)\n\n      row_id       weight\n   1:      1 0.0020590623\n   2:      2 0.0002230001\n   3:      3 0.0021290241\n   4:      4 0.0002044371\n   5:      5 0.0018758049\n  ---                    \n 996:    996 0.0007930756\n 997:    997 0.0020828039\n 998:    998 0.0022451720\n 999:    999 0.0011955923\n1000:   1000 0.0009944451\n\n\nHere is the observations that was given the highest and lowest weights.\n\ndata_with_wights <- cbind(data, weights)\n\n#=== highest (1st) and lowest (2nd) ===#\ndata_with_wights[weight %in% c(max(weight), min(weight)), ]\n\n            y     T        x1       x2        x3         x4        x5\n1:   9.795849 FALSE 2.5423094 0.713593 0.3197887 0.05861231 0.3453091\n2: 140.585346  TRUE 0.8604428 2.631900 2.4980926 0.62398495 0.2051564\n           x6        x7        x8        x9       x10    y_tilde    t_tilde\n1: 0.78236485 0.8942680 0.7315653 0.5546698 0.6760430 -13.150179 -0.5710382\n2: 0.09216643 0.4551321 0.2967835 0.3810359 0.3444893   1.000322  0.4291793\n   eta_t1 row_id       weight\n1:      0    563 2.400879e-03\n2:      0    987 1.020478e-05\n\n\nThen, we can use Equation 13.3 to calculate \\(\\hat{\\theta}(X_0)\\).\n\n(\ntheta_X0 <- sum(data_with_wights[, weight * (T-cf_trained$W.hat) * (y-cf_trained$Y.hat)]) / sum(data_with_wights[, weight * (T-cf_trained$W.hat)^2])\n)\n\n[1] 13.44869\n\n\nNote that \\(Y.hat\\) and \\(W.hat\\) attributes of cf_trained are the estimates of \\(E[Y|X]\\) and \\(E[T|X]\\), respectively. By subtracting them from \\(Y\\) and \\(T\\), \\(\\tilde{Y}\\) and \\(\\tilde{T}\\) are calculated in the above code.\nTo get the weights, we could have just used get_forest_weights() like below:\n\nweights <- get_forest_weights(cf_trained, X_0)\n\nLet’s compare the weights and see if we did it right.\n\nmean(abs(as.matrix(weights) - data_with_wights$weight))\n\n[1] 1.478887e-18\n\n\nGreat, virtually the same."
  },
  {
    "objectID": "E02-grf.html#references",
    "href": "E02-grf.html#references",
    "title": "17  Generalized Random Forest",
    "section": "References",
    "text": "References\n\n\nAthey, Susan, Julie Tibshirani, and Stefan Wager. 2019. “Generalized Random Forests.” The Annals of Statistics 47 (2): 1148–78."
  },
  {
    "objectID": "PROG-R-01-mlr3.html",
    "href": "PROG-R-01-mlr3.html",
    "title": "18  Machine Learning with mlr3",
    "section": "",
    "text": "The mlr3 package is an R package that makes it easy for you to implement standard machine learning procedures (e.g., training, prediction, cross-validation) in a unified framework. It is similar to the Python scikit-learn package. In this section, we cover the basic use cases of the mlr3 package accompanied by an real-world example at the end. The authors of the package is currently writing a book on how to use it. Materials covered in this section is basically a condensed (and far less comprehensive) version of the book. Yet, they should still be sufficient for the majority of ML tasks you typically implement in practice.\nTo start working with mlr3, it is convenient to load the mlr3verse package, which is a collection of packages that provide useful extensions (e.g., quick visualization (mlr3viz), machine learning methods (mlr3leaners), etc). It is like the tidyverse package.\nFor those who have been using solely R for their programming needs are not likely to be familiar with the way mlr3 works. It uses R6 classes provided the R6 package. The package provides an implementation of encapsulated object-oriented programing, which how Python works. So, if you are familiar with Python, then mlr3 should come quite natural to you. Fortunately, understanding how R6 works is not too hard (especially for us who are just using it). Reading the introduction of the R6 provided here should suffice.\nTo implement ML tasks, we need two core components at least: task, and learner. Roughly speaking, here are what they are.\nWe will take a deeper look at these two components first, and then training, prediction, and other ML tasks."
  },
  {
    "objectID": "PROG-R-01-mlr3.html#tasks",
    "href": "PROG-R-01-mlr3.html#tasks",
    "title": "18  Machine Learning with mlr3",
    "section": "18.1 Tasks",
    "text": "18.1 Tasks\n\n\nPackages to load for replication\n\nlibrary(data.table)\nlibrary(DoubleML)\nlibrary(mlr3verse)\nlibrary(tidyverse)\nlibrary(mltools)\nlibrary(parallel)\n\nA task in the mlr3 parlance is basically a dataset (called backend) with information on which variable is the dependent variable (called target) and which variables are explanatory variables (called features).\nHere, we will use mtcars data.\n\n#=== load mtcars ===#\ndata(\"mtcars\", package = \"datasets\")\n\n#=== see the first 6 rows ===#\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nWhen you create a task, you need to recognize what type of analysis you will be doing and use an appropriate class. Here, we will be running regression, so we use the TaskRegr class (see here for other task types).\nNow, let’s create a task using the TaskRegr class with\n\nmtcars as backend (data)\nmpg as target (dependent variable)\nexample as id (this is the id for the task, you can give any name)\n\nYou can instantiate a new TaskRegr instance using the new() methods on the TaskRegr class.\n\n(\nreg_task <-\n  TaskRegr$new(\n    id = \"example\",\n    backend = mtcars,\n    target = \"mpg\"\n  )\n)\n\n<TaskRegr:example> (32 x 11)\n* Target: mpg\n* Properties: -\n* Features (10):\n  - dbl (10): am, carb, cyl, disp, drat, gear, hp, qsec, vs, wt\n\n\nAs you can see, mpg is the Target and the rest was automatically set to Features.\nYou can use the cole_roles() method to return the roles of the variables.\n\nreg_task$col_roles \n\n$feature\n [1] \"am\"   \"carb\" \"cyl\"  \"disp\" \"drat\" \"gear\" \"hp\"   \"qsec\" \"vs\"   \"wt\"  \n\n$target\n[1] \"mpg\"\n\n$name\ncharacter(0)\n\n$order\ncharacter(0)\n\n$stratum\ncharacter(0)\n\n$group\ncharacter(0)\n\n$weight\ncharacter(0)\n\n\nYou can extract information from the task using various methods:\n\n$nrow: returns the number of rows\n$ncol: returns the number of columns\n$feature_names: returns the name of the feature variables\n$target_names: returns the name of the target variable(s)\n$row_ids: return row ids (integers starting from 1 to the number of rows)\n$data(): returns the backend (data) as a data.table\n\nLet’s see some of these.\n\n#=== row ids ===#\nreg_task$row_ids\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27 28 29 30 31 32\n\n\n\n#=== data ===#\nreg_task$data()\n\n     mpg am carb cyl  disp drat gear  hp  qsec vs    wt\n 1: 21.0  1    4   6 160.0 3.90    4 110 16.46  0 2.620\n 2: 21.0  1    4   6 160.0 3.90    4 110 17.02  0 2.875\n 3: 22.8  1    1   4 108.0 3.85    4  93 18.61  1 2.320\n 4: 21.4  0    1   6 258.0 3.08    3 110 19.44  1 3.215\n 5: 18.7  0    2   8 360.0 3.15    3 175 17.02  0 3.440\n 6: 18.1  0    1   6 225.0 2.76    3 105 20.22  1 3.460\n 7: 14.3  0    4   8 360.0 3.21    3 245 15.84  0 3.570\n 8: 24.4  0    2   4 146.7 3.69    4  62 20.00  1 3.190\n 9: 22.8  0    2   4 140.8 3.92    4  95 22.90  1 3.150\n10: 19.2  0    4   6 167.6 3.92    4 123 18.30  1 3.440\n11: 17.8  0    4   6 167.6 3.92    4 123 18.90  1 3.440\n12: 16.4  0    3   8 275.8 3.07    3 180 17.40  0 4.070\n13: 17.3  0    3   8 275.8 3.07    3 180 17.60  0 3.730\n14: 15.2  0    3   8 275.8 3.07    3 180 18.00  0 3.780\n15: 10.4  0    4   8 472.0 2.93    3 205 17.98  0 5.250\n16: 10.4  0    4   8 460.0 3.00    3 215 17.82  0 5.424\n17: 14.7  0    4   8 440.0 3.23    3 230 17.42  0 5.345\n18: 32.4  1    1   4  78.7 4.08    4  66 19.47  1 2.200\n19: 30.4  1    2   4  75.7 4.93    4  52 18.52  1 1.615\n20: 33.9  1    1   4  71.1 4.22    4  65 19.90  1 1.835\n21: 21.5  0    1   4 120.1 3.70    3  97 20.01  1 2.465\n22: 15.5  0    2   8 318.0 2.76    3 150 16.87  0 3.520\n23: 15.2  0    2   8 304.0 3.15    3 150 17.30  0 3.435\n24: 13.3  0    4   8 350.0 3.73    3 245 15.41  0 3.840\n25: 19.2  0    2   8 400.0 3.08    3 175 17.05  0 3.845\n26: 27.3  1    1   4  79.0 4.08    4  66 18.90  1 1.935\n27: 26.0  1    2   4 120.3 4.43    5  91 16.70  0 2.140\n28: 30.4  1    2   4  95.1 3.77    5 113 16.90  1 1.513\n29: 15.8  1    4   8 351.0 4.22    5 264 14.50  0 3.170\n30: 19.7  1    6   6 145.0 3.62    5 175 15.50  0 2.770\n31: 15.0  1    8   8 301.0 3.54    5 335 14.60  0 3.570\n32: 21.4  1    2   4 121.0 4.11    4 109 18.60  1 2.780\n     mpg am carb cyl  disp drat gear  hp  qsec vs    wt\n\n\nIt is possible to retrieve only a portion of the data using rows and cols options inside data() as follows:\n\nreg_task$data(rows = 1:10, cols = c(\"mpg\", 'wt'))\n\n     mpg    wt\n 1: 21.0 2.620\n 2: 21.0 2.875\n 3: 22.8 2.320\n 4: 21.4 3.215\n 5: 18.7 3.440\n 6: 18.1 3.460\n 7: 14.3 3.570\n 8: 24.4 3.190\n 9: 22.8 3.150\n10: 19.2 3.440\n\n\nTo retrieve the complete data from the task, you can apply as.data.table() to the task.\n\n(\ndata_extracted <- as.data.table(reg_task)\n)\n\n     mpg am carb cyl  disp drat gear  hp  qsec vs    wt\n 1: 21.0  1    4   6 160.0 3.90    4 110 16.46  0 2.620\n 2: 21.0  1    4   6 160.0 3.90    4 110 17.02  0 2.875\n 3: 22.8  1    1   4 108.0 3.85    4  93 18.61  1 2.320\n 4: 21.4  0    1   6 258.0 3.08    3 110 19.44  1 3.215\n 5: 18.7  0    2   8 360.0 3.15    3 175 17.02  0 3.440\n 6: 18.1  0    1   6 225.0 2.76    3 105 20.22  1 3.460\n 7: 14.3  0    4   8 360.0 3.21    3 245 15.84  0 3.570\n 8: 24.4  0    2   4 146.7 3.69    4  62 20.00  1 3.190\n 9: 22.8  0    2   4 140.8 3.92    4  95 22.90  1 3.150\n10: 19.2  0    4   6 167.6 3.92    4 123 18.30  1 3.440\n11: 17.8  0    4   6 167.6 3.92    4 123 18.90  1 3.440\n12: 16.4  0    3   8 275.8 3.07    3 180 17.40  0 4.070\n13: 17.3  0    3   8 275.8 3.07    3 180 17.60  0 3.730\n14: 15.2  0    3   8 275.8 3.07    3 180 18.00  0 3.780\n15: 10.4  0    4   8 472.0 2.93    3 205 17.98  0 5.250\n16: 10.4  0    4   8 460.0 3.00    3 215 17.82  0 5.424\n17: 14.7  0    4   8 440.0 3.23    3 230 17.42  0 5.345\n18: 32.4  1    1   4  78.7 4.08    4  66 19.47  1 2.200\n19: 30.4  1    2   4  75.7 4.93    4  52 18.52  1 1.615\n20: 33.9  1    1   4  71.1 4.22    4  65 19.90  1 1.835\n21: 21.5  0    1   4 120.1 3.70    3  97 20.01  1 2.465\n22: 15.5  0    2   8 318.0 2.76    3 150 16.87  0 3.520\n23: 15.2  0    2   8 304.0 3.15    3 150 17.30  0 3.435\n24: 13.3  0    4   8 350.0 3.73    3 245 15.41  0 3.840\n25: 19.2  0    2   8 400.0 3.08    3 175 17.05  0 3.845\n26: 27.3  1    1   4  79.0 4.08    4  66 18.90  1 1.935\n27: 26.0  1    2   4 120.3 4.43    5  91 16.70  0 2.140\n28: 30.4  1    2   4  95.1 3.77    5 113 16.90  1 1.513\n29: 15.8  1    4   8 351.0 4.22    5 264 14.50  0 3.170\n30: 19.7  1    6   6 145.0 3.62    5 175 15.50  0 2.770\n31: 15.0  1    8   8 301.0 3.54    5 335 14.60  0 3.570\n32: 21.4  1    2   4 121.0 4.11    4 109 18.60  1 2.780\n     mpg am carb cyl  disp drat gear  hp  qsec vs    wt\n\n\nYou can mutate tasks using the select() and filter() methods. It is important to remember here that the instance at which these mutations are implemented is indeed mutated. Let’s see what I mean by this.\n\n#=== first select few variables ===#\nreg_task$select(c(\"am\", \"carb\", \"cyl\"))\n\n#=== see the backend now ===#\nreg_task$data()\n\n     mpg am carb cyl\n 1: 21.0  1    4   6\n 2: 21.0  1    4   6\n 3: 22.8  1    1   4\n 4: 21.4  0    1   6\n 5: 18.7  0    2   8\n 6: 18.1  0    1   6\n 7: 14.3  0    4   8\n 8: 24.4  0    2   4\n 9: 22.8  0    2   4\n10: 19.2  0    4   6\n11: 17.8  0    4   6\n12: 16.4  0    3   8\n13: 17.3  0    3   8\n14: 15.2  0    3   8\n15: 10.4  0    4   8\n16: 10.4  0    4   8\n17: 14.7  0    4   8\n18: 32.4  1    1   4\n19: 30.4  1    2   4\n20: 33.9  1    1   4\n21: 21.5  0    1   4\n22: 15.5  0    2   8\n23: 15.2  0    2   8\n24: 13.3  0    4   8\n25: 19.2  0    2   8\n26: 27.3  1    1   4\n27: 26.0  1    2   4\n28: 30.4  1    2   4\n29: 15.8  1    4   8\n30: 19.7  1    6   6\n31: 15.0  1    8   8\n32: 21.4  1    2   4\n     mpg am carb cyl\n\n\nAs you can see, reg_task now holds only the variables that were selected (plus the target variable mpg). This behavior is similar to how data.table works when you create a new variable using data[, := ] syntax. And, this is different from how dplyr::select works.\n\n#=== create a dataset ===#\ndata_temp <- reg_task$data()\n\n#=== select mpg, carb ===#\ndplyr::select(data_temp, mpg, carb)\n\n     mpg carb\n 1: 21.0    4\n 2: 21.0    4\n 3: 22.8    1\n 4: 21.4    1\n 5: 18.7    2\n 6: 18.1    1\n 7: 14.3    4\n 8: 24.4    2\n 9: 22.8    2\n10: 19.2    4\n11: 17.8    4\n12: 16.4    3\n13: 17.3    3\n14: 15.2    3\n15: 10.4    4\n16: 10.4    4\n17: 14.7    4\n18: 32.4    1\n19: 30.4    2\n20: 33.9    1\n21: 21.5    1\n22: 15.5    2\n23: 15.2    2\n24: 13.3    4\n25: 19.2    2\n26: 27.3    1\n27: 26.0    2\n28: 30.4    2\n29: 15.8    4\n30: 19.7    6\n31: 15.0    8\n32: 21.4    2\n     mpg carb\n\n#=== look at data_temp ===#\ndata_temp\n\n     mpg am carb cyl\n 1: 21.0  1    4   6\n 2: 21.0  1    4   6\n 3: 22.8  1    1   4\n 4: 21.4  0    1   6\n 5: 18.7  0    2   8\n 6: 18.1  0    1   6\n 7: 14.3  0    4   8\n 8: 24.4  0    2   4\n 9: 22.8  0    2   4\n10: 19.2  0    4   6\n11: 17.8  0    4   6\n12: 16.4  0    3   8\n13: 17.3  0    3   8\n14: 15.2  0    3   8\n15: 10.4  0    4   8\n16: 10.4  0    4   8\n17: 14.7  0    4   8\n18: 32.4  1    1   4\n19: 30.4  1    2   4\n20: 33.9  1    1   4\n21: 21.5  0    1   4\n22: 15.5  0    2   8\n23: 15.2  0    2   8\n24: 13.3  0    4   8\n25: 19.2  0    2   8\n26: 27.3  1    1   4\n27: 26.0  1    2   4\n28: 30.4  1    2   4\n29: 15.8  1    4   8\n30: 19.7  1    6   6\n31: 15.0  1    8   8\n32: 21.4  1    2   4\n     mpg am carb cyl\n\n\nAs you can see, data_temp is not mutated after select(). To save the the result of dplyr::select(), you need to explicitly assign it to another R object.\n\n\nThe target variable cannot be selected in select(). It is automatically selected.\nIf you would like to keep the original task, you can use the clone() method to create a distinct instance.\n\n#=== create a clone ===#\nreg_task_independent <- reg_task$clone()\n\nLet’s filter the data using the filter() method.\n\n#=== filter ===#\nreg_task$filter(1:10)\n\n#=== see the backend ===#\nreg_task$data()\n\n     mpg am carb cyl\n 1: 21.0  1    4   6\n 2: 21.0  1    4   6\n 3: 22.8  1    1   4\n 4: 21.4  0    1   6\n 5: 18.7  0    2   8\n 6: 18.1  0    1   6\n 7: 14.3  0    4   8\n 8: 24.4  0    2   4\n 9: 22.8  0    2   4\n10: 19.2  0    4   6\n\n\nHowever, reg_task_independent is not affected.\n\nreg_task_independent$data()\n\n     mpg am carb cyl\n 1: 21.0  1    4   6\n 2: 21.0  1    4   6\n 3: 22.8  1    1   4\n 4: 21.4  0    1   6\n 5: 18.7  0    2   8\n 6: 18.1  0    1   6\n 7: 14.3  0    4   8\n 8: 24.4  0    2   4\n 9: 22.8  0    2   4\n10: 19.2  0    4   6\n11: 17.8  0    4   6\n12: 16.4  0    3   8\n13: 17.3  0    3   8\n14: 15.2  0    3   8\n15: 10.4  0    4   8\n16: 10.4  0    4   8\n17: 14.7  0    4   8\n18: 32.4  1    1   4\n19: 30.4  1    2   4\n20: 33.9  1    1   4\n21: 21.5  0    1   4\n22: 15.5  0    2   8\n23: 15.2  0    2   8\n24: 13.3  0    4   8\n25: 19.2  0    2   8\n26: 27.3  1    1   4\n27: 26.0  1    2   4\n28: 30.4  1    2   4\n29: 15.8  1    4   8\n30: 19.7  1    6   6\n31: 15.0  1    8   8\n32: 21.4  1    2   4\n     mpg am carb cyl\n\n\nYou can use the rbind() and cbind() methods to append data vertically and horizontally, respectively.\nHere is an example use of rbind().\n\nreg_task$rbind(\n  data.table(mpg = 20, am = 1, carb = 3, cyl = 99)\n)\n\n#=== see the change ===#\nreg_task$data()\n\n     mpg am carb cyl\n 1: 21.0  1    4   6\n 2: 21.0  1    4   6\n 3: 22.8  1    1   4\n 4: 21.4  0    1   6\n 5: 18.7  0    2   8\n 6: 18.1  0    1   6\n 7: 14.3  0    4   8\n 8: 24.4  0    2   4\n 9: 22.8  0    2   4\n10: 19.2  0    4   6\n11: 20.0  1    3  99"
  },
  {
    "objectID": "PROG-R-01-mlr3.html#learners",
    "href": "PROG-R-01-mlr3.html#learners",
    "title": "18  Machine Learning with mlr3",
    "section": "18.2 Learners",
    "text": "18.2 Learners\nIn Python, the majority of ML packages are written to be compatible with scikit-learn framework. However, in R, there is no single framework that is equivalent to scikit-learn to which all the developers of ML packages conform to. Fortunately, the author of the mlr3 package picked popular ML packages (e.g., ranger, xgboost) and made it easier for us to use those packages under the unified framework.\n\n\ntidymodels have their own collection of packages, which is very similar to what mlr3 has.\nHere is the list of learners that is available after loading the mlr3verse package.\n\nmlr_learners\n\n<DictionaryLearner> with 129 stored values\nKeys: classif.AdaBoostM1, classif.bart, classif.C50, classif.catboost,\n  classif.cforest, classif.ctree, classif.cv_glmnet, classif.debug,\n  classif.earth, classif.featureless, classif.fnn, classif.gam,\n  classif.gamboost, classif.gausspr, classif.gbm, classif.glmboost,\n  classif.glmnet, classif.IBk, classif.J48, classif.JRip, classif.kknn,\n  classif.ksvm, classif.lda, classif.liblinear, classif.lightgbm,\n  classif.LMT, classif.log_reg, classif.lssvm, classif.mob,\n  classif.multinom, classif.naive_bayes, classif.nnet, classif.OneR,\n  classif.PART, classif.qda, classif.randomForest, classif.ranger,\n  classif.rfsrc, classif.rpart, classif.svm, classif.xgboost,\n  clust.agnes, clust.ap, clust.cmeans, clust.cobweb, clust.dbscan,\n  clust.diana, clust.em, clust.fanny, clust.featureless, clust.ff,\n  clust.hclust, clust.kkmeans, clust.kmeans, clust.MBatchKMeans,\n  clust.meanshift, clust.pam, clust.SimpleKMeans, clust.xmeans,\n  dens.kde_ks, dens.locfit, dens.logspline, dens.mixed, dens.nonpar,\n  dens.pen, dens.plug, dens.spline, regr.bart, regr.catboost,\n  regr.cforest, regr.ctree, regr.cubist, regr.cv_glmnet, regr.debug,\n  regr.earth, regr.featureless, regr.fnn, regr.gam, regr.gamboost,\n  regr.gausspr, regr.gbm, regr.glm, regr.glmboost, regr.glmnet,\n  regr.IBk, regr.kknn, regr.km, regr.ksvm, regr.liblinear,\n  regr.lightgbm, regr.lm, regr.lmer, regr.M5Rules, regr.mars, regr.mob,\n  regr.randomForest, regr.ranger, regr.rfsrc, regr.rpart, regr.rvm,\n  regr.svm, regr.xgboost, surv.akritas, surv.blackboost, surv.cforest,\n  surv.coxboost, surv.coxtime, surv.ctree, surv.cv_coxboost,\n  surv.cv_glmnet, surv.deephit, surv.deepsurv, surv.dnnsurv,\n  surv.flexible, surv.gamboost, surv.gbm, surv.glmboost, surv.glmnet,\n  surv.loghaz, surv.mboost, surv.nelson, surv.obliqueRSF,\n  surv.parametric, surv.pchazard, surv.penalized, surv.ranger,\n  surv.rfsrc, surv.svm, surv.xgboost\n\n\nAs you can see, packages that we have seen earlier (e.g., glmnet, ranger, xgboost, gam) are available. the mlr3extralearners package provides you with additional but less-supported learners. You can check the complete list of learners here.\nYou set up a learner by giving the name of the learner you would like to implement from the list to lrn() like below.\n\nlearner <- lrn(\"regr.ranger\")\n\nNote that you need to pick the one with the appropriate prediction type prefix (the prefixes are self-explanatory). Here, since we are interested in regression, we picked \"regr.ranger\".\nOnce you set up a learner, you can see the set of the learner’s hyper-parameters.\n\nlearner$param_set\n\n<ParamSet>\n                              id    class lower upper nlevels        default\n 1:                        alpha ParamDbl  -Inf   Inf     Inf            0.5\n 2:       always.split.variables ParamUty    NA    NA     Inf <NoDefault[3]>\n 3:                      holdout ParamLgl    NA    NA       2          FALSE\n 4:                   importance ParamFct    NA    NA       4 <NoDefault[3]>\n 5:                   keep.inbag ParamLgl    NA    NA       2          FALSE\n 6:                    max.depth ParamInt     0   Inf     Inf               \n 7:                min.node.size ParamInt     1   Inf     Inf              5\n 8:                     min.prop ParamDbl  -Inf   Inf     Inf            0.1\n 9:                      minprop ParamDbl  -Inf   Inf     Inf            0.1\n10:                         mtry ParamInt     1   Inf     Inf <NoDefault[3]>\n11:                   mtry.ratio ParamDbl     0     1     Inf <NoDefault[3]>\n12:            num.random.splits ParamInt     1   Inf     Inf              1\n13:                  num.threads ParamInt     1   Inf     Inf              1\n14:                    num.trees ParamInt     1   Inf     Inf            500\n15:                    oob.error ParamLgl    NA    NA       2           TRUE\n16:                     quantreg ParamLgl    NA    NA       2          FALSE\n17:        regularization.factor ParamUty    NA    NA     Inf              1\n18:      regularization.usedepth ParamLgl    NA    NA       2          FALSE\n19:                      replace ParamLgl    NA    NA       2           TRUE\n20:    respect.unordered.factors ParamFct    NA    NA       3         ignore\n21:              sample.fraction ParamDbl     0     1     Inf <NoDefault[3]>\n22:                  save.memory ParamLgl    NA    NA       2          FALSE\n23: scale.permutation.importance ParamLgl    NA    NA       2          FALSE\n24:                    se.method ParamFct    NA    NA       2        infjack\n25:                         seed ParamInt  -Inf   Inf     Inf               \n26:         split.select.weights ParamUty    NA    NA     Inf               \n27:                    splitrule ParamFct    NA    NA       3       variance\n28:                      verbose ParamLgl    NA    NA       2           TRUE\n29:                 write.forest ParamLgl    NA    NA       2           TRUE\n                              id    class lower upper nlevels        default\n       parents value\n 1:  splitrule      \n 2:                 \n 3:                 \n 4:                 \n 5:                 \n 6:                 \n 7:                 \n 8:                 \n 9:  splitrule      \n10:                 \n11:                 \n12:  splitrule      \n13:                1\n14:                 \n15:                 \n16:                 \n17:                 \n18:                 \n19:                 \n20:                 \n21:                 \n22:                 \n23: importance      \n24:                 \n25:                 \n26:                 \n27:                 \n28:                 \n29:                 \n       parents value\n\n\nRight now, only num.threads is set explicitly.\n\nlearner$param_set$values\n\n$num.threads\n[1] 1\n\n\nYou can update or assign the value of a parameter like this:\n\n#=== set max.depth to 5 ===#\nlearner$param_set$values$max.depth <- 5\n\n#=== see the values ===#\nlearner$param_set$values\n\n$num.threads\n[1] 1\n\n$max.depth\n[1] 5\n\n\nWhen you would like to set values for multiple hyper-parameters at the same time, you can provide a named list to $param_set$values. Here is an example:\n\n#=== create a named vector ===#\nparameter_values <- list(\"min.node.size\" = 10, \"mtry\" = 5, \"num.trees\" = 500)\n\n#=== assign them ===#\nlearner$param_set$values <- parameter_values\n\n#=== see the values ===#\nlearner$param_set$values\n\n$min.node.size\n[1] 10\n\n$mtry\n[1] 5\n\n$num.trees\n[1] 500\n\n\nBut, notice that the values we set previously for max.depth and num.threads are gone."
  },
  {
    "objectID": "PROG-R-01-mlr3.html#train-predict-assessment",
    "href": "PROG-R-01-mlr3.html#train-predict-assessment",
    "title": "18  Machine Learning with mlr3",
    "section": "18.3 Train, predict, assessment",
    "text": "18.3 Train, predict, assessment\n\n18.3.1 Train\nTraining a model can be done using the $train() method on a leaner by supplying a task to it. Let’s first set up a task and learner.\n\n#=== define a task ===#\nreg_task <-\n  TaskRegr$new(\n    id = \"example\",\n    backend = mtcars,\n    target = \"mpg\"\n  )\n\n#=== set up a learner ===#\nlearner <- lrn(\"regr.ranger\")\nlearner$param_set$values <- \n  list(\n    \"min.node.size\" = 10, \n    \"mtry\" = 5, \n    \"num.trees\" = 500\n  )\n\nNotice that the model attribute of the learner is empty at this point.\n\nlearner$model\n\nNULL\n\n\nNow, let’s train.\n\nlearner$train(reg_task)\n\nWe now how information about the trained model in the model attribute.\n\nlearner$model\n\nRanger result\n\nCall:\n ranger::ranger(dependent.variable.name = task$target_names, data = task$data(),      case.weights = task$weights$weight, min.node.size = 10L,      mtry = 5L, num.trees = 500L) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      32 \nNumber of independent variables:  10 \nMtry:                             5 \nTarget node size:                 10 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       6.091714 \nR squared (OOB):                  0.8322955 \n\n\nNotice that this is exactly what you would get if you use ranger() to train your model.\nThe train() function has the row_ids() option, where you can specify which rows of the data backend in the task are used for training.\nLet’s split extract the row_ids attribute and then split it for train and test purposes.\n\n#=== extract row ids ===#\nrow_ids <- reg_task$row_ids\n\n#=== train ===#\ntrain_ids <- row_ids[1:(length(row_ids) / 2)]\n\n#=== test ===#\ntest_ids <- row_ids[!(row_ids %in% train_ids)]\n\n\n\n\n\n\n\nNote\n\n\n\nHe, we are doing the split manually. But, you should use resampling methods, which we will look at later.\n\n\nNow train using the train data.\n\n#=== train ===#\nlearner$train(reg_task, row_ids = train_ids)\n\n#=== seed the trained model ===#\nlearner$model\n\nRanger result\n\nCall:\n ranger::ranger(dependent.variable.name = task$target_names, data = task$data(),      case.weights = task$weights$weight, min.node.size = 10L,      mtry = 5L, num.trees = 500L) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      16 \nNumber of independent variables:  10 \nMtry:                             5 \nTarget node size:                 10 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       5.571253 \nR squared (OOB):                  0.6761402 \n\n\n\n\n18.3.2 Prediction\nWe can use the predict() method to make predictions by supplying a task to it. Just like the train() method, we can use the row_ids option inside predict_newdata() to apply prediction only on a portion of the data. Let’s use test_ids we created above.\n\nprediction <- learner$predict(reg_task, row_ids = test_ids)\n\nprediction is of a class called Prediction. You can make the prediction available as a data.table by using as.data.table().\n\nas.data.table(prediction)\n\n    row_ids truth response\n 1:      17  14.7 13.88969\n 2:      18  32.4 21.78838\n 3:      19  30.4 21.73563\n 4:      20  33.9 21.78873\n 5:      21  21.5 21.60558\n 6:      22  15.5 16.59382\n 7:      23  15.2 18.05873\n 8:      24  13.3 14.77104\n 9:      25  19.2 16.18795\n10:      26  27.3 21.73563\n11:      27  26.0 21.67934\n12:      28  30.4 21.60677\n13:      29  15.8 17.25357\n14:      30  19.7 20.49814\n15:      31  15.0 15.42272\n16:      32  21.4 21.62733\n\n\nYou can predict on a new dataset by supplying a new dataset as a data.frame/data.table to the predict_newdata() method. Here, we just use parts of mtcars (just pretend this is a newdataset).\n\nprediction <- learner$predict_newdata(mtcars)\n\n\n\n18.3.3 Performance assessment\nThere are many measure of performance we can use under mlr3. Here is the list:\n\nmlr_measures\n\n<DictionaryMeasure> with 63 stored values\nKeys: aic, bic, classif.acc, classif.auc, classif.bacc, classif.bbrier,\n  classif.ce, classif.costs, classif.dor, classif.fbeta, classif.fdr,\n  classif.fn, classif.fnr, classif.fomr, classif.fp, classif.fpr,\n  classif.logloss, classif.mbrier, classif.mcc, classif.npv,\n  classif.ppv, classif.prauc, classif.precision, classif.recall,\n  classif.sensitivity, classif.specificity, classif.tn, classif.tnr,\n  classif.tp, classif.tpr, clust.ch, clust.db, clust.dunn,\n  clust.silhouette, clust.wss, debug, oob_error, regr.bias, regr.ktau,\n  regr.mae, regr.mape, regr.maxae, regr.medae, regr.medse, regr.mse,\n  regr.msle, regr.pbias, regr.rae, regr.rmse, regr.rmsle, regr.rrse,\n  regr.rse, regr.rsq, regr.sae, regr.smape, regr.srho, regr.sse,\n  selected_features, sim.jaccard, sim.phi, time_both, time_predict,\n  time_train\n\n\nWe can access a measure using the msr() function.\n\n#=== get a measure ===#\nmeasure <- msr(\"regr.mse\")\n\n#=== check the class ===#\nclass(measure)\n\n[1] \"MeasureRegrSimple\" \"MeasureRegr\"       \"Measure\"          \n[4] \"R6\"               \n\n\nWe can do performance evaluation using the $score() method on a Prediction object by supplying a Measure object.\n\nprediction$score(measure)\n\nregr.mse \n16.35884"
  },
  {
    "objectID": "PROG-R-01-mlr3.html#resampling-cross-validation-and-cross-fitting",
    "href": "PROG-R-01-mlr3.html#resampling-cross-validation-and-cross-fitting",
    "title": "18  Machine Learning with mlr3",
    "section": "18.4 Resampling, cross-validation, and cross-fitting",
    "text": "18.4 Resampling, cross-validation, and cross-fitting\n\n18.4.1 Resampling\nmlr3 offers the following resampling methods:\n\nas.data.table(mlr_resamplings)\n\n           key                         label        params iters\n1:   bootstrap                     Bootstrap ratio,repeats    30\n2:      custom                 Custom Splits                  NA\n3:   custom_cv Custom Split Cross-Validation                  NA\n4:          cv              Cross-Validation         folds    10\n5:     holdout                       Holdout         ratio     1\n6:    insample           Insample Resampling                   1\n7:         loo                 Leave-One-Out                  NA\n8: repeated_cv     Repeated Cross-Validation folds,repeats   100\n9: subsampling                   Subsampling ratio,repeats    30\n\n\nYou can access a resampling method using the rsmp() function. You can specify parameters at the same time.\n\n(\nresampling <- rsmp(\"repeated_cv\", repeats = 2, folds = 3)\n)\n\n<ResamplingRepeatedCV>: Repeated Cross-Validation\n* Iterations: 6\n* Instantiated: FALSE\n* Parameters: repeats=2, folds=3\n\n\nYou can check the number of iterations (number of train-test datasets combinations) by accessing the iters attribute.\n\nresampling$iters\n\n[1] 6\n\n\nYou can override parameters just like you did for a leaner.\n\n#=== update ===#\nresampling$param_set$values = list(repeats = 3, folds = 4)\n\n#=== see the updates ===#\nresampling\n\n<ResamplingRepeatedCV>: Repeated Cross-Validation\n* Iterations: 12\n* Instantiated: FALSE\n* Parameters: repeats=3, folds=4\n\n\nWe can use the instantiate() method to implement the specified resampling method:\n\nresampling$instantiate(reg_task)\n\nYou can access the train and test datasets using the train_set() and test_set() method. respectively. Since repeats = 3 and folds = 4, we have 12 sets of train and test datasets. You indicate which set you want inside train_set() and test_set().\nFirst pair:\n\nresampling$train_set(1)\n\n [1]  2  3  4  8 10 17 30 31  5  9 16 18 20 23 24 32  1  6  7 19 22 25 27 29\n\nresampling$test_set(1)\n\n[1] 11 12 13 14 15 21 26 28\n\n\nLast pair:\n\nresampling$train_set(12)\n\n [1]  2  4 11 12 15 16 19 24  3  6  7 14 26 28 29 30  1  5 17 20 21 22 27 32\n\nresampling$test_set(12)\n\n[1]  8  9 10 13 18 23 25 31\n\n\n\n\n18.4.2 Cross-validation and cross-fitting\nNow that data splits are determined (along with a task and leaner), we can conduct a cross-validation using the resample() function like below.\n\ncv_results <- resample(reg_task, learner, resampling)\n\nINFO  [15:24:35.775] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 11/12) \nINFO  [15:24:35.796] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 10/12) \nINFO  [15:24:35.809] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 5/12) \nINFO  [15:24:35.818] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 8/12) \nINFO  [15:24:35.827] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 9/12) \nINFO  [15:24:35.837] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 12/12) \nINFO  [15:24:35.846] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 4/12) \nINFO  [15:24:35.856] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 3/12) \nINFO  [15:24:35.865] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 1/12) \nINFO  [15:24:35.875] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 6/12) \nINFO  [15:24:35.885] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 2/12) \nINFO  [15:24:35.897] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 7/12) \n\n\nThis code applies the method specified in leaner to each of the 12 train datasets, and evaluate the trained model on each of the 12 test datasets.\nYou can look at the prediction results using the predictions() method.\n\ncv_results$predictions()\n\n[[1]]\n<PredictionRegr> for 8 observations:\n    row_ids truth response\n         11  17.8 19.74880\n         12  16.4 17.17832\n         13  17.3 17.12910\n---                       \n         21  21.5 23.72262\n         26  27.3 29.29839\n         28  30.4 25.22943\n\n[[2]]\n<PredictionRegr> for 8 observations:\n    row_ids truth response\n          2  21.0 20.72939\n          3  22.8 26.72001\n          4  21.4 18.68237\n---                       \n         17  14.7 12.51768\n         30  19.7 20.24729\n         31  15.0 15.10645\n\n[[3]]\n<PredictionRegr> for 8 observations:\n    row_ids truth response\n          5  18.7 16.98578\n          9  22.8 23.43729\n         16  10.4 13.95478\n---                       \n         23  15.2 17.04231\n         24  13.3 15.32812\n         32  21.4 23.49904\n\n[[4]]\n<PredictionRegr> for 8 observations:\n    row_ids truth response\n          1  21.0 21.36250\n          6  18.1 19.87334\n          7  14.3 15.12675\n---                       \n         25  19.2 15.83393\n         27  26.0 25.49845\n         29  15.8 16.94153\n\n[[5]]\n<PredictionRegr> for 8 observations:\n    row_ids truth response\n          2  21.0 21.08216\n          3  22.8 25.61271\n          6  18.1 20.42328\n---                       \n         21  21.5 24.07409\n         27  26.0 26.04212\n         31  15.0 14.78513\n\n[[6]]\n<PredictionRegr> for 8 observations:\n    row_ids truth response\n          7  14.3 15.53954\n         10  19.2 18.64913\n         12  16.4 16.36077\n---                       \n         19  30.4 29.38506\n         24  13.3 15.68160\n         25  19.2 15.96820\n\n[[7]]\n<PredictionRegr> for 8 observations:\n    row_ids truth response\n         13  17.3 15.49761\n         16  10.4 14.17401\n         20  33.9 28.18790\n---                       \n         29  15.8 18.22896\n         30  19.7 21.46354\n         32  21.4 23.99942\n\n[[8]]\n<PredictionRegr> for 8 observations:\n    row_ids truth response\n          1  21.0 20.89334\n          4  21.4 18.54764\n          5  18.7 16.29278\n---                       \n         15  10.4 14.13126\n         26  27.3 29.04430\n         28  30.4 25.09802\n\n[[9]]\n<PredictionRegr> for 8 observations:\n    row_ids truth response\n          2  21.0 20.83557\n          4  21.4 18.97946\n         11  17.8 19.88023\n---                       \n         16  10.4 16.15204\n         19  30.4 29.17494\n         24  13.3 16.37231\n\n[[10]]\n<PredictionRegr> for 8 observations:\n    row_ids truth response\n          3  22.8 26.05216\n          6  18.1 19.57902\n          7  14.3 14.85265\n---                       \n         28  30.4 26.40531\n         29  15.8 17.67112\n         30  19.7 20.10260\n\n[[11]]\n<PredictionRegr> for 8 observations:\n    row_ids truth response\n          1  21.0 20.61631\n          5  18.7 16.81010\n         17  14.7 13.18634\n---                       \n         22  15.5 16.33569\n         27  26.0 25.60736\n         32  21.4 24.28578\n\n[[12]]\n<PredictionRegr> for 8 observations:\n    row_ids truth response\n          8  24.4 22.37342\n          9  22.8 21.61681\n         10  19.2 19.20952\n---                       \n         23  15.2 16.98023\n         25  19.2 15.79316\n         31  15.0 14.85194\n\n\nYou can get the all predictions combined using the prediction() method.\n\n#=== all combined ===#\nall_predictions <- cv_results$prediction()\n\n#=== check the class ===#\nclass(all_predictions)\n\n[1] \"PredictionRegr\" \"Prediction\"     \"R6\"            \n\n\nSince it is a Prediciton object, we can apply the score() method like this.\n\nall_predictions$score(msr(\"regr.mse\"))\n\nregr.mse \n6.820432 \n\n\nOf course, you are also cross-fitting when you are doing cross-validation. You can just take the prediction results and use them if you are implementing DML for example.\n\n(\ncross_fitted_yhat <-\n  as.data.table(all_predictions) %>%\n  .[, .(y_hat_cf = mean(response)), by = row_ids]\n)\n\n    row_ids y_hat_cf\n 1:      11 19.77759\n 2:      12 16.64932\n 3:      13 15.91240\n 4:      14 16.54014\n 5:      15 14.92358\n 6:      21 24.18697\n 7:      26 29.19309\n 8:      28 25.57758\n 9:       2 20.88237\n10:       3 26.12829\n11:       4 18.73649\n12:       8 22.98636\n13:      10 18.74485\n14:      17 13.15676\n15:      30 20.60448\n16:      31 14.91451\n17:       5 16.69622\n18:       9 22.95572\n19:      16 14.76028\n20:      18 27.57670\n21:      20 27.61440\n22:      23 17.36272\n23:      24 15.79401\n24:      32 23.92808\n25:       1 20.95738\n26:       6 19.95855\n27:       7 15.17298\n28:      19 29.31599\n29:      22 16.37949\n30:      25 15.86510\n31:      27 25.71598\n32:      29 17.61387\n    row_ids y_hat_cf"
  },
  {
    "objectID": "PROG-R-01-mlr3.html#hyper-parameter-tuning",
    "href": "PROG-R-01-mlr3.html#hyper-parameter-tuning",
    "title": "18  Machine Learning with mlr3",
    "section": "18.5 Hyper-parameter tuning",
    "text": "18.5 Hyper-parameter tuning\nIn conducting hyper-parameter tuning under the mlr3 framework, you define TuningInstance* class, select the tuning method, and then trigger it.\n\n18.5.1 TuningInstance\nThere are two tuning instance classes.\n\nTuningInstanceSingleCrit\nTuningInstanceMultiCrit\n\nThe difference should be clear by looking at the name of the classes. We focus on the TuningInstanceSingleCrit class here.\nTuning instance consists of six elements:\n\ntask\nlearner\nresampling\nmeasure\nsearch_space\nterminator\n\nWe have covered all except search_space and terminator. Let’s look at these two.\nLet’s quickly create the first four elements.\n\n#=== task ===#\nreg_task <-\n  TaskRegr$new(\n    id = \"example\",\n    backend = mtcars,\n    target = \"mpg\"\n  )\n\n#=== learner ===#\nlearner <- lrn(\"regr.ranger\")\nlearner$param_set$values$max.depth <- 10\n\n#=== resampling ===#\nresampling <- rsmp(\"cv\", folds = 3) # k-fold cv\n\n#=== measure ===#\nmeasure <- msr(\"regr.mse\")\n\nsearch_space defines which hyper-parameters to tune and their ranges. You can use ps() to create a search space. Before doing so, let’s look at the parameters of the learner.\n\nlearner$param_set\n\n<ParamSet>\n                              id    class lower upper nlevels        default\n 1:                        alpha ParamDbl  -Inf   Inf     Inf            0.5\n 2:       always.split.variables ParamUty    NA    NA     Inf <NoDefault[3]>\n 3:                      holdout ParamLgl    NA    NA       2          FALSE\n 4:                   importance ParamFct    NA    NA       4 <NoDefault[3]>\n 5:                   keep.inbag ParamLgl    NA    NA       2          FALSE\n 6:                    max.depth ParamInt     0   Inf     Inf               \n 7:                min.node.size ParamInt     1   Inf     Inf              5\n 8:                     min.prop ParamDbl  -Inf   Inf     Inf            0.1\n 9:                      minprop ParamDbl  -Inf   Inf     Inf            0.1\n10:                         mtry ParamInt     1   Inf     Inf <NoDefault[3]>\n11:                   mtry.ratio ParamDbl     0     1     Inf <NoDefault[3]>\n12:            num.random.splits ParamInt     1   Inf     Inf              1\n13:                  num.threads ParamInt     1   Inf     Inf              1\n14:                    num.trees ParamInt     1   Inf     Inf            500\n15:                    oob.error ParamLgl    NA    NA       2           TRUE\n16:                     quantreg ParamLgl    NA    NA       2          FALSE\n17:        regularization.factor ParamUty    NA    NA     Inf              1\n18:      regularization.usedepth ParamLgl    NA    NA       2          FALSE\n19:                      replace ParamLgl    NA    NA       2           TRUE\n20:    respect.unordered.factors ParamFct    NA    NA       3         ignore\n21:              sample.fraction ParamDbl     0     1     Inf <NoDefault[3]>\n22:                  save.memory ParamLgl    NA    NA       2          FALSE\n23: scale.permutation.importance ParamLgl    NA    NA       2          FALSE\n24:                    se.method ParamFct    NA    NA       2        infjack\n25:                         seed ParamInt  -Inf   Inf     Inf               \n26:         split.select.weights ParamUty    NA    NA     Inf               \n27:                    splitrule ParamFct    NA    NA       3       variance\n28:                      verbose ParamLgl    NA    NA       2           TRUE\n29:                 write.forest ParamLgl    NA    NA       2           TRUE\n                              id    class lower upper nlevels        default\n       parents value\n 1:  splitrule      \n 2:                 \n 3:                 \n 4:                 \n 5:                 \n 6:               10\n 7:                 \n 8:                 \n 9:  splitrule      \n10:                 \n11:                 \n12:  splitrule      \n13:                1\n14:                 \n15:                 \n16:                 \n17:                 \n18:                 \n19:                 \n20:                 \n21:                 \n22:                 \n23: importance      \n24:                 \n25:                 \n26:                 \n27:                 \n28:                 \n29:                 \n       parents value\n\n\nLet’s tune three parameters here: mtry.ratio, min.node.size, and num.trees. For each parameter to tune, we need to use an appropriate function to define the range. You can see what functions to use from class variable.\n\nlearner$param_set %>% \n  as.data.table() %>% \n  .[id %in% c(\"mtry.ratio\", \"min.node.size\"), .(id, class, lower, upper)]\n\n              id    class lower upper\n1: min.node.size ParamInt     1   Inf\n2:    mtry.ratio ParamDbl     0     1\n\n\nIn this case, we use p_int() for “min.node.size” as its class is ParamInt and use p_dbl() for “mtry.ratio” as its class is ParamDbl. You cannot specify the range that go beyond the lower and upper for each parameter.\n\nsearch_space <- \n  ps(\n    mtry.ratio = p_dbl(lower = 0.5, upper = 0.9),\n    min.node.size = p_int(lower = 1, upper = 20)\n  )\n\nLet’s define a terminator now. mlr3 offers five different options. We will just look at the most common one here, which is TerminatorEvals. TerminatorEvals terminates tuning after a given number of iterations specified by the user (see here for other terminator options).\nYou can use the trm() function to define a Terminator object.\n\nterminator <- trm(\"evals\", n_evals = 100) \n\nInside trm(), “evals” indicates that we would like to use the TerminatorEvals option.\nNow that we have all the components specified, we can instantiate (generate) a TuningInstanceSingleCrit class.\n\n(\ntuning_instance <- \n  TuningInstanceSingleCrit$new(\n    task = reg_task,\n    learner = learner,\n    resampling = resampling,\n    measure = measure,\n    search_space = search_space,\n    terminator = terminator\n  )\n)\n\n<TuningInstanceSingleCrit>\n* State:  Not optimized\n* Objective: <ObjectiveTuning:regr.ranger_on_example>\n* Search Space:\n              id    class lower upper nlevels\n1:    mtry.ratio ParamDbl   0.5   0.9     Inf\n2: min.node.size ParamInt   1.0  20.0      20\n* Terminator: <TerminatorEvals>\n\n\n\n\n18.5.2 Tuner\nLet’s now define the method of tuning. mlr3 offers four options:\n\nGrid Search (TunerGridSearch)\nRandom Search (TunerRandomSearch)\nGeneralized Simulated Annealing (TunerGenSA)\nNon-Linear Optimization (TunerNLoptr)\n\nHere we use TunerGridSearch. We can set up a tuner using the tnr() function.\n\ntuner <- tnr(\"grid_search\", resolution = 4) \n\nresolution = 4 means that each parameter takes four values where the values are equidistant between the upper and lower bounds specified in search_space. So, this tuning will look at \\(4^2 = 16\\) parameter configurations. Notice that we set the number of evaluations to 100 above. So, all \\(16\\) cases will be evaluated. However, if you set n_evals lower than \\(16\\), then the tuning will not look at all the cases.\n\n\n18.5.3 Tuning\nYou can trigger tuning by supplying a tuning instance to the optimizer() method on tuner.\n\ntuner$optimize(tuning_instance)\n\nSince the execution of this tuning prints so many lines of results, it is not presented here.\nOne the tuning is done, you can get the optimized parameters by accessing the result_learner_param_values attribute of the tuning instance.\n\ntuning_instance$result_learner_param_vals\n\n$num.threads\n[1] 1\n\n$max.depth\n[1] 10\n\n$mtry.ratio\n[1] 0.6333333\n\n$min.node.size\n[1] 1\n\n\nYou can look at the evaluation results of other parameter configurations by accessing the archive attribute of the tuning instance.\n\nas.data.table(tuning_instance$archive) %>% head()\n\n   mtry.ratio min.node.size  regr.mse x_domain_mtry.ratio\n1:  0.6333333             1  6.704947           0.6333333\n2:  0.5000000             1  6.832022           0.5000000\n3:  0.6333333            14 10.352118           0.6333333\n4:  0.9000000             1  6.836336           0.9000000\n5:  0.6333333            20 13.434402           0.6333333\n6:  0.9000000            14 10.324315           0.9000000\n   x_domain_min.node.size runtime_learners           timestamp batch_nr\n1:                      1            0.040 2022-08-15 15:24:36        1\n2:                      1            0.027 2022-08-15 15:24:36        2\n3:                     14            0.020 2022-08-15 15:24:36        3\n4:                      1            0.033 2022-08-15 15:24:36        4\n5:                     20            0.021 2022-08-15 15:24:36        5\n6:                     14            0.016 2022-08-15 15:24:36        6\n   warnings errors      resample_result\n1:        0      0 <ResampleResult[22]>\n2:        0      0 <ResampleResult[22]>\n3:        0      0 <ResampleResult[22]>\n4:        0      0 <ResampleResult[22]>\n5:        0      0 <ResampleResult[22]>\n6:        0      0 <ResampleResult[22]>\n\n\n\n\n18.5.4 AutoTuner\nAutoTuner sounds like it is a tuner, but it is really learner where tuning is automatically implemented when training is triggered with a task. An AutoTuner class can be instantiated using the new() method on AutoTuner class like below.\n\n(\nauto_tunning_learner <- \n  AutoTuner$new(\n    learner = learner,\n    resampling = resampling,\n    measure = measure,\n    search_space = search_space,\n    terminator = terminator,\n    tuner = tuner\n  )\n)\n\n<AutoTuner:regr.ranger.tuned>\n* Model: -\n* Search Space:\n<ParamSet>\n              id    class lower upper nlevels        default value\n1:    mtry.ratio ParamDbl   0.5   0.9     Inf <NoDefault[3]>      \n2: min.node.size ParamInt   1.0  20.0      20 <NoDefault[3]>      \n* Packages: mlr3, mlr3tuning, mlr3learners, ranger\n* Predict Type: response\n* Feature Types: logical, integer, numeric, character, factor, ordered\n* Properties: hotstart_backward, importance, oob_error, weights\n\n\nNote that unlike TuningInstance, AutoTuner does not take a task as its element and takes a Tuner instead. Once an AutoTuner is instantiated, you can use it like a learner and invoke the train() method with a task to train a model. The difference from a regular leaner is that it automatically tune the parameters internally and use the optimized parameter values to train.\n\nauto_tunning_learner$train(reg_task)\n\nINFO  [15:24:37.122] [bbotk] Starting to optimize 2 parameter(s) with '<TunerGridSearch>' and '<TerminatorEvals> [n_evals=100, k=0]' \nINFO  [15:24:37.123] [bbotk] Evaluating 1 configuration(s) \nINFO  [15:24:37.132] [mlr3] Running benchmark with 3 resampling iterations \nINFO  [15:24:37.134] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 1/3) \nINFO  [15:24:37.144] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 3/3) \nINFO  [15:24:37.155] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 2/3) \nINFO  [15:24:37.165] [mlr3] Finished benchmark \nINFO  [15:24:37.176] [bbotk] Result of batch 1: \nINFO  [15:24:37.176] [bbotk]  mtry.ratio min.node.size regr.mse warnings errors runtime_learners \nINFO  [15:24:37.176] [bbotk]         0.5            14 11.78706        0      0            0.019 \nINFO  [15:24:37.176] [bbotk]                                 uhash \nINFO  [15:24:37.176] [bbotk]  99f51d8c-fa7b-4762-9c12-c11f7ec905e8 \nINFO  [15:24:37.177] [bbotk] Evaluating 1 configuration(s) \nINFO  [15:24:37.187] [mlr3] Running benchmark with 3 resampling iterations \nINFO  [15:24:37.189] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 1/3) \nINFO  [15:24:37.199] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 3/3) \nINFO  [15:24:37.208] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 2/3) \nINFO  [15:24:37.218] [mlr3] Finished benchmark \nINFO  [15:24:37.236] [bbotk] Result of batch 2: \nINFO  [15:24:37.237] [bbotk]  mtry.ratio min.node.size regr.mse warnings errors runtime_learners \nINFO  [15:24:37.237] [bbotk]         0.9            14 11.54836        0      0            0.018 \nINFO  [15:24:37.237] [bbotk]                                 uhash \nINFO  [15:24:37.237] [bbotk]  c2d1fbc2-f172-46cd-aa81-9b6285a5a256 \nINFO  [15:24:37.237] [bbotk] Evaluating 1 configuration(s) \nINFO  [15:24:37.246] [mlr3] Running benchmark with 3 resampling iterations \nINFO  [15:24:37.249] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 1/3) \nINFO  [15:24:37.258] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 2/3) \nINFO  [15:24:37.267] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 3/3) \nINFO  [15:24:37.277] [mlr3] Finished benchmark \nINFO  [15:24:37.288] [bbotk] Result of batch 3: \nINFO  [15:24:37.289] [bbotk]  mtry.ratio min.node.size regr.mse warnings errors runtime_learners \nINFO  [15:24:37.289] [bbotk]   0.6333333            14 11.80731        0      0            0.018 \nINFO  [15:24:37.289] [bbotk]                                 uhash \nINFO  [15:24:37.289] [bbotk]  e3621121-9429-41a0-98ae-ae36f92ba315 \nINFO  [15:24:37.289] [bbotk] Evaluating 1 configuration(s) \nINFO  [15:24:37.298] [mlr3] Running benchmark with 3 resampling iterations \nINFO  [15:24:37.301] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 3/3) \nINFO  [15:24:37.310] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 1/3) \nINFO  [15:24:37.319] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 2/3) \nINFO  [15:24:37.334] [mlr3] Finished benchmark \nINFO  [15:24:37.346] [bbotk] Result of batch 4: \nINFO  [15:24:37.347] [bbotk]  mtry.ratio min.node.size regr.mse warnings errors runtime_learners \nINFO  [15:24:37.347] [bbotk]   0.7666667            14 12.06097        0      0            0.019 \nINFO  [15:24:37.347] [bbotk]                                 uhash \nINFO  [15:24:37.347] [bbotk]  79d63e96-da42-4729-ab27-18002ea5c0c7 \nINFO  [15:24:37.348] [bbotk] Evaluating 1 configuration(s) \nINFO  [15:24:37.357] [mlr3] Running benchmark with 3 resampling iterations \nINFO  [15:24:37.359] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 2/3) \nINFO  [15:24:37.370] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 3/3) \nINFO  [15:24:37.381] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 1/3) \nINFO  [15:24:37.393] [mlr3] Finished benchmark \nINFO  [15:24:37.404] [bbotk] Result of batch 5: \nINFO  [15:24:37.405] [bbotk]  mtry.ratio min.node.size regr.mse warnings errors runtime_learners \nINFO  [15:24:37.405] [bbotk]         0.9             7 8.914293        0      0            0.026 \nINFO  [15:24:37.405] [bbotk]                                 uhash \nINFO  [15:24:37.405] [bbotk]  057c0518-92cb-4c86-9d33-d25bf69ebf70 \nINFO  [15:24:37.405] [bbotk] Evaluating 1 configuration(s) \nINFO  [15:24:37.414] [mlr3] Running benchmark with 3 resampling iterations \nINFO  [15:24:37.417] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 1/3) \nINFO  [15:24:37.426] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 3/3) \nINFO  [15:24:37.441] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 2/3) \nINFO  [15:24:37.451] [mlr3] Finished benchmark \nINFO  [15:24:37.462] [bbotk] Result of batch 6: \nINFO  [15:24:37.463] [bbotk]  mtry.ratio min.node.size regr.mse warnings errors runtime_learners \nINFO  [15:24:37.463] [bbotk]         0.9            20 15.14286        0      0            0.018 \nINFO  [15:24:37.463] [bbotk]                                 uhash \nINFO  [15:24:37.463] [bbotk]  48442780-7ef7-415d-8535-72e8a641d715 \nINFO  [15:24:37.464] [bbotk] Evaluating 1 configuration(s) \nINFO  [15:24:37.473] [mlr3] Running benchmark with 3 resampling iterations \nINFO  [15:24:37.476] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 1/3) \nINFO  [15:24:37.484] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 3/3) \nINFO  [15:24:37.493] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 2/3) \nINFO  [15:24:37.502] [mlr3] Finished benchmark \nINFO  [15:24:37.513] [bbotk] Result of batch 7: \nINFO  [15:24:37.514] [bbotk]  mtry.ratio min.node.size regr.mse warnings errors runtime_learners \nINFO  [15:24:37.514] [bbotk]   0.7666667            20 15.23424        0      0            0.016 \nINFO  [15:24:37.514] [bbotk]                                 uhash \nINFO  [15:24:37.514] [bbotk]  1414f1fe-21ab-4ce4-96f2-7945b0fa1a61 \nINFO  [15:24:37.515] [bbotk] Evaluating 1 configuration(s) \nINFO  [15:24:37.524] [mlr3] Running benchmark with 3 resampling iterations \nINFO  [15:24:37.526] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 1/3) \nINFO  [15:24:37.551] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 3/3) \nINFO  [15:24:37.567] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 2/3) \nINFO  [15:24:37.583] [mlr3] Finished benchmark \nINFO  [15:24:37.595] [bbotk] Result of batch 8: \nINFO  [15:24:37.595] [bbotk]  mtry.ratio min.node.size regr.mse warnings errors runtime_learners \nINFO  [15:24:37.595] [bbotk]         0.9             1 7.542293        0      0             0.04 \nINFO  [15:24:37.595] [bbotk]                                 uhash \nINFO  [15:24:37.595] [bbotk]  d76cd2a6-a9ce-4368-a455-257158761887 \nINFO  [15:24:37.596] [bbotk] Evaluating 1 configuration(s) \nINFO  [15:24:37.605] [mlr3] Running benchmark with 3 resampling iterations \nINFO  [15:24:37.608] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 1/3) \nINFO  [15:24:37.619] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 3/3) \nINFO  [15:24:37.629] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 2/3) \nINFO  [15:24:37.641] [mlr3] Finished benchmark \nINFO  [15:24:37.652] [bbotk] Result of batch 9: \nINFO  [15:24:37.653] [bbotk]  mtry.ratio min.node.size regr.mse warnings errors runtime_learners \nINFO  [15:24:37.653] [bbotk]   0.6333333             7 8.534506        0      0            0.021 \nINFO  [15:24:37.653] [bbotk]                                 uhash \nINFO  [15:24:37.653] [bbotk]  9f2d6cc1-6eb6-47b7-966e-449a3d7c568f \nINFO  [15:24:37.653] [bbotk] Evaluating 1 configuration(s) \nINFO  [15:24:37.668] [mlr3] Running benchmark with 3 resampling iterations \nINFO  [15:24:37.672] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 1/3) \nINFO  [15:24:37.688] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 3/3) \nINFO  [15:24:37.702] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 2/3) \nINFO  [15:24:37.716] [mlr3] Finished benchmark \nINFO  [15:24:37.728] [bbotk] Result of batch 10: \nINFO  [15:24:37.728] [bbotk]  mtry.ratio min.node.size regr.mse warnings errors runtime_learners \nINFO  [15:24:37.728] [bbotk]         0.5             1 7.411416        0      0            0.031 \nINFO  [15:24:37.728] [bbotk]                                 uhash \nINFO  [15:24:37.728] [bbotk]  67a788fc-6591-4215-9fa7-7d90b6a255e0 \nINFO  [15:24:37.729] [bbotk] Evaluating 1 configuration(s) \nINFO  [15:24:37.739] [mlr3] Running benchmark with 3 resampling iterations \nINFO  [15:24:37.741] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 2/3) \nINFO  [15:24:37.751] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 1/3) \nINFO  [15:24:37.760] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 3/3) \nINFO  [15:24:37.769] [mlr3] Finished benchmark \nINFO  [15:24:37.781] [bbotk] Result of batch 11: \nINFO  [15:24:37.782] [bbotk]  mtry.ratio min.node.size regr.mse warnings errors runtime_learners \nINFO  [15:24:37.782] [bbotk]   0.6333333            20 14.74427        0      0            0.018 \nINFO  [15:24:37.782] [bbotk]                                 uhash \nINFO  [15:24:37.782] [bbotk]  f528569d-cdd5-4df9-9717-3337c21d9771 \nINFO  [15:24:37.782] [bbotk] Evaluating 1 configuration(s) \nINFO  [15:24:37.800] [mlr3] Running benchmark with 3 resampling iterations \nINFO  [15:24:37.803] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 1/3) \nINFO  [15:24:37.814] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 2/3) \nINFO  [15:24:37.825] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 3/3) \nINFO  [15:24:37.837] [mlr3] Finished benchmark \nINFO  [15:24:37.849] [bbotk] Result of batch 12: \nINFO  [15:24:37.849] [bbotk]  mtry.ratio min.node.size regr.mse warnings errors runtime_learners \nINFO  [15:24:37.849] [bbotk]         0.5             7 9.065184        0      0            0.022 \nINFO  [15:24:37.849] [bbotk]                                 uhash \nINFO  [15:24:37.849] [bbotk]  51a77a22-48ab-4cde-9b3c-7457815af994 \nINFO  [15:24:37.850] [bbotk] Evaluating 1 configuration(s) \nINFO  [15:24:37.859] [mlr3] Running benchmark with 3 resampling iterations \nINFO  [15:24:37.862] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 1/3) \nINFO  [15:24:37.876] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 3/3) \nINFO  [15:24:37.892] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 2/3) \nINFO  [15:24:37.907] [mlr3] Finished benchmark \nINFO  [15:24:37.918] [bbotk] Result of batch 13: \nINFO  [15:24:37.919] [bbotk]  mtry.ratio min.node.size regr.mse warnings errors runtime_learners \nINFO  [15:24:37.919] [bbotk]   0.6333333             1 7.830295        0      0            0.036 \nINFO  [15:24:37.919] [bbotk]                                 uhash \nINFO  [15:24:37.919] [bbotk]  bebf8e18-9159-491d-a795-af72732b9d96 \nINFO  [15:24:37.920] [bbotk] Evaluating 1 configuration(s) \nINFO  [15:24:37.936] [mlr3] Running benchmark with 3 resampling iterations \nINFO  [15:24:37.939] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 3/3) \nINFO  [15:24:37.948] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 1/3) \nINFO  [15:24:37.956] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 2/3) \nINFO  [15:24:37.965] [mlr3] Finished benchmark \nINFO  [15:24:37.976] [bbotk] Result of batch 14: \nINFO  [15:24:37.977] [bbotk]  mtry.ratio min.node.size regr.mse warnings errors runtime_learners \nINFO  [15:24:37.977] [bbotk]         0.5            20 14.68091        0      0            0.015 \nINFO  [15:24:37.977] [bbotk]                                 uhash \nINFO  [15:24:37.977] [bbotk]  55fbbdfe-4d6b-4fe8-9ac7-41d1e2653b6e \nINFO  [15:24:37.977] [bbotk] Evaluating 1 configuration(s) \nINFO  [15:24:37.986] [mlr3] Running benchmark with 3 resampling iterations \nINFO  [15:24:37.988] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 2/3) \nINFO  [15:24:37.998] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 3/3) \nINFO  [15:24:38.009] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 1/3) \nINFO  [15:24:38.020] [mlr3] Finished benchmark \nINFO  [15:24:38.035] [bbotk] Result of batch 15: \nINFO  [15:24:38.036] [bbotk]  mtry.ratio min.node.size regr.mse warnings errors runtime_learners \nINFO  [15:24:38.036] [bbotk]   0.7666667             7 8.826943        0      0            0.022 \nINFO  [15:24:38.036] [bbotk]                                 uhash \nINFO  [15:24:38.036] [bbotk]  0f5236e2-5d8a-4a63-ad2b-f07791142183 \nINFO  [15:24:38.037] [bbotk] Evaluating 1 configuration(s) \nINFO  [15:24:38.047] [mlr3] Running benchmark with 3 resampling iterations \nINFO  [15:24:38.049] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 2/3) \nINFO  [15:24:38.063] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 3/3) \nINFO  [15:24:38.076] [mlr3] Applying learner 'regr.ranger' on task 'example' (iter 1/3) \nINFO  [15:24:38.091] [mlr3] Finished benchmark \nINFO  [15:24:38.102] [bbotk] Result of batch 16: \nINFO  [15:24:38.103] [bbotk]  mtry.ratio min.node.size regr.mse warnings errors runtime_learners \nINFO  [15:24:38.103] [bbotk]   0.7666667             1 7.680444        0      0            0.032 \nINFO  [15:24:38.103] [bbotk]                                 uhash \nINFO  [15:24:38.103] [bbotk]  87d5f575-7546-4700-9a85-c8f0c7992b49 \nINFO  [15:24:38.105] [bbotk] Finished optimizing after 16 evaluation(s) \nINFO  [15:24:38.105] [bbotk] Result: \nINFO  [15:24:38.105] [bbotk]  mtry.ratio min.node.size learner_param_vals  x_domain regr.mse \nINFO  [15:24:38.105] [bbotk]         0.5             1          <list[4]> <list[2]> 7.411416 \n\n\nYou can access the optimized learner by accessing the model$learner attribute.\n\nauto_tunning_learner$model$learner\n\n<LearnerRegrRanger:regr.ranger>\n* Model: ranger\n* Parameters: num.threads=1, max.depth=10, mtry.ratio=0.5,\n  min.node.size=1\n* Packages: mlr3, mlr3learners, ranger\n* Predict Type: response\n* Feature types: logical, integer, numeric, character, factor, ordered\n* Properties: hotstart_backward, importance, oob_error, weights\n\n\nYou can look at the history of tuning process like below:\n\nauto_tunning_learner$model$tuning_instance\n\n<TuningInstanceSingleCrit>\n* State:  Optimized\n* Objective: <ObjectiveTuning:regr.ranger_on_example>\n* Search Space:\n              id    class lower upper nlevels\n1:    mtry.ratio ParamDbl   0.5   0.9     Inf\n2: min.node.size ParamInt   1.0  20.0      20\n* Terminator: <TerminatorEvals>\n* Result:\n   mtry.ratio min.node.size regr.mse\n1:        0.5             1 7.411416\n* Archive:\n    mtry.ratio min.node.size  regr.mse\n 1:  0.5000000            14 11.787063\n 2:  0.9000000            14 11.548363\n 3:  0.6333333            14 11.807311\n 4:  0.7666667            14 12.060974\n 5:  0.9000000             7  8.914293\n 6:  0.9000000            20 15.142857\n 7:  0.7666667            20 15.234245\n 8:  0.9000000             1  7.542293\n 9:  0.6333333             7  8.534506\n10:  0.5000000             1  7.411416\n11:  0.6333333            20 14.744271\n12:  0.5000000             7  9.065184\n13:  0.6333333             1  7.830295\n14:  0.5000000            20 14.680911\n15:  0.7666667             7  8.826943\n16:  0.7666667             1  7.680444\n\n\nYou can of course use the predict() method as well.\n\nauto_tunning_learner$predict(reg_task)\n\n<PredictionRegr> for 32 observations:\n    row_ids truth response\n          1  21.0 20.89420\n          2  21.0 20.89920\n          3  22.8 24.02480\n---                       \n         30  19.7 19.90290\n         31  15.0 14.87800\n         32  21.4 21.96653"
  },
  {
    "objectID": "PROG-R-01-mlr3.html#sec-mlr3-in-action",
    "href": "PROG-R-01-mlr3.html#sec-mlr3-in-action",
    "title": "18  Machine Learning with mlr3",
    "section": "18.6 mlr3 in action",
    "text": "18.6 mlr3 in action\nHere, an example usage of the mlr3 framework as a part of a research process is presented. Suppose our goal is to estimate the heterogeneous treatment effect using causal forest using the R grf package. The grf::causal_forest() function implements causal forest as an R-learner and it uses random forest for its first-stage estimations by default. However, the function allows the users to provide their own estimated values of \\(y\\) (dependent variable) and \\(T\\) (treatment). We will code the process of conducting the first stage using mlr3 and then use grf::causal_forest() to estimate heterogeneous treatment effects.\n\n#=== load the Treatment dataset ===#\ndata(\"Treatment\", package = \"Ecdat\")\n\n#=== convert to a data.table ===#\n(\ndata <- \n  data.table(Treatment)\n)\n\n      treat age educ     ethn married    re74    re75     re78   u74   u75\n   1:  TRUE  37   11    black    TRUE     0.0     0.0  9930.05  TRUE  TRUE\n   2:  TRUE  30   12    black   FALSE     0.0     0.0 24909.50  TRUE  TRUE\n   3:  TRUE  27   11    black   FALSE     0.0     0.0  7506.15  TRUE  TRUE\n   4:  TRUE  33    8    black   FALSE     0.0     0.0   289.79  TRUE  TRUE\n   5:  TRUE  22    9    black   FALSE     0.0     0.0  4056.49  TRUE  TRUE\n  ---                                                                     \n2671: FALSE  47    8    other    TRUE 44667.4 33837.1 38568.70 FALSE FALSE\n2672: FALSE  32    8    other    TRUE 47022.4 67137.1 59109.10 FALSE FALSE\n2673: FALSE  47   10    other    TRUE 48198.0 47968.1 55710.30 FALSE FALSE\n2674: FALSE  54    0 hispanic    TRUE 49228.5 44221.0 20540.40 FALSE FALSE\n2675: FALSE  40    8    other    TRUE 50940.9 55500.0 53198.20 FALSE FALSE\n\n\nThe dependent variable is re78 (\\(Y\\)), which is real annual earnings in 1978 (after treatment). The treatment variable of interest is treat (\\(T\\)), which is TRUE if a person had gone through a training, FALSE otherwise. The features that are included as potential drivers of the heterogeneity in the impact of treat on re78 is age, educ, ethn, and married (\\(X\\)). Note that the focus of this section is just showcasing the use of mlr3 and no attention is paid to potential endogeneity problems.\n\n18.6.1 First stage\nWe will use ranger() and xgboost() as learners. While ranger() accepts factor variables, xgboost() does not. So, we will one-hot-encode the data using mltools::one_hot() so that we can just create a single Task and use it for both. We also turn treat to a factor so it is amenable with classification jobs by the two learner functions.\n\n(\ndata_trt <- \n  mltools::one_hot(data) %>% \n  .[, treat := factor(treat)]\n)\n\nNote that we need separate procedures for estimating \\(E[Y|X]\\) and \\(E[T|X]\\). The former is a regression and the latter is a classification task.\nLet’s first work on estimating \\(E[Y|X]\\). First, we set up a task.\n\ny_est_task <- \n  TaskRegr$new(\n    id = \"estimate_y_on_x\",\n    backend = data_trt[, .(\n      re78, age, educ, married, \n      ethn_other, ethn_black, ethn_hispanic \n    )],\n    target = \"re78\"\n  )\n\nWe consider two modeling approaches: random forest by ranger and gradient boosted forest by xgboost.\n\ny_learner_ranger <- lrn(\"regr.ranger\")\ny_learner_xgboost <- lrn(\"regr.xgboost\")\n\nWe will implement a K-fold cross-validation to select the better model with optimized hyper-parameter values. We do this by applying triggering Tuner classed defined separately for the two learners.\nLet’s define the resampling method, measure, terminator, and tuner that will be shared by the two approaches.\n\nresampling_y <- rsmp(\"cv\", folds = 4)\nmeasure_y <- msr(\"regr.mse\")\nterminator <- trm(\"evals\", n_evals = 100)\ntuner <- tnr(\"grid_search\", resolution = 4)\n\nNow, when we compare multiple models, we should use the same CV splits to have a fair comparison their model performance. To ensure this, we need to use an instantiated Resampling object.\n\n\nIf you provide an un-instantiated Resampling object to an AutoTuner, it will instantiate the Resampling object internally and two separate AutoTuners can result in two distinct splits.\n\n#=== instantiate ===#\nresampling_y$instantiate(y_est_task)\n\n#=== confirm it is indeed instantiated ===#\nresampling_y\n\n<ResamplingCV>: Cross-Validation\n* Iterations: 4\n* Instantiated: TRUE\n* Parameters: folds=4\n\n\nLet’s define search space for each of the learners.\n\nsearch_space_ranger <-\n  ps(\n    mtry = p_int(lower = 1, upper = length(y_est_task$feature_names)),\n    min.node.size = p_int(lower = 1, upper = 20)\n  )\n\nsearch_space_xgboost <-\n  ps(\n    nrounds = p_int(lower = 100, upper = 400),\n    eta = p_dbl(lower = 0.01, upper = 1)\n  )\n\nWe have all the ingredients to set up TuningInstances for the learners.\n\ntuning_instance_ranger <-\n  TuningInstanceSingleCrit$new(\n    task = y_est_task,\n    learner = y_learner_ranger,\n    resampling = resampling_y,\n    measure = measure_y,\n    search_space = search_space_ranger,\n    terminator = terminator\n  )\n\ntuning_instance_xgboost <-\n  TuningInstanceSingleCrit$new(\n    task = y_est_task,\n    learner = y_learner_xgboost,\n    resampling = resampling_y,\n    measure = measure_y,\n    search_space = search_space_xgboost,\n    terminator = terminator\n  )\n\nLet’s tune them now (this can take a while).\n\n\nWe use using the same tuner here, but you can use different tuning processes for the learners. For example, you can have resolution = 5 for tuning regr.xgboost.\n\n#=== tune ranger ===#\ntuner$optimize(tuning_instance_ranger)\n\n#=== tune xgboost ===#\ntuner$optimize(tuning_instance_xgboost)\n\nHere are the MSEs from the two individually tuned learners.\n\ntuning_instance_ranger$result_y\n\n regr.mse \n190679415 \n\ntuning_instance_xgboost$result_y\n\n regr.mse \n195496531 \n\n\nSo, in this example, we go with regr.ranger with its optimized hyper-parameter values. Let’s update our learner with the optimized hyper-parameter values.\n\n(\ny_learner_ranger$param_set$values <- tuning_instance_ranger$result_learner_param_vals\n)\n\n$num.threads\n[1] 1\n\n$mtry\n[1] 2\n\n$min.node.size\n[1] 1\n\n\nNow that we have decided on the model to use for predicting \\(E[y|X]\\), let’s implement cross-fitting.\n\ncv_results_y <-\n  resample(\n    y_est_task, \n    y_learner_ranger, \n    rsmp(\"repeated_cv\", repeats = 4, folds = 3) \n  )\n\nINFO  [15:25:05.866] [mlr3] Applying learner 'regr.ranger' on task 'estimate_y_on_x' (iter 6/12) \nINFO  [15:25:05.999] [mlr3] Applying learner 'regr.ranger' on task 'estimate_y_on_x' (iter 12/12) \nINFO  [15:25:06.129] [mlr3] Applying learner 'regr.ranger' on task 'estimate_y_on_x' (iter 3/12) \nINFO  [15:25:06.259] [mlr3] Applying learner 'regr.ranger' on task 'estimate_y_on_x' (iter 9/12) \nINFO  [15:25:06.396] [mlr3] Applying learner 'regr.ranger' on task 'estimate_y_on_x' (iter 8/12) \nINFO  [15:25:06.530] [mlr3] Applying learner 'regr.ranger' on task 'estimate_y_on_x' (iter 10/12) \nINFO  [15:25:06.668] [mlr3] Applying learner 'regr.ranger' on task 'estimate_y_on_x' (iter 1/12) \nINFO  [15:25:06.806] [mlr3] Applying learner 'regr.ranger' on task 'estimate_y_on_x' (iter 5/12) \nINFO  [15:25:06.940] [mlr3] Applying learner 'regr.ranger' on task 'estimate_y_on_x' (iter 4/12) \nINFO  [15:25:07.077] [mlr3] Applying learner 'regr.ranger' on task 'estimate_y_on_x' (iter 11/12) \nINFO  [15:25:07.210] [mlr3] Applying learner 'regr.ranger' on task 'estimate_y_on_x' (iter 2/12) \nINFO  [15:25:07.343] [mlr3] Applying learner 'regr.ranger' on task 'estimate_y_on_x' (iter 7/12) \n\n#=== all combined ===#\nall_predictions_y <- \n  cv_results_y$prediction() %>% \n  as.data.table() %>% \n  .[, .(y_hat = mean(response)), by = row_ids] %>%\n  .[order(row_ids), ]\n\nWe basically follow the same process for estimating \\(E[T|X]\\). Let’s set up tuning processes for the learners.\n\nt_est_task <-\n  TaskClassif$new(\n    id = \"estimate_t_on_x\",\n    backend = \n      data_trt[, .(\n        treat, age, educ, married, \n        ethn_other, ethn_black, ethn_hispanic \n      )],\n    target = \"treat\"\n  )\n\nt_learner_ranger <- lrn(\"classif.ranger\")\nt_learner_xgboost <- lrn(\"classif.xgboost\")\n\nresampling_t <- rsmp(\"cv\", folds = 4)\nresampling_y$instantiate(t_est_task)\nmeasure_t <- msr(\"classif.ce\")\nterminator <- trm(\"evals\", n_evals = 100)\ntuner <- tnr(\"grid_search\", resolution = 4)\n\ntuning_instance_ranger <-\n  TuningInstanceSingleCrit$new(\n    task = t_est_task,\n    learner = t_learner_ranger,\n    resampling = resampling_y,\n    measure = measure_t,\n    search_space = search_space_ranger,\n    terminator = terminator\n  )\n\ntuning_instance_xgboost <-\n  TuningInstanceSingleCrit$new(\n    task = t_est_task,\n    learner = t_learner_xgboost,\n    resampling = resampling_y,\n    measure = measure_t,\n    search_space = search_space_xgboost,\n    terminator = terminator\n  )\n\nHere are the classification error from the two individually tuned learners.\n\n#=== tune ranger ===#\ntuner$optimize(tuning_instance_ranger)\ntuning_instance_ranger$result_y\n\n\n#=== tune xgboost ===#\ntuner$optimize(tuning_instance_xgboost)\ntuning_instance_xgboost$result_y\n\nSo, we are picking the classif.ranger option here as well as it has a lower classification error.\n\nt_learner_ranger$param_set$values <- tuning_instance_ranger$result_learner_param_vals\n\nNow that we have decided on the model to use for predicting \\(E[T|X]\\), let’s implement cross-fitting. Before cross-fitting, we need to tell t_learner_ranger to predict probability instead of classification (either 0 or 1).\n\nt_learner_ranger$predict_type <- \"prob\"\n\ncv_results_t <-\n  resample(\n    t_est_task, \n    t_learner_ranger, \n    rsmp(\"repeated_cv\", repeats = 4, folds = 3) \n  )\n\nINFO  [15:25:27.166] [mlr3] Applying learner 'classif.ranger' on task 'estimate_t_on_x' (iter 3/12) \nINFO  [15:25:27.302] [mlr3] Applying learner 'classif.ranger' on task 'estimate_t_on_x' (iter 4/12) \nINFO  [15:25:27.431] [mlr3] Applying learner 'classif.ranger' on task 'estimate_t_on_x' (iter 10/12) \nINFO  [15:25:27.558] [mlr3] Applying learner 'classif.ranger' on task 'estimate_t_on_x' (iter 1/12) \nINFO  [15:25:27.687] [mlr3] Applying learner 'classif.ranger' on task 'estimate_t_on_x' (iter 11/12) \nINFO  [15:25:27.829] [mlr3] Applying learner 'classif.ranger' on task 'estimate_t_on_x' (iter 5/12) \nINFO  [15:25:27.968] [mlr3] Applying learner 'classif.ranger' on task 'estimate_t_on_x' (iter 8/12) \nINFO  [15:25:28.100] [mlr3] Applying learner 'classif.ranger' on task 'estimate_t_on_x' (iter 7/12) \nINFO  [15:25:28.227] [mlr3] Applying learner 'classif.ranger' on task 'estimate_t_on_x' (iter 12/12) \nINFO  [15:25:28.358] [mlr3] Applying learner 'classif.ranger' on task 'estimate_t_on_x' (iter 6/12) \nINFO  [15:25:28.488] [mlr3] Applying learner 'classif.ranger' on task 'estimate_t_on_x' (iter 2/12) \nINFO  [15:25:28.617] [mlr3] Applying learner 'classif.ranger' on task 'estimate_t_on_x' (iter 9/12) \n\n#=== all combined ===#\nall_predictions_t <- \n  cv_results_t$prediction() %>% \n  as.data.table() %>% \n  .[, .(t_hat = mean(prob.TRUE)), by = row_ids] %>% \n  .[order(row_ids), ]\n\nWe now use all_predictions_y and all_predictions_t for Y.hat and W.hat in grf::causal_forest().\n\ngrf::causal_forest(\n  X = data_trt[, .(age, educ, married, ethn_other, ethn_black, ethn_hispanic)] %>% as.matrix(),\n  Y = data_trt[, re78],\n  W = data_trt[, fifelse(treat == TRUE, 1, 0)],\n  Y.hat = all_predictions_y[, y_hat],\n  W.hat = all_predictions_t[, t_hat]\n)\n\nGRF forest object of type causal_forest \nNumber of trees: 2000 \nNumber of training samples: 2675 \nVariable importance: \n    1     2     3     4     5     6 \n0.508 0.302 0.110 0.027 0.047 0.007"
  },
  {
    "objectID": "PROG-R-02-reticulate.html",
    "href": "PROG-R-02-reticulate.html",
    "title": "19  Running Python from R",
    "section": "",
    "text": "There are many Python packages that implement machine learning methods that are not available in R at the moment. Fortunately for R users, we can easily call Python functions from within R to take advantage of those Python packages. In this chapter, we learn how to do so using the reticulate package. Its package website is very helpful and cover more topics than this chapter does.\nWe will run mainly the following python codes in different ways on R."
  },
  {
    "objectID": "PROG-R-02-reticulate.html#set-up-a-python-virtual-environment",
    "href": "PROG-R-02-reticulate.html#set-up-a-python-virtual-environment",
    "title": "19  Running Python from R",
    "section": "19.1 Set up a Python virtual environment",
    "text": "19.1 Set up a Python virtual environment\nIt is recommended that you have a python virtual environment (VE) specific for each of your projects. A VE is independent of the rest of the Python environment on your computer and avoid breaking codes from your old projects that may depend on a specific versions of Python packages. It also made it easier for others to replicate your work.\nLet’s first load the reticulate package.\n\nlibrary(reticulate)\n\nFor inexperienced Python users, the reticulate package offers an easy way to set up and manage a Python VE. First, you can create a VE using virtualenv_create(). The following code for example create a VE called ml-learning.\n\nvirtualenv_create(\"ml-learning\")\n\nThe newly created VE have only minimal packages. Here is the list of packages I have in this new VE.\n\n\n\n\n\nThis is despite all the packages that I have installed in my main Python installation.\nYou can install packages to a VE using virtualenv_install(). For example, the code below attempts to install the econml package to the VE called ml-learning.\n\nvirtualenv_install(\"ml-learning\", \"econml\")\n\nIn order to use a VE, you use use_virtualenv() after you load the reticulate package.\n\nuse_virtualenv(\"ml-learning\")\n\nAfter this, all your python codes will be run on the VE when you are running them through the reticulate package. We will now look at how we actually run Python codes from R."
  },
  {
    "objectID": "PROG-R-02-reticulate.html#sec-r-python-repl",
    "href": "PROG-R-02-reticulate.html#sec-r-python-repl",
    "title": "19  Running Python from R",
    "section": "19.2 R and Python REPL simultaneously",
    "text": "19.2 R and Python REPL simultaneously\n\n\nRPEL stands for read-evaluate-print-loop.\nWithin your R session, you can start a Python session that is embedded in it by running repl_python() from R. This will allow you to go back and forth between R and Python. Objects created in Python can be accessed from R using the py object as described above. This provides an ideal environment where you can test the interactions of your R and Python codes in a seamless manner.\nYou initiate a Python environment using repl_python(). If you are using RStudio, this is what it looks like.\n\nrepl_python()\n\n\n\n\n\n\nAs you can see on the console tab, a python session has been initiated, which is connected your R session. As long as you are using RStudio, you do not have to worry about switching between R and Python manually. RStudio is smart enough to know where the codes are coming from and execute codes where they should be executed. That is, if you run codes from a python (R) file, they will be sent to the python (R) console.\nPicture below shows what the console looks like after I executed library(data.table) while I am still on the python console. You can see >>> quit (exit from python) before library(data.table) was executed on R.\n\n\n\n\n\nLet’s now define an object called a in python.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFrom R, you can access objects defined on Python by using py$ prefix to the object name on python. This is like you are accessing an element of a list on R. py has a list of objects defined on Python.\n\n\nLet’s now confirm this by accessing a on R.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhen you access R objects in Python, you can use r. in front of the object name. We first run b <- 1:10 and then confirm r.b can get you the Python equivalent of b in R.\n\n\n\n\n\n\n\nGreat. This seamless integration is really nice especially when you are writing an R-centric program that include pythons codes and are tying to debug.\nWhenever R and Python exchange objects, they are automatically converted to their counterparts in the receiving program. For example, a vector with multiple elements in R will be converted to a list in Python, and vice versa. See here for more examples."
  },
  {
    "objectID": "PROG-R-02-reticulate.html#importing-python-modules",
    "href": "PROG-R-02-reticulate.html#importing-python-modules",
    "title": "19  Running Python from R",
    "section": "19.3 Importing Python modules",
    "text": "19.3 Importing Python modules\nYou can import Python modules and use them as if they are R functions. This way of interacting with Python involves the least direct interactions with Python among all the options that the reticulate package provides us with. Unlike the approach above (Section 19.2), you do not see outcomes of your Python codes on the Python console. Rather, you will see the outcomes translated to R object on R.\nYou can import a python module using import() like below, which imports the Python os package.\n\n(\nos <- import(\"os\")\n)\n\nModule(os)\n\n\nThe imported module is assigned to os on R. You can access functions in a module using $ instead of . as done in Python.\nFor example, the following code use listdir() from the os module.\n\nos$listdir(\".\") %>% head()\n\n[1] \"B03-cross-validation_files\"              \n[2] \"C01-dml.qmd\"                             \n[3] \"PROG-R-03-model-selection-prediction.qmd\"\n[4] \".Rhistory\"                               \n[5] \"P03-xgb.html\"                            \n[6] \"L00-prediction-vs-causal-inference.html\" \n\n\nLet’s rewrite the above sample Python code using this method. First, we import all the modules (not the functions) we need.\n\nsk_ds <- import(\"sklearn.datasets\")\nsk_ms <- import(\"sklearn.model_selection\")\nsk_ensemble <- import(\"sklearn.ensemble\")\nnp <- import(\"numpy\")\n\nIf you try this, you will get a complaint.\n\nmake_regression <- import(\"sklearn.datasets.make_regression\")\n\nError in py_module_import(module, convert = convert): ModuleNotFoundError: No module named 'sklearn.datasets.make_regression'\n\n\nNow, let’s define parameters used in make_regression().\n\nn_samples <- as.integer(2000)\nn_features <- as.integer(20)\nn_informative <- as.integer(15)\nnoise <- 2\nrng <- np$random$RandomState(as.integer(123))\n\nNote that the values of n_sample, n_features, and n_informative are made integers explicitly. This is because Python accepts only integers for those parameters (see here). noise is a float and you can be loose about what type of numeric value you provide.\nNow, in the Python code below, make_regression create a tuple (like a list on R) of tuple of length 2: the first one is an array of 2000 by 20 (assigned to X) and the second one is an array of 2000 by 1 (assigned to y). Python has a convenient way of assigning the elements of a tuple to new objects as shown below.\n\nX, y = make_regression(\n  n_samples, \n  n_features, \n  n_informative = n_informative, \n  noise = noise, \n  random_state = rng\n)\n\nR is a bit clumsy on this. So, we can just assign the list of arrays into a single object as a list like below and then extract its elements separately later.\n\nsynth_data <-\n  sk_ds$make_regression(\n    n_samples, \n    n_features, \n    n_informative = n_informative, \n    noise = noise, \n    random_state = rng\n  )\n\nHere is the structure of the data created.\n\nstr(synth_data)\n\nList of 2\n $ : num [1:2000, 1:20] 1.465 -0.8946 1.1291 -0.0123 -0.124 ...\n $ : num [1:2000(1d)] 168.3 -95.3 163 31.4 -112.7 ...\n\n\nWe can then generate \\(X\\) and \\(y\\) like below.\n\nX <- synth_data[[1]]\ny <- synth_data[[2]]\n\nWe now split the dataset into the train and test datasets using train_test_split().\n\ntrain_test_ls <- sk_ms$train_test_split(X, y, random_state = rng)\n\nAssign each element of train_test_ls to R objects with appropriate names.\n\nX_train <- train_test_ls[[1]]\nX_test <- train_test_ls[[2]]\ny_train <- train_test_ls[[3]]\ny_test <- train_test_ls[[4]]\n\nThen, train RF on the train data, and then test the fit.\n\n#=== set up an RF ===#\nreg_rf <- sk_ensemble$RandomForestRegressor(max_features = \"sqrt\")\n\n#=== train an RF on the train data ===#\nreg_rf$fit(X_train, y_train)\n\nRandomForestRegressor(max_features='sqrt')\n\n#=== test the fit ===#\nreg_rf$score(X_test, y_test)\n\n[1] 0.6956042"
  },
  {
    "objectID": "PROG-R-02-reticulate.html#sourcing-python-scripts",
    "href": "PROG-R-02-reticulate.html#sourcing-python-scripts",
    "title": "19  Running Python from R",
    "section": "19.4 Sourcing Python scripts",
    "text": "19.4 Sourcing Python scripts\nAnother way to run Python codes from R is to source a python script to make python functions and modules available.\nHere is the Python code in the file called import_modules.py.\n\n# | eval: false\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_regression\nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy\n\nYou can source this file using source_python().\n\nsource_python(\"import_modules.py\")\n\n\n\nimport_modules.py is included in the github repository.\nThe first three lines are importing functions (train_test_split, make_regression, and RandomForestRegressor), while the last line is importing a module (numpy). When functions are imported in this manner, they can be used without py$ as if they are R functions.\n\nsynth_data <-\n  make_regression(\n    n_samples, \n    n_features, \n    n_informative = n_informative, \n    noise = noise, \n    random_state = rng\n  )\n\nlength(synth_data)\n\n[1] 2\n\n\nHowever, a module imported in this manner cannot be used without py$. So, you cannot do this.\n\nnumpy$random$RandomState(as.integer(123))\n\nError in eval(expr, envir, enclos): object 'numpy' not found\n\n\nRather, you need to do this:\n\npy$numpy$random$RandomState(as.integer(123))\n\nRandomState(MT19937) at 0x2D5205640\n\n\nYou can source user-defined functions on Python as well. Here is the python code in the file called make_data.py. train_test_RF() takes some of the arguments for make_regression() as its arguments, split the data, train RF on the train data, evaluate the fit, and then return the test score.\n\ndef train_test_RF(n_samples, n_features, n_informative, noise, rng):\n\n  from sklearn.model_selection import train_test_split\n  from sklearn.datasets import make_regression\n  from sklearn.ensemble import RandomForestRegressor\n  import numpy as np\n  \n  X, y = make_regression(\n    n_samples, \n    n_features, \n    n_informative = n_informative, \n    noise = noise, \n    random_state = rng\n  )\n  \n  X_train, X_test, y_train, y_test = train_test_split(\n      X, y, random_state = rng\n  )\n\n  reg_rf = RandomForestRegressor(max_features = \"sqrt\")\n\n  reg_rf.fit(X_train, y_train)\n\n  test_score = reg_rf.score(X_test, y_test)\n\n  return test_score\n\nSourcing this file using source_python(),\n\nsource_python(\"run_rf.py\")\n\nOnce this is done, train_test_RF() is now available to use on R.\n\ntrain_test_RF(\n  n_samples, \n  n_features, \n  n_informative = n_informative, \n  noise = noise, \n  rng = rng\n)\n\n[1] 0.7053812"
  },
  {
    "objectID": "PROG-R-02-reticulate.html#python-in-rmarkdown-or-quarto",
    "href": "PROG-R-02-reticulate.html#python-in-rmarkdown-or-quarto",
    "title": "19  Running Python from R",
    "section": "19.5 Python in Rmarkdown or Quarto",
    "text": "19.5 Python in Rmarkdown or Quarto\nUsing R and Python in an Rmarkdown or Quarto file is extremely easy. You can have both R and Python codes in the same Rmd file. When you write R (python) codes, you use R (python) code chunks indicated by {R} ({python}) like below.\n\n\n\n\n\nFirst code chunk is an R code chunk and loads libraries including reticulate.\n\n\n\n\n\n\nTip\n\n\n\nIf you do not load the reticulate package, python code chunks still run, but the objects defined on python would not be available on R.\n\n\nThe second code chunk is a python code chunk and generate synthetic data using make_regression from sklearn.datasets.\nThe third code chunk is an R code chunk. It uses X and y generated on Python to train an RF using ranger()."
  },
  {
    "objectID": "PROG-R-03-model-selection-prediction.html",
    "href": "PROG-R-03-model-selection-prediction.html",
    "title": "20  Model Selection (Prediction)",
    "section": "",
    "text": "When you use a DML approach, the first stage estimations are prediction tasks. In this section, R codes to select a model for the first stage estimations are presented. We first implement the task using mlr3 (Section 20.1) and then use the reticulate pacakge to emulate the task done in Python in ?sec-python-model-selection from within R (Section 20.2)."
  },
  {
    "objectID": "PROG-R-03-model-selection-prediction.html#sec-mlr3-model-selection",
    "href": "PROG-R-03-model-selection-prediction.html#sec-mlr3-model-selection",
    "title": "20  Model Selection (Prediction)",
    "section": "20.1 R implementation using mlr3",
    "text": "20.1 R implementation using mlr3\nYou can use benchmark_grid() and benchmark() to imitate GridSearchCVList() from the Python econml package with a bit of extra coding.\nLet’s use mtcars data for demonstration.\n\n#=== load the mtcars data ===#\ndata(\"mtcars\", package = \"datasets\")\n\nreg_task <-\n  TaskRegr$new(\n    id = \"regression\",\n    backend = mtcars,\n    target = \"mpg\"\n  )\n\nWe can create a list of learners that vary in the value of hyper-parameters we would like to tune. Suppose you are interested in using random forest and extreme gradient boosting. We can create such a list for each of them and then combine. Let’s do that for random forest first. It is much like what we did for GridSearchCVList() in ?sec-python-model-selection.\nFirst create a sequence of values for parameters to be tuned.\n\n#=== sequence of values ===#\nmtry_seq <- c(1, 3, 5)\nmin_node_size_seq <- c(5, 10, 20)\n\nWe can create a grid of values ourselves.\n\n(\nsearch_space_ranger <- \n  data.table::CJ(\n    mtry = mtry_seq,\n    min_node_size = min_node_size_seq\n  )\n)\n\n   mtry min_node_size\n1:    1             5\n2:    1            10\n3:    1            20\n4:    3             5\n5:    3            10\n6:    3            20\n7:    5             5\n8:    5            10\n9:    5            20\n\n\nWe now loop over the row of search_space_ranger to create learners with each pair (row) stored in search_space_ranger.\n\nlrn_list_ranger <-\n  lapply(\n    seq_len(nrow(search_space_ranger)),\n    function(x) {\n      lrn(\n        \"regr.ranger\", \n        mtry = search_space_ranger[x, mtry], \n        min.node.size = search_space_ranger[x, min_node_size]\n      )\n    }\n  )\n\nWe can do the same for xgboost.\n\n#=== sequence of values ===#\nnrounds_seq <- c(100, 400, by = 100)\neta_seq <- seq(0.01, 1, length = 5)\n\n#=== define the grid ===#\nsearch_space_xgb <- \n  data.table::CJ(\n    nrounds = nrounds_seq,\n    eta = eta_seq\n  )\n\nlrn_list_xgboost <-\n  lapply(\n    seq_len(nrow(search_space_xgb)),\n    function(x) {\n      lrn(\n        \"regr.xgboost\",\n        nrounds = search_space_xgb[x, nrounds],\n        eta = search_space_xgb[x, eta]\n      )\n    }\n  )\n\nNow, we combine all the learners we want to evaluate.\n\nall_learners <- c(lrn_list_ranger,lrn_list_xgboost)\n\nWe need all the learners to use the same resampled datasets. So, we need to used an instantiated resampling.\n\nresampling <- rsmp(\"cv\", folds = 3)\nresampling$instantiate(reg_task)\n\nWe can then supply all_learners to benchmark_grid() along with a task and resampling method, and ten call benchmark() on it.\n\n#=== benchmark design ===#\nbenchmark_design <- \n  benchmark_grid(\n    task = reg_task, \n    learners = all_learners,\n    resampling = resampling\n  )\n\n#=== benchmark implementation ===#\nbmr_results <- benchmark(benchmark_design)\n\nINFO  [15:25:44.222] [mlr3] Running benchmark with 72 resampling iterations \nINFO  [15:25:44.255] [mlr3] Applying learner 'regr.ranger' on task 'regression' (iter 3/3) \nINFO  [15:25:44.279] [mlr3] Applying learner 'regr.ranger' on task 'regression' (iter 3/3) \nINFO  [15:25:44.290] [mlr3] Applying learner 'regr.ranger' on task 'regression' (iter 3/3) \nINFO  [15:25:44.301] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 3/3) \nINFO  [15:25:44.349] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 2/3) \nINFO  [15:25:44.375] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 1/3) \nINFO  [15:25:44.402] [mlr3] Applying learner 'regr.ranger' on task 'regression' (iter 1/3) \nINFO  [15:25:44.411] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 3/3) \nINFO  [15:25:44.422] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 1/3) \nINFO  [15:25:44.438] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 1/3) \nINFO  [15:25:44.449] [mlr3] Applying learner 'regr.ranger' on task 'regression' (iter 2/3) \nINFO  [15:25:44.458] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 3/3) \nINFO  [15:25:44.470] [mlr3] Applying learner 'regr.ranger' on task 'regression' (iter 1/3) \nINFO  [15:25:44.478] [mlr3] Applying learner 'regr.ranger' on task 'regression' (iter 1/3) \nINFO  [15:25:44.488] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 1/3) \nINFO  [15:25:44.499] [mlr3] Applying learner 'regr.ranger' on task 'regression' (iter 1/3) \nINFO  [15:25:44.508] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 1/3) \nINFO  [15:25:44.518] [mlr3] Applying learner 'regr.ranger' on task 'regression' (iter 2/3) \nINFO  [15:25:44.527] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 2/3) \nINFO  [15:25:44.560] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 2/3) \nINFO  [15:25:44.571] [mlr3] Applying learner 'regr.ranger' on task 'regression' (iter 3/3) \nINFO  [15:25:44.579] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 2/3) \nINFO  [15:25:44.590] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 3/3) \nINFO  [15:25:44.617] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 3/3) \nINFO  [15:25:44.628] [mlr3] Applying learner 'regr.ranger' on task 'regression' (iter 1/3) \nINFO  [15:25:44.638] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 2/3) \nINFO  [15:25:44.649] [mlr3] Applying learner 'regr.ranger' on task 'regression' (iter 3/3) \nINFO  [15:25:44.659] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 1/3) \nINFO  [15:25:44.693] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 1/3) \nINFO  [15:25:44.719] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 1/3) \nINFO  [15:25:44.730] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 1/3) \nINFO  [15:25:44.741] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 1/3) \nINFO  [15:25:44.752] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 3/3) \nINFO  [15:25:44.763] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 2/3) \nINFO  [15:25:44.774] [mlr3] Applying learner 'regr.ranger' on task 'regression' (iter 2/3) \nINFO  [15:25:44.788] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 2/3) \nINFO  [15:25:44.815] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 3/3) \nINFO  [15:25:44.827] [mlr3] Applying learner 'regr.ranger' on task 'regression' (iter 1/3) \nINFO  [15:25:44.838] [mlr3] Applying learner 'regr.ranger' on task 'regression' (iter 3/3) \nINFO  [15:25:44.848] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 1/3) \nINFO  [15:25:44.859] [mlr3] Applying learner 'regr.ranger' on task 'regression' (iter 2/3) \nINFO  [15:25:44.869] [mlr3] Applying learner 'regr.ranger' on task 'regression' (iter 2/3) \nINFO  [15:25:44.879] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 1/3) \nINFO  [15:25:44.891] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 2/3) \nINFO  [15:25:44.904] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 3/3) \nINFO  [15:25:44.937] [mlr3] Applying learner 'regr.ranger' on task 'regression' (iter 2/3) \nINFO  [15:25:44.946] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 2/3) \nINFO  [15:25:44.959] [mlr3] Applying learner 'regr.ranger' on task 'regression' (iter 1/3) \nINFO  [15:25:44.969] [mlr3] Applying learner 'regr.ranger' on task 'regression' (iter 1/3) \nINFO  [15:25:44.980] [mlr3] Applying learner 'regr.ranger' on task 'regression' (iter 2/3) \nINFO  [15:25:44.989] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 3/3) \nINFO  [15:25:45.002] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 2/3) \nINFO  [15:25:45.031] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 2/3) \nINFO  [15:25:45.049] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 2/3) \nINFO  [15:25:45.078] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 1/3) \nINFO  [15:25:45.106] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 3/3) \nINFO  [15:25:45.137] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 2/3) \nINFO  [15:25:45.150] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 3/3) \nINFO  [15:25:45.167] [mlr3] Applying learner 'regr.ranger' on task 'regression' (iter 3/3) \nINFO  [15:25:45.177] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 2/3) \nINFO  [15:25:45.190] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 3/3) \nINFO  [15:25:45.203] [mlr3] Applying learner 'regr.ranger' on task 'regression' (iter 2/3) \nINFO  [15:25:45.214] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 1/3) \nINFO  [15:25:45.243] [mlr3] Applying learner 'regr.ranger' on task 'regression' (iter 1/3) \nINFO  [15:25:45.253] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 2/3) \nINFO  [15:25:45.267] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 3/3) \nINFO  [15:25:45.280] [mlr3] Applying learner 'regr.ranger' on task 'regression' (iter 3/3) \nINFO  [15:25:45.294] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 3/3) \nINFO  [15:25:45.307] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 1/3) \nINFO  [15:25:45.318] [mlr3] Applying learner 'regr.xgboost' on task 'regression' (iter 3/3) \nINFO  [15:25:45.345] [mlr3] Applying learner 'regr.ranger' on task 'regression' (iter 3/3) \nINFO  [15:25:45.353] [mlr3] Applying learner 'regr.ranger' on task 'regression' (iter 2/3) \nINFO  [15:25:45.364] [mlr3] Finished benchmark \n\n\nWe can calculate MSE for each of the learners using the aggregate() method with msr(\"regr.mse\").\n\n(\nbmr_perf <- bmr_results$aggregate(msr(\"regr.mse\"))\n)\n\n    nr      resample_result    task_id   learner_id resampling_id iters\n 1:  1 <ResampleResult[22]> regression  regr.ranger            cv     3\n 2:  2 <ResampleResult[22]> regression  regr.ranger            cv     3\n 3:  3 <ResampleResult[22]> regression  regr.ranger            cv     3\n 4:  4 <ResampleResult[22]> regression  regr.ranger            cv     3\n 5:  5 <ResampleResult[22]> regression  regr.ranger            cv     3\n 6:  6 <ResampleResult[22]> regression  regr.ranger            cv     3\n 7:  7 <ResampleResult[22]> regression  regr.ranger            cv     3\n 8:  8 <ResampleResult[22]> regression  regr.ranger            cv     3\n 9:  9 <ResampleResult[22]> regression  regr.ranger            cv     3\n10: 10 <ResampleResult[22]> regression regr.xgboost            cv     3\n11: 11 <ResampleResult[22]> regression regr.xgboost            cv     3\n12: 12 <ResampleResult[22]> regression regr.xgboost            cv     3\n13: 13 <ResampleResult[22]> regression regr.xgboost            cv     3\n14: 14 <ResampleResult[22]> regression regr.xgboost            cv     3\n15: 15 <ResampleResult[22]> regression regr.xgboost            cv     3\n16: 16 <ResampleResult[22]> regression regr.xgboost            cv     3\n17: 17 <ResampleResult[22]> regression regr.xgboost            cv     3\n18: 18 <ResampleResult[22]> regression regr.xgboost            cv     3\n19: 19 <ResampleResult[22]> regression regr.xgboost            cv     3\n20: 20 <ResampleResult[22]> regression regr.xgboost            cv     3\n21: 21 <ResampleResult[22]> regression regr.xgboost            cv     3\n22: 22 <ResampleResult[22]> regression regr.xgboost            cv     3\n23: 23 <ResampleResult[22]> regression regr.xgboost            cv     3\n24: 24 <ResampleResult[22]> regression regr.xgboost            cv     3\n    nr      resample_result    task_id   learner_id resampling_id iters\n     regr.mse\n 1:  9.570194\n 2: 12.086689\n 3: 14.298429\n 4:  7.775000\n 5:  8.997337\n 6: 11.274749\n 7:  7.190620\n 8:  8.104033\n 9: 10.612477\n10: 79.063709\n11: 79.063709\n12:  7.748481\n13:  7.748481\n14:  9.430555\n15:  9.430555\n16: 11.757810\n17: 11.757810\n18:  7.063300\n19:  7.063300\n20:  8.681047\n21:  7.748481\n22:  9.430555\n23: 11.757810\n24:  7.063300\n     regr.mse\n\n\nWe then pick the best learner which minimized regr.mse.\n\n#=== get the index ===#\nbest_nr <- as.data.table(bmr_perf) %>% \n  .[which.min(regr.mse), nr]\n\n#=== get the best learner by the index ===#\nbest_learner <- all_learners[[best_nr]]"
  },
  {
    "objectID": "PROG-R-03-model-selection-prediction.html#sec-reticulate-model-selection",
    "href": "PROG-R-03-model-selection-prediction.html#sec-reticulate-model-selection",
    "title": "20  Model Selection (Prediction)",
    "section": "20.2 reticulate implementation in R",
    "text": "20.2 reticulate implementation in R\nHere is the R codes that emulates what is done in ?sec-python-model-selection using the reticulate package from within R.\n\n#--------------------------\n# Import modules  \n#--------------------------\nlibrary(reticulate)\nsk_ensemble <- import(\"sklearn.ensemble\") \nsk_lm <- import(\"sklearn.linear_model\")\nsk_ms <- import(\"sklearn.model_selection\")\neconml_sk_ms <- import(\"econml.sklearn_extensions.model_selection\")\neconml_dml <- import(\"econml.dml\")\n\nsk_data <- import(\"sklearn.datasets\")\nmake_regression <- sk_data$make_regression\nnp <- import(\"numpy\")\n\n#--------------------------\n# Generate synthetic data\n#--------------------------\n#=== set parameters for data generation ===#\nn_samples <- 2000L\nn_features <- 20L\nn_informative <- 15L\nnoise <- 2L\nrng <- np$random$RandomState(8934L)\n\n#=== generate synthetic data ===#\ndata <-\n  make_regression(\n    n_samples, \n    n_features, \n    n_informative = n_informative, \n    noise = noise, \n    random_state = rng\n  )\n\n#=== Create data matrices ===#\nXT <- data[[1]]\nT <- XT[, 1]\nX <- XT[, -1]\nY  <- data[[2]]\n\n#--------------------------\n# Set up GridSearchCVList\n#--------------------------\n#=== list of estimators ===#\nest_list <- \n  c(\n    sk_lm$Lasso(max_iter=1000L), \n    sk_ensemble$GradientBoostingRegressor(),\n    sk_ensemble$RandomForestRegressor(min_samples_leaf = 5L)\n  ) \n\n#=== list of parameter values ===#\npar_grid_list <- \n  list(\n    list(alpha = c(0.001, 0.01, 0.1, 1, 10)),\n    list(\n      max_depth = as.integer(c(3, 5, 10)), \n      n_estimators = as.integer(c(50, 100, 200))\n    ),\n    list(\n      max_depth = as.integer(c(3, 5, 10)), \n      max_features = as.integer(c(3, 5, 10, 20))\n    )\n  )\n\n#=== CV specification ===#\nrk_cv <- \n  sk_ms$RepeatedKFold(\n    n_splits = 4L, \n    n_repeats = 2L, \n    random_state = 123L\n  )\n\n#=== set up a GridSearchCVList ===#\nfirst_stage <-\n  econml_sk_ms$GridSearchCVList(\n    estimator_list = est_list,\n    param_grid_list = par_grid_list,\n    cv = rk_cv\n  )\n\n#--------------------------\n# Run CV\n#--------------------------\n#=== Y ===#\nmodel_y <- first_stage$fit(X, Y)$best_estimator_\n\n#=== T ===#\nmodel_t <- first_stage$fit(X, T)$best_estimator_\n\n#--------------------------\n# DML\n#--------------------------\n#=== set up a linear DML ===#\nest = econml_dml$LinearDML(\n    model_y = model_y, \n    model_t = model_t\n)\n\n#=== train ===#\nest$fit(Y, T, X=X, W=X)"
  },
  {
    "objectID": "PROG-P-01-scikitlearn.html#random-forest",
    "href": "PROG-P-01-scikitlearn.html#random-forest",
    "title": "21  Prediction",
    "section": "21.2 Random Forest",
    "text": "21.2 Random Forest"
  },
  {
    "objectID": "PROG-P-01-scikitlearn.html#gradient-boosting",
    "href": "PROG-P-01-scikitlearn.html#gradient-boosting",
    "title": "21  Prediction",
    "section": "21.3 Gradient Boosting",
    "text": "21.3 Gradient Boosting"
  },
  {
    "objectID": "PROG-P-01-scikitlearn.html#extreme-gradient-boosting",
    "href": "PROG-P-01-scikitlearn.html#extreme-gradient-boosting",
    "title": "21  Prediction",
    "section": "21.4 Extreme Gradient Boosting",
    "text": "21.4 Extreme Gradient Boosting\nxgboost"
  },
  {
    "objectID": "PROG-P-01-scikitlearn.html#light-gbm",
    "href": "PROG-P-01-scikitlearn.html#light-gbm",
    "title": "21  Prediction",
    "section": "21.5 Light GBM",
    "text": "21.5 Light GBM\nlightgbm"
  },
  {
    "objectID": "PROG-P-02-CATE-econml.html",
    "href": "PROG-P-02-CATE-econml.html",
    "title": "22  Treatment Effect Estimation",
    "section": "",
    "text": "This chapter presents CATE estimation using the econml package (Keith Battocchi 2019). The causalml package by Uber (Chen et al. 2020) is less complete than econml at the moment, and we do not cover it.\nLet’s also generate synthetic dataset using make_regression()."
  },
  {
    "objectID": "PROG-P-02-CATE-econml.html#average-treatment-effect",
    "href": "PROG-P-02-CATE-econml.html#average-treatment-effect",
    "title": "22  Treatment Effect Estimation",
    "section": "22.1 Average Treatment Effect",
    "text": "22.1 Average Treatment Effect\nDoubleML\n\nimport DoubleML"
  },
  {
    "objectID": "PROG-P-02-CATE-econml.html#s--x--and-t-learner",
    "href": "PROG-P-02-CATE-econml.html#s--x--and-t-learner",
    "title": "22  Treatment Effect Estimation",
    "section": "22.2 S-, X-, and T-learner",
    "text": "22.2 S-, X-, and T-learner\nThis section shows how to train S-, X-, and T-learner. See Chapter 12 for how these learners work, which would help you understand what you need to specify for each of the learners.\n\n22.2.1 S-learner\nTo train an S-learner, you need to specify only one estimator, which estimates \\(E[Y|T, X, W]\\). This can be done using overall_model in SLearner.\n\n#=== specify the overall model ===#\noverall_model = GradientBoostingRegressor(\n  n_estimators=100,\n  max_depth=6,\n  min_samples_leaf=10\n)\n\n#=== set up an S-learner ===#\nS_learner = SLearner(overall_model=overall_model)\n\n#=== train ===#\nS_learner.fit(Y, T, X=X)\n\n<econml.metalearners._metalearners.SLearner object at 0x30c645eb0>\n\n\nEstimate \\(\\theta(X)\\) using the effect method,\n\nS_te = S_learner.effect(X_test)\n\n#=== see the first 10 ===#\nprint(S_te[:10])\n\n[ 0.07676244 -0.15449469  0.0595073   0.01511768  0.09292546  2.33848765\n  1.01326725 -0.0965191   1.10031727  0.11364357]\n\n\n\n\n22.2.2 T-learner\nTo train a T-learner, you need to specify only one estimator, which estimates \\(E[Y|T=1, X, W]\\) and \\(E[Y|T=0, X, W]\\) sparately. This can be done using models in TLearner.\n\n#=== set up an estimator ===#\nmodels = GradientBoostingRegressor(\n  n_estimators=100, \n  max_depth=6, \n  min_samples_leaf=10\n)\n\n#=== set up a T-learner ===#\nT_learner = TLearner(models=models)\n\n#=== train ===#\nT_learner.fit(Y, T, X=X)\n\n<econml.metalearners._metalearners.TLearner object at 0x30c73edc0>\n\n\nEstimate \\(\\theta(X)\\) using the effect method,\n\nT_te = T_learner.effect(X_test)\n\n#=== see the first 10 ===#\nprint(T_te[:10])\n\n[ 0.5454421  -0.49436235 -1.12997239 -0.31692142  0.95678029  2.03697363\n  0.11513357 -0.51682216 -0.39907222 -0.601359  ]\n\n\n\n\n22.2.3 X-learner\nTo train an X-learner, you need to specify two estimators\n\nEstimator that estimates \\(E[Y|T=1, X, W]\\) and \\(E[Y|T=0, X, W]\\) sparately just like T-learner.\nEstimator that estimates \\(E[T|X, W]\\) for weighting by propensity score\n\nThis can be done using models (for the first) and propensity_model (for the second) in XLearner.\n\n#=== set up an estimator for 1 ===#\nmodels = GradientBoostingRegressor(\n  n_estimators=100, \n  max_depth=6, \n  min_samples_leaf=10\n)\n#=== set up a propensity model ===#\npropensity_model = RandomForestClassifier(\n  n_estimators=100,\n  max_depth=6,\n  min_samples_leaf=10\n)\n\n#=== set up an X-learner ===#\nX_learner = XLearner(models=models, propensity_model=propensity_model)\n\n#=== train ===#\nX_learner.fit(Y, T, X=X)\n\n<econml.metalearners._metalearners.XLearner object at 0x30c761430>\n\n\nEstimate \\(\\theta(X)\\) using the effect method,\n\nX_te = X_learner.effect(X_test)\n\n#=== see the first 10 ===#\nprint(X_te[:10])\n\n[-0.16448101 -0.13047179 -0.69563225 -0.17402517  0.13194752  0.21497962\n -0.08160524 -0.27964704  0.21067333 -1.00625867]"
  },
  {
    "objectID": "PROG-P-02-CATE-econml.html#r-learner",
    "href": "PROG-P-02-CATE-econml.html#r-learner",
    "title": "22  Treatment Effect Estimation",
    "section": "22.3 R-learner",
    "text": "22.3 R-learner\nThis section shows how to run various estimators that fall under R-learner, which is referred to as the _Rlearner class in econml. As we saw in Chapter 12, R-learner is a DML and econml offers many estimators under _Rlearner.\n\nDML\n\nLinearDML\nSparseLinearDML\nKernelDML\n\nNonParamDML\nCausalForestDML\n\nAll the estimators under _Rlearner require that estimators for \\(E[Y|X]\\) and \\(E[T|X]\\) are specified. This can be done by model_y for \\(E[Y|X]\\) and model_t \\(E[T|X]\\). However, some estimators require that you specify the final (second stage) model using model_final while others do not.\n\n22.3.1 DML\nIn this example, let’s use gradient boosting regression for both model_y and model_t and use lasso with cross-validation for model_final. Let’s import GradientBoostingRegressor() and LassoCV() from the scikitlearn package.\n\n\n22.3.2 LinearDML\nLinearDML is a DML estimator that uses  unregularlized  linear model in the second stage. So, it assumes that \\(\\theta(X)\\) can be written as follows in Equation 12.1:\n\\[\n\\begin{aligned}\n\\theta(X) = \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_k x_k\n\\end{aligned}\n\\]\nIt solves the following minimization problem,\n\\[\n\\begin{aligned}\nargmin_{\\beta_1, \\dots, \\beta_k} \\sum_{i=1}^N (\\tilde{Y}_i - \\beta_1 x_1\\cdot \\tilde{T}- \\beta_2 x_2\\cdot \\tilde{T}, \\dots, - \\beta_K x_K\\cdot \\tilde{T})^2\n\\end{aligned}\n\\]\nThis can be solved by simply regressing \\(\\tilde{Y}_i\\) on \\(x_1\\cdot \\tilde{T}\\) through \\(x_K\\cdot \\tilde{T}\\). Once \\(\\beta_1, \\dots,\\beta_K\\) are estimated, then \\(\\hat{\\theta}(X)\\) is\n\\[\n\\begin{aligned}\n\\hat{\\theta}(X) = \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\dots + \\hat{\\beta}_k x_k\n\\end{aligned}\n\\]\nSo, it is easy to interpret how \\(X\\) affects treatment effects using LinearDML. This estimator should be used only when there are small numbers of heterogeneity drivers, \\(X\\).\nSince LinearDML runs a linear-in-parameter model without regularization, you do not need to specify the estimator for the final stage. We use GradientBoostingRegressor() for model_y and GradientBoostingClassifier() for model_t. Let’s set up our LinearDML,\n\nest = LinearDML(\n  model_y = GradientBoostingRegressor(),\n  model_t = GradientBoostingClassifier()\n)\n\nWe now invoke the fit method. Here, \\(W=X\\).\n\nest.fit(Y, T, X = X, W = X)\n\n<econml.dml.dml.LinearDML object at 0x30c78fcd0>\n\n\nPredict \\(\\theta(X)\\) for X_test.\n\nldml_te = est.effect(X_test)\n\nprint(ldml_te[:5])\n\n[-4.17455233 -4.05561813 -3.82697554 -3.70857405 -3.8411808 ]\n\n\n\n\n22.3.3 NonParamDML\nAs the name suggests, it runs non-parametric regression (e.g., reandom forest) at the second stage. Unlike LinearDML, we need to specify model_final. Internally, it solves the following problem:\n\\[\n\\begin{aligned}\n\\sum_{i=1}^N \\tilde{T}_i^2(\\frac{\\tilde{Y}_i}{\\tilde{T}_i} - \\theta(X)) = 0\n\\end{aligned}\n\\]\nThe estimator specified for model_final regress \\(\\frac{\\tilde{Y}_i}{\\tilde{T}_i}\\) and \\(X_i\\) with sample weight of \\(\\tilde{T}_i^2\\).\nLet’s use GradientBoostingRegressor() as the final model.\n\nest = NonParamDML(\n  model_y = GradientBoostingRegressor(),\n  model_t = GradientBoostingClassifier(),\n  model_final = GradientBoostingRegressor()\n)\n\nWe now invoke the fit method. Here, \\(W=X\\).\n\nest.fit(Y, T, X = X, W = X)\n\n<econml.dml.dml.NonParamDML object at 0x30c7d8fd0>\n\n\nPredict \\(\\theta(X)\\) for X_test.\n\nldml_te = est.effect(X_test)\n\nprint(ldml_te[:5])\n\n[-0.46165735 -0.98081282 -0.13231993 -0.07102961 -0.56087185]\n\n\n\n\n22.3.4 CausalForestDML"
  },
  {
    "objectID": "PROG-P-02-CATE-econml.html#orthogonal-forest",
    "href": "PROG-P-02-CATE-econml.html#orthogonal-forest",
    "title": "22  Treatment Effect Estimation",
    "section": "22.4 Orthogonal Forest",
    "text": "22.4 Orthogonal Forest\n\n\n\n\n\n\nChen, Huigang, Totte Harinen, Jeong-Yoon Lee, Mike Yung, and Zhenyu Zhao. 2020. “CausalML: Python Package for Causal Machine Learning.” https://arxiv.org/abs/2002.11631.\n\n\nKeith Battocchi, Maggie Hei, Eleanor Dillon. 2019. “EconML: A Python Package for ML-Based Heterogeneous Treatment Effects Estimation.” https://github.com/microsoft/EconML."
  },
  {
    "objectID": "PROG-P-03-model-selection.html",
    "href": "PROG-P-03-model-selection.html",
    "title": "23  Model selection",
    "section": "",
    "text": "The first stage of DML approaches involves training prediction models to predict \\(E[Y|X, W]\\) and \\(E[T|X, W]\\). Suppose you are considering multiple classes of estimators with various sets of hyper-parameter values for each model class. We can use GridSearchCVList() from the econml pacakge to conduct cross-validation to see which model works the best in predicting \\(E[Y|X, W]\\) and \\(E[T|X, W]\\) in terms of CV MSE. GridSearchCVList() is an extension of GridSearchCV() from the sklearn package. While GridSearchCV() conducts CV for a  single model class with various sets of hyper-parameter values, GridSearchCVList() conducts CV for  multiple classes of models with various sets of hyper-parameter values.\nLet’s go through an example use of GridSearchCVList(). First, we import all the functions we will be using.\n\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import RepeatedKFold\nfrom econml.dml import LinearDML\nfrom econml.sklearn_extensions.model_selection import GridSearchCVList\n\nLet’s also generate synthetic dataset using make_regression().\n\nfrom sklearn.datasets import make_regression\nimport numpy as np\n\n#=== set parameters for data generation ===#\nn_samples, n_features, n_informative, noise = 2000, 20, 15, 2\nrng = np.random.RandomState(8934)\n\n#=== generate synthetic data ===#\nXT, y = make_regression(\n  n_samples, \n  n_features, \n  n_informative = n_informative, \n  noise = noise, \n  random_state = rng\n)\n\nT = XT[:, 0] # first column as the treatment variable\nX = XT[:, 1:] # the rest as X\n\nSome of the key parameters for GridSearchCVList() are:\n\nestimator_list: List of estimators. Each estimator needs to implement the scikit-learn estimator interface.\nparam_grid: List of the name of parameters to tune with their values for each model.\ncv: Specification of how CV is conducted\n\nLet’s define them one by one.\n\nestimator_list\n\n\nest_list = [\n    Lasso(max_iter=1000), \n    GradientBoostingRegressor(),\n    RandomForestRegressor(min_samples_leaf = 5)\n]\n\nSo, the model classes we consider are lasso, random forest, and boosted forest. Note that you can fix the value of parameters that you do not vary in CV. For example, RandomForestRegressor(min_samples_leaf = 5) sets min_samples_leaf at 5.\n\nparam_grid\n\n\npar_grid_list = [\n    {\"alpha\": [0.001, 0.01, 0.1, 1, 10]},\n    {\"max_depth\": [3, 5, None], \"n_estimators\": [50, 100, 200]},\n    {\"max_depth\": [3, 5, 10], \"max_features\": [3, 5, 10, 20]},\n]\n\nThe \\(n\\)th entry of param_grid is for the \\(n\\)th entry of estimator_list. For example, two hyper-parameters will be tried for RandomForestRegressor(): max_depth and max_features. The complete combinations of the values from the parameters will be evaluated. For example, here are all the set of parameter values tried for RandomForestRegressor().\n\n\n   max_depth max_features\n1          3            3\n2          5            3\n3         10            3\n4          3            5\n5          5            5\n6         10            5\n7          3           10\n8          5           10\n9         10           10\n10         3           20\n11         5           20\n12        10           20\n\n\n\ncv\n\n\nrk_cv = RepeatedKFold(n_splits = 4, n_repeats = 3, random_state = 123)\n\n#=== check the number of splits ===#  \nrk_cv.get_n_splits()\n\n12\n\n\nCross-validation specification can be done using the sklearn.model_selection module. Here, we are using RepeatedKFold, and it is a 4-fold CV repeated 2 times.\nLet’s now specify GridSearchCVList() using the above parameters.\n\nfirst_stage = GridSearchCVList(\n    estimator_list = est_list,\n    param_grid_list= par_grid_list,\n    cv = rk_cv,\n    scoring = \"neg_mean_squared_error\"\n)\n\n\n\nscoring\nWe now conduct CV with the fit() method and then access the best estimator by referring to best_estimator_ attribute.\n\nmodel_y = first_stage.fit(X, y).best_estimator_\nmodel_y\n\nLasso(alpha=0.1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoLasso(alpha=0.1)\n\n\nWe do the same for \\(T\\).\n\nmodel_t = first_stage.fit(X, T).best_estimator_\nmodel_t\n\nLasso(alpha=0.1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LassoLasso(alpha=0.1)\n\n\nWe can now run DML with the best first stage models like below.\n\n#=== set up a linear DML ===#\nest = LinearDML(\n    model_y=model_y, \n    model_t=model_t\n)\n\n#=== train ===#\nest.fit(Y, T, X=X, W=X)"
  },
  {
    "objectID": "PROG-P-03-model-selection.html#sec-python-causal-model-selection",
    "href": "PROG-P-03-model-selection.html#sec-python-causal-model-selection",
    "title": "23  Model selection",
    "section": "23.2 Causal model selection",
    "text": "23.2 Causal model selection\n\nfrom econml.dml import DML, LinearDML, SparseLinearDML, NonParamDML\nfrom econml.metalearners import XLearner, TLearner, SLearner, DomainAdaptationLearner\nfrom econml.dr import DRLearner\n\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LassoCV\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nimport numpy as np\n\n# split data in train-validation\nX_train, X_val, T_train, T_val, Y_train, Y_val = train_test_split(X, T, y, test_size=.4)\n\n# define list of CATE estimators to select among\nreg = lambda: RandomForestRegressor(min_samples_leaf=20)\nclf = lambda: RandomForestClassifier(min_samples_leaf=20)\n\nmodels = [('xlearner', XLearner(models=reg(), cate_models=reg(), propensity_model=clf())),\n          ('slearner', SLearner(overall_model=reg()))\n]\n\n# fit cate models on train data\nmodels = [(name, mdl.fit(Y_train, T_train, X=X_train)) for name, mdl in models]\n\n# score cate models on validation data\nscorer = RScorer(model_y=reg(), model_t=clf(),\n                 discrete_treatment=True, cv=5, mc_iters=2, mc_agg='median')\n\nscorer.fit(Y_val, T_val, X=X_val)\n\nrscore = [scorer.score(mdl) for _, mdl in models]\n# select the best model\nmdl, _ = scorer.best_model([mdl for _, mdl in models])\n# create weighted ensemble model based on score performance\nmdl, _ = scorer.ensemble([mdl for _, mdl in models])"
  },
  {
    "objectID": "A01-mc-simulation.html",
    "href": "A01-mc-simulation.html",
    "title": "Appendix A — Monte Carlo (MC) Simulation",
    "section": "",
    "text": "Monte Carlo (MC) simulation is an important tool to test econometric hypothesis  numerically and it is highly desirable that you can conduct your own MC simulations that fit your need. Suppose you are interested in learning whether the OSL estimator of a simple linear-in-parameter model is unbiased when the error term is correlated with one of the explanatory variables. Well, it has been theoretically proven that the OLS estimator is biased under such a data generating process. So, we do not really have to show this numerically. But, what if you are facing with a more complex econometric task for which the answer is not clear for you? For example, what is the impact of over-fitting the first stage estimations in a double machine learning approach to the bias and efficiency of the estimation of treatment effect in the second stage? We can partially answer to this question (though not generalizable unlike theoretical expositions) using MC simulations. Indeed, this book uses MC simulations often to get insights into econometric problems for which the answers are not clear or to just confirm if econometric theories are correct.\nIt is important to first recognize that it is  impossible  to test econometric theory using real-wold data. That is simply because you never know the underlying data generating process of real-world data. In MC simulations, we generate data according to the data generating process we specify. This allows us to check if the econometric outcome is consistent with the data generating process or not. This is the reason every journal article with newly developed statistical procedures published in an econometric journal has MC simulations to check the new econometric theories are indeed correct (e.g., whether the new estimator is unbiased or not).\nHere, we learn how to program MC simulations using very simple econometric examples."
  },
  {
    "objectID": "A01-mc-simulation.html#random-number-generator",
    "href": "A01-mc-simulation.html#random-number-generator",
    "title": "Appendix A — Monte Carlo (MC) Simulation",
    "section": "A.1 Random number generator",
    "text": "A.1 Random number generator\nTo create a dateset, we use pseudo random number generators. In most cases, runif() and rnorm() are sufficient.\n\n#=== uniform ===#\nx_u <- runif(1000) \n\nhead(x_u)\n\n[1] 0.70477077 0.06572347 0.82278887 0.67470012 0.10318660 0.65792823\n\nhist(x_u)\n\n\n\n\n\n#=== normal ===#\nx_n <- rnorm(1000, mean = 0, sd = 1)\n\nhead(x_n)\n\n[1] -0.56131265 -0.21137285  1.71807607 -0.09692634  0.58177286 -0.19401035\n\nhist(x_n)\n\n\n\n\nWe can use runif() to draw from the Bernouli distribution, which can be useful in generating a treatment variable.\n\n#=== Bernouli (0.7) ===#\nrunif(30) > 0.3\n\n [1]  TRUE FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[13] FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE\n[25]  TRUE  TRUE  TRUE FALSE FALSE  TRUE\n\n\nThey are called  pseudo  random number generators because they are not truly random. What sequence of numbers you get is determined by  seed . In R, you can use set.seed() to set seed.\n\nset.seed(43230)\nrunif(10)\n\n [1] 0.78412711 0.06118740 0.02052893 0.80489633 0.69706142 0.65549966\n [7] 0.18618487 0.87417016 0.39325872 0.06729849\n\n\nIf you run the code on your computer, then you would get exactly the same set of numbers. So, pseudo random generators generate random-looking numbers, but it is not truly random. You are simply drawing from a pre-determined sequence of number that  act like random numbers. This is a very important feature of pseudo random number generators. The fact that  anybody can generate the same sequence of numbers mean that any results based on pseudo random number generators can be reproducible. When you use MC simulations, you  must set a seed so that your results are reproducible."
  },
  {
    "objectID": "A01-mc-simulation.html#mc-simulation-steps",
    "href": "A01-mc-simulation.html#mc-simulation-steps",
    "title": "Appendix A — Monte Carlo (MC) Simulation",
    "section": "A.2 MC simulation steps",
    "text": "A.2 MC simulation steps\n\nStep 1: Generate data based on the data generating process  you  specify\nStep 2: Get an estimate based on the generated data (e.g. OLS, mean)\nStep 3: Repeat Steps 1 and 2 many times (e.g., 1000)\nStep 4: Compare your estimates with the true parameter specified in Step 1\n\nGoing though Steps 1 and 2 only once is not going to give you an idea of how the estimator of interest performs. So, you repeat Steps 1 and 2 many times to see what you can expect form the estimator on average.\nLet’s use a very simple example to better understand the MC steps. The statistical question of interest here is whether sample mean is an unbiased estimator of the expected value: \\(E[\\frac{1}{n}\\sum_{i=1}^n x_i] = E[x]\\), where \\(x_i\\) is an independent random draw from the same distribution.\n\n\nOf course, \\(x_i\\) does not have to be independent. But, just making things as simple as possible.\nHere is Step 1.\n\nx <- runif(100) \n\nHere, \\(x\\) follows \\(Unif(0, 1)\\) and \\(E[x] = 0.5\\). This is the data generating process. And, data (x) has been generated using x <- runif(100).\nStep 2 is the estimation of \\(E[x]\\). The estimator is the mean of the observed values of x.\n\n(\nmean_x <- mean(x)\n)\n\n[1] 0.5226597\n\n\nOkay, pretty close. But, remember this is just a single realization of the estimator. Let’s move on to Step 3 (repeating the above many times). Let’s write a function that does Steps 1 and 2.\n\nget_estimate <- function()\n{\n  x <- runif(100) \n  mean_x <- mean(x)\n  return(mean_x)\n}\n\nYou can now repeat get_estimate() many times. There are numerous ways to do this in R. But, let’s use lapply() here.\n\nestimates <-\n  lapply(\n    1:1000,\n    function(x) get_estimate()\n  ) %>% \n  unlist()\n\nHere is the mean of the estimates (the estimate of \\(E[\\frac{1}{n}\\sum_{i=1}^n x_i]\\)).\n\nmean(estimates)\n\n[1] 0.4991132\n\n\nVery close. Of course, you will not get the exact number you are hoping to get, which is \\(0.5\\) in this case as MC simulation is a random process.\nWhile this example may seem excessively simple, no matter what you are trying to test, the basic steps will be exactly the same."
  },
  {
    "objectID": "A01-mc-simulation.html#another-example",
    "href": "A01-mc-simulation.html#another-example",
    "title": "Appendix A — Monte Carlo (MC) Simulation",
    "section": "A.3 Another Example",
    "text": "A.3 Another Example\nLet’s work on a slightly more complex MC simulations. We are interested in understanding what happens to \\(\\beta_1\\) if \\(E[u|x]\\ne 0\\) when estimating \\(y=\\beta_0+\\beta_1 x + u\\) (classic endogeneity problem).\nLet’s set some parameters first.\n\nB <- 1000 # the number of iterations\nN <- 100 # sample size\n\nLet’s write a code to generate data for a single iteration (Step 1).\n\nmu <- rnorm(N) # the common term shared by both x and u\nx <- rnorm(N) + mu # independent variable\nu <- rnorm(N) + mu # error\ny <- 1 + x + u # dependent variable\ndata <- data.frame(y = y, x = x)\n\nSo, the target parameter (\\(\\beta_1\\)) is 1 in this data generating process. x and u are correlated because they share the common term mu.\n\ncor(x, u)\n\n[1] 0.5222601\n\n\nThis code gets the OLS estimate of \\(\\beta_1\\) (Step 2).\n\nlm(y ~ x, data = data)$coefficient[\"x\"]\n\n       x \n1.518179 \n\n\nOkay, things are not looking good for OLS already.\nLet’s repeat Steps 1 and 2 many times (Step 3).\n\nget_ols_estimate <- function()\n{\n  mu <- rnorm(N) # the common term shared by both x and u\n  x <- rnorm(N) + mu # independent variable\n  u <- rnorm(N) + mu # error\n  y <- 1 + x + u # dependent variable\n  data <- data.frame(y = y, x = x)\n\n  beta_hat <- lm(y ~ x, data = data)$coefficient[\"x\"]\n\n  return(beta_hat)\n}\n\nestimates <-\n  lapply(\n    1:1000,\n    function(x) get_ols_estimate()\n  ) %>% \n  unlist()\n\nYes, the OLS estimator of \\(\\beta_1\\) is biased as we expected.\n\nmean(estimates)\n\n[1] 1.50085\n\nhist(estimates)"
  },
  {
    "objectID": "A02-method-of-moment.html",
    "href": "A02-method-of-moment.html",
    "title": "Appendix B — Primer on method of moment",
    "section": "",
    "text": "Method of moment is a class of statistical methods that derive estimators from moment conditions. We will go over the basic concepts of method of moments here because it will help us understand how double machine learning methods work."
  },
  {
    "objectID": "A02-method-of-moment.html#moments",
    "href": "A02-method-of-moment.html#moments",
    "title": "Appendix B — Primer on method of moment",
    "section": "B.1 Moments",
    "text": "B.1 Moments\nLet’s first review what moments are.\n\n\n\n\n\n\nDefinition\n\n\n\n\\(n\\)th raw moment of a random variable \\(x\\) (denoted as \\(\\mu_n\\)) is defined as\n\\[\n\\begin{aligned}\n\\mu_n = E[x^n]\n\\end{aligned}\n\\]\n\\(n\\)th central moment of a random variable \\(x\\) (denoted as \\(\\mu_n'\\)) is defined as\n\\[\n\\begin{aligned}\n\\mu_n' = E[(x-\\mu)^n]\n\\end{aligned}\n\\]\n\n\nThe statistics that we use all the time, expected value and variance of \\(x\\), are the 1st raw moment of \\(x\\) and second central moment of \\(x\\), respectively.\n\\[\n\\begin{aligned}\n\\mu_1  & = E[x] \\\\\n\\mu_2' & = E[(x-\\mu_1)^2]\n\\end{aligned}\n\\]\nSample analogs  of moments are defined as follows:\n\n\n\n\n\n\nSample analog of moment conditions\n\n\n\nSuppose you have \\(N\\) realized values of a random variable \\(x\\) (\\(x_1, x_2, \\dots, x_N\\)), then the sample analog of \\(n\\)th raw and central moments are\n\\[\n\\begin{aligned}\n\\mu_n = E[x^n] & \\Rightarrow \\frac{1}{n}\\sum_{i=1}^N x_i^n \\\\\n\\mu_n' = E[(x-\\mu_1)^n] & \\Rightarrow \\frac{1}{n} \\sum_{i=1}^N (x_i-\\bar{x})^n\n\\end{aligned}\n\\]\n, respectively."
  },
  {
    "objectID": "A02-method-of-moment.html#method-of-moments-estimator",
    "href": "A02-method-of-moment.html#method-of-moments-estimator",
    "title": "Appendix B — Primer on method of moment",
    "section": "B.2 Method of moments estimator",
    "text": "B.2 Method of moments estimator\nIn general, method of moments work like this.\n\n\n\n\n\n\nMethod of moments in general\n\n\n\n\n\nFor the given statistics of interest (say, \\(\\theta\\)), write equations that define \\(\\theta\\) using moments either implicitly or explicitly.\n\n\nReplace the moment conditions with their sample analogs\n\n\nSolve for \\(\\theta\\)\n\n\n\n\nIt is best to see some examples to understand this better.\n\nB.2.1 Simple example\nWe would like to estimate the expected value of a random variable \\(x\\).\nStep 1: In this example, the statistics of interest (\\(\\theta\\)) is the expected value of a random variable \\(x\\). The moment condition is simply\n\\[\n\\begin{aligned}\n\\theta = E[x]\n\\end{aligned}\n\\]\nStep 2: The sample analog of \\(E[x]\\) is \\(\\frac{1}{n}\\sum_{i=1}^N x_i\\). So,\n\\[\n\\begin{aligned}\n\\theta = \\frac{1}{n}\\sum_{i=1}^N x_i\n\\end{aligned}\n\\]\nStep 3: Well, the equation is already solve with respect to \\(\\theta\\).\n\\[\n\\begin{aligned}\n\\hat{\\theta} = \\frac{1}{n}\\sum_{i=1}^N x_i\n\\end{aligned}\n\\]\nThe method of moment estimator of the expected value of a random variable is sample mean (\\(\\frac{1}{n}\\sum_{i=1}^N x_i\\)).\n\nN <- 1000\nx <- rnorm(N)\n(\ntheta_hat_mm <- mean(x)\n)\n\n[1] 0.03424469\n\n\n\n\nB.2.2 Method of moments to estimate a linear-in-parameter model\nNow, let’s look at an estimation task that is more familiar and relevant to our work: estimating the coefficients of a linear model.\nConsider the following linear model.\n\\[\n\\begin{aligned}\ny = \\alpha + \\beta x + \\mu\n\\end{aligned}\n\\tag{B.1}\\]\nBy the assumption of zero conditional mean (\\(E[\\mu|X] = 0\\)), where \\(X = \\{1, x\\}\\) (the intercept and \\(x\\)), the following hold as the moment conditions for this problem:\n\n\n\\[\n\\begin{aligned}\nE[\\mu \\cdot x] & = E_x[E_{\\mu}[\\mu \\cdot x|x]] \\\\\n             & = E_x[xE_{\\mu}[\\mu|x]] \\\\\n             & = E_x[x\\cdot 0] \\;\\; \\mbox{(by the assumption)} \\\\\n             & = 0\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nE[\\mu\\cdot 1] & = 0\\\\\nE[\\mu\\cdot x] & = 0\n\\end{aligned}\n\\tag{B.2}\\]\nFrom Equation B.1, we can see that \\(\\mu = y - \\alpha + \\beta x\\). Substituting this into Equation B.2,\n\\[\n\\begin{aligned}\nE[(y - \\alpha + \\beta x)\\cdot 1] & = 0\\\\\nE[(y - \\alpha + \\beta x)\\cdot x] & = 0\n\\end{aligned}\n\\tag{B.3}\\]\n\n\n\n\n\n\nTerminology alert:  Score function \n\n\n\nScore function is a function of parameters to estimate inside \\(E[]\\) of the moment conditions. \\(\\Psi(\\cdot)\\) is often used to represent a score function.\nSo, the score functions in Equation B.3 are\n\n\\(\\Psi_1(\\alpha, \\beta) = y - \\alpha + \\beta x\\)\n\\(\\Psi_2(\\alpha, \\beta) = (y - \\alpha + \\beta x)\\cdot x\\)\n\nfor the first and second moment conditions.\n\n\nNow, the sample analogs of these moment conditions are,\n\\[\n\\begin{aligned}\n\\sum_{i=1}^N(y_i - \\alpha + \\beta x_i) & = 0\\\\\n\\sum_{i=1}^N(y_i - \\alpha + \\beta x_i)\\cdot x_i & = 0\n\\end{aligned}\n\\]\nDo these look familiar to you? They should be because they are identical to the first order conditions of OLS. Solving the equations,\n\\[\n\\begin{aligned}\n\\hat{\\alpha}_{mm} & = \\frac{1}{N}\\sum_{i=1}^N y_i \\\\\n\\hat{\\beta}_{mm} & = \\frac{\\sum_{i=1}^N (y_i-\\bar{y})(x_i - \\bar{x})}{\\sum_{i=1}^N (x_i-\\bar{x})^2}\n\\end{aligned}\n\\]\n\n\nB.2.3 Instrumental variable approach as a method of moment estimator\nNow, consider the following model,\n\\[\n\\begin{aligned}\ny = \\alpha + \\beta x + \\mu\n\\end{aligned}\n\\]\nwhere \\(E[\\mu|x] = f(x)\\). So, \\(x\\) is endogenous. Fortunately, we have found an external instrument \\(z\\) such that \\(E[u|z] = 0\\) and \\(z\\) has explanatory power on \\(x\\) (\\(z\\) is not a weak instrument). According to these assumptions, we can write the following moment conditions:\n\\[\n\\begin{aligned}\nE[\\mu\\cdot 1] & = 0 \\;\\; \\mbox{(w.r.t to the intercept)}\\\\\nE[\\mu\\cdot z] & = 0\n\\end{aligned}\n\\tag{B.4}\\]\nThe key difference from the previous case is that we are not using \\(E[\\mu\\cdot x] = 0\\) because we believe that this moment condition is not satisfied. Instead, we are using \\(E[\\mu\\cdot z] = 0\\) because we believe this condition is satisfied.\nSubstituting \\(\\mu = y - \\alpha + \\beta x\\) into Equation B.4,\n\\[\n\\begin{aligned}\nE[(y-\\alpha + \\beta x)\\cdot 1] & = 0 \\\\\nE[(y-\\alpha + \\beta x)\\cdot z] & = 0\n\\end{aligned}\n\\]\nThe sample analogs of these conditions are,\n\\[\n\\begin{aligned}\n\\sum_{i=1}^N(y_i - \\alpha + \\beta x_i) & = 0\\\\\n\\sum_{i=1}^N(y_i - \\alpha + \\beta x_i)\\cdot z_i & = 0\n\\end{aligned}\n\\]\nNow, we can solve these conditions with respect to \\(\\alpha\\) and \\(\\beta\\), which are instrumental variable estimators."
  },
  {
    "objectID": "E02-grf.html#clustering",
    "href": "E02-grf.html#clustering",
    "title": "17  Generalized Random Forest",
    "section": "17.6 Clustering",
    "text": "17.6 Clustering\nIn this section, we will conduct MC simulations to see how cluserting affects CF estimation of CATE (\\(\\theta(X)\\)) and the standard error of \\(\\hat{\\theta}(X)\\). We use the folloinwg data generating process:\n\n\n\n\n\n\nDGP 1\n\n\n\n\\[\n\\begin{aligned}\nY_i & = (3 x_{i,1} + 0.5(x_{i,2} x_{i,3}))\\cdot T_i + x_{i,3} + 2 x_{i,3} x_{i,4} + \\mu_i \\\\\nT_i|X_i & = Bernouli((1+x_{i,1})/2) \\\\\n\\mu_i|X_i & = N(0,1)\n\\end{aligned}\n\\]\n\n\\(x_1 \\sim U[0,1]\\)\n\\(x_2\\sim U[0,2]\\)\n\\(x_3\\sim U[0,5]\\)\n\\(x_4\\dots, x_10\\sim N(0,1)^7\\)\n\\(\\mu\\sim N(0, 9)\\)\n\n\n\n\n\nCode\ngen_cl_var <-  function(G, Ng){\n  var <-\n    MASS::mvrnorm(\n      G, mu = rep(0, Ng), \n      Sigma = matrix(0.75, nrow = Ng, ncol = Ng) + 0.25 * diag(Ng)\n    ) %>% t() %>% c()\n\n  return(var)\n}\n\ngen_data <- function(N = 1000, G = 40, K = 10){\n\n  Ng <- N/G # number of observations per group\n\n  #=== error term ===#    \n  mu <- gen_cl_var(G, Ng) * 3\n\n  #=== X ===#\n  X <- \n    lapply(\n      1:K,\n      function(x) gen_cl_var(G, Ng)\n    ) %>%\n    do.call(\"cbind\", .) %>%\n    data.table() %>% \n    setnames(names(.), paste0(\"x\", seq_len(K)))\n\n  group <- rep(1:G, each = Ng)\n\n  data <-\n    cbind(X, mu, group) %>% \n    #=== convert x1 and x3 to unif ===#\n    .[, `:=`(\n      x1 = pnorm(x1, sd = sqrt(2)),\n      x2 = pnorm(x2, sd = sqrt(2)) * 2,\n      x3 = pnorm(x3, sd = sqrt(2)) * 5\n    )] %>% \n    .[, T := runif(N) < ((0.5+x1)/2)] %>% \n    .[, theta_x := 3 * x1 + x2 * x3 / 2] %>%\n    .[, g_x := x3 + 2 * x1 * x4] %>%\n    .[, det_Y := theta_x * T + g_x] %>%\n    .[, Y := det_Y + mu] %>% \n    .[, id := 1:.N]\n\n  return(data)\n}\n\n\nHere is a dataset that is generated according to the DGP.\n\n\nUnfold the Code chunk above to see how gen_data() is defined.\n\nset.seed(428943)\ndata <- gen_data() \n\nFigure 17.4 presents the values of individual outcomes (\\(Y\\)), error (\\(\\mu\\)), and outcomes less errors (\\(\\theta(X)\\cdot T + g(X)\\)) and their respective means by cluster (group). You can observe that data are clustered by group.\n\n\nCode\nplot_data <-\n  data[, .(id, Y, det_Y, mu, group)] %>% \n  melt(id.var = c(\"id\", \"group\")) %>% \n  .[, type := fcase(\n    variable == \"Y\", \"Y\",\n    variable == \"det_Y\", \"Y - Error\",\n    variable == \"mu\", \"Error\"\n  )] %>% \n  .[, type := factor(type, levels = c(\"Y\", \"Error\", \"Y - Error\"))]\n\nggplot(data = plot_data) +\n  geom_point(aes(y = value, x = factor(group)), size = 0.3) +\n  geom_point(\n    data = plot_data[, .(value = mean(value)), by = .(group, type)],\n    aes(y = value, x = factor(group)),\n    color = \"red\"\n  ) +\n  xlab(\"group\") +\n  facet_grid(type ~ .) +\n  theme(\n    axis.text.x = element_text(size = 6, angle = 90)\n  ) +\n  theme_bw() \n\n\n\n\n\nFigure 17.4: Visualization of clustered data\n\n\n\n\nWe now run MC simulations. We use the same test data for all the iterations so that we can find the standard error estimates averaged over iterations. The following function trains CFs with and without clustering by group, estimate \\(\\theta(X)\\) at the test data along with the estimate of the variance of \\(\\hat{\\theta}(X)\\).\n\n#=== use fixed test data ===#\ntest_data <- gen_data() %>% \n  .[, test_id := 1:.N]\n\nrun_MC_sim <- function(i){\n\n  print(i)\n\n  train_data <- gen_data()\n\n  cf_trained_no_cluster <-\n    causal_forest(\n      X = select(train_data, starts_with(\"x\")),\n      Y = train_data[, Y],\n      W = train_data[, T]\n    )\n\n  thta_hat_no_cl <- \n    predict(\n      cf_trained_no_cluster, \n      select(test_data, starts_with(\"x\")),\n      estimate.variance = TRUE\n    )  %>% \n    data.table() %>% \n    .[, theta_true := test_data$theta_x] %>% \n    .[, id := test_data$id] %>% \n    .[, type := \"No clustering\"]\n\n  cf_trained_with_cluster <-\n    causal_forest(\n      X = select(train_data, starts_with(\"x\")),\n      Y = train_data[, Y],\n      W = train_data[, T],\n      cluster = train_data[, group]\n    )\n\n  thta_hat_with_cl <- \n    predict(\n      cf_trained_with_cluster, \n      select(test_data, starts_with(\"x\")),\n      estimate.variance = TRUE\n    ) %>% \n    data.table() %>% \n    .[, theta_true := test_data$theta_x] %>% \n    .[, id := test_data$id] %>% \n    .[, type := \"With clustering\"]\n\n  return_data <- \n    rbind(thta_hat_no_cl, thta_hat_with_cl) %>% \n    .[, sim := i]\n\n  return(return_data)\n}\n\nLet’s repeat the experiment 200 times.\n\nset.seed(428943)\n\nmc_results <-\n  lapply(\n    seq_len(200),\n    run_MC_sim\n  )\n\n\n\n\n\n\n\nFigure 17.5 shows the distribution of MSE of CATE estimation by CF with and without clustering by group. As you can see, clustering is making things worse in this instance.\n\n\nCode\nmc_results %>% \n  .[, .(mse = mean((theta_true - predictions)^2)), by = .(sim, type)] %>% \n  ggplot(data = .) + \n    geom_density(aes(x = mse, fill = type), alpha = 0.5) +\n    theme_bw()\n\n\n\n\n\nFigure 17.5: MSE of CATE estimation by CF with and without clustering by group\n\n\n\n\nFigure 17.6 shows the distribution of the ratio of the standard deviation of \\(\\hat{\\theta}(X)\\) to the mean of the estimate of the standard deviation of \\(\\hat{\\theta}(X)\\). Ratio less than 1 indicates that the estimate of the standard deviation of \\(\\hat{\\theta}(X)\\) is under-estimating the true standard deviation of \\(\\hat{\\theta}(X)\\).\n\n\nCode\nmc_results %>% \n  .[, .(sd = sd(predictions), sd_est = mean(sqrt(variance.estimates))), by = .(id, type)] %>% \n  .[, ratio := sd_est/sd] %>% \n  ggplot(data = .) + \n    geom_density(aes(x = ratio, fill = type), alpha = 0.5) +\n    theme_bw()\n\n\n\n\n\nFigure 17.6: Ratio of the estimated standard error of CATE to the standard deviation of CATE"
  },
  {
    "objectID": "E02-grf.html#hyper-parmeter-tuning",
    "href": "E02-grf.html#hyper-parmeter-tuning",
    "title": "17  Generalized Random Forest",
    "section": "17.7 Hyper-parmeter tuning",
    "text": "17.7 Hyper-parmeter tuning"
  },
  {
    "objectID": "E02-grf.html#hyper-parameter-tuning",
    "href": "E02-grf.html#hyper-parameter-tuning",
    "title": "17  Generalized Random Forest",
    "section": "17.7 Hyper-parameter tuning",
    "text": "17.7 Hyper-parameter tuning\nYou can write your own codes to conduct cross-validation just like any other ML methods as we saw in Chapter 3 (cross-validation is a general approach that can be applied to any ML methods) to tune hyper-parameters. However, the grf package offers internal tuning options based on out-of-bag error (see ?sec-rf-implementation for what out-of-bag prediction is).\nYou can define which hyper-parameters to tune using tune.parameters option. For example, tune.parameters = c(\"min.node.size\", \"mtry\") would tune min.node.size and mtry.\nThe cross-validation procedure takes the following steps:\n\n\n\n\n\n\nCross-validation steps\n\n\n\n\nDraw tune.num.reps numbers of random points in the space of possible values of parameters specified by tune.parameters\nFor each of the tune.num.reps sets of parameter values, tune.num.trees of trees are built and out-of-bag errors are calculated\nRegress the error estimates on parameter values using smoothing functions\nRedraw tune.num.draws numbers of random points in the space of possible values of parameters\nPick the set of parameter values that minimizes out-of-bag error."
  }
]