[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Machine Learning for Economists",
    "section": "",
    "text": "This book will provide an introduction to machine learning methods. The main target audience is economists (and possibly other scientific fields that value causal identification). This was originally written for my students to enhance their productivity and their collaborative work with me. So, this book will by no means cover all the things that you would like to know. Rather, it covers a small subset of the vast world of machine learning methods that benefits my students and me. If you are still interested in reading the book. Suit yourself."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Introduction to Machine Learning for Economists",
    "section": "License",
    "text": "License\nThis website is (and will always be) free to use, and is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 4.0 License."
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "1  Preface: Prediction v.s. Causal Inference",
    "section": "",
    "text": "Examples where  prediction matters:\n\nprediction of the future price of corn when the modeler is interested in using the predicted price to make money in the futures market\nprediction of the crop yield by field when the modeler is interested in using the field-level predicted crop yields as an dependent or explanatory variable in a regression analysis (e.g., the impact of weather on crop yield)\nprediction of what is in the vicinity of a self-driving car (the user)\n\nWhat is common among these examples is that the users wants to use the  level  or state of the dependent variable to drive their decisions.\nExamples where  causal inference matters:\n\nunderstand the impact of a micro-finance program on welfare in developing countries when the modelers is interested in whether they should implement such a program or not (does the benefit of implementing the program worth the cost?). The modelers do not care about what level of welfare people are gonna be at. They care about how much improvement (change) in welfare the program would make.\nunderstand the impact of water use limits for farmers on groundwater usage when the modeler (water managers) are interested in predicting how much water use reduction (change) they can expect.\nunderstand the impact of fertilizer on yield for when the modelers are interested in identifying the profit-maximizing fertilizer level. The modelers do not care about what the yield levels are going to be at different fertilizer levels. They care about how much yield improvement (change) can be achieved when more fertilizer is applied.\n\nWhat is common among these examples is that the users wants to use the information about the  change  in the dependent variable after changing the value of an explanatory variable (implementing a policy) in driving their decisions.\nNow, you may think that once you can predict the  level  of the dependent variable as a function of explanatory variables \\(X\\), say \\(\\hat{f}(X)\\), where \\(\\hat{f}(\\cdot)\\) is the trained model, then you can simply take the difference in the predicted values of the dependent variable evaluated at \\(X\\) before (\\(X_0\\)) and after (\\(X_1\\)) to find the change in the dependent variable caused by the change in \\(X\\).\n\\[\n\\begin{aligned}\n\\hat{f}(X_1) - \\hat{f}(X_0)\n\\end{aligned}\n\\]\nYou are indeed right and you can predict the change in the dependent variable when the value of an explanatory variable changes once the model is trained to predict the level of the dependent variable. However, this way of predicting the impact of \\(X\\) (the continuous treatment version of the so-called S-learner) is often biased. Instead, (most of) causal machine learning methods razor-focus on  directly  estimating the change in the dependent variable when the value of an explanatory variable changes and typically performs better.\nThe goal of this book is to learn such causal machine learning methods to add them to your econometric tool box for practical applications. This, however, does not mean we do not learn any prediction-oriented (traditional) machine learning methods. Indeed, it is essential to understand them because the prominent causal machine learning methods do use prediction-oriented ML methods in its process as we will see later. It is just that we do not use prediction-oriented ML methods by themselves for the task of identifying the causal impact of a treatment."
  },
  {
    "objectID": "B01-nonlinear.html",
    "href": "B01-nonlinear.html",
    "title": "2  Non-linear function estimation",
    "section": "",
    "text": "The purpose of this section is to introduce you to the idea of semi-parametric and non-parametric regression methods. We only scratch the surface by just looking at smoothing splines and K-nearest neighbor regression methods. The world of semi-parametric and non-parametric regression is much deeper. But, that’s out of the scope of this section. The primary goal of this section is to familiarize you with the concepts of over-fitting, regularization, hyper-parameters, and parameter tuning using smoothing splines and K-nearest neighbor regression methods as examples."
  },
  {
    "objectID": "B01-nonlinear.html#sec-functional-form",
    "href": "B01-nonlinear.html#sec-functional-form",
    "title": "2  Non-linear function estimation",
    "section": "\n2.1 Flexible functional form estimation",
    "text": "2.1 Flexible functional form estimation\n\n\nPackages to load for replications:\n\nlibrary(data.table)\nlibrary(tidyverse)\nlibrary(mgcv)\nlibrary(caret)\n\nThere is a clear limit to liner (in parameter) parametric models in flexibility to represent quantitative relationships between variables. For example, consider crop yield response to fertilizer. Typically, yield increases at the diminishing rate as fertilizer rate increases. However, at a high enough fertilizer rate, yield stops increasing (fertilizer is not a limiting factor at that point). This relationship is illustrated in the figure below.\n\nset.seed(83944)\n\n#=== generate data ===#\nN <- 300 # number of observations\nx <- seq(1, 250, length = N) \ny_det <- 240 * (1 - 0.4 * exp(- 0.03 * x))\ne <- 50 * runif(N) # error\ndata <- data.table(x = x, y = y_det + e, y_det = y_det)\n\n#=== plot ===#\n(\ng_base <- ggplot(data) +\n  geom_line(aes(y = y_det, x = x)) +\n  theme_bw()\n)\n\n\n\n\nLet’s try to fit this data using linear parametric models with \\(sqrt(x)\\), \\(log(x)\\), and \\(x + x^2\\), where the dependent variable is y_det, which is \\(E[y|x]\\) (no error added).\n\n#=== sqrt ===#\nlm_sq <- lm(y_det ~ sqrt(x), data = data)\ndata[, y_hat_sqrt := lm_sq$fit]\n\n#=== log ===#\nlm_log <- lm(y_det ~ log(x), data = data)\ndata[, y_hat_log := lm_log$fit]\n\n#=== quadratic ===#\nlm_quad <- lm(y_det ~ x + x^2, data = data)\ndata[, y_hat_quad := lm_quad$fit]\n\n\nCodeplot_data <-\n  melt(data, id.var = \"x\") %>% \n  .[variable != \"y\", ] %>% \n  .[, fit_case := fcase(\n    variable == \"y_det\", \"True response\",\n    variable == \"y_hat_sqrt\", \"sqrt\",\n    variable == \"y_hat_log\", \"log\",\n    variable == \"y_hat_quad\", \"quadratic\"\n  )]\n\nggplot(plot_data) +\n  geom_line(aes(y = value, x = x, color = fit_case)) +\n  scale_color_discrete(name = \"\") +\n  theme_bw()\n\n\n\n\nNone of the specifications do quite well. Indeed, you cannot represent the relationship well using well-known popular functional forms. Let’s now look at methods that are flexible enough to capture the relationship. First, smoothing splines, and then K-nearest neighbor next."
  },
  {
    "objectID": "B01-nonlinear.html#smoothing-splines-semi-parametric",
    "href": "B01-nonlinear.html#smoothing-splines-semi-parametric",
    "title": "2  Non-linear function estimation",
    "section": "\n2.2 Smoothing Splines (semi-parametric)",
    "text": "2.2 Smoothing Splines (semi-parametric)\nDetailed discussion of smoothing splines is out of the scope of this book. Only its basic ideas will be presented in this chapter. See Wood (2006) for a fuller treatment of this topic.\nConsider a simple quantitative relationship of two variables \\(y\\) and \\(x\\): \\(y = f(x)\\).\n\\[\n\\begin{aligned}\ny = f(x)\n\\end{aligned}\n\\]\nIt is possible to characterize this function by using many functions in additive manner: \\(b_1(x), \\dots, b_K(x)\\).\n\\[\n\\begin{aligned}\ny = \\sum_{k=1}^K \\beta_k b_k(x)\n\\end{aligned}\n\\]\nwhere \\(\\beta_k\\) is the coefficient on \\(b_k(x)\\).\nHere are what \\(b_1(x), \\dots, b_K(x)\\) may look like (1 intercept and 9 cubic spline functions).\n\nCodebasis_data <-\n  gam(y_det ~ s(x, k = 10, bs = \"cr\"), data = data) %>% \n  predict(., type = \"lpmatrix\") %>% \n  data.table() %>% \n  .[, x := data[, x]] %>% \n  melt(id.var = \"x\")\n\nggplot(data = basis_data) +\n  geom_line(aes(y = value, x = x)) +\n  facet_grid(variable ~ .) +\n  theme_bw()\n\n\n\n\nBy assigning different values to \\(b_1(x), \\dots, b_K(x)\\), their summation can represent different functional relationships.\nHere is what \\(\\sum_{k=1}^K \\beta_k b_k(x)\\) looks like when \\(\\beta_1\\) through \\(\\beta_10\\) are all \\(200\\).\n\n\n\\[\ny = \\sum_{k=1}^10 200 b_k(x)\n\\]\n\nCodedata.table(\n  variable = unique(basis_data$variable),\n  coef = rep(200, 10)\n) %>% \n.[basis_data, on = \"variable\"] %>% \n.[, sum(coef * value), by = x] %>% \nggplot(data = .) +\n  geom_line(aes(y = V1, x = x)) +\n  theme_bw()\n\n\n\n\nHere is what \\(\\sum_{k=1}^K \\beta_k b_k(x)\\) looks like when \\(\\beta_1\\) through \\(\\beta_4\\) are all \\(50\\) and \\(\\beta_5\\) through \\(\\beta_9\\) are all \\(200\\).\n\n\n\\[\ny = \\sum_{k=1}^5 50 b_k(x) + \\sum_{k=6}^10 200 b_k(x)\n\\]\n\nCodedata.table(\n  variable = unique(basis_data$variable),\n  coef = c(rep(50, 5), rep(200, 5))\n) %>% \n.[basis_data, on = \"variable\"] %>% \n.[, sum(coef * value), by = x] %>% \nggplot(data = .) +\n  geom_line(aes(y = V1, x = x)) +\n  theme_bw()\n\n\n\n\nIn practice, we fit the model to a dataset to find coefficient estimates that fit the data well. Here, we use the gam() function from the mgcv package. Note that, we use \\(E[y|x]\\) (y_det) as the dependent variable to demonstrate the ability of smoothing splines to imitate the true function.\n\n\ngam stands for  Generalized Additive Model. It is a much wider class of model than our examples in this section. See Wood (2006) for more details.\n\ngam_fit <- gam(y_det ~ s(x, k = 10, bs = \"cr\"), data = data)\n\ns(x, k = 10, bs = \"cr\") in the regression formula tells gam() to use 10 knots, which results in an intercept and nine spline basis functions. bs = \"cr\" tells gam() to use cubic spline basis functions.\n\n\nThere are many other spline basis options offered by the mgcv package. Interested readers are referred to Wood (2006).\nHere are the coefficient estimates:\n\ngam_fit$coefficient\n\n(Intercept)      s(x).1      s(x).2      s(x).3      s(x).4      s(x).5 \n 227.421061   -1.486636   16.747228   28.309674   32.115070   34.173085 \n     s(x).6      s(x).7      s(x).8      s(x).9 \n  35.178339   34.541380   38.586536   21.979396 \n\n\nThis translate into the following fitted curve.\n\nCodedata.table(\n  variable = unique(basis_data$variable),\n  coef = gam_fit$coefficient[-1]\n) %>% \n.[basis_data, on = \"variable\"] %>% \n.[, .(y_no_int = sum(coef * value)), by = x] %>% \n.[, y_hat := gam_fit$coefficient[1] + y_no_int] %>% \nggplot(data = .) +\n  geom_line(aes(y = y_hat, x = x, color = \"gam-fitted\")) +\n  geom_line(data = data, aes(y = y_det, x = x, color = \"True\")) +\n  scale_color_manual(\n    name = \"\",\n    values = c(\"gam-fitted\" = \"red\", \"True\" = \"blue\")\n  ) +\n  ylab(\"y\") +\n  xlab(\"x\") +\n  theme_bw()\n\nWarning in as.data.table.list(x, keep.rownames = keep.rownames, check.names\n= check.names, : Item 2 has 9 rows but longest item has 10; recycled with\nremainder.\n\n\n\n\n\nAs you can see, the trained model is almost perfect in representing the functional relationship of \\(y\\) and \\(x\\).\nNow, when gam() fits a model to a dataset, it penalizes the wiggliness (how wavy the curve is) of the estimated function to safe-guard against fitting the model too well to the data. Specifically, it finds coefficients that minimizes the sum of the squared residuals (for regression) plus an additional term that captures how wavy the resulting function is.\n\n\nHere is an example of wiggly (first) v.s. smooth (second) functions.\n\nCodegam_fit_wiggly <- gam(y ~ s(x, k = 40, bs = \"cr\", sp = 0), data = data)\nplot(gam_fit_wiggly, se = FALSE)\n\n\n\nCodegam_fit_smooth <- gam(y ~ s(x, k = 5, bs = \"cr\"), data = data)\nplot(gam_fit_smooth, se = FALSE)\n\n\n\n\n\\[\n\\begin{aligned}\nMin_{\\hat{f}(x)} \\sum_{i=1}^N(y_i - \\hat{f}(x_i))^2 + \\lambda \\Omega(\\hat{f}(x))\n\\end{aligned}\n\\]\nwhere \\(\\Omega(\\hat{f}(x)) > 0\\) is a function that captures how wavy the resulting function is. It takes a higher value when \\(\\hat{f}(x)\\) is more wiggly. \\(\\lambda > 0\\) is the penalization parameter. As \\(\\lambda\\) gets larger, a greater penalty on the wiggliness of \\(\\hat{f}(x)\\), thus resulting in a smoother curve.\nYou can specify \\(\\lambda\\) by sp parameter in gam(). When sp is not specified by the user, gam() finds the optimal value of sp internally using cross-validation (cross-validation will be introduce formally in Chapter 4). For now, just consider it as a method to find parameters that make the trained model a good representation of the underlying conditional mean function (\\(E[y|x]\\)).\n\n\nMore specifically, it uses generalized cross-validation (GCV). A special type of cross-validation that can be done when the model is linear in parameter.\nIf you do not pick the value of sp well, the estimated curve will be very wiggly. Let’s see an example by setting the value of sp to 0, meaning no punishment for being very wiggly. We also set the number of splines to \\(39\\) so that \\(\\sum_{k=1}^K \\beta_k b_k(x)\\) is  very flexible.\n\n#=== fit ===#\ngam_fit_wiggly <- gam(y ~ s(x, k = 40, bs = \"cr\", sp = 0), data = data)\n\n#=== assign the fitted values to a variable ===#\ndata[, y_hat := gam_fit_wiggly$fitted.values]\n\n#=== plot ===#\nggplot(data = data) +\n  geom_line(aes(y = y_hat, x = x)) +\n  geom_point(aes(y = y, x = x)) +\n  theme_bw()\n\n\n\n\nWe call this phenomenon over-fitting (of the data by the model). An over-fitted model does well in predicting \\(y\\) when applied to the data the model used to train itself. However, it would do a terrible job in prediction on the data it has never seen clearly because it is not predicting \\(E[y|x]\\) well.\n\n\n\n\n\n\nImportant\n\n\n\n\n\n Hyper-parameter: parameters that one needs to specify  before fitting the model and affect the fitting process in ways that change the outcome of the fitting.\n\n Parameter tuning: process that attempts to find the optimal set of hyper-parameters.\n\n\n\nIn mgcv::gam(), the hyper-parameters are the penalty parameter \\(\\lambda\\) (specified by. sp), the number of knots (specified by k)\\(^1\\), the type of splines (specified by bs). Coefficient estimates (\\(\\alpha\\), \\(\\beta_1, \\dots, \\beta_K\\)) change when the value of sp is altered. Here is what happens when k \\(= 3\\) (less flexible than the k \\(= 39\\) case above).\n\n\n\\(^1\\) or more precisely, how many knots and where to place them\n\n#=== fit ===#\ngam_fit_wiggly <- gam(y ~ s(x, k = 3, bs = \"cr\", sp = 0), data = data)\n\n#=== assign the fitted values to a variable ===#\ndata[, y_hat := gam_fit_wiggly$fitted.values]\n\n#=== plot ===#\nggplot(data = data) +\n  geom_line(aes(y = y_hat, x = x)) +\n  geom_point(aes(y = y, x = x)) +\n  theme_bw()\n\n\n\n\nHyper-parameters can significantly influence the outcome. Since the user get to pick any numbers, it can be potentially used to twist the results in a way that favors the outcomes they want to have. Therefore, it is important to pick the values of hyper-parameters wisely. One way of achieving the goal is cross-validation, which is a data-driven way to finding the best value of hyper-parameters. We will discuss cross-validation in ?sec-cv in detail.\nHere is the fitted curve when the optimal value of sp is picked by gam() automatically given k = 40 and bs = \"cr\" using cross-validation.\n\n\nThat is, we are not tuning k and bs here.\n\n#=== fit ===#\ngam_fit_cved <- gam(y ~ s(x, k = 40, bs = \"cr\"), data = data)\n\n#=== assign the fitted values to a variable ===#\ndata[, y_hat := gam_fit_cved$fitted.values]\n\n#=== plot ===#\nggplot(data = data) +\n  geom_line(aes(y = y_hat, x = x)) +\n  geom_point(aes(y = y, x = x)) +\n  theme_bw()\n\n\n\n\nYou can see that the tuning of sp is successful and has resulted in a much better fitted curve compared to the case where sp was forced to be 0. As you will see, hyper-parameter tuning will be critical for many of the machine learning methods we will look at later."
  },
  {
    "objectID": "B01-nonlinear.html#k-nearest-neighbor-non-parametric",
    "href": "B01-nonlinear.html#k-nearest-neighbor-non-parametric",
    "title": "2  Non-linear function estimation",
    "section": "\n2.3 K-nearest neighbor (non-parametric)",
    "text": "2.3 K-nearest neighbor (non-parametric)\nThe idea of K-nearest neighbor (KNN) regression (a special case of kernel regression with a uniform kernel) is very simple. The prediction of \\(y\\) conditional on \\(x\\) is simply the average of \\(y\\) observed for the K closest (in terms of distance to \\(x\\)) observations in the data.\nLet’s illustrate the method using the data generated in Section 2.1. Suppose you are interested in estimating \\(E[y|x = 100]\\). Here is a visualization of the 10 closest data points to \\(x = 100\\). Blue points are the 10-closest observed data points. The red point is the 10-nearest neighbor estimate of \\(E[y|x = 100]\\).\n\nCodeneighbors <-\n  copy(data)[, abs_dist_100 := abs(x -100)] %>% \n  .[order(abs_dist_100), ] %>%\n  .[1:10, ] \n\ny_estimate <- neighbors[, mean(y)]\n\nggplot() +\n  geom_point(data = data, aes(y = y, x = x), color = \"darkgray\") +\n  geom_point(data = neighbors, aes(y = y, x = x), color = \"blue\") +\n  geom_point(aes(y = y_estimate, x = 100), color = \"red\") +\n  geom_line(\n    data  = data.table(y = y_estimate, x = 0:100),\n    aes(y = y, x = x),\n    color = \"red\",\n    linetype = 2\n  )\n\n\n\n\nOne critical difference between KNN and smoothing splines is that KNN fits data locally while smoothing splines fit the data globally, meaning use all the data to fit a single curve at the same time.\nThe hyper-parameter for KNN regression is k (the number of neighbors). The choice of the value of \\(k\\) has a dramatic impacts on the fitted curve.\n\nplot_data <-\n  data.table(\n    k = c(1, 5, 10, 20)\n  ) %>% \n  rowwise() %>% \n  mutate(knn_fit = list(\n    knnreg(y ~ x, k = k, data = data)\n  )) %>% \n  mutate(eval_data = list(\n    data.table(x = seq(0, 250, length = 1000)) %>% \n    .[, y := predict(knn_fit, newdata = .)]\n  )) %>% \n  dplyr::select(k, eval_data) %>% \n  unnest() %>% \n  mutate(k_txt = paste0(\"k = \", k)) %>% \n  mutate(k_txt = factor(k_txt, levels = paste0(\"k = \", c(1, 5, 10, 20))))\n\nWarning: `cols` is now required when using unnest().\nPlease use `cols = c(eval_data)`\n\nggplot() +\n  geom_point(data = data, aes(y = y, x = x), color = \"darkgray\") +\n  geom_line(data = plot_data, aes(y = y, x = x), color = \"red\") +\n  facet_wrap(k_txt ~ ., nrow = 2)\n\n\n\n\nAs you can see, at k \\(= 1\\), the fitted curve fits perfectly with the observed data, and it is highly over-fitted. While increasing k to \\(5\\) alleviates the over-fitting problem, the fitted curve is still very much wiggly. Nobody uses KNN for any practical applications. However, KNN is great to illustrate the importance of the choice of an arbitrary parameter (hyper-parameter). As discussed above, we will later look at cross-validation as a way to tune hyper-parameters."
  },
  {
    "objectID": "B01-nonlinear.html#references",
    "href": "B01-nonlinear.html#references",
    "title": "2  Non-linear function estimation",
    "section": "References",
    "text": "References\n\n\nWood, Simon N. 2006. Generalized Additive Models: An Introduction with r. chapman; hall/CRC."
  },
  {
    "objectID": "B02-bias-variance-tradeoff.html",
    "href": "B02-bias-variance-tradeoff.html",
    "title": "\n3  Bias-variance Trade-off\n",
    "section": "",
    "text": "Packages to load for replication:\n\nlibrary(data.table)\nlibrary(tidyverse)\nlibrary(mgcv)\nlibrary(caret)\nlibrary(parallel)\n\nSuppose you have a dataset: \\(D = \\{X_1, y_1\\}, \\{X_2, y_2\\}, \\dots, \\{X_N, y_N\\}\\), where \\(X_i\\) is a collection of features and \\(y_i\\) is the dependent variable for \\(i\\)th observation. Further, suppose you use \\(D\\) for training ML models and \\(\\hat{f}()\\) is a trained model.\nA most common measure of how good a model is mean squared error (MSE) defined as below:\n\\[\nMSE = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{f}(x_i))^2\n\\]\n\\(\\hat{f}(x_i)\\) is the value of \\(y\\) predicted by the trained model \\(\\hat{f}()\\), so \\(y_i - \\hat{f}(x_i)\\) is the residual (termed error more often).\nWhen you get the MSE of a trained model for the very data that is used to train the model, then we may call it training MSE.\nHowever, we are typically interested in how the trained model performs for the data that we have not seen. Let \\(D^{test} = \\{X^{test}_1, y^{test}_1\\}, \\{X^{test}_2, y^{test}_2\\}, \\dots, \\{X^{test}_M, y^{test}_M\\}\\) denote new data set with \\(M\\) data points. Then the test MSE would be:\n\\[\nMSE_{test} = \\frac{1}{M} \\sum_{i=1}^M (y^{test}_{i} - \\hat{f}(x^{test}_{i}))^2\n\\]\nTypically, we try different ML approaches (Random Forest, Support Vector Machine, Causal Forest, Boosted Regression Forest, Neural Network, etc). We also try different values of hyper-parameters for the same approach (e.g., tree depth and minimum observations per leaf for RF). Ideally, we would like to pick the model that has the smallest test \\(MSE\\) among all the models.\nSuppose you do not have a sufficiently large dataset to split to train and test datasets (often the case). So, you used all the available observations to train a model. That means you can get only training \\(MSE\\).\n\n\n\n\n\n\nImportant\n\n\n\nIn this case, can we trust the model that has the lowest training \\(MSE\\)?\n\n\nThe quick answer is no. The problem is that the model with the lowest training \\(MSE\\) does not necessarily achieve the lowest test \\(MSE\\).\nLet’s run some simulations to see this. The data generating process is as follows:\n\\[\ny  = (x - 2.5)^3 + \\mu\n\\]\nwhere \\(\\mu\\) is the error term.\n\nset.seed(943843)\n\n# define the data generating process\n# x fixed\ngen_data <- function(x) {\n  \n  ey <- (x - 2.5)^3 # E[y|x]\n  y <- ey + 3 * rnorm(length(x)) # y = E[y|x] + u\n  return_data <- data.table(y = y, x = x, ey = ey)\n\n  return(return_data)\n}\n\n## generate train data\ntrain_data <- gen_data(x = runif(100) * 5)\n\n  \n## generate test data\n# test data is large to stabilize test MSE \ntest_data <- gen_data(x = runif(10000) * 5)\n\nVisually, here is the relationship between \\(E[y]\\) and \\(x\\):\n\nggplot(data = train_data) +\n  geom_line(aes(y = ey, x = x)) +\n  theme_bw()\n\n\n\n\nNow, let’s define a function that runs regression with different levels of flexibility using a generalized additive model from the mgcv package, predict \\(y\\) for both the train and test datasets, and find train and test MSEs. Specifically, we vary the value of k (the number of knots) in gam() while intentionally setting sp to \\(0\\) so that the wiggliness of the fitted curve is not punished.\n\n\nA very brief introduction of generalized additive mode is available in Chapter 2\n\nest_gam <- function(k, train_data, test_data) \n{\n\n  # define train and test data\n  train_data <- copy(train_data)\n  test_data <- copy(test_data)\n\n  #--- train a model ---#  \n  trained_model <- gam(y ~ s(x, k = k), sp = 0, data = train_data)\n\n  #--- predict y for the train and test datasets ---#\n  train_data[, y_hat := predict(trained_model, newdata = train_data)] \n  train_data[, type := \"Train\"]\n  test_data[, y_hat := predict(trained_model, newdata = test_data)] \n  test_data[, type := \"Test\"]\n\n  #--- combine before returning ---#\n  return_data <- \n    rbind(train_data, test_data) %>%\n    .[, num_knots := k]\n\n  return(return_data)\n}\n\nWe now loop over the number of knots (num_knots).\n\nsim_results <- \n  lapply(1:50, function(x) est_gam(x, train_data, test_data)) %>%\n  rbindlist()\n\nFigure 3.1 presents the fitted regression lines for num_knots \\(= 1, 4, 5, 15, 25\\), and \\(50\\), along with the observed data points in the train dataset.\n\nCodeggplot(sim_results[num_knots  %in% c(1, 4, 15, 25, 50), ]) +\n  geom_point(data = train_data, aes(y = y, x = x), color = \"grey\") +\n  geom_line(aes(y = y_hat, x = x, color = factor(num_knots))) +\n  theme_bw()\n\n\n\nFigure 3.1: Fitted curves by gam() with different numbers of knots\n\n\n\n\nWhen the number of knots is \\(1\\), gam is not flexible enough to capture the underlying cubic function. However, once the number of knots becomes \\(4\\), it is capable of capturing the underlying non-linearity. However, when you increase the number of knots to 15, you see that the fitted curve is very wiggly (sudden and large changes in \\(y\\) when \\(x\\) is changed slighly). When num_knots \\(= 50\\), the fitted curve looks crazy and does not resemble the underlying smooth cubic curve.\nNow, let’s check how train and test MSEs change as k changes. As you can see in Figure 3.2 below, train MSE goes down as k increases (the more complex the model is, the better fit you will get for the train data). However, test MSE is the lowest when num_knots \\(= 4\\), and it goes up afterward instead of going down. As we saw earlier, when model is made too flexible, it is trained to fit the trained data too well and lose generalizability (predict well for the dataset that has not been seen). This phenomenon is called over-fitting.\n\nCode#--- get train and test MSEs by k ---#\nsummary <- \n  sim_results[, .(mse = mean((y_hat - y)^2)), by = .(num_knots, type)]\n\nggplot(summary) +\n  geom_line(aes(y = mse, x = num_knots, color = type)) +\n  geom_point(aes(y = mse, x = num_knots, color = type)) +\n  xlab(\"Number of knots\") +\n  ylab(\"MSE\") +\n  scale_color_discrete(name = \"\") +\n  theme_bw()\n\n\n\nFigure 3.2: Train and test MSEs as a function of the number of knots\n\n\n\n\nIf we were to trust train MSE in picking the model, we would pick the model with k \\(= 50\\) in this particular instance. This clearly tells us that we should  NOT  use training MSE to pick the best model."
  },
  {
    "objectID": "B02-bias-variance-tradeoff.html#variance-bias-trade-off",
    "href": "B02-bias-variance-tradeoff.html#variance-bias-trade-off",
    "title": "\n3  Bias-variance Trade-off\n",
    "section": "\n3.2 Variance-bias trade-off",
    "text": "3.2 Variance-bias trade-off\nExpected test MSE at \\(x = x_0\\) can be written in general (no matter what the trained model is) as follows:\n\\[\nE[(y_0 - \\hat{f}(x_0))^2] = Var(\\hat{f}(x_0)) + E[y - \\hat{f}(x_0)]^2 + Var(\\mu)\n\\]\nThe first term is the variance of predicted value at \\(x_0\\), the second term is the squared bias of \\(\\hat{f}(x_0)\\) (how much \\(\\hat{f}(x_0)\\) differs from \\(E[y_0]\\) on average), and \\(Var(\\mu)\\) is the variance of the error term.\nTo illustrate this trade-off, we will run Monte Carlo simulations. We repeat the folowing steps 500 times.\n\nstep 1: generate train and test datasets\nstep 2: train gam with different values of \\(k\\) \\((1, 5, 15, 25, 50)\\) using the train dataset\nstep 3: predict \\(y\\) using the test dataset\n\nOnce all the iterations are completed, simulation results are summarized to estimate \\(Var(\\hat{f}(x_0))\\) and \\(E[y - \\hat{f}(x_0)]^2\\) for all the values of \\(x_0\\) (all the \\(x\\) values observed in the test dataset) by \\(k\\). We then average them to find the overall \\(Var(\\hat{f}(x_0))\\) and \\(E[y - \\hat{f}(x_0)]^2\\) by \\(k\\).\n\nx_train <- runif(100) * 5\n# x_test is fixed to make it easier to get average conditonal on a given value of x later  \nx_test <- runif(100) * 5\n\n# function that performs steps 1 ~ 3 (a single iteration) \nrun_mc <- function(i, x_train, x_test)\n{\n  print(i) # track progress\n  train_data <- gen_data(x_train) # generate data\n  test_data <- gen_data(x_test) # generate data\n  # run gam for K = 1, ..., 50\n  sim_results <- \n    lapply(\n      c(1, 5, 15, 25, 50), \n      function(x) est_gam(x, train_data, test_data)\n    ) %>%\n    rbindlist()\n\n  return(sim_results)\n}\n\n# runs run_mc 500 times\nmc_results <-\n  mclapply(\n    1:500,\n    function(x) run_mc(x, x_train, x_test),\n    mc.cores = 12\n  ) %>%\n  rbindlist(idcol = \"sim_id\") \n\nFigure 3.3 shows plots fitted curves for all the 500 simulations by \\(k\\) (grey lines). The blue line is the true \\(E[y|x]\\). The red line is \\(E[\\hat{f}(x)]\\)1. Figure Figure 3.4 plots the average2 \\(Var(\\hat{f}(x))\\) (red), \\(E[y - \\hat{f}(x)]^2\\) (blue), and test MSE (darkgreen) from the test datasets for different values of \\(k\\).\n\nCodemc_results_sum <- \n    mc_results %>%\n    .[type == \"Test\", ] %>% \n    .[, .(mean_y_hat = mean(y_hat)), by = .(x, ey, num_knots)]\n\nggplot() +\n    geom_line(data = mc_results[type == \"Test\", ], aes(y = y_hat, x = x, group = sim_id), color = \"gray\") +\n    geom_line(data = mc_results_sum, aes(y = mean_y_hat, x = x), color = \"red\") +\n    geom_line(data = mc_results_sum, aes(y = ey, x= x), color = \"blue\") +\n    facet_wrap(. ~ num_knots, ncol = 5) +\n    theme_bw()\n\n\n\nFigure 3.3: Bias-variance trade-off of GAM models with differing number of knots\n\n\n\n\nAs you can see in Figure 3.3, when \\(k = 1\\), it clearly has a significant bias in estimating \\(E[y|x]\\) except for several values of \\(x\\) at which \\(E[\\hat{f}(x)]\\) only happens to be unbiased. The model is simply too restrictive and suffers significant bias. However, the variance of \\(\\hat{f}(x)\\) is the smallest as shown in Figure 3.4. As we increase the value of \\(k\\) (making the model more flexible), bias dramatically reduces. However, the variance of \\(\\hat{f}(x)\\) slightly increases. Going from \\(k = 5\\) to \\(k = 15\\) further reduces bias. That is, even though individual fitted curves may look very bad, on average they perform well (as you know that what bias measures). However, the variance of \\(\\hat{f}(x)\\) dramatically increases (this is why individual fitted curves look terrible). Moving to a even higher value of \\(k\\) does not reduce bias, but increases the variance of \\(\\hat{f}(x)\\) even further. That is, increasing \\(k\\) from 15 to a higher value of \\(k\\) increases the variance of \\(\\hat{f}(x)\\) while not reducing bias at all.\nAccording to MSE presented in Figure 3.4, \\(k = 5\\) is the best model among all the models tried in this experiment. In this experiment, we had test datasets available. However, in practice, we need to pick the best model when test datasets are not available most of the time. For such a case, we would like a clever way to estimate test MSE even when test datasets are not available. We will later talk about cross-validation as a means to do so.\n\nCodesum_stat <- \n  mc_results %>%\n  .[type == \"Test\", ] %>% \n  .[\n    , \n    .(\n      var_hat = var(y_hat), # varianc of y_hat\n      bias_sq = mean(y_hat - ey)^2, # squared bias\n      mse = mean((y - y_hat)^2)\n    ),\n    by = .(x, num_knots)\n  ] %>%\n  .[\n    ,\n    .(\n      mean_var_hat = mean(var_hat),\n      mean_bias_sq = mean(bias_sq),\n      mean_mse = mean(mse)\n    ),\n    by = .(num_knots)\n  ]\n\nggplot(data = sum_stat) +\n  geom_line(aes(y = mean_var_hat, x = num_knots, color = \"Variance\")) +\n  geom_point(aes(y = mean_var_hat, x = num_knots, color = \"Variance\")) +\n  geom_line(aes(y = mean_bias_sq, x = num_knots, color = \"Bias\")) +\n  geom_point(aes(y = mean_bias_sq, x = num_knots, color = \"Bias\")) +\n  geom_line(aes(y = mean_mse, x = num_knots, color = \"test MSE\")) +\n  geom_point(aes(y = mean_mse, x = num_knots, color = \"test MSE\")) +\n  scale_color_manual(\n    values = c(\"Variance\" = \"red\", \"Bias\" = \"blue\", \"test MSE\" = \"darkgreen\"),\n    name = \"\"\n    ) +\n  ylab(\"\") +\n  xlab(\"Number of knots\") +\n  theme_bw()\n\n\n\nFigure 3.4: Expected variance of predicted values, bias, and test ME"
  },
  {
    "objectID": "B02-bias-variance-tradeoff.html#additional-example-k-nearest-neighbor-regression",
    "href": "B02-bias-variance-tradeoff.html#additional-example-k-nearest-neighbor-regression",
    "title": "\n3  Bias-variance Trade-off\n",
    "section": "\n3.3 Additional Example (K-nearest neighbor regression)",
    "text": "3.3 Additional Example (K-nearest neighbor regression)\nAnother example of bias-variance trade-off is presented using KNN as the regression method. Its hyper-parameter - the number of neighbors (k) - is varied to see its effect.\n\nfit_knn <- function(k, train_data, test_data) \n{\n\n  # define train and test data\n  train_data <- copy(train_data)\n  test_data <- copy(test_data)\n\n  #--- train a model ---#  \n  trained_model <- knnreg(y ~ x, k = k, data = train_data)\n\n  #--- predict y for the train and test datasets ---#\n  train_data[, y_hat := predict(trained_model, newdata = train_data)] \n  train_data[, type := \"Train\"]\n  test_data[, y_hat := predict(trained_model, newdata = test_data)] \n  test_data[, type := \"Test\"]\n\n  #--- combine before returning ---#\n  return_data <- \n    rbind(train_data, test_data) %>%\n    .[, num_nbs := k]\n\n  return(return_data)\n}\n\nWe now loop over the number of neighbors (k).\n\n## generate train data\ntrain_data <- gen_data(x = runif(1000) * 5)\n  \n## generate test data\ntest_data <- gen_data(x = runif(1000) * 5)\n\nsim_results <- \n  lapply(1:50, function(x) fit_knn(x, train_data, test_data)) %>%\n  rbindlist()\n\nFigure 3.5 presents the fitted regression lines for \\(k = 1, 5, 15, 25\\), and \\(50\\) using knnreg(), along with the observed data points in the train dataset.\n\nCodeggplot(sim_results[num_nbs  %in% c(1, 5, 15, 25, 50), ]) +\n  geom_point(data = train_data, aes(y = y, x = x), color = \"gray\") +\n  geom_line(aes(y = y_hat, x = x), color = \"red\") +\n  facet_grid(num_nbs ~ .) +\n  theme_bw()\n\n\n\nFigure 3.5: Fitted curves by knnreg() with different numbers of neighbors\n\n\n\n\n\nCode#--- get train and test MSEs by k ---#\nsummary <- \n  sim_results[, .(mse = mean((y_hat - y)^2)), by = .(num_nbs, type)]\n\nggplot(summary) +\n  geom_line(aes(y = mse, x = num_nbs, color = type)) +\n  geom_point(aes(y = mse, x = num_nbs, color = type)) +\n  xlab(\"Number of neighbors\") +\n  ylab(\"MSE\") +\n  scale_color_discrete(name = \"\") +\n  theme_bw()\n\n\n\nFigure 3.6: Train and test MSEs as a function of the number of knots"
  },
  {
    "objectID": "B02-bias-variance-tradeoff.html#references",
    "href": "B02-bias-variance-tradeoff.html#references",
    "title": "\n3  Bias-variance Trade-off\n",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "B03-cross-validation.html",
    "href": "B03-cross-validation.html",
    "title": "4  Cross-validation based on mean squared error (MSE)",
    "section": "",
    "text": "No model works the best all the time, and searching for the best modeling approach and specifications is an essential part of modeling applications.\nFor example, we may consider five approaches with varying modeling specifications for each of the approaches:\n\nRandom Forest (RF)\n\nnumber of trees (1000, 2000)\nnumber of variables used in each tree (3, 5, 8)\nand many other hyper parameters\n\n\nLASSO\n\npenalty parameter (1, 2, 3, etc)\n\n\nGAM\n\nnumber of knots\npenalty parameter\n\n\nBoosted Regression Forest (BRF)\n\nnumber of trees (1000, 2000)\nnumber of variables used in each tree (3, 5, 8)\nand many other hyper parameters\n\n\nConvolutional Neural Network (CNN)\n\nconvolution matrix dimension\nthe order of convolution\nlearning rate\nand many other hyper parameters\n\n\n\nOur goal here is to find the model that would performs the best when applied to the data that has not been seen yet.\nWe saw earlier that training MSE is not appropriate for that purpose as picking the model with the lowest training MSE would very much likely to lead you to the over-fitted model. In this lecture, we consider a better way of selecting a model using only train data."
  },
  {
    "objectID": "B03-cross-validation.html#leave-one-out-cross-validation-loocv",
    "href": "B03-cross-validation.html#leave-one-out-cross-validation-loocv",
    "title": "4  Cross-validation based on mean squared error (MSE)",
    "section": "\n4.2 Leave-One-Out Cross-Validation (LOOCV)",
    "text": "4.2 Leave-One-Out Cross-Validation (LOOCV)\n\n\nPackages to load for replication.\n\nlibrary(data.table)\nlibrary(tidyverse)\nlibrary(mgcv)\nlibrary(rsample)\nlibrary(parallel)\n\nConsider a dataset: \\(D = \\{X_1, y_1\\}, \\{X_2, y_2\\}, \\dots, \\{X_N, y_N\\}\\), where \\(X_i\\) is a collection of features and \\(y_i\\) is the dependent variable for \\(i\\)th observation. Further, suppose you use \\(D\\) for training ML models and \\(\\hat{f}()\\) is a trained model.\nLOOCV leaves out a single observation (say \\(i\\)), and train a model (say, GAM with the number of knots of 10) using the all the other observations (-\\(i\\)), and then find MSE for the left-out observation. This process is repeated for all the observations, and then the average of the individual MSEs is calculated.\n\n4.2.1 R demonstration using mgcv::gam()\n\nLet’s demonstrate this using R. Here is the dataset we use.\n\nset.seed(943843)\n\n# define the data generating process\n# x fixed\ngen_data <- function(x) {\n  \n  ey <- (x - 2.5)^3 # E[y|x]\n  y <- ey + 3 * rnorm(length(x)) # y = E[y|x] + u\n  return_data <- data.table(y = y, x = x, ey = ey)\n\n  return(return_data)\n}\n\n## generate train data\ndata <- gen_data(x = runif(100) * 5)\n\nVisually, here is the relationship between \\(E[y]\\) and \\(x\\):\n\nggplot(data = data) +\n  geom_line(aes(y = ey, x = x))\n\n\n\n\nFor example, for the case where the first observation is left out for validation,\n\n# leave out the first observation\nleft_out_observation <- data[1, ]\n\n# all the rest\ntrain_data <- data[-1, ]\n\nNow we train a gam model using the train_data, predict \\(y\\) for the first observation, and find the MSE.\n\n#=== train the model ===#\nfitted <- gam(y ~ s(x, k = 10), sp = 0, data = train_data)\n\n#=== predict y for the first observation ===#\ny_fitted <- predict(fitted, newdata = left_out_observation)\n\n#=== get MSE ===#\nMSE <- (left_out_observation[, y] - y_fitted) ^ 2\n\nAs described above, LOOCV repeats this process for every single observation of the data. Now, let’s write a function that does the above process for any \\(i\\) you specify.\n\n#=== define the modeling approach ===#\ngam_k_10 <- function(train_data) \n{\n  gam(y ~ s(x, k = 30), sp = 0, data = train_data)\n}\n\n#=== define the process of getting MSE for ith observation ===#\nget_mse <- function(i, model)\n{\n  left_out_observation <- data[i, ]\n\n  # all the rest\n  train_data <- data[-i, ]\n\n  #=== train the model ===#\n  fitted <- model(train_data)\n\n  #=== predict y for the first observation ===#\n  y_fitted <- predict(fitted, newdata = left_out_observation)\n\n  #=== get MSE ===#\n  MSE <- (left_out_observation[, y] - y_fitted) ^ 2 \n\n  return(MSE)\n} \n\nFor example, this gets MSE for the 10th observation.\n\nget_mse(10, gam_k_10)\n\n       1 \n1.523446 \n\n\nLet’s now loop over \\(i = 1:100\\).\n\nmse_indiv <-\n  lapply(\n    1:100,\n    function(x) get_mse(x, gam_k_10)\n  ) %>% \n  #=== list to a vector ===#\n  unlist() \n\nHere is the distribution of MSEs.\n\nhist(mse_indiv)\n\n\n\n\nWe now get the average MSE.\n\nmse_average <- mean(mse_indiv)\n\n\n4.2.2 Selecting the best GAM specification: Illustration\nNow, let’s try to find the best (among the ones we try) GAM specification using LOOCV. We will try ten different GAM specifications which vary in penalization parameter. Penalization parameter can be set using the sp option for mgcv::gam(). A greater value of sp leads to a more smooth fitted curve.\n\nspecify_gam <- function(sp) {\n  function(train_data) {\n    gam(y ~ s(x, k = 30), sp = sp, data = train_data)\n  }\n}\n\nget_mse_by_sp <- function(sp)\n{\n\n  temp_gam <- specify_gam(sp)\n\n  mse_indiv <-\n    lapply(\n      1:100,\n      function(x) get_mse(x, temp_gam)\n    ) %>% \n    #=== list to a vector ===#\n    unlist() %>% \n    mean()\n\n  return_data <-\n    data.table(\n      mse = mse_indiv,\n      sp = sp\n    )\n\n  return(return_data)\n}\n\nFor example, the following code gets you the average MSE for sp \\(= 3\\).\n\nget_mse_by_sp(3)\n\n        mse sp\n1: 11.56747  3\n\n\nNow, let’s loop over ten values of sp: 0, 0.2, 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2.\n\n(\nmse_data <- \n  lapply(\n    seq(0, 2, by = 0.2),\n    function(x) get_mse_by_sp(x)\n  ) %>% \n  rbindlist()\n)\n\n          mse  sp\n 1: 12.460156 0.0\n 2:  9.909992 0.2\n 3:  9.957858 0.4\n 4: 10.049327 0.6\n 5: 10.164749 0.8\n 6: 10.293142 1.0\n 7: 10.427948 1.2\n 8: 10.565081 1.4\n 9: 10.701933 1.6\n10: 10.836829 1.8\n11: 10.968701 2.0\n\n\n\n\n\nSo, according to the LOOCV, we should pick sp \\(= 0.2\\) as the penalty parameter.\nNow, that we know sp \\(= 0.2\\) produces the lowest LOOCV MSE, we rerun gam() using the entire dataset (not leaving out any of the observations) and make it our final trained model.\n\nfinal_gam_spec <- specify_gam(sp = 1)\n\nfit_gam <- final_gam_spec(train_data)\n\nHere is what the fitted curve looks like:\n\nplot(fit_gam)\n\n\n\n\nLooks good. By the way, here are the fitted curves for some other sp values.\n\nfitted_curves <- \n  lapply(\n    c(0, 0.6, 1, 2),\n    function(x) {\n      temp_gam <- specify_gam(sp = x)\n      fit_gam <- temp_gam(train_data)   \n    }\n  )  \n\nfor (plot in fitted_curves) {\n  plot(plot)\n}\n\n\n\n\n\n(a) k = 0\n\n\n\n\n\n\n(b) k = 0.6\n\n\n\n\n\n\n\n\n(c) k = 1\n\n\n\n\n\n\n(d) k = 2\n\n\n\n\nFigure 4.1: Fitted curves at various penalization parameters\n\n\n\nLOOCV is perfectly general and can be applied to any statistical methods. However, it can be extremely computationally burdensome because you need to fit the same model for as many as the number of observations. So, if you have 10,000 observations, then you need to fit the model 10,000 times, which can take a long long time.\n\n4.2.3 Summary\n\n\n\n\n\n\nNote\n\n\n\nLOOCV is perfectly general and can be applied to any statistical methods.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nLOOCV can be highly computation-intensive when the dataset is large"
  },
  {
    "objectID": "B03-cross-validation.html#k-fold-cross-validation-kcv",
    "href": "B03-cross-validation.html#k-fold-cross-validation-kcv",
    "title": "4  Cross-validation based on mean squared error (MSE)",
    "section": "\n4.3 K-fold Cross-Validation (KCV)",
    "text": "4.3 K-fold Cross-Validation (KCV)\nKCV is a type of cross-validation that overcomes the LOOCV’s drawback of being computationally too intensive when the dataset is large. KCV first splits the entire dataset intro \\(K\\) folds (K groups) randomly. It then leaves out a chunk of observations that belongs to a fold (group), trains the model using the rest of the observations in the other folds, evaluate the trained model using the left-out group. It repeats this process for all the groups and average the MSEs obtained for each group.\nLet’s demonstrate this process using R.\n\nset.seed(89534)\ndata <- gen_data(x = runif(500) * 5)\n\nYou can use rsample::vfold_cv() to split the data into groups.\n\n#=== split into 5 groups ===#\n(\ndata_folds <- rsample::vfold_cv(data, v = 5)\n)\n\n#  5-fold cross-validation \n# A tibble: 5 × 2\n  splits            id   \n  <list>            <chr>\n1 <split [400/100]> Fold1\n2 <split [400/100]> Fold2\n3 <split [400/100]> Fold3\n4 <split [400/100]> Fold4\n5 <split [400/100]> Fold5\n\n\nAs you can see, rsample::vfold_cv() creates \\(v\\) (\\(=5\\) here) splits. And each split has both train and test datasets. <split [400/100]> means that \\(400\\) and \\(100\\) observations for the train and test datasets, respectively. Note that, the \\(100\\) observations in the first split (called Fold 1) are in the train datasets of the rest of the splits (Fold 2 through Fold 5).\nYou can extract the train and test datasets like below using the training() and testing() functions.\n\ntrain_data <- data_folds[1, ]$splits[[1]] %>% training()\ntest_data <- data_folds[1, ]$splits[[1]] %>% testing()\n\nNow, let’s get MSE for the first fold.\n\n#=== train the model ===#\nfitted_model <- gam(y ~ s(x, k = 30), sp = 0, data = train_data)\n\n#=== predict y for the test data ===#\ny_hat <- predict(fitted_model, test_data)\n\n#=== calculate MSE for the fold ===#\n(test_data[, y] - y_hat)^2 %>% mean()\n\n[1] 12.05604\n\n\nNow that we know how to get MSE for a single fold, let’s loop over folds and get MSE for each of the folds. We first create a function that gets us MSE for a single fold.\n\nget_mse_by_fold <- function(data, fold, model)\n{\n\n  test_data <- data_folds[fold, ]$splits[[1]] %>% testing()\n  train_data <- data_folds[fold, ]$splits[[1]] %>% training()\n\n  #=== train the model ===#\n  fitted_model <- model(train_data)\n\n  #=== predict y for the test data ===#\n  y_hat <- predict(fitted_model, test_data)\n\n  #=== calculate MSE for the fold ===#\n  mse <- (test_data[, y] - y_hat)^2 %>% mean() \n\n  return_data <- \n    data.table(\n      k = fold, \n      mse = mse\n    )\n\n  return(return_data)\n}\n\nThis will get you MSE for the third fold.\n\nget_mse_by_fold(data, 3, gam_k_10)\n\n   k      mse\n1: 3 10.65129\n\n\nNow, let’s loop over the row number of data_folds (loop over splits).\n\n(\nmse_all <-\n  lapply(\n    seq_len(nrow(data_folds)),\n    function(x) get_mse_by_fold(data, x, gam_k_10)\n  ) %>% \n  rbindlist()\n)\n\n   k       mse\n1: 1 12.056038\n2: 2  9.863496\n3: 3 10.651289\n4: 4 10.705704\n5: 5 10.166634\n\n\nBy averaging MSE values, we get\n\nmse_all[, mean(mse)]\n\n[1] 10.68863\n\n\n\n4.3.1 Selecting the best GAM specification: Illustration\nJust like we found the best gam specification (choice of penalization parameter), we do the same now using KCV.\n\nget_mse_by_sp_kcv <- function(sp)\n{\n\n  temp_gam <- specify_gam(sp)\n\n  mse_by_k <-\n    lapply(\n      seq_len(nrow(data_folds)),\n      function(x) get_mse_by_fold(train_data, x, temp_gam)\n    ) %>% \n    rbindlist()\n\n  return_data <-\n    mse_by_k %>% \n    .[, sp := sp]\n\n  return(return_data[])\n}\n\nFor example, the following code gets you the MSE for all the folds for sp \\(= 3\\).\n\nget_mse_by_sp_kcv(3)\n\n   k      mse sp\n1: 1 13.56925  3\n2: 2 11.22831  3\n3: 3 11.45746  3\n4: 4 10.48461  3\n5: 5 11.29421  3\n\n\nNow, let’s loop over ten values of sp: 0, 0.2, 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2.\n\n(\nmse_results <- \n  lapply(\n    seq(0, 2, by = 0.2),\n    function(x) get_mse_by_sp_kcv(x)\n  ) %>% \n  rbindlist()\n)\n\n    k       mse  sp\n 1: 1 12.056038 0.0\n 2: 2  9.863496 0.0\n 3: 3 10.651289 0.0\n 4: 4 10.705704 0.0\n 5: 5 10.166634 0.0\n 6: 1 11.476153 0.2\n 7: 2  9.840434 0.2\n 8: 3  9.813155 0.2\n 9: 4 10.289085 0.2\n10: 5  9.895722 0.2\n11: 1 11.620416 0.4\n12: 2  9.882467 0.4\n13: 3  9.916178 0.4\n14: 4 10.239119 0.4\n15: 5  9.992636 0.4\n16: 1 11.785544 0.6\n17: 2  9.952820 0.6\n18: 3 10.033789 0.6\n19: 4 10.218321 0.6\n20: 5 10.090156 0.6\n21: 1 11.957640 0.8\n22: 2 10.040814 0.8\n23: 3 10.156511 0.8\n24: 4 10.211168 0.8\n25: 5 10.189377 0.8\n26: 1 12.130712 1.0\n27: 2 10.140277 1.0\n28: 3 10.281839 1.0\n29: 4 10.213564 1.0\n30: 5 10.290230 1.0\n31: 1 12.301318 1.2\n32: 2 10.246975 1.2\n33: 3 10.408152 1.2\n34: 4 10.223465 1.2\n35: 5 10.392306 1.2\n36: 1 12.467400 1.4\n37: 2 10.357879 1.4\n38: 3 10.534197 1.4\n39: 4 10.239448 1.4\n40: 5 10.495093 1.4\n41: 1 12.627767 1.6\n42: 2 10.470802 1.6\n43: 3 10.659006 1.6\n44: 4 10.260390 1.6\n45: 5 10.598085 1.6\n46: 1 12.781780 1.8\n47: 2 10.584159 1.8\n48: 3 10.781853 1.8\n49: 4 10.285370 1.8\n50: 5 10.700824 1.8\n51: 1 12.929162 2.0\n52: 2 10.696811 2.0\n53: 3 10.902210 2.0\n54: 4 10.313617 2.0\n55: 5 10.802917 2.0\n    k       mse  sp\n\n\nLet’s now get the average MSE by sp:\n\n(\nmean_mse_data <- mse_results[, .(mean_mse = mean(mse)), by = sp]\n)\n\n     sp mean_mse\n 1: 0.0 10.68863\n 2: 0.2 10.26291\n 3: 0.4 10.33016\n 4: 0.6 10.41613\n 5: 0.8 10.51110\n 6: 1.0 10.61132\n 7: 1.2 10.71444\n 8: 1.4 10.81880\n 9: 1.6 10.92321\n10: 1.8 11.02680\n11: 2.0 11.12894\n\n\n\n\n\nSo, according to the KCV, we should pick sp \\(= 0.2\\) as the penalty parameter.\nBy the way, here is what MSE values look like for each fold based on the value of sp.\n\nggplot(data = mse_results) +\n  geom_line(aes(y = mse, x = sp, color = factor(k))) +\n  scale_color_discrete(name = \"\")\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nEven though we compared different specification of the same approach (GAM), we can compare across different models as well. For example, you can find KCV for an RF model with a particular specifications of its hyper-parameters and compare the KCV with those of the GAM model specifications and see what comes at the top."
  },
  {
    "objectID": "B03-cross-validation.html#repeated-k-fold-cross-validation-kcv",
    "href": "B03-cross-validation.html#repeated-k-fold-cross-validation-kcv",
    "title": "4  Cross-validation based on mean squared error (MSE)",
    "section": "\n4.4 Repeated K-fold Cross-Validation (KCV)",
    "text": "4.4 Repeated K-fold Cross-Validation (KCV)\nAs its name suggest, repeated KCV repeats the process of KCV multiple times. Each KCV iteration splits the original data into k-fold in a different way. A single KCV may not be reliable as the original data was split into such a way that favors one parameter set of or model class over the others. However, if we repeat KCV multiple times, then we can safe-guard against this randomness in a KCV procedure. Repeated KCV is preferred over a single KCV.\nYou can use rsample::vfold_cv() to create repeated k-fold datasets by using the repeats argument.\n\n#=== split into 5 groups ===#\n(\ndata_folds <- rsample::vfold_cv(data, v = 5, repeats = 5)\n)\n\n#  5-fold cross-validation repeated 5 times \n# A tibble: 25 × 3\n   splits            id      id2  \n   <list>            <chr>   <chr>\n 1 <split [400/100]> Repeat1 Fold1\n 2 <split [400/100]> Repeat1 Fold2\n 3 <split [400/100]> Repeat1 Fold3\n 4 <split [400/100]> Repeat1 Fold4\n 5 <split [400/100]> Repeat1 Fold5\n 6 <split [400/100]> Repeat2 Fold1\n 7 <split [400/100]> Repeat2 Fold2\n 8 <split [400/100]> Repeat2 Fold3\n 9 <split [400/100]> Repeat2 Fold4\n10 <split [400/100]> Repeat2 Fold5\n# … with 15 more rows\n\n\nThe output has 5 (number of folds) times 5 (number of repeats) splits. It also has an additional column that indicates which repeat each row is in (id). You can apply get_mse_by_fold() (this function is defined above and calculate MSE) to each row (split) one by one and calculate MSE just like we did above.\n\n(\nmean_mse <-\n  lapply(\n    seq_len(nrow(data_folds)),\n    function(x) get_mse_by_fold(data, x, gam_k_10)\n  ) %>% \n  rbindlist() %>% \n  .[, mean(mse)]\n)\n\n[1] 10.65908"
  },
  {
    "objectID": "B03-cross-validation.html#does-kcv-really-work",
    "href": "B03-cross-validation.html#does-kcv-really-work",
    "title": "4  Cross-validation based on mean squared error (MSE)",
    "section": "\n4.5 Does KCV really work?",
    "text": "4.5 Does KCV really work?\nLOOCV and KCV use the train data to estimate test MSE. But, does it really work? In other words, does it let us pick the parameter that minimizes the test MSE? We will run a simple MC simulations to test this. We continue to use the same data generating process and gam models for this simulation as well.\nNow, since we know the data generating process, we can actually use the following metric instead of MSE.\n\\[\n\\sum_{i=1}^N(\\hat{f}(x_i) - E[y|x_i])^2\n\\]\nThis measure removes the influence of the error term that appears in the test MSE. Your objective is to minimize this measure. Of course, you cannot do this in practice because you do not observe \\(E[y|x]\\). Let’s call this “pure” MSE.\nFirst, we define a function that gets you MSE from KCV and MSE using the test data as a function of sp.\n\nget_mse_by_sp <- function(sp)\n{\n\n  #=== generate train data and test data ===#\n  train_data <- gen_data(x = runif(500) * 5)\n  test_data <- gen_data(x = runif(500) * 5)\n\n  #/*----------------------------------*/\n  #' ## MSE from KCV \n  #/*----------------------------------*/\n  #=== split train_data into 5 groups ===#\n  data_folds <- rsample::vfold_cv(train_data, v = 5)\n  \n  #=== specify the model ===#\n  temp_gam <- specify_gam(sp)\n\n  #=== get MSE by fold ===#\n  mse_by_k <-\n    lapply(\n      seq_len(nrow(data_folds)),\n      function(x) get_mse_by_fold(train_data, x, temp_gam)\n    ) %>% \n    rbindlist()\n\n  #=== find the average MSE (over folds) ===#\n  mse_kcv <-\n    mse_by_k %>% \n    .[, .(mse = mean(mse))] %>% \n    .[, sp := sp] %>% \n    .[, type := \"KCV\"]\n\n  #/*----------------------------------*/\n  #' ## pure MSE from the test data  \n  #/*----------------------------------*/\n  #=== train using the entire train dataset ===#\n  fitted <- temp_gam(train_data)\n\n  #=== find the average MSE (over observations) ===#\n  mse_test <- \n    test_data %>% \n    #=== predict y ===#\n    .[, y_hat := predict(fitted, newdata = .)] %>% \n    .[, .(mse = mean((ey - y_hat)^2))] %>% \n    .[, sp := sp] %>% \n    .[, type := \"Pure\"]\n\n  #/*----------------------------------*/\n  #' ## Combine and return\n  #/*----------------------------------*/\n  return_data <- rbind(mse_kcv, mse_test)\n\n  return(return_data)\n}\n\n\n\n\n\n\n\nWarning\n\n\n\nNote that you do not have to use an independent test data to obtain pure MSE above even though the code does it so. You could just use the train_data in getting pure MSE and the results would be essentially the same.\n\n\nFor example, this will give you MSE from KCV and MSE using the test data for sp \\(= 2\\).\n\nget_mse_by_sp(2)\n\n          mse sp type\n1: 11.0072347  2  KCV\n2:  0.7131384  2 Pure\n\n\nNow, we define a function that loops over all the sp values we test.\n\nsp_seq <- seq(0, 2, by = 0.2)\n\nget_mse <- function(i)\n{ \n  print(i) # progress tracker\n\n  lapply(\n    sp_seq,\n    function(x) get_mse_by_sp(x)\n  ) %>% \n  rbindlist()\n}\n\nFor example, the following gives you MSE values for all the sp values for a single iteration.\n\nget_mse(1)\n\n[1] 1\n\n\n            mse  sp type\n 1: 10.49305386 0.0  KCV\n 2:  0.46854532 0.0 Pure\n 3: 10.07060222 0.2  KCV\n 4:  0.06385132 0.2 Pure\n 5: 10.14490012 0.4  KCV\n 6:  0.28102534 0.4 Pure\n 7: 10.23670256 0.6  KCV\n 8:  0.29782934 0.6 Pure\n 9: 10.33852029 0.8  KCV\n10:  0.46728439 0.8 Pure\n11: 10.44652013 1.0  KCV\n12:  0.78308220 1.0 Pure\n13: 10.55803026 1.2  KCV\n14:  0.15087488 1.2 Pure\n15: 10.67109831 1.4  KCV\n16:  0.64433189 1.4 Pure\n17: 10.78429633 1.6  KCV\n18:  0.59293569 1.6 Pure\n19: 10.89658887 1.8  KCV\n20:  0.86996493 1.8 Pure\n21: 11.00723472 2.0  KCV\n22:  0.92992710 2.0 Pure\n            mse  sp type\n\n\nFinally, we run get_mse() 500 times.\n\nmse_results <-\n  mclapply(\n    1:500,\n    get_mse,\n    mc.cores = 12\n  ) %>% \n  rbindlist(idcol = \"sim_id\")\n\n#=== use this if you are a Windows user ===#\n# mse_results <-\n#   lapply(\n#     1:500,\n#     get_mse\n#   )\n\nFor each simulation round, let’s find the best sp using KCV and pure MSE.\n\n(\nwhich_sp_optimal <-\n  mse_results %>% \n  .[, .SD[which.min(mse), ], by = .(type, sim_id)] %>% \n  #=== drop mse ===#\n  .[, mse := NULL] %>% \n  dcast(sim_id ~ type, value.var = \"sp\") %>% \n  .[, num_comb := .N, by = .(KCV, Pure)]\n)\n\nFor example, for the first simulation, sp that would have minimized pure MSE (least error relative to the true \\(E[y|x]\\) across varying values of \\(x\\)) was 0.2. On the other hand, if you relied on KCV, you would have chosen 0.2.\nThe following figure shows the relationship between KCV-based and pure MSE-based sp values.\n\n#=== plot the frequency of sp chosen by KCV ===#\nggplot(data = which_sp_optimal) +\n  geom_point(aes(y = Pure, x = KCV, size = num_comb)) +\n  scale_y_continuous(breaks = sp_seq) +\n  scale_x_continuous(breaks = sp_seq) \n\n\n\n\nAs you can see KCV gives you sp that is close to the sp based on pure MSE in many cases. But, you can also see that KCV can suggest you a very different number as well. KCV is not perfect, which is kind of obvious."
  },
  {
    "objectID": "B04-regularization.html",
    "href": "B04-regularization.html",
    "title": "\n5  Regression Shrinkage Methods\n",
    "section": "",
    "text": "We have talked about variance-bias trade-off. When you “shrink” coefficients towards zero, you may be able to achieve lower variance of \\(\\hat{f}(x)\\) while increasing bias, which can result in a lower MSE.\nConsider the following generic linear model:\n\\[\ny = X\\beta + \\mu\n\\]\n\n\n\\(y\\): dependent variable\n\n\\(X\\): a collection of explanatory variables (\\(K\\) variables)\n\n\\(\\beta\\): a collection of coefficients on the explanatory variables \\(X\\)\n\n\n\\(\\mu\\): error term\n\nBorrowing from the documentation of the glmnet package(), the minimization problem shrinkage methods solve to estimate coefficients for a linear model can be written as follows:\n\\[\nMin_{\\beta} \\sum_{i=1}^N (y_i - X_i\\beta)^2 + \\lambda \\huge[\\normalsize(1-\\alpha)||\\beta||^2_2/2 + \\alpha ||\\beta||_1\\huge]\n\\tag{5.1}\\]\n\n\\(||\\beta||_1 = |\\beta_1| + |\\beta_2| + \\dots+ |\\beta_K|\\) (called L1 norm)\n\\(||\\beta||_2 = (|\\beta_1|^2 + |\\beta_2|^2 + \\dots+ |\\beta_K|^2)^{\\frac{1}{2}}\\) (called L2 norm)\n\n\\(\\lambda (> 0)\\) is the penalization parameter that governs how much coefficients shrinkage happens (more details later).\nThe shrinkage method is called Lasso when \\(\\alpha = 0\\), Ridge regression when \\(\\alpha = 1\\), and elastic net when \\(\\alpha \\in (0, 1)\\).\n\n\n\n\n\n\nNote\n\n\n\n\n\nLasso : \\(\\alpha = 0\\)\n\n\nRidge : \\(\\alpha = 1\\)\n\n\nElastic net : \\(0 < \\alpha < 1\\)\n\n\n\n\nRidge regression and elastic net are rarely used. So, we are going to cover only Lasso here."
  },
  {
    "objectID": "B04-regularization.html#lasso",
    "href": "B04-regularization.html#lasso",
    "title": "\n5  Regression Shrinkage Methods\n",
    "section": "\n5.2 Lasso",
    "text": "5.2 Lasso\nWhen there are many potential variables to include, it is hard to know which ones to include. Lasso can be used to select variables to build a more parsimonious model, which may help reducing MSE.\nAs mentioned above, Lasso is a special case of shrinkage methods where \\(\\alpha = 1\\) in Equation 5.1. So, the optimization problem of Lasso is\n\\[\nMin_{\\beta} \\sum_{i=1}^N (y_i - X_i\\beta)^2 + \\lambda \\sum_{k=1}^K |\\beta_k|\n\\tag{5.2}\\]\n, where \\(\\lambda\\) is the penalization parameter.\nAlternatively, we can also write the optimization problem as the constrained minimization problem as follows1:\n\\[\nMin_{\\beta} \\sum_{i=1}^N (y_i - X_i\\beta)^2 \\\\\n\\mbox{subject to } \\sum_{k=1}^K |\\beta_k| < t\n\\tag{5.3}\\]\nA graphical representation of the minimization problem is highly illustrative on what Lasso does. Consider the following data generating process:\n\\[\ny  = 0.2 x_1 + 2 * x_2 + \\mu\n\\]\nWhen \\(t\\) is set to 1 in Equation 5.3, Lasso tries to estimate the coefficient on \\(x_1\\) and \\(x_2\\) by solving the following problem:\n\\[\n\\begin{align}\nMin_{\\beta} \\sum_{i=1}^N (y_i - \\beta_1 x_1 - \\beta_2 x_2)^2 \\\\\n\\mbox{subject to } \\sum_{k=1}^K |\\beta_k| < \\textcolor{red}{1}    \n\\end{align}\n\\]\nThis means that, we need to look for the combinations of \\(\\beta_1\\) and \\(\\beta_2\\) such that the sum of their absolute values is less than 1. Graphically, here is what the constraint looks like:\n\nCodeggplot() +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  geom_segment(aes(x = 1, y = 0, xend = 0, yend = 1), color = \"red\", size = 1.2) +\n  geom_segment(aes(x = 1, y = 0, xend = 0, yend = -1), color = \"red\", size = 1.2) +\n  geom_segment(aes(x = -1, y = 0, xend = 0, yend = 1), color = \"red\", size = 1.2) +\n  geom_segment(aes(x = -1, y = 0, xend = 0, yend = -1), color = \"red\", size = 1.2) +\n  coord_equal() +\n  xlab(\"beta_1\") +\n  ylab(\"beta_2\")\n\n\n\n\nNow, let’s calculate what value the objective function takes at different values of \\(\\beta_1\\) and \\(\\beta_2\\).\nWe first generate data.\n\nN <- 1000 # number of observations\nx_1 <- rnorm(N)\nx_2 <- rnorm(N)\nmu <- rnorm(N) # error term\ny <- 2 * x_1 + 0.2 * x_2 + mu\n\ndata <-\n  data.table(\n    y = y,\n    x_1 = x_1,\n    x_2 = x_2\n  )\n\nWithout the constraint, here is the combination of \\(\\beta_1\\) and \\(\\beta_2\\) that minimizes the objective function of Equation 5.3, which is the same as OLS estimates.\n\n(\nols_coefs <- lm(y ~ x_1 + x_2, data = data)$coefficient\n)\n\n(Intercept)         x_1         x_2 \n 0.01666832  2.00711701  0.18010906 \n\n\nWe now calculate the value of the objective functions at different values of \\(\\beta_1\\) and \\(\\beta_2\\). Here is the set of \\(\\{\\beta_1, \\beta_2\\}\\) combinations we look at.\n\n(\nbeta_table <- \n  data.table::CJ(\n    beta_1 = seq(-2, 2, length = 50),\n    beta_2 = seq(-1, 1, length = 50) \n  )\n)\n\n      beta_1     beta_2\n   1:     -2 -1.0000000\n   2:     -2 -0.9591837\n   3:     -2 -0.9183673\n   4:     -2 -0.8775510\n   5:     -2 -0.8367347\n  ---                  \n2496:      2  0.8367347\n2497:      2  0.8775510\n2498:      2  0.9183673\n2499:      2  0.9591837\n2500:      2  1.0000000\n\n\n\n\n\n\n\n\nNote\n\n\n\ndata.table::CJ() takes more than one set of vectors and find the complete combinations the values of the vectors. Trying\n\ndata.table::CJ(x1 = c(1, 2, 3), x2 = c(4, 5, 6))\n\nwill help you understand exactly what it does.\n\n\nLoop over the row numbers of beta_table to find SSE for all the rows (all the combinations of \\(\\beta_1\\) and \\(\\beta_2\\)).\n\n#=== define the function to get SSE ===#\nget_sse <- function(i, data)\n{\n  #=== extract beta_1 and beta_2 for ith observation  ===#\n  betas <- beta_table[i, ]\n\n  #=== calculate SSE ===#\n  sse <-\n    copy(data) %>% \n    .[, y_hat := x_1 * betas[, beta_1] + x_2 * betas[, beta_2]] %>% \n    .[, se := (y - y_hat)^2] %>% \n    .[, sum(se)]\n\n  return(sse)\n}\n\n#=== calculate SSE for each row of beta_table ===#\nsse_all <-\n  lapply(\n    1:nrow(beta_table),\n    function(x) get_sse(x, data)\n  ) %>% \n  unlist()\n\n#=== assign the calculated sse values as a variable ===#\n(\nbeta_table[, sse := sse_all]\n)\n\nHere is the contour map of SSE as a function of \\(\\beta_1\\) and \\(\\beta_2\\). The solution to the unconstrained problem (OLS estimates) is represented by the red point. Since Lasso needs to find a point within the red square, the solution would be \\(\\beta_1 = 1\\) and \\(\\beta_2 = 0\\) (yellow point). Lasso did not give anything to \\(\\beta_2\\) as \\(x_1\\) is a much bigger contributor of the two included variables. Lasso tends to give the coefficient of \\(0\\) to some of the variables when the constraint is harsh, effectively eliminating them from the model. For this reason, Lasso is often used as a variable selection method.\n\nCodeggplot() +\n  stat_contour(\n    data = beta_table, \n    aes(x = beta_1, y = beta_2, z = sse, color = ..level..),\n    size = 1.2,\n    breaks = \n      round(\n        quantile(beta_table$sse, seq(0, 1, 0.05)),\n        0\n      )\n    ) +\n  scale_color_viridis_c(name = \"SSE\") +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  geom_segment(aes(x = 1, y = 0, xend = 0, yend = 1), color = \"red\", size = 1.2) +\n  geom_segment(aes(x = 1, y = 0, xend = 0, yend = -1), color = \"red\", size = 1.2) +\n  geom_segment(aes(x = -1, y = 0, xend = 0, yend = 1), color = \"red\", size = 1.2) +\n  geom_segment(aes(x = -1, y = 0, xend = 0, yend = -1), color = \"red\", size = 1.2) +\n  #=== OLS point estimates (solutions without the constraint) ===#\n  geom_point(\n    aes(x = ols_coefs[\"x_1\"], y = ols_coefs[\"x_2\"]),\n    color = \"red\",\n    size = 3\n  ) +\n  geom_point(\n    aes(x = 1, y = 0),\n    color = \"yellow\",\n    size = 3\n  ) +\n  coord_equal()\n\n\n\n\nLet’s consider a different data generating process: \\(y = x_1 + x_2 + \\mu\\). Here, \\(x_1\\) and \\(x_2\\) are equally important unlike the previous case. Here is what happens:\n\nCodeN <- 1000 # number of observations\nx_1 <- rnorm(N)\nx_2 <- rnorm(N)\nmu <- rnorm(N) # error term\ny <- x_1 + x_2 + mu\n\ndata <-\n  data.table(\n    y = y,\n    x_1 = x_1,\n    x_2 = x_2\n  )\n\nols_coefs <- lm(y ~ x_1 + x_2, data = data)$coefficient\n\n#=== calculate sse for each row of beta_table ===#\nsse_all <-\n  lapply(\n    1:nrow(beta_table),\n    function(x) {\n      betas <- beta_table[x, ]\n      sse <-\n        copy(data) %>% \n        .[, y_hat := x_1 * betas[, beta_1] + x_2 * betas[, beta_2]] %>% \n        .[, se := (y - y_hat)^2] %>% \n        .[, sum(se)]\n      return(sse)\n    }\n  ) %>% \n  unlist()\n\n#=== assign the calculated sse values as a variable ===#\nbeta_table[, sse := sse_all]\n\n#=== visualize ===#\nggplot() +\n  stat_contour(\n    data = beta_table, \n    aes(x = beta_1, y = beta_2, z = sse, color = ..level..),\n    size = 1.2,\n    breaks = \n      round(\n        quantile(beta_table$sse, seq(0, 1, 0.05)),\n        0\n      )\n    ) +\n  scale_color_viridis_c(name = \"SSE\") +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  geom_segment(aes(x = 1, y = 0, xend = 0, yend = 1), color = \"red\", size = 1.2) +\n  geom_segment(aes(x = 1, y = 0, xend = 0, yend = -1), color = \"red\", size = 1.2) +\n  geom_segment(aes(x = -1, y = 0, xend = 0, yend = 1), color = \"red\", size = 1.2) +\n  geom_segment(aes(x = -1, y = 0, xend = 0, yend = -1), color = \"red\", size = 1.2) +\n  #=== OLS point estimates (solutions without the constraint) ===#\n  geom_point(\n    aes(x = ols_coefs[\"x_1\"], y = ols_coefs[\"x_2\"]),\n    color = \"red\",\n    size = 3\n  ) +\n  geom_point(\n    aes(x = 0.5, y = 0.5),\n    color = \"yellow\",\n    size = 3\n  ) +\n  coord_equal()\n\n\n\n\nIn this case, the solution would be (very close to) \\(\\{\\beta_1 = 0.5, \\beta_2 = 0.5\\}\\), with neither of them sent to zero. This is because \\(x_1\\) and \\(x_2\\) are equally important in explaining \\(y\\)."
  },
  {
    "objectID": "B04-regularization.html#lasso-implementation-r",
    "href": "B04-regularization.html#lasso-implementation-r",
    "title": "\n5  Regression Shrinkage Methods\n",
    "section": "\n5.3 Lasso implementation: R",
    "text": "5.3 Lasso implementation: R\nYou can use the glmnet() from the glmnet package run Lasso. For demonstration, we use the QuickStartExample data.\n\n#=== get the data ===#\ndata(QuickStartExample)\n\n#=== see the structure ===#\nstr(QuickStartExample)\n\nList of 2\n $ x: num [1:100, 1:20] 0.274 2.245 -0.125 -0.544 -1.459 ...\n $ y: num [1:100, 1] -1.275 1.843 0.459 0.564 1.873 ...\n\n\nAs you can see, QuickStartExample is a list of two elements. First one (x) is a matrix of dimension 100 by 20, which is the data of explanatory variables. Second one (y) is a matrix of dimension 100 by 1, which is the data for the dependent variable.\n\n\n\n\n\n\nNote\n\n\n\nIf you are used to running regressions in R, you should have specified a model using formula (e.g., y ~ x). However, most of the machine learning functions in R accept the dependent variable and explanatory variables in a matrix form (or data.frame). This is almost always the case for ML methods in Python as well.\n\n\nBy default, alpha parameter for glmnet() (\\(\\alpha\\) in Equation 5.1) is set to 1. So, to run Lasso, you can simply do the following:\n\n#=== extract X and y ===#\nX <- QuickStartExample$x\ny <- QuickStartExample$y\n\n#=== run Lasso ===#\nlasso <- glmnet(X, y)\n\nBy looking at the output below, you can see that glmnet() tried many different values of \\(\\lambda\\).\n\nlasso\n\n\nCall:  glmnet(x = X, y = y) \n\n   Df  %Dev  Lambda\n1   0  0.00 1.63100\n2   2  5.53 1.48600\n3   2 14.59 1.35400\n4   2 22.11 1.23400\n5   2 28.36 1.12400\n6   2 33.54 1.02400\n7   4 39.04 0.93320\n8   5 45.60 0.85030\n9   5 51.54 0.77470\n10  6 57.35 0.70590\n11  6 62.55 0.64320\n12  6 66.87 0.58610\n13  6 70.46 0.53400\n14  6 73.44 0.48660\n15  7 76.21 0.44330\n16  7 78.57 0.40400\n17  7 80.53 0.36810\n18  7 82.15 0.33540\n19  7 83.50 0.30560\n20  7 84.62 0.27840\n21  7 85.55 0.25370\n22  7 86.33 0.23120\n23  8 87.06 0.21060\n24  8 87.69 0.19190\n25  8 88.21 0.17490\n26  8 88.65 0.15930\n27  8 89.01 0.14520\n28  8 89.31 0.13230\n29  8 89.56 0.12050\n30  8 89.76 0.10980\n31  9 89.94 0.10010\n32  9 90.10 0.09117\n33  9 90.23 0.08307\n34  9 90.34 0.07569\n35 10 90.43 0.06897\n36 11 90.53 0.06284\n37 11 90.62 0.05726\n38 12 90.70 0.05217\n39 15 90.78 0.04754\n40 16 90.86 0.04331\n41 16 90.93 0.03947\n42 16 90.98 0.03596\n43 17 91.03 0.03277\n44 17 91.07 0.02985\n45 18 91.11 0.02720\n46 18 91.14 0.02479\n47 19 91.17 0.02258\n48 19 91.20 0.02058\n49 19 91.22 0.01875\n50 19 91.24 0.01708\n51 19 91.25 0.01557\n52 19 91.26 0.01418\n53 19 91.27 0.01292\n54 19 91.28 0.01178\n55 19 91.29 0.01073\n56 19 91.29 0.00978\n57 19 91.30 0.00891\n58 19 91.30 0.00812\n59 19 91.31 0.00739\n60 19 91.31 0.00674\n61 19 91.31 0.00614\n62 20 91.31 0.00559\n63 20 91.31 0.00510\n64 20 91.31 0.00464\n65 20 91.32 0.00423\n66 20 91.32 0.00386\n67 20 91.32 0.00351\n\n\nYou can access the coefficients for each value of \\(lambdata\\) by applying coef() method to lasso.\n\n#=== get coefficient estimates ===#\ncoef_lasso <- coef(lasso)\n\n#=== check the dimension ===#\ndim(coef_lasso)\n\n[1] 21 67\n\n#=== take a look at the first and last three ===#\ncoef_lasso[, c(1:3, 65:67)]\n\n21 x 6 sparse Matrix of class \"dgCMatrix\"\n                   s0           s1         s2          s64          s65\n(Intercept) 0.6607581  0.631235043  0.5874616  0.111208836  0.111018972\nV1          .          0.139264992  0.2698292  1.378068980  1.378335220\nV2          .          .            .          0.023067319  0.023240539\nV3          .          .            .          0.762792114  0.763209604\nV4          .          .            .          0.059619334  0.060253956\nV5          .          .            .         -0.901460720 -0.901862151\nV6          .          .            .          0.613661389  0.614081490\nV7          .          .            .          0.117323876  0.117960550\nV8          .          .            .          0.396890604  0.397260052\nV9          .          .            .         -0.030538991 -0.031073136\nV10         .          .            .          0.127412702  0.128222375\nV11         .          .            .          0.246801359  0.247227761\nV12         .          .            .         -0.063941712 -0.064471794\nV13         .          .            .         -0.045935249 -0.046242852\nV14         .         -0.005878595 -0.1299063 -1.158552963 -1.159038292\nV15         .          .            .         -0.137103471 -0.138012175\nV16         .          .            .         -0.045085698 -0.045661882\nV17         .          .            .         -0.047272446 -0.048039238\nV18         .          .            .          0.051702567  0.052180547\nV19         .          .            .         -0.001791685 -0.002203174\nV20         .          .            .         -1.144262012 -1.144641845\n                     s66\n(Intercept)  0.110845721\nV1           1.378578220\nV2           0.023398270\nV3           0.763589908\nV4           0.060832496\nV5          -0.902227796\nV6           0.614464085\nV7           0.118540773\nV8           0.397596878\nV9          -0.031560145\nV10          0.128960349\nV11          0.247615990\nV12         -0.064955124\nV13         -0.046522983\nV14         -1.159480668\nV15         -0.138840304\nV16         -0.046186890\nV17         -0.048737920\nV18          0.052615915\nV19         -0.002578088\nV20         -1.144987654\n\n\nApplying plot() method gets you how the coefficient estimates change as the value of \\(\\lambda\\) changes:\n\nplot(lasso)\n\n\n\n\nA high L1 Norm is associated with a “lower” value of \\(\\lambda\\) (weaker shrinkage). You can see that as \\(\\lambda\\) increases (L1 Norm decreases), coefficients on more and more variables are set to 0.\nNow, the obvious question is which \\(\\lambda\\) should we pick? One way to select a \\(\\lambda\\) is K-fold cross-validation (KCV), which we covered in section. We can implement KCV using the cv.glmnet() function. You can set the number of folds using the nfolds option (the default is 10). Here, let’s 5-fold CV.\n\ncv_lasso <- cv.glmnet(X, y, nfolds = 5)\n\nThe results of KCV can be readily visualized by applying the plot() method:\n\nplot(cv_lasso)\n\n\n\n\nThere are two vertical dotted lines. The left one indicates the value of \\(\\lambda\\) where CV MSE is minimized (called lambda.min). The right one indicates the  highest  (most regularized) value of \\(\\lambda\\) such that the CV error is within one standard error of the minimum (called lambda.1se).\nYou can access the MSE-minimizing \\(\\lambda\\) as follows:\n\ncv_lasso$lambda.min\n\n[1] 0.06896889\n\n\nYou can access the coefficient estimates when \\(\\lambda\\) is lambda.min as follows\n\ncoef(cv_lasso, s = \"lambda.min\")\n\n21 x 1 sparse Matrix of class \"dgCMatrix\"\n                      s1\n(Intercept)  0.147927056\nV1           1.337393911\nV2           .          \nV3           0.704086481\nV4           .          \nV5          -0.842853150\nV6           0.549403480\nV7           0.032703914\nV8           0.342430521\nV9           .          \nV10          0.001206608\nV11          0.178995989\nV12          .          \nV13          .          \nV14         -1.079993473\nV15          .          \nV16          .          \nV17          .          \nV18          .          \nV19          .          \nV20         -1.061382444\n\n\nThe following code gives you the coefficient estimates when \\(\\lambda\\) is lambda.1se\n\ncoef(cv_lasso, s = \"lambda.1se\")\n\n21 x 1 sparse Matrix of class \"dgCMatrix\"\n                    s1\n(Intercept)  0.1536661\nV1           1.3019575\nV2           .        \nV3           0.6422426\nV4           .        \nV5          -0.7892388\nV6           0.4944794\nV7           .        \nV8           0.2943189\nV9           .        \nV10          .        \nV11          0.1058440\nV12          .        \nV13          .        \nV14         -1.0402312\nV15          .        \nV16          .        \nV17          .        \nV18          .        \nV19          .        \nV20         -0.9791172\n\n\n\n\n\n\n\n\nNote\n\n\n\nglmnet() can be used to much broader class of models (e.g., Logistic regression, Poisson regression, Cox regression, etc). As the name suggests it’s elastic  net  methods for  generalized  linear  model."
  },
  {
    "objectID": "B04-regularization.html#lasso-implementation-python",
    "href": "B04-regularization.html#lasso-implementation-python",
    "title": "\n5  Regression Shrinkage Methods\n",
    "section": "\n5.4 Lasso implementation: Python",
    "text": "5.4 Lasso implementation: Python\nComing later."
  },
  {
    "objectID": "B04-regularization.html#scaling",
    "href": "B04-regularization.html#scaling",
    "title": "\n5  Regression Shrinkage Methods\n",
    "section": "\n5.5 Scaling",
    "text": "5.5 Scaling\nUnlike linear model estimation without shrinkage (regularization), shrinkage method is sensitive to the scaling of independent variables. Scaling of a variable has basically no consequence in linear model without regularization. It simply changes the interpretation of the scaled variable and the coefficient estimates on all the other variables remain unaffected. However, scaling of a single variable has a ripple effect to the other variables in shrinkage methods. This is because the penalization term: \\(\\lambda \\huge[\\normalsize(1-\\alpha)||\\beta||^2_2/2 + \\alpha ||\\beta||_1\\huge]\\). As you can see, \\(\\lambda\\) is applied universally to all the coefficients without any consideration of the scale of the variables.\nLet’s scale the first variable in X (this variable is influential as it survived even when \\(\\lambda\\) is very low) by 1/1000 and see what happens. Now, by default, the standardize option is set to TRUE. So, we need to set it to FALSE explicitly to see the effect.\nHere is before scaling:\n\ncv.glmnet(X, y, nfolds = 5, standardize = FALSE) %>% \n  coef(s = \"lambda.min\")\n\n21 x 1 sparse Matrix of class \"dgCMatrix\"\n                    s1\n(Intercept)  0.1482955\nV1           1.3381070\nV2           .        \nV3           0.6936938\nV4           .        \nV5          -0.8249778\nV6           0.5448439\nV7           0.0230429\nV8           0.3398575\nV9           .        \nV10          .        \nV11          0.1648445\nV12          .        \nV13          .        \nV14         -1.0801745\nV15          .        \nV16          .        \nV17          .        \nV18          .        \nV19          .        \nV20         -1.0338882\n\n\nHere is after scaling:\n\n#=== scale the first variable ===#\nX_scaled <- X\nX_scaled[, 1] <- X_scaled[, 1] / 1000\n\ncv.glmnet(X_scaled, y, nfolds = 5, standardize = FALSE) %>% \n  coef(s = \"lambda.min\")\n\n21 x 1 sparse Matrix of class \"dgCMatrix\"\n                     s1\n(Intercept)  0.49789782\nV1           .         \nV2           .         \nV3           0.60938060\nV4          -0.02028196\nV5          -0.79629764\nV6           0.75033487\nV7           0.05928865\nV8           0.09916745\nV9           .         \nV10          .         \nV11          0.49890470\nV12          .         \nV13          .         \nV14         -1.03083531\nV15          .         \nV16          .         \nV17          .         \nV18          0.12642122\nV19          .         \nV20         -1.32068655\n\n\nAs you can see, the coefficient on the first variable is 0 after scaling. Setting standardize = TRUE (or not doing anything with this option) gives you very similar results whether the data is scaled or not.\n\n#=== not scaled ===#\ncv.glmnet(X, y, nfolds = 5, standardize = TRUE) %>% \n  coef(s = \"lambda.min\")\n\n21 x 1 sparse Matrix of class \"dgCMatrix\"\n                     s1\n(Intercept)  0.14867414\nV1           1.33377821\nV2           .         \nV3           0.69787701\nV4           .         \nV5          -0.83726751\nV6           0.54334327\nV7           0.02668633\nV8           0.33741131\nV9           .         \nV10          .         \nV11          0.17105029\nV12          .         \nV13          .         \nV14         -1.07552680\nV15          .         \nV16          .         \nV17          .         \nV18          .         \nV19          .         \nV20         -1.05278699\n\n#=== scaled ===#\ncv.glmnet(X_scaled, y, nfolds = 5, standardize = TRUE) %>% \n  coef(s = \"lambda.min\")\n\n21 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s1\n(Intercept)  1.479271e-01\nV1           1.337394e+03\nV2           .           \nV3           7.040865e-01\nV4           .           \nV5          -8.428531e-01\nV6           5.494035e-01\nV7           3.270391e-02\nV8           3.424305e-01\nV9           .           \nV10          1.206608e-03\nV11          1.789960e-01\nV12          .           \nV13          .           \nV14         -1.079993e+00\nV15          .           \nV16          .           \nV17          .           \nV18          .           \nV19          .           \nV20         -1.061382e+00\n\n\nWhile you do not have to worry about scaling issues as long as you are using glmnet(), this is something worth remembering."
  },
  {
    "objectID": "B05-bootstrap.html",
    "href": "B05-bootstrap.html",
    "title": "\n6  Bootstrap\n",
    "section": "",
    "text": "Bootstrap can be used to quantify the uncertainty associated with an estimator. For example, you can use it to estimate the standard error (SE) of a coefficient of a linear model. Since there are closed-form solutions for that, bootstrap is not really bringing any benefits to this case. However, the power of bootstrap comes in handy when you do NOT have a closed form solution. We will first demonstrate how bootstrap works using a linear model, and then apply it to a case where no-closed form solution is available."
  },
  {
    "objectID": "B05-bootstrap.html#how-it-works",
    "href": "B05-bootstrap.html#how-it-works",
    "title": "\n6  Bootstrap\n",
    "section": "\n6.2 How it works",
    "text": "6.2 How it works\n\n\nPackages to load for replication.\n\nlibrary(data.table)\nlibrary(tidyverse)\nlibrary(mgcv)\nlibrary(caret)\nlibrary(parallel)\nlibrary(fixest)\nlibrary(ranger)\n\nHere are the general steps of a bootstrap:\n\nStep 1: Sample the data with replacement (You can sample the same observations more than one times. You draw a ball and then you put it back in the box.)\nStep 2: Run a statistical analysis to estimate whatever quantity you are interested in estimating\nRepeat Steps 1 and 2 many times and store the estimates\nDerive uncertainty measures from the collection of estimates obtained above\n\nLet’s demonstrate this using a very simple linear regression example.\nHere is the data generating process:\n\nset.seed(89343)\nN <- 100\nx <- rnorm(N)\ny <- 2 * x + 2 * rnorm(N)\n\nWe would like to estimate the coefficient on \\(x\\) by applying OLS to the following model:\n\\[\ny = \\alpha + \\beta_x + \\mu\n\\]\nWe know from the econometric theory class that the SE of \\(\\hat{\\beta}_{OLS}\\) is \\(\\frac{\\sigma}{\\sqrt{SST_x}}\\), where \\(\\sigma^2\\) is the variance of the error term (\\(\\mu\\)) and \\(SST_x = \\sum_{i=1}^N (x_i - \\bar{x})^2\\) (\\(\\bar{x}\\) is the mean of \\(x\\)).\n\nmean_x <- mean(x)\nsst_x <- ((x-mean(x))^2) %>% sum()\n(\nse_bhat <- sqrt(4 / sst_x)\n)\n\n[1] 0.217933\n\n\nSo, we know that the true SE of \\(\\hat{\\beta}_{OLS}\\) is 0.217933. There is not really any point in using bootstrap in this case, but this is a good example to see if bootstrap works or not.\nLet’s implement a single iteration of the entire bootstrap steps (Steps 1 and 2).\n\n#=== set up a dataset ===#\ndata <-\n  data.table(\n    y = y,\n    x = x\n  )\n\nNow, draw observations with replacement so the resulting dataset has the same number of observations as the original dataset.\n\nnum_obs <- nrow(data)\n\n#=== draw row numbers ===#\n(\nrow_indices <- sample(seq_len(num_obs), num_obs, replace = TRUE)\n)\n\n  [1] 29 56 39 83 52 66 70 19  4 34 28 34 81 32  9 95 99 86  4 79 30 15 41 97 43\n [26] 89 60 41 16 19 66 96 34 91 86 67 75 28 74 50 71 95 74 87 58 27  9 65 80 41\n [51] 71 64 21 47 45 77 97 94 72 50 23 10 33 45 14 17 82 56 33 75 70 63 78 81 64\n [76] 16 84 90  2 17  5 46 53 37 93 85 72 63 10 35 42 20 70 49 74 32 25 73 76 32\n\n\nUse the sampled indices to create a bootstrapped dataset:\n\n\nYou could also use bootstraps() from the rsample package.\n\ntemp_data <- data[row_indices, ]\n\nNow, apply OLS to get a coefficient estimate on \\(x\\) using the bootstrapped dataset.\n\nlm(y ~ x, data = temp_data)$coefficient[\"x\"]\n\n       x \n2.040957 \n\n\nThis is the end of Steps 1 and 2. Now, let’s repeat this step 1000 times. First, we define a function that implements Steps 1 and 2.\n\nget_beta <- function(i, data)\n{\n  num_obs <- nrow(data)\n\n  #=== sample row numbers ===#\n  row_indices <- sample(seq_len(num_obs), num_obs, replace = TRUE)\n\n  #=== bootstrapped data ===#\n  temp_data <- data[row_indices, ]\n\n  #=== get coefficient ===#\n  beta_hat <- lm(y ~ x, data = temp_data)$coefficient[\"x\"]\n\n  return(beta_hat)\n}\n\nNow repeat get_beta() many times:\n\nbeta_store <-\n  lapply(\n    1:1000,\n    function(x) get_beta(x, data)\n  ) %>% \n  unlist()\n\nCalculate standard deviation of \\(\\hat{\\beta}_{OLS}\\),\n\nsd(beta_store)\n\n[1] 0.2090611\n\n\nNot, bad. What if we make the number of observations to 1000 instead of 100?\n\nset.seed(67343)\n\n#=== generate data ===#\nN <- 1000\nx <- rnorm(N)\ny <- 2 * x + 2 * rnorm(N)\n\n#=== set up a dataset ===#\ndata <-\n  data.table(\n    y = y,\n    x = x\n  )\n\n#=== true SE ===#\nmean_x <- mean(x)\nsst_x <- sum(((x-mean(x))^2))\n(\nse_bhat <- sqrt(4 / sst_x)\n)\n\n[1] 0.06243842\n\n\n\n#=== bootstrap-estimated SE ===#\nbeta_store <-\n  lapply(\n    1:1000,\n    function(x) get_beta(x, data)\n  ) %>% \n  unlist()\n\nsd(beta_store)\n\n[1] 0.06147708\n\n\nThis is just a single simulation. So, we cannot say bootstrap works better when the number of sample size is larger only from these experiments. But, it is generally true that bootstrap indeed works better when the number of sample size is larger."
  },
  {
    "objectID": "B05-bootstrap.html#more-complicated-example",
    "href": "B05-bootstrap.html#more-complicated-example",
    "title": "\n6  Bootstrap\n",
    "section": "\n6.3 More complicated example",
    "text": "6.3 More complicated example\nConsider a simple production function (yield response functions for agronomists):\n\\[\ny = \\beta_1 x + \\beta_2 x^2 + \\mu\n\\]\n\n\n\\(y\\): output\n\n\\(x\\): input\n\n\\(\\mu\\): error\n\nThe price of \\(y\\) is 5 and the cost of \\(x\\) is 2. Your objective is to identify the amount of input that maximizes profit. You do not know \\(\\beta_1\\) and \\(\\beta_2\\), and will be estimating them using the data you have collected. Letting \\(\\hat{\\beta_1}\\) and \\(\\hat{\\beta_2}\\) denote the estimates of \\(\\beta_1\\) and \\(\\beta_2\\), respectively, the mathematical expression of the optimization problem is:\n\\[\nMax_x 5(\\hat{\\beta}_1 x + \\hat{\\beta}_2 x^2) - 2 x\n\\]\nThe F.O.C is\n\\[\n5\\hat{\\beta}_1 + 10 \\hat{\\beta}_2 x - 2 = 0\n\\]\nSo, the estimated profit-maximizing input level is \\(\\hat{x}^* = \\frac{2-5\\hat{\\beta}_1}{10\\hat{\\beta}_2}\\). What we are interested in knowing is the SE of \\(x^*\\). As you can see, it is a non-linear function of the coefficients, which makes it slightly harder than simply getting the SE of \\(\\hat{\\beta_1}\\) or \\(\\hat{\\beta_2}\\). However, bootstrap can easily get us an estimate of the SE of \\(\\hat{x}^*\\)1. The bootstrap process will be very much the same as the first bootstrap example except that we will estimate \\(x^*\\) in each iteration instead of stopping at estimating just coefficients. Let’s work on a single iteration first.\nHere is the data generating process:\n\nset.seed(894334)\n\nN <-  1000\nx <-  runif(N) * 3\ney <- 6 * x - 2 * x^2\nmu <- 2 * rnorm(N)\ny <- ey + mu\n\ndata <- \n  data.table(\n    x = x,\n    y = y,\n    ey = ey\n  )\n\nUnder the data generating process, here is the production function looks like:\n\nggplot(data = data) +\n  geom_line(aes(y = ey, x = x))\n\n\n\n\n\nnum_obs <- nrow(data)\nrow_indices <- sample(seq_len(num_obs), num_obs, replace = TRUE)\nboot_data <- data[row_indices, ]\nreg <- lm(y ~ x + I(x^2), data = boot_data)\n\nNow that we have estimated \\(\\beta_1\\) and \\(\\beta_2\\), we can easily estimate \\(x^*\\) using its analytical formula.\n\n(\nx_star <- (2 - 5 * reg$coef[\"x\"])/ (10 * reg$coef[\"I(x^2)\"])\n)\n\n      x \n1.40625 \n\n\nWe can repeat this many times to get a collection of \\(x^*\\) estimates and calculate the standard deviation.\n\nget_x_star <- function(i)\n{\n  row_indices <- sample(seq_len(num_obs), num_obs, replace = TRUE)\n  boot_data <- data[row_indices, ]\n  reg <- lm(y ~ x + I(x^2), data = boot_data)\n  x_star <- (2 - 5 * reg$coef[\"x\"])/ (10 * reg$coef[\"I(x^2)\"])\n}\n\n\nx_stars <- \n  lapply(\n    1:1000,\n    get_x_star\n  ) %>%\n  unlist()\n\nHere is the histogram:\n\nhist(x_stars, breaks = 30)\n\n\n\n\nSo, it seems to follow a normal distribution. You can get standard deviation of x_stars as an estimate of the SE of \\(\\hat{x}^*\\).\n\nsd(x_stars)\n\n[1] 0.01783721\n\n\nYou can get the 95% confidence interval (CI) like below:\n\nquantile(x_stars, prob = c(0.025, 0.975))\n\n    2.5%    97.5% \n1.364099 1.430915"
  },
  {
    "objectID": "B05-bootstrap.html#one-more-example-with-a-non-parametric-model",
    "href": "B05-bootstrap.html#one-more-example-with-a-non-parametric-model",
    "title": "\n6  Bootstrap\n",
    "section": "\n6.4 One more example with a non-parametric model",
    "text": "6.4 One more example with a non-parametric model\nWe now demonstrate how we can use bootstrap to get an estimate of the SE of \\(\\hat{x}^*\\) when we use random forest (RF) as our regression method instead of OLS. When RF is used, we do not have any coefficients like the OLS case above. Even then, bootstrap allows us to estimate the SE of \\(\\hat{x}^*\\).\nThe procedure is exactly the same except that we use RF to estiamte the production function and also that we need to conduct numerical optimization as no analytical formula is available unlike the case above.\nWe first implement a single iteration.\n\n#=== get bootstrapped data ===#\nrow_indices <- sample(seq_len(num_obs), num_obs, replace = TRUE)\nboot_data <- data[row_indices, ]\n\n#=== train RF ===#\nreg_rf <- ranger(y ~ x, data = boot_data)\n\nOnce you train RF, we can predict yield at a range of values of \\(x\\), calculate profit, and then pick the value of \\(x\\) that maximizes the estimated profit. Here is what the estimated production function looks like:\n\n#=== create series of x values at which yield will be predicted ===#\neval_data <- data.table(x = seq(0, 3, length = 1000))\n\n#=== predict yield based on the trained RF ===#\neval_data[, y_hat := predict(reg_rf, eval_data)$predictions]\n\n#=== plot ===#\nggplot(data = eval_data) +\n  geom_line(aes(y = y_hat, x = x))\n\n\n\n\nWell, it is very spiky (we need to tune hyper-parameters using KCV. But, more on this later. The quality of RF estimation has nothing to do with the goal of this section).\nWe can now predict profit at each value of \\(x\\).\n\n#=== calculate profit ===#\neval_data[, profit_hat := 5 * y_hat - 2 * x]\n\nhead(eval_data)\n\n             x      y_hat profit_hat\n1: 0.000000000 -0.4807528  -2.403764\n2: 0.003003003 -0.4807528  -2.409770\n3: 0.006006006 -0.4807528  -2.415776\n4: 0.009009009  1.0193781   5.078872\n5: 0.012012012  1.0193781   5.072866\n6: 0.015015015  1.2466564   6.203252\n\n\nThe only thing left for us to do is to find the \\(x\\) value that maximizes profit.\n\neval_data[which.max(profit_hat), ]\n\n          x    y_hat profit_hat\n1: 1.273273 8.675033   40.82862\n\n\nOkay, so 1.2732733 is the \\(\\hat{x}^*\\) from this iteration.\nAs you might have guessed already, we can just repeat this step to get an estimate of the SE of \\(\\hat{x}^*_{RF}\\).\n\nget_x_star_rf <- function(i)\n{\n  print(i) # progress tracker\n  \n  #=== get bootstrapped data ===#\n  row_indices <- sample(seq_len(num_obs), num_obs, replace = TRUE)\n  boot_data <- data[row_indices, ]\n\n  #=== train RF ===#\n  reg_rf <- ranger(y ~ x, data = boot_data)\n\n  #=== create series of x values at which yield will be predicted ===#\n  eval_data <- data.table(x = seq(0, 3, length = 1000))\n\n  #=== predict yield based on the trained RF ===#\n  eval_data[, y_hat := predict(reg_rf, eval_data)$predictions]\n  \n  #=== calculate profit ===#\n  eval_data[, profit_hat := 5 * y_hat - 2 * x]\n  \n  #=== find x_star_hat ===#\n  x_star_hat <- eval_data[which.max(profit_hat), x]\n  \n  return(x_star_hat)\n}\n\n\nx_stars_rf <- \n  mclapply(\n    1:1000,\n    get_x_star_rf,\n    mc.cores = 12\n  ) %>%\n  unlist()\n\n#=== Windows user ===#\n# library(future.apply)\n# plan(\"multisession\", workers = detectCores() - 2)\n# x_stars_rf <- \n#   future_lapply(\n#     1:1000,\n#     get_x_star_rf\n#   ) %>%\n#   unlist()\n\nHere are the estimate of the SE of \\(\\hat{x}^*_{RF}\\) and 95% CI.\n\nsd(x_stars_rf)\n\n[1] 0.4044706\n\nquantile(x_stars_rf, prob = c(0.025, 0.975))\n\n     2.5%     97.5% \n0.5465465 2.0570571 \n\n\nAs you can see, the estimation of \\(x^*\\) is much more inaccurate than the previous OLS approach. This is likely due to the fact that we are not doing a good job of tuning the hyper-parameters of RF (but, again, more on this later).\nThis conclude the illustration of the power of using bootstrap to estimate the uncertainty of the statistics of interest (\\(x^*\\) here) when the analytical formula of the statistics is non-linear or not even known."
  },
  {
    "objectID": "P01-random-forest.html",
    "href": "P01-random-forest.html",
    "title": "\n7  Random Forest\n",
    "section": "",
    "text": "library(tidyverse)\nlibrary(data.table)\nlibrary(rpart)\nlibrary(rattle)\nlibrary(wooldridge)"
  },
  {
    "objectID": "P01-random-forest.html#sec-rt",
    "href": "P01-random-forest.html#sec-rt",
    "title": "\n7  Random Forest\n",
    "section": "\n7.1 Regression tree",
    "text": "7.1 Regression tree\n\n7.1.1 What is it?\nHere is an example of regression tree to explain logged salary (lsalary) using the mlb1 data from the wooldridge package.\n\nCode#=== get mlb1 data (from wooldridge) ===#\ndata(mlb1)\n\n#=== build a simple tree ===#\nsimple_tree <-\n  rpart(\n    lsalary ~ hits + runsyr, \n    data = mlb1, \n    control = rpart.control(minsplit = 200)\n  )\n\nfancyRpartPlot(simple_tree)\n\n\n\n\nHere is how you read the figure. At the first node, all the observations belong to it (\\(n=353\\)) and the estimate of lsalary is 13. Now, the whole datasets are split into two based on the criteria of whether hits is less than 262 or not. If yes, then such observations will be grouped into the node with “2” on top (the leftmost node), and the estimated lsalary for all the observations in that group (\\(n = 132\\)) is 12. If no, then such observations will be grouped into the node with “3” on top, and the estimated lsalary for all the observations in that group (\\(n = 221\\)) is 14. This node is further split into two groups based on whether runsyr is less than 44 or not. For those observations with runsyr \\(< 44\\) (second node a the bottom), estimated lsalary is 14. For those with runsyr \\(>= 44\\) (rightmost node at the bottom), estimated lsalary is 15. The nodes that do not have any further bifurcations below are called terminal nodes or leafs.\nAs illustrated in the figure above, a regression tree splits the data into groups based on the value of explanatory variables, and all the observations in the same group will be assigned the same estimate (the sample average of the dependent variable of the group).\nAnother way of illustrating this grouping is shown below:\n\nCodeggplot(mlb1) +\n  geom_point(aes(y = hits, x = runsyr, color = lsalary)) +\n  scale_color_viridis_c() +\n  geom_hline(yintercept = 262) +\n  geom_line(\n    data = data.table(x = 44, y = seq(262, max(mlb1$hits), length = 100)), \n    aes(y = y, x = x)\n  ) +\n  annotate(\n    \"text\", \n    x = 44, y = 111, \n    label = \"Region 2\", \n    color = \"red\"\n  ) +\n  annotate(\n    \"text\", \n    x = 22, y = 1500, \n    label = \"Region 6\", \n    color = \"red\"\n  ) +\n  annotate(\n    \"text\", \n    x = 75, y = 1500, \n    label = \"Region 7\", \n    color = \"red\"\n  )\n\n\n\n\nThe mechanism called recursive binary splitting is used to split the predictor space like the example above. Suppose you have K explanatory variables (\\(X_1, \\dots, X_k\\)). Further, let \\(c\\) denote the cutpoint that splits the sample into two regions: {\\(X|X_k < c\\)} and {\\(X|X_k \\geq c\\)}.\n\n\n{\\(X|X_k < c\\)} means observations that satisfy the condition stated right to the vertical bar (|). Here, it means all the observations for which its \\(X_k\\) value is less than \\(c\\).\n\nStep 1: For each of the explanatory variables (\\(X_1\\) through \\(X_K\\)), find the cutpoint that leads to the lowest sum of the squared residuals.\nStep 2: Among all the splits (as many as the number of explanatory variables), pick the variable-cutpoint combination that leads to the lowest sum of the squared residuals.\n\nThe data is then split according to the chosen criteria and then the same process is repeated for each of the branches, ad infinitum until the user-specified stopping criteria is met.\nLet’s try to write (an inefficient version) this process for the first split from the beginning node using the mlb1 data as an illustration based on a simple grid search to find the optimal cutpoints (Step 1).\n\n#=== get data ===#\nlibrary(wooldridge)\ndata(mlb1)\n\nmlb1_dt <- \n  mlb1 %>% \n  data.table() %>% # turn into data.table \n  .[, salary := NULL] %>% # remove salary (use lsalary instead)\n  na.omit() # remove observations with NA in any of the variables\n\nLet’s work on splitting based on hruns. First, we define a sequence of values of cutpoints for hruns.\n\nvalue_seq <- \n  quantile(\n    mlb1_dt$hruns, \n    prob = seq(0.001, 0.999, length = 100)\n  ) %>% \n  unique()\n\nFor each value in value_seq, we find the RSS. For example, for the 50th value in value_seq,\n\ncopy(mlb1_dt) %>% \n  #=== find the mean of lsalary by whether hruns is less than the cutpoint or not ===#\n  .[, y_hat := mean(lsalary), by = (hruns < value_seq[50])] %>% \n  #=== get squared residuals ===#\n  .[, (lsalary - y_hat)^2] %>% \n  #=== get RSS ===#\n  sum()\n\n[1] 318.2431\n\n\nHow about 70th value in value_seq?\n\ncopy(mlb1_dt) %>% \n  #=== find the mean of lsalary by whether hruns is less than the cutpoint or not ===#\n  .[, y_hat := mean(lsalary), by = (hruns < value_seq[70])] %>% \n  #=== get squared residuals ===#\n  .[, (lsalary - y_hat)^2] %>% \n  #=== get RSS ===#\n  sum()\n\n[1] 419.1821\n\n\nThis means value_seq[70] (209.0880707) is a better cutpoint than value_seq[50] (71.7563535).\nOkay, let’s consider all the candidate values, not just 50th and 70th, and then pick the best.\n\nget_rss <- function(i, var_name, value_seq, data)\n{\n  rss <-\n    copy(data) %>% \n    setnames(var_name, \"var\") %>% \n    .[, y_hat := mean(lsalary), by = (var < value_seq[i])] %>% \n    .[, (lsalary - y_hat)^2] %>% \n    sum()\n\n  return_data <-\n    data.table(\n      rss = rss,\n      var_name = var_name,\n      var_value = value_seq[i]\n    )\n\n  return(return_data)\n}\n\nHere are RSS values at every value in value_seq.\n\nrss_value <-\n  lapply(\n    seq_len(length(value_seq)),\n    function(x) get_rss(x, \"hruns\", value_seq, mlb1_dt) \n  ) %>% \n  rbindlist()\n\nhead(rss_value)\n\n        rss var_name var_value\n1: 445.0615    hruns         0\n2: 396.8287    hruns         1\n3: 383.2393    hruns         2\n4: 362.5779    hruns         3\n5: 337.0760    hruns         4\n6: 325.4865    hruns         5\n\ntail(rss_value)\n\n        rss var_name var_value\n1: 419.1821    hruns  209.0881\n2: 419.5111    hruns  231.0000\n3: 428.4163    hruns  242.4425\n4: 434.3318    hruns  258.5674\n5: 441.9307    hruns  333.4414\n6: 443.3740    hruns  426.0780\n\n\nFinding the cutpoint value that minimizes RSS,\n\nrss_value[which.min(rss), ]\n\n        rss var_name var_value\n1: 260.3094    hruns  28.47488\n\n\nOkay, so, the best cutpoint for hruns is 28.475\nSuppose we are considering only five explanatory variables in building a regression tree: hruns, years, rbisyr, allstar, runsyr, hits, and bavg. We do the same operation we did for hruns for all the variables.\n\nget_rss_by_var <- function(var_name, data)\n{\n  temp_data <- copy(data) \n\n  #=== define a sequence of values of hruns ===#\n  value_seq <- \n    quantile(\n      temp_data[, ..var_name] %>% unlist(), \n      prob = seq(0.001, 0.999, length = 100)\n    ) %>% \n    unique()\n\n  #=== get RSS ===#\n  rss_value <-\n    lapply(\n      seq_len(length(value_seq)),\n      function(x) get_rss(x, var_name, value_seq, temp_data) \n    ) %>% \n    rbindlist() %>% \n    .[which.min(rss),]\n\n  return(rss_value)\n}\n\nLooping over the set of variables,\n\n(\nmin_rss_by_var <-\n  lapply(\n    c(\"hruns\", \"years\", \"rbisyr\", \"allstar\", \"runsyr\", \"hits\", \"bavg\"),\n    function(x) get_rss_by_var(x, mlb1_dt)\n  ) %>% \n  rbindlist()\n)\n\n        rss var_name  var_value\n1: 260.3094    hruns  28.474879\n2: 249.9090    years   4.000000\n3: 265.1082   rbisyr  32.774949\n4: 279.7165  allstar   8.209356\n5: 251.6343   runsyr  38.079145\n6: 205.1488     hits 354.811444\n7: 375.0281     bavg 252.359257\n\n\nSo, the variable-cutpoint combination that minimizes RSS is hits - 354.81. We now have the first split. This tree is developed further by splitting nodes like this.\n\n7.1.2 Training a regression tree in R\nYou can fit a regression tree using rpart() from the rpart package. Its syntax is similar to that of lm() for a quick fitting.\n\nrpart(\n  formula,\n  data\n)\n\nUsing mlb1, let’s fit a regression tree where lsalary is the dependent variable and hruns, years, rbisyr, allstar, runsyr, hits, and bavg are the explanatory variables.\n\n#=== fit a tree ===#\nfitted_tree <-\n  rpart(\n    lsalary ~ hruns + years + rbisyr + allstar + runsyr + hits + bavg, \n    data = mlb1_dt\n  )\n\nHere is the visualization of the fitted tree using fancyRpartPlot() from the rattle package.\n\nfancyRpartPlot(fitted_tree)\n\n\n\n\nNow, you may wonder why rpart() is not building a tree that has as many leaves as the number of observations so that we have a perfect prediction for the train data (mlb1). If we are simply implementing recursive binary splitting, then it should not have stopped where it stopped. This is because rpart() sets parameter values that control the development of a tree by default. Those default parameters can be seen below:\n\nrpart.control()\n\n$minsplit\n[1] 20\n\n$minbucket\n[1] 7\n\n$cp\n[1] 0.01\n\n$maxcompete\n[1] 4\n\n$maxsurrogate\n[1] 5\n\n$usesurrogate\n[1] 2\n\n$surrogatestyle\n[1] 0\n\n$maxdepth\n[1] 30\n\n$xval\n[1] 10\n\n\nFor example, minsplit is the minimum number of observations that must exist in a node in order for a split to be attempted. cp refers to the complexity parameter. For a given value of cp, a tree is build to minimize the following:\n\\[\n\\sum_{t=1}^T\\sum_{x_i\\in R_t} (y_i - \\hat{y}_{R_t})^2 + cp\\cdot T\n\\]\nwhere \\(R_t\\) is the \\(t\\)th region and \\(\\hat{y_{R_t}}\\) is the estimate of \\(y\\) for all the observations that reside in \\(R_t\\). So, the first term is RSS. The objective function has a penalization term (the second term) just like shrinkage methods we saw in Section 5.1. A higher value of cp leads to a less complex tree with less leaves.\nIf you want to build a much deeper tree that has many leaves, then you can do so using the control option like below.\n\nfull_tree <-\n  rpart(\n    lsalary ~ hruns + years + rbisyr + allstar + runsyr + hits + bavg, # formula\n    data = mlb1_dt, # data\n    control = # control of the hyper parameters\n      rpart.control(\n        minsplit = 2, \n        cp = 0 # complexity parameter\n      )\n  )\n\nLet’s see how amazing this tree is by comparing the observed and fitted lsalary values.\n\n#=== get fitted values ===#\nmlb1_dt[, y_hat := predict(full_tree, newdata = mlb1_dt)]\n\n#=== visualize the fit ===#\nggplot(data = mlb1_dt) +\n  geom_point(aes(y = lsalary, x = y_hat)) +\n  geom_abline(slope = 1, color = \"red\")\n\n\n\n\nYes, perfect prediction accuracy! At least for the train data anyway. But, we all know we want nothing to do with this kind of model. It is clearly over-fitting the train data.\nIn order to find a reasonable model, we can use KCV over cp. Fortunately, when we run rpart(), it automatically builds multiple trees at different values of cp that controls the number of leaves and conduct KCV. You can visualize this using plotcp().\n\nplotcp(fitted_tree)\n\n\n\n\nMSE and cp are presented on the y- and x-axis, respectively. According to the KCV results, cp \\(= 0.018\\) provides the tree with the smallest number of leaves (the most simple) where the MSE value is within one standard deviation from the lowest MSE. You can access the tree built under cp \\(= 0.018\\) like below.\n\n#=== get the best tree ===#\nbest_tree <- prune(full_tree, cp = 0.018)\n\n#=== visualize it ===#\nfancyRpartPlot(best_tree)\n\n\n\n\nEven though how a regression tree is build in R. In practice, you never use a regression tree itself as the final model for your research as its performance is rather poor and tend to over-fit compared to other competitive methods. But, understanding how building a regression tree is important to understand its derivatives like random forest, boosted regression forest."
  },
  {
    "objectID": "P01-random-forest.html#sec-rf",
    "href": "P01-random-forest.html#sec-rf",
    "title": "\n7  Random Forest\n",
    "section": "\n7.2 Random Forest (RF)",
    "text": "7.2 Random Forest (RF)\nRegression tree approach is often not robust and suffers from high variance. Here, we look at the process called  bagging  and how it can be used to train RF model, which is much more robust than a regression tree.\n\n7.2.1 Bagging (Bootstrap aggregation)\nConsider two random variables \\(x_1\\) and \\(x_2\\) from the identical distribution, where \\(E[x_i] = \\alpha\\) and \\(Var(x_i) = \\sigma^2\\). You are interested in estimating \\(E[x_i]\\). We all know that the following relationship holds in general:\n\\[\n\\begin{aligned}\nVar(\\frac{x_1 + x_2}{2}) & = \\frac{Var(x_1)}{4} + \\frac{Var(x_2)}{4} + \\frac{Cov(x_1, x_2)}{2} \\\\\n& = \\frac{\\sigma^2}{2} + \\frac{Cov(x_1, x_2)}{2}\n\\end{aligned}\n\\]\nSo, instead of using a single draw from \\(x_1\\) and using it as an estimate for \\(E[x_i]\\), it is better to use the values from both \\(x_1\\) and \\(x_2\\) and average them to obtain an estimate for \\(E[x_i]\\) as long as \\(x_1\\) and \\(x_2\\) are not perfectly  positively  correlated (in this case \\(Cov(x_1, x_2) = Var(x_1) = Var(x_1) = \\sigma^2\\)). The benefit of averaging is greater when the value of \\(Cov(x_1, x_2)\\) is smaller.\nLet’s do a little experiment to see this. We consider three cases:\n\n#=== set the number of observations to 1000 ===#\nN <- 1000\n\n\n#=== first case (no correlation) ===#\nx_1 <- rnorm(N)\nx_2 <- rnorm(N)\n\ncor(x_1, x_2)\n\n[1] -0.04817562\n\n#=== second case (positively correlated) ===#\nx_1 <- rnorm(N)\nx_2 <- 0.5 * x_1 + sqrt(1-(0.5)^2) * rnorm(N)\n\ncor(x_1, x_2)\n\n[1] 0.5265198\n\n#=== third case (negatively correlated) ===#\nx_1 <- rnorm(N)\nx_2 <- - 0.8 * x_1 - sqrt(1-(0.8)^2) * rnorm(N)\n\ncor(x_1, x_2)\n\n[1] -0.8031794\n\n\n\nget_alpha <- function(i)\n{\n  #=== base case ===#\n  alpha_hat_0 <- rnorm(1)\n\n  #=== first case (no correlation) ===#\n  x_1 <- rnorm(1)\n  x_2 <- rnorm(1)\n\n  alpha_hat_1 <- (x_1 + x_2) / 2\n\n  #=== second case (positively correlated) ===#\n  x_1 <- rnorm(1)\n  x_2 <- 0.5 * x_1 + sqrt(1-(0.5)^2) * rnorm(1)\n\n  alpha_hat_2 <- (x_1 + x_2) / 2\n\n  #=== third case (negatively correlated) ===#\n  x_1 <- rnorm(1)\n  x_2 <- - 0.8 * x_1 - sqrt(1-(0.8)^2) * rnorm(1)\n\n  alpha_hat_3 <- (x_1 + x_2) / 2\n\n  return_data <-\n    data.table(\n      alpha_hat_0 = alpha_hat_0,\n      alpha_hat_1 = alpha_hat_1,\n      alpha_hat_2 = alpha_hat_2,\n      alpha_hat_3 = alpha_hat_3\n    )\n\n  return(return_data)\n\n} \n\n\nset.seed(234934)\n\nsim_results <-\n  lapply(\n    1:1000,\n    get_alpha\n  ) %>% \n  rbindlist() %>% \n  melt()\n\nWarning in melt.data.table(.): id.vars and measure.vars are internally\nguessed when both are 'NULL'. All non-numeric/integer/logical type columns are\nconsidered id.vars, which in this case are columns []. Consider providing at\nleast one of 'id' or 'measure' vars in future.\n\n\nAs you can see below, they are all pretty much unbiased. However, all the cases that averaged two values (cases 1, 2, and 3) outperformed the base case that relied on a single value each iteration. You can see that when the random variables are negatively correlated, the power of averaging is greater compared to when they are independent or positively correlated. The independent case (case 1) is better than the positive correlation case (case 2).\n\n#=== expected value ===#\nsim_results[, mean(value), by = variable]\n\n      variable          V1\n1: alpha_hat_0 0.037836191\n2: alpha_hat_1 0.008031971\n3: alpha_hat_2 0.008486035\n4: alpha_hat_3 0.004259832\n\n#=== standard error ===#\nsim_results[, sd(value), by = variable]\n\n      variable        V1\n1: alpha_hat_0 0.9840232\n2: alpha_hat_1 0.6969730\n3: alpha_hat_2 0.8660699\n4: alpha_hat_3 0.3103408\n\n\nBagging takes advantage of the power of averaging. Specifically, bagging takes the following steps:\n\nGenerate many bootstrapped datasets (say \\(B\\) datasets)\nTrain a model on each of the bootstrapped datasets (\\(\\hat{f}^1, \\dots, \\hat{f}^B\\))\nAverage the estimates from all the trained models to come up with an estimate\n\n\\[\n\\hat{f}(X) = \\frac{\\hat{f}^1(X) + \\dots + \\hat{f}^B(X)}{B}\n\\]\nLet’s implement this for \\(B = 10\\) using mlb1_dt. First, define a function (named train_a_tree()) that bootstrap data, fit a regression tree, and then return the fitted values.\n\ntrain_a_tree <- function(i, data)\n{\n  #=== number of observations ===#\n  N <- nrow(data)\n\n  #=== bootstrapped data ===#\n  boot_data <- data[sample(1:N, N, replace = TRUE), ]\n\n  #=== train a regression tree ===#\n  rpart <-\n    rpart(\n      lsalary ~ hruns + years + rbisyr + allstar + runsyr + hits + bavg, \n      data = boot_data\n    )\n\n  #=== predict ===#\n  return_data <-\n    copy(data) %>% \n    .[, y_hat := predict(rpart, newdata = data)] %>% \n    .[, .(id, y_hat)]\n\n  return(return_data)\n}\n\nWe now repeat train_a_tree() 10 times.\n\n#=== create observation id for later group-by averaging ===#\nmlb1_dt[, id := 1:.N]\n\n(\ny_estimates <-\n  lapply(\n    1:10,\n    function(x) train_a_tree(x, mlb1_dt) \n  ) %>% \n  rbindlist()\n)\n\n       id    y_hat\n   1:   1 15.15792\n   2:   2 14.23816\n   3:   3 15.15792\n   4:   4 14.23816\n   5:   5 14.23816\n  ---             \n3296: 326 13.17676\n3297: 327 12.64863\n3298: 328 12.64863\n3299: 329 13.61157\n3300: 330 12.02302\n\n\nBy averaging \\(y\\) estimates by id, we can get bagging estimates.\n\ny_estimates[, mean(y_hat), by = id]\n\n      id       V1\n  1:   1 15.09137\n  2:   2 14.66143\n  3:   3 14.61425\n  4:   4 14.31035\n  5:   5 13.74852\n ---             \n326: 326 13.04455\n327: 327 12.79717\n328: 328 12.79717\n329: 329 13.67179\n330: 330 12.02051\n\n\nNow, let’s take a look at the individual estimates of \\(y\\) for the first observation.\n\ny_estimates[id == 1, ]\n\n    id    y_hat\n 1:  1 15.15792\n 2:  1 15.04446\n 3:  1 15.21238\n 4:  1 15.07311\n 5:  1 14.92840\n 6:  1 14.98571\n 7:  1 15.12060\n 8:  1 15.14064\n 9:  1 15.05676\n10:  1 15.19375\n\n\nHmm, the estimates look very similar. Actually, that is the case for all the observations. This is because the trained trees are very similar for many reasons, and the trees are highly “positively” correlated with each other. From our very simple experiment above, we know that the power of bagging is not very high when that is the case. While RF does use bagging, popular R and python packages does it in a much better way than I demonstrated here. We see this next.\n\n7.2.2 Random Forest (RF)\nUnlike a naive bagging approach demonstrated above, RF does it in a clever way to decorrelate trees. Specifically, for any leave of any tree, they consider only a randomly select subset of the explanatory variables when deciding how to split a leave. A typical choice of the number of variables considered at each split is \\(\\sqrt{K}\\), where \\(K\\) is the number of the explanatory variables specified by the user. In the naive example above, all \\(K\\) variables are considered for all the split decisions of all the trees. Some variables are more influential than others and they get to be picked as the splitting variable at similar places, which can result in highly correlated trees. Instead, RF gives other variables a chance, which helps decorrelate the trees.\nWe can use ranger() from the ranger package to train an RF model.\n\n\nAnother compelling R package for RF is the randomForest package.\nThe ranger() function has many options you can specify that determine how trees are built. Here are some of the important ones (see here for the complete description of the hyper-parameters.):\n\n\nmtry: the number of variables considered in each split (default is the square root of the total numbers of explanatory variables rounded down.)\n\nnum.trees: the number of tree to be built (default is 500)\n\nmin.node.size: minimum number of observations in each node (default varies based on the the type of analysis)\n\nreplace: where sample with or without replacement when bootstrapping samples (default is TRUE)\n\nsample.fraction: the fraction of the entire observations that are used in each tree (default is 1 if sampling with replacement, 0.632 if sampling without replacement)\n\nLet’s try fitting an RF with ranger() with the default parameters.\n\n#=== load the package ===#\nlibrary(ranger)\n\n\nAttaching package: 'ranger'\n\n\nThe following object is masked from 'package:rattle':\n\n    importance\n\n#=== fit and RF ===#\n(\nrf_fit <- \n  ranger(\n    lsalary ~ hruns + years + rbisyr + allstar + runsyr + hits + bavg, \n    data = mlb1_dt\n  )\n)\n\nRanger result\n\nCall:\n ranger(lsalary ~ hruns + years + rbisyr + allstar + runsyr +      hits + bavg, data = mlb1_dt) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      330 \nNumber of independent variables:  7 \nMtry:                             2 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       0.3668547 \nR squared (OOB):                  0.7288123 \n\n\n\n\nSince we have many trees, it is no longer possible to have a nice graphical representation of the trained RF model like we did with a regression tree.\nIn the output, you can see OOB prediction error (MSE). OOB stands for  out-of-bag. When bootstrapping (whether you do it with replacement or not), some of the train data will not be used to build a tree.\n\nn_obs <- nrow(mlb1_dt)\n\n#=== bootstrapped data ===#\nboot_data <- mlb1_dt[sample(1:n_obs, n_obs, replace = TRUE), ]\n\n#=== which rows (observations) from the original datasets are missing? ===#\nmlb1_dt[, id %in% unique(boot_data$id)] %>% mean()\n\n[1] 0.6212121\n\n\nSo, only \\(65\\%\\) of the rows from the original data (mlb1_dt) in this bootstrapped sample (many duplicates of the original observations). The observations that are NOT included in the bootstrapped sample is called out-of-bag observations. This provides a great opportunity to estimate test MSE while training an RF model! For a given regression tree, you can apply it to the out-of-bag samples to calculate MSE. You can repeat this for all the trees and average the MSEs, effectively conducting cross-validation. When the number of trees is large enough, OOB MSE is almost equivalent to MSE from LOOCV (James et al., n.d.). This means that we can tune hyper-parameters by comparing OOB MSEs of different configurations of them.\nYou can use a simple grid-search to find the best hyper-parameters. Grid-search is simply a brute-force optimization methods that goes through all the combinations of hyper-parameters and see which combination comes at the top. The computational intensity of grid-search depends on how many hyper-parameters you want to vary and how many values you would like to look at for each of the hyper-parameters. Here, let’s tune mtry, min.node.size, and sample.fraction.\n\n#=== define set of values you want to look at ===#\nmtry_seq <- c(2, 4, 7)\nmin_node_size_seq <- c(2, 5, 10)\nsample_fraction_seq <- c(0.5, 0.75, 1)\n\n#=== create a complete combinations of the three parameters ===#\n(\nparameters <-\n  data.table::CJ(\n    mtry = mtry_seq,\n    min_node_size = min_node_size_seq,\n    sample_fraction = sample_fraction_seq\n  )\n)\n\n    mtry min_node_size sample_fraction\n 1:    2             2            0.50\n 2:    2             2            0.75\n 3:    2             2            1.00\n 4:    2             5            0.50\n 5:    2             5            0.75\n 6:    2             5            1.00\n 7:    2            10            0.50\n 8:    2            10            0.75\n 9:    2            10            1.00\n10:    4             2            0.50\n11:    4             2            0.75\n12:    4             2            1.00\n13:    4             5            0.50\n14:    4             5            0.75\n15:    4             5            1.00\n16:    4            10            0.50\n17:    4            10            0.75\n18:    4            10            1.00\n19:    7             2            0.50\n20:    7             2            0.75\n21:    7             2            1.00\n22:    7             5            0.50\n23:    7             5            0.75\n24:    7             5            1.00\n25:    7            10            0.50\n26:    7            10            0.75\n27:    7            10            1.00\n    mtry min_node_size sample_fraction\n\n\nIn total, we have 27 (\\(3 \\times 3 \\times 3\\)) cases. You can see how quickly the number of cases increases as you increase the number of parameters to tune and the values of each parameter. We can now loop over the rows of this parameter data (parameters) and get OOB MSE for each of them.\n\noob_mse_all <-\n  lapply(\n    seq_len(nrow(parameters)),\n    function(x) {\n\n      #=== Fit the mode ===#\n      rf_fit <- \n        ranger(\n          lsalary ~ hruns + years + rbisyr + allstar + runsyr + hits + bavg, \n          data = mlb1_dt,\n          num.trees = 1000,\n          mtry = parameters[x, mtry],\n          min.node.size = parameters[x, min_node_size],\n          sample.fraction = parameters[x, sample_fraction]\n        )\n\n      #=== return OOB SME ===#\n      return(rf_fit$prediction.error)\n      \n    }\n  ) %>% \n  unlist()\n\n#=== assign OOB MSE to the parameters data ===#\nparameters[, oob_mse := oob_mse_all]\n\n#=== take a look ===#\nparameters\n\n    mtry min_node_size sample_fraction   oob_mse\n 1:    2             2            0.50 0.3634654\n 2:    2             2            0.75 0.3632539\n 3:    2             2            1.00 0.3703735\n 4:    2             5            0.50 0.3609772\n 5:    2             5            0.75 0.3640391\n 6:    2             5            1.00 0.3657121\n 7:    2            10            0.50 0.3632176\n 8:    2            10            0.75 0.3590104\n 9:    2            10            1.00 0.3647577\n10:    4             2            0.50 0.3639452\n11:    4             2            0.75 0.3678789\n12:    4             2            1.00 0.3762502\n13:    4             5            0.50 0.3625460\n14:    4             5            0.75 0.3682806\n15:    4             5            1.00 0.3696961\n16:    4            10            0.50 0.3632466\n17:    4            10            0.75 0.3641201\n18:    4            10            1.00 0.3693227\n19:    7             2            0.50 0.3697563\n20:    7             2            0.75 0.3773171\n21:    7             2            1.00 0.3807321\n22:    7             5            0.50 0.3749875\n23:    7             5            0.75 0.3731546\n24:    7             5            1.00 0.3857696\n25:    7            10            0.50 0.3688267\n26:    7            10            0.75 0.3758684\n27:    7            10            1.00 0.3764331\n    mtry min_node_size sample_fraction   oob_mse\n\n\nSo, the best choice among the ones tried is:\n\nparameters[which.min(oob_mse), ]\n\n   mtry min_node_size sample_fraction   oob_mse\n1:    2            10            0.75 0.3590104"
  },
  {
    "objectID": "P01-random-forest.html#over-fitting",
    "href": "P01-random-forest.html#over-fitting",
    "title": "\n7  Random Forest\n",
    "section": "\n7.3 Over-fitting",
    "text": "7.3 Over-fitting"
  },
  {
    "objectID": "P01-random-forest.html#resources",
    "href": "P01-random-forest.html#resources",
    "title": "\n7  Random Forest\n",
    "section": "\n7.4 Resources",
    "text": "7.4 Resources\n\n\nGradient Boosting with Scikit-Learn, XGBoost, LightGBM, and CatBoost by Jason Brownlee\nA Gentle Introduction to XGBoost for Applied Machine Learning\n\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. n.d. An Introduction to Statistical Learning. Vol. 112. Springer."
  },
  {
    "objectID": "P02-boosted-regression-forest.html",
    "href": "P02-boosted-regression-forest.html",
    "title": "\n8  Boosted Regression Forest\n",
    "section": "",
    "text": "library(tidyverse)\nlibrary(data.table)\nlibrary(rpart)\nlibrary(rattle)\nlibrary(wooldridge)\n\n\n#=== get data ===#\ndata(mlb1)\n\nmlb1_dt <- \n  mlb1 %>% \n  data.table() %>% # turn into data.table \n  .[, salary := NULL] %>% # remove salary (use lsalary instead)\n  na.omit() # remove observations with NA in any of the variables"
  },
  {
    "objectID": "P02-boosted-regression-forest.html#gradient-boosting",
    "href": "P02-boosted-regression-forest.html#gradient-boosting",
    "title": "\n8  Boosted Regression Forest\n",
    "section": "\n8.1 Gradient Boosting",
    "text": "8.1 Gradient Boosting\nIn training RF that uses the idea of bagging, the original data is used to generate many bootstrapped datasets, a regression tree is trained on each of them  independently , and then they are averaged to reach the final model. Boosting is similar to bagging (bootstrap aggregation) in that it trains many statistical models and then combine them to reach the final model. However, instead of building many trees independently, it builds trees  sequentially  in a manner that improves prediction step by step.\nWhile there are many variants of boosting methods (see Chapter 10 of Hastie et al. (2009)), we will look at gradient boosting using trees for regression in particular (Algorithm 10.3 in Hastie et al. (2009) presents the generic gradient tree boosting algorithm), where squared error is used as the loss function.\n\nSet \\(f_0(X_i) = \\frac{\\sum_{i=1}^N y_i}{N}\\) for all \\(i = 1, \\dots, N\\)\n\nFor b = 1 to B,\n\n\nFor \\(i = 1, \\dots, N\\), calculate \\[\n    r_{i,b} =  (y_i - f_{b-1}(X_i))\n    \\]\n\nFit a regression tree to \\(r_{i, b}\\), which generates terminal regions \\(R_{j,b}\\), \\(j = 1, \\dots, J\\), and denote the predicted value of region \\(R_{j,b}\\) as \\(\\gamma_{j,b}\\).\nSet \\(f_b(X_i) = f_{b-1}(X_i) + \\lambda \\cdot \\sum_{j=1}^J\\gamma_{j, b}\\cdot I(X_i \\in R_{j,b})\\)\n\n\n\nFinally, \\(\\hat{f}(X_i) = f_B(X_i)\\)\n\n\nLet’s try to go through this algorithm a bit to have it sink in for you.\nStep 1\nStep 1 finds the mean of the dependent variable. This quantity is used as the starting estimate for the dependent variable.\n\n(\nf_0 <- mean(mlb1_dt$lsalary)\n)\n\n[1] 13.51172\n\n\nStep 2: \\(b = 1\\)\nNow, we get residuals:\n\nmlb1_dt[, resid_1 := lsalary - f_0]\n\nThe residuals contain information in lsalary that was left unexplained. By training a regression tree using the residuals as the dependent variable, we are finding a tree that can explain the unexplained parts of lsalary using the explanatory variables.\n\ntree_fit_b1 <- \n  rpart(\n    resid_1 ~ ., # . means all variables\n    data = mlb1_dt \n  )\n\nHere is the fitted value of the residuals (\\(\\sum_{j=1}^J\\gamma_{j, b}\\cdot I(X_i \\in R_{j,b})\\))\n\nresid_1_hat <- predict(tree_fit_b1, newdata = mlb1_dt)\nhead(resid_1_hat)\n\n         1          2          3          4          5          6 \n 1.7134881  1.7134881  1.2414996  1.2414996  0.5054178 -0.1851016 \n\n\nNow, we update our prediction according to \\(f_b(X_i) = f_{b-1}(X_i) + \\lambda \\cdot \\sum_{j=1}^J\\gamma_{j, b}\\cdot I(X_i \\in R_{j,b})\\). We set \\(\\lambda\\) to be \\(0.2\\) in this illustration.\n\nlambda <- 0.2\nf_1 <- f_0 + lambda * resid_1_hat\nhead(f_1)\n\n       1        2        3        4        5        6 \n13.85441 13.85441 13.76002 13.76002 13.61280 13.47470 \n\n\nDid we actually improve prediction accuracy? Let’s compare f_0 and f_1.\n\nsum((mlb1_dt$lsalary - f_0)^2)\n\n[1] 445.0615\n\nsum((mlb1_dt$lsalary - f_1)^2)\n\n[1] 288.3205\n\n\nGreat. Let’s move on to \\(b = 2\\).\n\n#=== get negative of the residuals ===#\nmlb1_dt[, resid_2 := lsalary - f_1]\n\n#=== fit a regression tree ===#\ntree_fit_b2 <- \n  rpart(\n    resid_2 ~ ., # . means all variables\n    data = mlb1_dt \n  )\n\n#=== get predicted values ===#\nresid_2_hat <- predict(tree_fit_b2, newdata = mlb1_dt)\n\n#=== update ===#\nf_2 <- f_1 + lambda * resid_2_hat\n\n\nsum((mlb1_dt$lsalary - f_1)^2)\n\n[1] 288.3205\n\nsum((mlb1_dt$lsalary - f_2)^2)\n\n[1] 186.9229\n\n\nWe further improved our predictions. We repeat this process until certain user-specified stopping criteria is met.\nAs you probably have noticed, there are several key parameters in the process above that controls the performance of gradient boosting forest. \\(\\lambda\\) controls the speed of learning. The lower \\(\\lambda\\) is, slower the learning speed is. \\(B\\) (the number of trees) determines how many times we want to make small improvements to the original prediction. When you increase the value of \\(\\lambda\\), you should decrease the value of \\(B\\). Too high values of \\(\\lambda\\) and \\(B\\) can lead to over-fitting.\nYou may have been wondering why this algorithm is called Gradient boosting. Gradient boosting is a much more general than the one described here particularly for gradient tree boosting for regression. It can be applied to both regression and classification1. In general, Step 2.a can be written as follows:\n\\[\nr_{i,b} = - \\huge[\\normalsize\\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)}\\huge]\\normalsize_{f = f_{b-1}}\n\\]\nwhere \\(L(y_i, f(x_i))\\) is the loss function. For regression, the loss function is almost always squared error: \\((y_i - f(x_i))^2\\). For, \\(L(y_i, f(x_i)) = (y_i - f(x_i))^2\\), the negative of the derivative of the loss function with respect to \\(f(x_i)\\) is\n\\[\n- \\huge[\\normalsize\\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)}\\huge]\\normalsize_{f = f_{b-1}} = - (- 2 (y_i - f(x_i))) = 2 (y_i - f(x_i))\n\\]\nThis is why we have \\(r_{i,b} = (y_i - f_{b-1}(X_i))\\) at Step 2.a. And, as you just saw, we are using the gradient of the loss function for model updating, which is why it is called  gradient  boosting. Note that it does not really matter whether you have \\(2\\) in front of the residuals or not the fitted residuals is multiplied (scaled) by \\(\\lambda\\) to when updating the model. You can always find the same \\(\\lambda\\) that would result in the same results as when just non-scaled residuals are used.\nMost R and python packages allow you to use a fraction of the train sample that are randomly selected and/or to use a subset of the included variables in building a tree within Step 2. This generate randomness in the algorithm and they are referred to as  stochastic gradient boosting."
  },
  {
    "objectID": "P02-boosted-regression-forest.html#implementation",
    "href": "P02-boosted-regression-forest.html#implementation",
    "title": "\n8  Boosted Regression Forest\n",
    "section": "\n8.2 Implementation",
    "text": "8.2 Implementation\nWe can use the gbm package to train a gradient boosting regression. Just like ranger(), gbm takes formula and data like below.\n\nlibrary(gbm)\n\nLoaded gbm 2.1.8\n\n#=== fit a gbm model ===#\ngbm_fit <- \n  gbm(\n    lsalary ~ hruns + years + rbisyr + allstar + runsyr + hits + bavg, \n    data = mlb1_dt \n  )\n\nDistribution not specified, assuming gaussian ...\n\n\nHere is the list of some parameters to be aware of:\n\n\nn.trees: Number of trees (\\(B\\)). Default is \\(100\\).\n\ninteraction.depth: 1 implies an additive model without interactions between included variables2, 2 implies a model with 2-way interactions. Default is 1.\n\nn.minobsinnode: Minimum number of observations in a terminal node (leaf).\n\nshrinkage: Learning rate (\\(\\lambda\\)). Default is 0.1.\n\nbag.fraction: The fraction of the train data observations that are select randomly in building a tree. Default is 0.5.\n\ncv.folds: The number of folds in conducting KCV\n\nBy specifying cv.folds, gbm() automatically conducts cross-validation for you.\n\n#=== gbm fit with CV ===#\ngbm_fit <- \n  gbm(\n    lsalary ~ hruns + years + rbisyr + allstar + runsyr + hits + bavg, # . means all variables\n    data = mlb1_dt,\n    cv.folds = 5,\n  )\n\nDistribution not specified, assuming gaussian ...\n\n#=== see the MSE history ===#  \ngbm_fit$cv.error\n\n  [1] 1.2229690 1.1075832 1.0157435 0.9417138 0.8668569 0.8107364 0.7674985\n  [8] 0.7151831 0.6745636 0.6450490 0.6157606 0.5943775 0.5728772 0.5541941\n [15] 0.5367066 0.5229922 0.5095516 0.4941631 0.4824212 0.4687584 0.4572636\n [22] 0.4479893 0.4428348 0.4380302 0.4307389 0.4254884 0.4208747 0.4165191\n [29] 0.4116251 0.4056642 0.4013533 0.3991657 0.3970355 0.3964611 0.3937440\n [36] 0.3915728 0.3921816 0.3900771 0.3911436 0.3906951 0.3914275 0.3901961\n [43] 0.3896571 0.3879309 0.3861445 0.3847895 0.3831043 0.3826466 0.3801617\n [50] 0.3802345 0.3801612 0.3798797 0.3783574 0.3772985 0.3762190 0.3756384\n [57] 0.3746196 0.3742810 0.3730617 0.3748269 0.3756662 0.3762339 0.3757384\n [64] 0.3748527 0.3745331 0.3745911 0.3751994 0.3747395 0.3737588 0.3739640\n [71] 0.3735709 0.3730999 0.3720065 0.3718486 0.3716349 0.3718658 0.3720764\n [78] 0.3714145 0.3733023 0.3727223 0.3724057 0.3731439 0.3721333 0.3716534\n [85] 0.3715348 0.3714628 0.3719048 0.3722881 0.3710087 0.3709487 0.3703085\n [92] 0.3698065 0.3694505 0.3706741 0.3703749 0.3697160 0.3700435 0.3693750\n [99] 0.3693814 0.3694650\n\n\nYou can visualize the CV results using gbm.perf().\n\ngbm.perf(gbm_fit)\n\n\n\n\n[1] 98\n\n\nNote that it will tell you what the optimal number of trees is  given  the values of the other hyper-parameters (here default values). If you want to tune other parameters as well, you need to program it yourself.\n\n\n\n\nHastie, Trevor, Robert Tibshirani, Jerome H Friedman, and Jerome H Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Vol. 2. Springer."
  },
  {
    "objectID": "P03-xgb.html",
    "href": "P03-xgb.html",
    "title": "\n9  Extreme Gradient Boosting\n",
    "section": "",
    "text": "Extreme gradient boosting (XGB) is a variant of gradient boosting that has been extremely popular due to its superb performance. The basic concept is the same as the gradient boosting algorithm described above, however, it has its own way of building a tree, which is more mindful of avoiding over-fitting trees."
  },
  {
    "objectID": "P03-xgb.html#preparation",
    "href": "P03-xgb.html#preparation",
    "title": "\n9  Extreme Gradient Boosting\n",
    "section": "Preparation",
    "text": "Preparation\n\nlibrary(tidyverse)\nlibrary(data.table)"
  },
  {
    "objectID": "P03-xgb.html#tree-updating-in-xgb-general",
    "href": "P03-xgb.html#tree-updating-in-xgb-general",
    "title": "\n9  Extreme Gradient Boosting\n",
    "section": "\n9.1 Tree updating in XGB (general)",
    "text": "9.1 Tree updating in XGB (general)\nLet \\(f_{i,b}(x_i)\\) be the prediction for the \\(i\\)th observation at the \\(b\\)-th iteration. Further, let \\(w_t(x_i)\\) is the term that is added to \\(f_{i,b}(x_i)\\) to obtain \\(f_{i,b+1}(x_i)\\). In XGB, \\(w_t(x_i)\\) is such that it minimizes the following objective:\n\\[\n\\Psi_t = \\sum_{i=1}^N [L(y_i, f_{i,b}(x_i) + w_t(x_i))] + \\Omega(w_t)\n\\tag{9.1}\\]\nwhere \\(L()\\) is the user-specified loss-function that is differentiable and \\(\\Omega(w_t)\\) is the regularization term. Instead of Equation 9.1, XGB uses the second order Taylor expansion of \\(L()\\) about \\(w\\)1.\n\\[\n\\tilde{\\Psi}_t = \\sum_{i=1}^N [L(y_i, f_{i,b}(x_i)) + g_i w_t(x_i) + \\frac{1}{2}h_i w_t(x_i)^2] + \\Omega(w_t)\n\\tag{9.2}\\]\nwhere \\(g_i = \\frac{\\partial L(y_i, p_i)}{\\partial p_i}\\) (first-order derivative) and \\(h_i = \\frac{\\partial^2 L(y_i, p_i)}{\\partial p_i^2}\\) (second-order derivative). Since \\(L(y_i, f_{i,b}(x_i))\\) is just a constant, we can safely remove it from the objective function, which leads to\n\\[\n\\tilde{\\Psi}_t = \\sum_{i=1}^N [g_i w_t(x_i) + \\frac{1}{2}h_i w_t(x_i)^2] + \\Omega(w_t)\n\\tag{9.3}\\]\nLet \\(I_j\\) denote a set of observations that belong to leaf \\(j\\) (\\(j = 1, \\dots, J\\)). Then, Equation 9.3 is written as follows:\n\\[\n\\tilde{\\Psi}_t = \\sum_{j=1}^J\\huge[\\normalsize (\\sum_{i\\in I_j}g_i)w_j + \\frac{1}{2}(\\sum_{i\\in I_j}h_i + \\lambda)w_j^2 \\huge]\\normalsize + \\gamma J\n\\tag{9.4}\\]\n\n\nRemember that all the observations in the same leaf shares the same prediction. So, for all \\(i\\)s that belong to leaf \\(j\\), the prediction is denoted as \\(w_j\\) in Equation 9.4. That is, \\(w_t(x_i)\\) that belongs to leaf \\(j\\) is \\(w_j\\).\nFor a given tree structure (denoted as \\(q(x)\\)), the leaves can be treated independently in minimizing this objective.\nTaking the derivative of \\(\\tilde{\\Psi}_t\\) w.r.t \\(w_j\\),\n\\[\n\\begin{aligned}\n(\\sum_{i\\in I_j}g_i) + (\\sum_{i\\in I_j}h_i + \\lambda)w_j = 0 \\\\\n\\Rightarrow w_j^* = \\frac{-\\sum_{i\\in I_j}g_i}{\\sum_{i\\in I_j}h_i + \\lambda}\n\\end{aligned}\n\\tag{9.5}\\]\nThe minimized value of \\(\\tilde{\\Psi}_t\\) is then (obtained by plugging \\(w_j^*\\) into Equation 9.4),\n\\[\n\\begin{aligned}\n\\tilde{\\Psi}_t(q)^* & = \\sum_{j=1}^J\\huge[\\normalsize (\\sum_{i\\in I_j}g_i)\\frac{-\\sum_{i\\in I_j}g_i}{\\sum_{i\\in I_j}h_i + \\lambda} + \\frac{1}{2}(\\sum_{i\\in I_j}h_i + \\lambda)(\\frac{-\\sum_{i\\in I_j}g_i}{\\sum_{i\\in I_j}h_i + \\lambda})^2 \\huge]\\normalsize + \\gamma J \\\\\n& = \\sum_{j=1}^J\\huge[\\normalsize \\frac{-(\\sum_{i\\in I_j}g_i)^2}{\\sum_{i\\in I_j}h_i + \\lambda} + \\frac{1}{2}\\frac{(\\sum_{i\\in I_j}g_i)^2}{\\sum_{i\\in I_j}h_i + \\lambda} \\huge]\\normalsize + \\gamma J \\\\\n& = -\\frac{1}{2} \\sum_{j=1}^J \\huge[\\normalsize\\frac{(\\sum_{i\\in I_j}g_i)^2}{\\sum_{i\\in I_j}h_i + \\lambda}\\huge]\\normalsize + \\gamma J\n\\end{aligned}\n\\tag{9.6}\\]\nFor notatinal convenience, we call \\(\\frac{(\\sum_{i\\in I_j}g_i)^2}{\\sum_{i\\in I_j}h_i + \\lambda}\\) quality score and denote it by \\(Q_j\\) ( Quality score for leaf \\(j\\)).\nWe could find the best tree structure by finding \\(w_j^*(q)\\) according to Equation 9.4 and calculate \\(\\tilde{\\Psi}_t(q)^*\\) according to Equation 9.6 for each of all the possible tree structures, and then pick the tree structure q(x) that has the lowest \\(\\tilde{\\Psi}_t(q)^*\\).\nHowever, it is impossible to consider all possible tree structures practically. So, a greedy (myopic) approach that starts from a single leaf and iteratively splits leaves is used instead.\nConsider splitting an existing leaf \\(s\\) (where in the tree it may be located) into two leaves \\(L\\) and \\(R\\) when there are \\(J\\) existing leaves. Then, we find \\(w_j^*\\) and calculate \\(\\tilde{\\Psi}_t(q)^*\\) for each leaf, and the resulting minimized objective is\n\\[\n-\\frac{1}{2} \\huge[\\normalsize Q_L + Q_R + \\Gamma \\huge]\\normalsize + \\gamma(J+1)\n\\]\nwhere \\(\\Gamma\\) is the sum of quality scores for all the leaves except \\(L\\) and \\(R\\).\n\n\n\\[\n\\Gamma = \\sum_{j\\ne \\{L, R\\}}^J Q_j\n\\]\nThe minimized objective before splitting is\n\\[\n-\\frac{1}{2} \\huge[\\normalsize Q_s + \\Gamma \\huge]\\normalsize + \\gamma J\n\\]\nSo, the reduction  in loss after the split is\n\\[\nG(s, L, R) = \\frac{1}{2} \\huge[\\normalsize Q_L + Q_R - Q_s \\huge]\\normalsize - \\gamma\n\\]\nLet’s call \\(G(s, L, R)\\) simply a gain (of the split).\n\n\nA more positive value of gain (\\(G(s, L, R)\\)) means a more successful split.\nWe can try many different patterns of \\(I_L\\) and \\(I_R\\) (how to split tree \\(s\\)), calculate the gain for each of them and pick the split that has the highest gain.\n\n\nDifferent patterns of \\(I_L\\) and \\(I_R\\) arise from different variable-cutpoint combinations\nIf the highest gain is negative, then the leaf under consideration for splitting is not split.\nOnce the best tree is chosen (the tree that has the highest gain among the ones investigated), then we update our prediction based on \\(w^*\\) of the tree. For observation \\(i\\) that belongs to leaf \\(j\\) of the tree,\n\\[\n\\begin{aligned}\nf_{i,b} = f_{i,b-1} + \\eta \\cdot w_j^*\n\\end{aligned}\n\\tag{9.7}\\]\nwhere \\(\\eta\\) is the learning rate."
  },
  {
    "objectID": "P03-xgb.html#tree-updating-in-xgb-regression",
    "href": "P03-xgb.html#tree-updating-in-xgb-regression",
    "title": "\n9  Extreme Gradient Boosting\n",
    "section": "\n9.2 Tree updating in XGB (regression)",
    "text": "9.2 Tree updating in XGB (regression)\nWe now make the general tree updating algorithm specific to regression problems, where the loss function is squared error: \\(L(y_i, p_i) = \\frac{1}{2}(y_i - p_i)^2\\), where \\(p_i\\) is the predicted value for \\(i\\).\n\nFirst, let’s find \\(g_i\\) and \\(h_i\\) for \\(L(y_i, p_i) = \\frac{1}{2}(y_i - p_i)^2\\).\n\\[\n\\begin{aligned}\ng_i = \\frac{\\partial L(y_i, p_i)}{\\partial p_i}  = -(y_i - p_i)\\\\\nh_i = \\frac{\\partial^2 L(y_i, p_i)}{\\partial p_i^2} = 1 \\\\\n\\end{aligned}\n\\]\nSo, \\(g_i\\) is simply the negative of the residual for \\(i\\).\nNow, suppose your are at iteration \\(b\\) and the predicted value for \\(i\\) is denoted as \\(f_{i,b}(x_i)\\). Further, let \\(r_{i,b}\\) denote the residual (\\(y_i - f_{i,b}(x_i)\\)).\nPlugging these into Equation 9.5,\n\\[\n\\begin{aligned}\nw_j^* & = \\frac{\\sum_{i\\in I_j}r_{i,b}}{\\sum_{i\\in I_j}1 + \\lambda} \\\\\n      & = \\frac{\\sum_{i\\in I_j}r_{i,b}}{N_j + \\lambda}\n\\end{aligned}\n\\tag{9.8}\\]\nThat is for a given leaf \\(j\\), the optimal predicted value (\\(w_j^*\\)) is the sum of the residuals of all the observations in leaf \\(j\\) divided by the number of observations in leaf \\(j\\) plus \\(\\lambda\\). When \\(\\lambda = 0\\), the optimal predicted value (\\(w_j^*\\)) is simply the mean of the residuals.\nThe quality score for leaf \\(j\\) is then,\n\\[\nQ_j = \\frac{(\\sum_{i\\in I_j}r_{i,b})^2}{N_j + \\lambda}\n\\tag{9.9}\\]"
  },
  {
    "objectID": "P03-xgb.html#illustration-of-xgb-for-regression",
    "href": "P03-xgb.html#illustration-of-xgb-for-regression",
    "title": "\n9  Extreme Gradient Boosting\n",
    "section": "\n9.3 Illustration of XGB for regression",
    "text": "9.3 Illustration of XGB for regression\nIn order to further our understanding of the entire XGB algorithm, let’s take a lookt at a simple regression problem as an illustration. We consider a four-observation data as follows:\n\n(\ndata <-\n  data.table(\n    y = c(-3, 7, 8, 12),\n    x = c(1, 4, 6, 8)\n  )\n)\n\n    y x\n1: -3 1\n2:  7 4\n3:  8 6\n4: 12 8\n\n\n\n(\ng_0 <-\n  ggplot(data) +\n  geom_point(aes(y = y, x = x))\n)\n\n\n\n\nFirst step (\\(b = 0\\)) is to make an initial prediction. This can be any number, but let’s use the mean of y and set it as the predicted value for all the observations.\n\n(\nf_0 <- mean(data$y) # f_0: the predicted value for all the observations\n)\n\n[1] 6\n\n\nLet’s set \\(\\gamma\\), \\(\\lambda\\), and \\(\\eta\\) to \\(10\\), \\(1\\), and \\(0.3\\), respectively.\n\ngamma <- 10\nlambda <- 1\neta <- 0.3\n\nWe have a single-leaf tree at the moment. And the quality score for this leaf is\n\n\nquality score for leaf \\(j\\) is \\(\\frac{(\\sum_{i\\in I_j}r_{i,b})^2}{N_j + \\lambda}\\)\n\n#=== get residuals ===#\ndata[, resid := y - f_0]\n\n#=== get quality score ===#\n(\nq_0 <- (sum(data$resid))^2/(nrow(data) + 1)\n)\n\n[1] 0\n\n\nQuality score of the leaf is 0.\n\n\nSince we are using the mean of \\(y\\) as the prediction, of course, the sum of the residuals is zero, which then means that the quality score is zero.\nNow, we have three potential to split patterns: {x, 2}, {x, 5}, {x, 7}.\n\n\n{x, 2} means the leaf is split into two leaves: \\({x | x <2}\\) and \\({x | x >= 2}\\). Note that any number between \\(1\\) and \\(4\\) will result in the same split results.\nLet’s consider them one by one.\n\n9.3.0.1  Split: {x, 2} \n\nHere are graphical representations of the split:\n\ng_0 +\n  geom_vline(xintercept = 2, color = \"red\") +\n  annotate(\"text\", x = 1.25, y = 6, label = \"leaf L\", color = \"red\") +\n  annotate(\"text\", x = 5, y = 6, label = \"leaf R\", color = \"red\")\n\n\n\n\n\n\n\nCodeDiagrammeR::grViz(\n\"\ndigraph {\n  graph [ranksep = 0.2]\n  node [shape = box]\n    T1R [label = 'L: -9']\n    T1L [label = 'R: 1 , 2 , 6']\n    T0 [label = '-9, 1 , 2 , 6']\n  edge [minlen = 2]\n    T0->T1L\n    T0->T1R\n  { rank = same; T1R; T1L}\n}\n\"\n)\n\n\n\n\n\nLet’s split the data.\n\n#=== leaf L ===#\n(\ndata_L_1 <- data[x < 2, ]\n)\n\n    y x resid\n1: -3 1    -9\n\n#=== leaf R ===#\n(\ndata_R_1 <- data[x >= 2, ]\n)\n\n    y x resid\n1:  7 4     1\n2:  8 6     2\n3: 12 8     6\n\n\nUsing Equation 9.8,\n\n\n\\(w_j^* = \\frac{\\sum_{i\\in I_j}r_{i,b}}{N_j + \\lambda}\\)\n\nw_L <- (sum(data_L_1$resid))/(nrow(data_L_1) + lambda)\nw_R <- (sum(data_R_1$resid))/(nrow(data_R_1) + lambda)\n\n\\[\n\\begin{aligned}\nw_L^* & = -9 / (1 + 1) = -4.5 \\\\\nw_R^* & = 1 + 2 + 6 / (3 + 1) = 2.25\n\\end{aligned}\n\\]\nUsing Equation 9.9, the quality scores for the leaves are\n\n\n\\(Q_j = \\frac{(\\sum_{i\\in I_j}r_{i,b})^2}{N_j + \\lambda}\\)\n\nq_L <- (sum(data_L_1$resid))^2/(nrow(data_L_1) + lambda)\nq_R <- (sum(data_R_1$resid))^2/(nrow(data_R_1) + lambda)\n\n\n\n\nCodeDiagrammeR::grViz(\n  paste0(\n  \"\n  digraph {\n    graph [ranksep = 0.2]\n    node [shape = box]\n      T1R [label = 'L: -9 \\n Q score = \", round(q_L, digits = 2), \"']\n      T1L [label = 'R: 1 , 2 , 6 \\n Q score = \", round(q_R, digits = 2), \"']\n      T0 [label = '-9, 1 , 2 , 6']\n    edge [minlen = 2]\n      T0->T1L\n      T0->T1R\n    { rank = same; T1R; T1L}\n  }\n  \"\n  )\n\n)\n\n\n\n\n\n\\[\n\\begin{aligned}\nq_L^* & = (-9)^2 / (1 + 1) = 40.5 \\\\\nq_R^* & = (1 + 2 + 6)^2 / (3 + 1) = 20.25\n\\end{aligned}\n\\]\nNotice that residuals are first summed and then squared in the denominator of the quality score (the higher, the better). This means that if the prediction is off in the same direction (meaning they are similar) among the observations within the leaf, then the quality score is higher. On the other hand, if the prediction is off in both directions (meaning they are not similar), then the residuals cancel each other out, resulting in a lower quality score. Since we would like to create leaves consisting of similar observations, a more successful split has a higher quality score.\nFinally, the gain of this split is\n\n\n\\[\nG(s, L, R) = \\frac{1}{2} \\huge[\\normalsize Q_L + Q_R - Q_s \\huge]\\normalsize - \\gamma\n\\]\nwhere \\(s\\) is the leaf before split, \\(L\\) and \\(R\\) are leaves after the split of leaf \\(s\\).\n\ngain_1 <- (q_L + q_R - q_0)/2 - gamma\n\n\\[\nG_1 = \\frac{40.5 + 20.25 - 0}{2} - 10 = 20.375\n\\]\nNow that we have gone through the process of finding update value (\\(w\\)), quality score (\\(q\\)), and gain (\\(G\\)) for a given split structure, let’s write a function that returns the values of these measures by feeding the cutpoint before moving onto the next split candidate.\n\nget_info <- function(data, cutpoint, lambda, gamma)\n{\n  q_0 <- (sum(data$resid))^2/(nrow(data) + lambda)\n\n  data_L <- data[x < cutpoint, ]\n  data_R <- data[x >= cutpoint, ]\n\n  w_L <- (sum(data_L$resid))/(nrow(data_L) + lambda)\n  w_R <- (sum(data_R$resid))/(nrow(data_R) + lambda)\n\n  q_L <- (sum(data_L$resid))^2/(nrow(data_L) + lambda)\n  q_R <- (sum(data_R$resid))^2/(nrow(data_R) + lambda)\n\n  gain <- (q_L + q_R - q_0)/2 - gamma\n\n  return(list(\n    w_L = w_L, \n    w_R = w_R, \n    q_L = q_L, \n    q_R = q_R, \n    gain = gain \n  ))\n}\n\n\n9.3.0.2  Split: {x, 5} \n\n\nmeasures_2 <- get_info(data, 5, lambda, gamma)\n\n\ng_0 +\n  geom_vline(xintercept = 5, color = \"red\") +\n  annotate(\"text\", x = 3, y = 6, label = \"leaf L\", color = \"red\") +\n  annotate(\"text\", x = 7, y = 6, label = \"leaf R\", color = \"red\")\n\n\n\n\n\n\n\nCodeDiagrammeR::grViz(\n  paste0(\n    \"\n    digraph {\n      graph [ranksep = 0.2]\n      node [shape = box]\n        T1R [label = 'L: -9, 1 \\n Q score = \", round(measures_2$q_L, digits = 2), \"']\n        T1L [label = 'R: 2 , 6 \\n Q score = \", round(measures_2$q_R, digits = 2), \"']\n        T0 [label = '-9, 1 , 2 , 6']\n      edge [minlen = 2]\n        T0->T1L\n        T0->T1R\n      { rank = same; T1R; T1L}\n    }\n    \"\n  )\n)\n\n\n\n\n\n\\[\n\\begin{aligned}\nq_L^* & = (-9)^2 / (2 + 1) = 21.33 \\\\\nq_R^* & = (1 + 2 + 6)^2 / (2 + 1) = 21.33\n\\end{aligned}\n\\]\n\\[\nG_2 = \\frac{21.33 + 21.33 - 0}{2} - 10 = 11.3333333\n\\]\n\n9.3.0.3  Split: {x, 7} \n\n\nmeasures_3 <- get_info(data, 7, lambda, gamma)\n\n\ng_0 +\n  geom_vline(xintercept = 7, color = \"red\") +\n  annotate(\"text\", x = 4, y = 6, label = \"leaf L\", color = \"red\") +\n  annotate(\"text\", x = 8, y = 6, label = \"leaf R\", color = \"red\")\n\n\n\n\n\n\n\nCodeDiagrammeR::grViz(\n  paste0(\n    \"\n    digraph {\n      graph [ranksep = 0.2]\n      node [shape = box]\n        T1R [label = 'L: -9, 1, 2 \\n Q score = \", round(measures_3$q_L, digits = 2), \"']\n        T1L [label = 'R: 6 \\n Q score = \", round(measures_3$q_R, digits = 2), \"']\n        T0 [label = '-9, 1 , 2 , 6']\n      edge [minlen = 2]\n        T0->T1L\n        T0->T1R\n      { rank = same; T1R; T1L}\n    }\n    \"\n  )\n)\n\n\n\n\n\n\\[\n\\begin{aligned}\nq_L^* & = (-9)^2 / (1 + 1) = 9 \\\\\nq_R^* & = (1 + 2 + 6)^2 / (3 + 1) = 18\n\\end{aligned}\n\\]\n\\[\nG_3 = \\frac{9 + 18 - 0}{2} - 10 = 3.5\n\\]\nAmong all the splits we considered, the first case (Split: {x, 2}) has the highest score. This is easy to confirm visually and shows picking a split based on the gain measure indeed makes sense.\nNow we consider how to split leaf R (leaf L cannot be split further as it has only one observation). We have two split candidates: {x, 5} and {x, 7}. Let’s get the gain measures using get_info().\n\n#=== first split ===#\nget_info(data_R_1, 5, lambda, gamma)$gain \n\n[1] -9.208333\n\n#=== second split ===#\nget_info(data_R_1, 7, lambda, gamma)$gain\n\n[1] -9.625\n\n\nSo, neither of the splits has a positive gain value. Therefore, we do not adopt either of the splits. For this iteration (\\(b=1\\)), this is the end of tree building.\n\n\n\n\n\n\nNote\n\n\n\nIf the value of \\(\\gamma\\) is lower (say, 0), then we would have adopted the second split.\n\nget_info(data_R_1, 5, lambda, 0)$gain # first split\n\n[1] 0.7916667\n\nget_info(data_R_1, 7, lambda, 0)$gain # second split\n\n[1] 0.375\n\n\nAs you can see, a higher value of \\(\\gamma\\) leads to a more aggressive tree pruning.\n\n\nSo, the final tree for this iteration (\\(b = 1\\)) is\n\nCodeDiagrammeR::grViz(\n  paste0(\n  \"\n  digraph {\n    graph [ranksep = 0.2]\n    node [shape = box, width = 0.3, height = 0.15, fontsize = 3, fixedsize = TRUE, penwidth = 0.2]\n      T1R [label = 'L: -9 \\n w* = \", round(w_L, digits = 2), \"']\n      T1L [label = 'R: 1 , 2 , 6 \\n w* = \", round(w_R, digits = 2), \"']\n      T0 [label = '-9, 1 , 2 , 6']\n    edge [penwidth = 0.2, arrowsize = 0.3, len = 0.3]\n      T0->T1L\n      T0->T1R\n    { rank = same; T1R; T1L}\n  }\n  \"\n  )\n\n)\n\n\n\n\n\nWe now use \\(w^*\\) from this tree to update our prediction according to Equation 9.7.\n\n\n\\(f_{i,b} = f_{i,b-1} + \\eta \\cdot w_j^*\\)\n\nmeasures_1 <- get_info(data, 2, lambda, gamma)\n\nSince the first observation is in \\(L\\),\n\\[\nf_{i = 1,b = 1} = 6 + 0.3 \\times -4.5 = 4.65\n\\]\nSince the second, third, and fourth observations are in \\(R\\),\n\\[\n\\begin{aligned}\nf_{i = 2,b = 1} = 6 + 0.3 \\times 2.25 = 6.68 \\\\\nf_{i = 3,b = 1} = 6 + 0.3 \\times 2.25  = 6.68\\\\\nf_{i = 4,b = 1} = 6 + 0.3 \\times 2.25 = 6.68\n\\end{aligned}\n\\]\n\ndata %>% \n  .[, f_0 := f_0] %>% \n  .[1, f_1 := f_0 + measures_1$w_L * eta] %>%\n  .[2:4, f_1 := f_0 + measures_1$w_R * eta]\n\nThe prediction updates can be seen below. Though small, we made small improvements in our prediction.\n\nCodeggplot(data = data) +\n  geom_point(aes(y = y, x = x, color = \"observed\")) +\n  geom_point(aes(y = f_1, x = x, color = \"after (f1)\")) +\n  geom_point(aes(y = f_0, x = x, color = \"before (f0)\")) +\n  scale_color_manual(\n    values = \n      c(\n        \"before (f0)\" = \"blue\", \n        \"after (f1)\" = \"red\",\n        \"observed\" = \"black\"\n      ), \n    name = \"\"\n  ) +\n  geom_segment(\n    aes(y = f_0, x = x, yend = f_1, xend = x), \n    color = \"blue\",\n    arrow = arrow(length = unit(0.1, \"cm\"))\n  ) +\n  theme_bw()\n\n\n\n\nNow, we move on to \\(b=2\\). We first update residuals:\n\ndata[, resid := y - f_1]\n\ndata\n\n    y x  resid f_0   f_1\n1: -3 1 -7.650   6 4.650\n2:  7 4  0.325   6 6.675\n3:  8 6  1.325   6 6.675\n4: 12 8  5.325   6 6.675\n\n\nJust like at \\(b=1\\), all the possible splits are {x, 2}, {x, 5}, {x, 7}. Let’s find the gain for each split.\n\nlapply(\n  c(2, 5, 7),\n  function(x) get_info(data, x, lambda, gamma)$gain\n)\n\n[[1]]\n[1] 10.66639\n\n[[2]]\n[1] 6.267458\n\n[[3]]\n[1] 1.543344\n\n\nSo, the first split is again the best split. Should we split the right leaf, which has the observations except the first one?\n\nlapply(\n  c(5, 7),\n  function(x) get_info(data[2:3, ], x, lambda, gamma)$gain\n)\n\n[[1]]\n[1] -9.988437\n\n[[2]]\n[1] -10\n\n\nAll the splits have negative gains. So, we do not split this leaf just like at \\(b=1\\).\nSo, the final tree for this iteration (\\(b = 1\\)) is\n\nmeasures_b2 <- get_info(data, 2, lambda, gamma)\n\n#| code-fold: true\n#| fig-height: 2\n#| fig-width: 4\n\nDiagrammeR::grViz(\n  paste0(\n  \"\n  digraph {\n    graph [ranksep = 0.2]\n    node [shape = box, width = 0.4, height = 0.15, fontsize = 3, fixedsize = TRUE, penwidth = 0.2]\n      T1R [label = 'L: -8.18 \\n w* = \", round(measures_b2$w_L, digits = 2), \"']\n      T1L [label = 'R: 0.71 , 1.71 , 5.71 \\n w* = \", round(measures_b2$w_R, digits = 2), \"']\n      T0 [label = '-8.18, 0.71 , 1.71 , 5.71']\n    edge [penwidth = 0.2, arrowsize = 0.3, len = 0.3]\n      T0->T1L\n      T0->T1R\n    { rank = same; T1R; T1L}\n  }\n  \"\n  )\n\n)\n\n\n\n\n\nLet’s now update our predictions.\n\ndata %>% \n  .[1, f_2 := f_1 + measures_b2$w_L * eta] %>%  \n  .[2:4, f_2 := f_1 + measures_b2$w_R * eta] \n\n\nCodeggplot(data = data) +\n  geom_point(aes(y = y, x = x, color = \"observed\")) +\n  geom_point(aes(y = f_2, x = x, color = \"f2\")) +\n  geom_point(aes(y = f_1, x = x, color = \"f1\")) +\n  geom_point(aes(y = f_0, x = x, color = \"f0\")) +\n  scale_color_manual(\n    values = \n      c(\n        \"f0\" = \"blue\", \n        \"f1\" = \"red\",\n        \"f2\" = \"red\",\n        \"observed\" = \"black\"\n      ), \n    name = \"\"\n  ) +\n  geom_segment(\n    aes(y = f_0, x = x, yend = f_1, xend = x), \n    color = \"blue\",\n    arrow = arrow(length = unit(0.1, \"cm\"))\n  ) +\n  geom_segment(\n    aes(y = f_1, x = x, yend = f_2, xend = x), \n    color = \"blue\",\n    arrow = arrow(length = unit(0.1, \"cm\"))\n  ) +\n  theme_bw()\n\n\n\n\nAgain, we made small improvements in our predictions. This process continues until user-specified stopping criteria is met.\n\n\n\n\n\n\nTip\n\n\n\n\n\n\\(\\lambda\\):\n\nA higher value of \\(\\lambda\\) leads to a lower value of prediction updates (\\(w^*\\)).\nA higher value of \\(\\lambda\\) leads to a lower value of quality score (\\(Q\\)), thus leading to a lower value of gain (\\(G\\)), which then leads to more aggressive pruning for a given value of \\(\\gamma\\).\n\n\n\n\\(\\gamma\\):\n\nA higher value of \\(\\gamma\\) leads to more aggressive pruning.\n\n\n\n\\(\\eta\\):\n\nA higher value of \\(\\eta\\) leads to faster learning."
  },
  {
    "objectID": "P03-xgb.html#implementation",
    "href": "P03-xgb.html#implementation",
    "title": "\n9  Extreme Gradient Boosting\n",
    "section": "\n9.4 Implementation",
    "text": "9.4 Implementation\n\n\nR\nPython\n\n\n\nYou can use the xgboost package to implement XGB modeling.\n\nlibrary(xgboost)\n\n\nAttaching package: 'xgboost'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\n\nThe first task is to create a class of matrix called xgb.DMatrix using the xgb.DMatrix() function. You provide the explanatory variable data matrix to the data option and the dependent variable matrix (vector) to the label option in xgb.DMatrix() like below.\nLet’s get the mlb1 data from the wooldridge package for demonstration.\n\nlibrary(wooldridge)\ndata(mlb1)\n\nmlb1_dt <- \n  mlb1 %>% \n  data.table() %>% # turn into data.table \n  .[, salary := NULL] %>% # remove salary (use lsalary instead)\n  na.omit() # remove observations with NA in any of the variables\n\n\nmlb1_dm_X <- \n  xgb.DMatrix(\n    data = as.matrix(mlb1_dt[, .(hruns, years, rbisyr, allstar, runsyr, hits, bavg)]),\n    label = as.matrix(mlb1_dt[, lsalary])\n  )\n\nWe can then use xgb.train() to train a model using the XGB algorithm.\n\nxgb_fit <-\n  xgb.train(\n    data = mlb1_dm_X, # independent variable\n    nrounds = 100, # number of iterations (trees to add)\n    eta = 1, # learning rate\n    objective = \"reg:squarederror\" # objective function\n  )"
  },
  {
    "objectID": "causal-ml.html",
    "href": "causal-ml.html",
    "title": "Causal Machine Learning (CML) Methods",
    "section": "",
    "text": "\\[\nTE(X) = \\theta(X)\\cdot T\n\\]\n\\(\\theta(X)\\) is the impact of the treatment when \\(T\\) is binary and marginal impact of the treatment when \\(T\\) is continuous. \\(\\theta(X)\\) is a function of attributes (\\(X\\)), meaning that the impact of the treatment varies (heterogeneous) based on the value of the attributes.\n\n\n\\(T\\) is 0 if not treated, 1 if treated.\nCML considers the following model (following the documentation of the econml Python package)\n\\[\n\\begin{aligned}\nY & = \\theta(X)\\cdot T + g(X, W) + \\varepsilon \\\\\nT & = f(X, W) + \\eta\n\\end{aligned}\n\\]\n\\(W\\) are the collection of attributes that affect \\(Y\\) along with \\(X\\) (represented by \\(g(X, W)\\)), but not as drivers of the heterogeneity in the impact of the treatment. \\(X\\) not just affects \\(Y\\) as drivers of the heterogeneity in the impact of the treatment (\\(\\theta(X)\\cdot T\\)), but also directly along with \\(W\\).\nBoth \\(X\\) and \\(W\\) are potential confounders. While we do control for them (eliminating their influence) by partialing out \\(f(X, W)\\) and \\(g(X, W)\\), the sole focus is on the estimation of \\(\\theta(X)\\). This is in stark contrast to the focus of the ML methods we have seen in earlier sections, which primarily focuses on the accurate prediction of the  level of the dependent variable, rather than how the level of the dependent variable  changes  when treated like CML methods.\nUnderstanding the how treatment effects vary can be highly valuable in many circumstances.\n Example 1:  If we come to know a particular drug is effective on elderly people but detrimental to kids, then doctors can make a smart decision of prescribing the drug to elderly people, but not to kids.\n\n\nIn this example, the heterogeneity driver (\\(X\\)) is age.\n Example 3:  If we come to know fertilizer is more effective in increasing corn yield in soil type A than B, then farmers can apply more fertilized on the parts of the field where soil type is A but less on where soil type is B.\n\n\nIn this example, the heterogeneity driver (\\(X\\)) is soil type.\nAs you can see these examples, knowledge on the heterogeneity of the treatment effects and its causes can help decision makers smart-target the treatment."
  },
  {
    "objectID": "A01-mc-simulation.html",
    "href": "A01-mc-simulation.html",
    "title": "Appendix A — Monte Carlo (MC) Simulation",
    "section": "",
    "text": "Monte Carlo (MC) simulation is an important tool to test econometric hypothesis  numerically and it is highly desirable that you can conduct your own MC simulations that fit your need. Suppose you are interested in learning whether the OSL estimator of a simple linear-in-parameter model is unbiased when the error term is correlated with one of the explanatory variables. Well, it has been theoretically proven that the OLS estimator is biased under such a data generating process. So, we do not really have to show this numerically. But, what if you are facing with a more complex econometric task for which the answer is not clear for you? For example, what is the impact of over-fitting the first stage estimations in a double machine learning approach to the bias and efficiency of the estimation of treatment effect in the second stage? We can partially answer to this question (though not generalizable unlike theoretical expositions) using MC simulations. Indeed, this book uses MC simulations often to get insights into econometric problems for which the answers are not clear or to just confirm if econometric theories are correct.\nIt is important to first recognize that it is  impossible  to test econometric theory using real-wold data. That is simply because you never know the underlying data generating process of real-world data. In MC simulations, we generate data according to the data generating process we specify. This allows us to check if the econometric outcome is consistent with the data generating process or not. This is the reason every journal article with newly developed statistical procedures published in an econometric journal has MC simulations to check the new econometric theories are indeed correct (e.g., whether the new estimator is unbiased or not).\nHere, we learn how to program MC simulations using a very simple econometric example. Specifically, suppose you are interested in checking what happens to OLS estimators if \\(E[u|x]=0\\) (the error term and \\(x\\) are not correlated) is violated in estimating \\(y = \\alpha + \\beta x + u\\)."
  },
  {
    "objectID": "A01-mc-simulation.html#random-number-generator",
    "href": "A01-mc-simulation.html#random-number-generator",
    "title": "Appendix A — Monte Carlo (MC) Simulation",
    "section": "\nA.1 Random number generator",
    "text": "A.1 Random number generator\nTo create a dateset, we use pseudo random number generators. In most cases, runif() and rnorm() are sufficient.\n\n#=== uniform ===#\nx_u <- runif(1000) \n\nhead(x_u)\n\n[1] 0.2819268 0.5544102 0.3399541 0.8785103 0.5901369 0.9546177\n\nhist(x_u)\n\n\n\n\n\n#=== normal ===#\nx_n <- rnorm(1000, mean = 0, sd = 1)\n\nhead(x_n)\n\n[1] -0.81693399 -0.28328661  0.17030182 -0.05521493 -1.16520506  0.37030996\n\nhist(x_n)\n\n\n\n\nWe can use runif() to draw from the Bernouli distribution, which can be useful in generating a treatment variable.\n\n#=== Bernouli (0.7) ===#\nrunif(30) > 0.3\n\n [1]  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n[13]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n[25]  TRUE  TRUE FALSE FALSE  TRUE  TRUE\n\n\nThey are called  pseudo  random number generators because they are not truly random. What sequence of numbers you get is determined by  seed . In R, you can use set.seed() to set seed.\n\nset.seed(43230)\nrunif(10)\n\n [1] 0.78412711 0.06118740 0.02052893 0.80489633 0.69706142 0.65549966\n [7] 0.18618487 0.87417016 0.39325872 0.06729849\n\n\nIf you run the code on your computer, then you would get exactly the same set of numbers. So, pseudo random generators generate random-looking numbers, but it is not truly random. You are simply drawing from a pre-determined sequence of number that  act like random numbers. This is a very important feature of pseudo random number generators. The fact that  anybody can generate the same sequence of numbers mean that any results based on pseudo random number generators can be reproducible. When you use MC simulations, you  must set a seed so that your results are reproducible."
  },
  {
    "objectID": "A01-mc-simulation.html#mc-simulation-steps",
    "href": "A01-mc-simulation.html#mc-simulation-steps",
    "title": "Appendix A — Monte Carlo (MC) Simulation",
    "section": "\nA.2 MC simulation steps",
    "text": "A.2 MC simulation steps\n\nStep 1: Generate data based on the data generating process  you  specify\nStep 2: Get an estimate based on the generated data (e.g. OLS, mean)\nStep 3: Repeat Steps 1 and 2 many times (e.g., 1000)\nStep 4: Compare your estimates with the true parameter specified in Step 1\n\nGoing though Steps 1 and 2 only once is not going to give you an idea of how the estimator of interest performs. So, you repeat Steps 1 and 2 many times to what you can expect form the estimator on average.\nLet’s use a very simple example to better understand the MC steps. The statistical question of interest here is whether sample mean is an unbiased estimator of the expected value: \\(E[\\frac{1}{n}\\sum_{i=1}^n x_i] = E[x]\\), where \\(x_i\\) is an independent random draw from the same distribution.\n\n\nOf course, \\(x_i\\) does not have to be independent. But, just making things as simple as possible.\nHere is Steps 1.\n\nx <- runif(100) \n\nHere, \\(x\\) follows \\(Unif(0, 1)\\) and \\(E[x] = 0.5\\). This is the data generating process. And, data (x) has been generated using x <- runif(100).\nStep 2 is the estimation of \\(E[x]\\). The estimator is the mean of the observed values of x.\n\n(\nmean_x <- mean(x)\n)\n\n[1] 0.5226597\n\n\nOkay, pretty close. But, remember this is just a single realization of the estimator. Let’s move on to Step 3 (repeating the above many times). Let’s write a function that does Steps 1 and 2.\n\nget_estimate <- function()\n{\n  x <- runif(100) \n  mean_x <- mean(x)\n  return(mean_x)\n}\n\nYou can now repeat get_estimate() many times. There are numerous ways to do this in R. But, let’s use lapply() here.\n\nestimates <-\n  lapply(\n    1:1000,\n    function(x) get_estimate()\n  ) %>% \n  unlist()\n\nHere is the mean of the estimates (the estimate of \\(E[\\frac{1}{n}\\sum_{i=1}^n x_i]\\)).\n\nmean(estimates)\n\n[1] 0.4991132\n\n\nVery close. Of course, you will not get the exact number you are hoping to get, which is \\(0.5\\) in this case as MC simulation is a random process.\nWhile this example may seem excessively simple, no matter what you are trying to test, the basic steps will be exactly the same."
  },
  {
    "objectID": "A01-mc-simulation.html#another-example",
    "href": "A01-mc-simulation.html#another-example",
    "title": "Appendix A — Monte Carlo (MC) Simulation",
    "section": "\nA.3 Another Example",
    "text": "A.3 Another Example\nLet’s work on a slightly more complex MC simulations. We are interested in understanding what happens to \\(\\beta_1\\) if \\(E[u|x]\\ne 0\\) when estimating \\(y=\\beta_0+\\beta_1 x + u\\) (classic endogeneity problem).\nLet’s set some parameters first.\n\nB <- 1000 # the number of iterations\nN <- 100 # sample size\n\nLet’s write a code to generate data for a single iteration (Step 1).\n\nmu <- rnorm(N) # the common term shared by both x and u\nx <- rnorm(N) + mu # independent variable\nu <- rnorm(N) + mu # error\ny <- 1 + x + u # dependent variable\ndata <- data.frame(y = y, x = x)\n\nSo, the target parameter (\\(\\beta_1\\)) is 1 in this data generating process. x and u are correlated because they share the common term mu.\n\ncor(x, u)\n\n[1] 0.5222601\n\n\nThis code gets the OLS estimate of \\(\\beta_1\\) (Step 2).\n\nlm(y ~ x, data = data)$coefficient[\"x\"]\n\n       x \n1.518179 \n\n\nOkay, things are not looking good for OLS already.\nLet’s repeat Steps 1 and 2 many times (Step 3).\n\nget_ols_estimate <- function()\n{\n  mu <- rnorm(N) # the common term shared by both x and u\n  x <- rnorm(N) + mu # independent variable\n  u <- rnorm(N) + mu # error\n  y <- 1 + x + u # dependent variable\n  data <- data.frame(y = y, x = x)\n\n  beta_hat <- lm(y ~ x, data = data)$coefficient[\"x\"]\n\n  return(beta_hat)\n}\n\nestimates <-\n  lapply(\n    1:1000,\n    function(x) get_ols_estimate()\n  ) %>% \n  unlist()\n\nYes, the OLS estimator of \\(\\beta_1\\) is biased as we expected.\n\nmean(estimates)\n\n[1] 1.50085\n\nhist(estimates)"
  }
]