[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Machine Learning for Economists",
    "section": "",
    "text": "This book provides an introduction to machine learning methods for economists with a strong focus on causal machine learning methods. It is still under development and it still has a long way to go before it is completed. One thing that may distinguish this book from the books on machine learning methods on the market is that it attempts to be illustrative and easy to understand for those who are not familiar with machine learning methods. This is done by providing step-by-step R codes to enhance understanding of various algorithms."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Introduction to Machine Learning for Economists",
    "section": "License",
    "text": "License\nThis website is (and will always be) free to use, and is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 4.0 License."
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface: Prediction v.s. Causal Inference",
    "section": "",
    "text": "It is critical to understand the distinctions between  prediction  and  causal inference  for anybody who is interested in using machine learning (ML) methods. This is because a method designed to for the former objective may not work well for the latter objective, and vice versa.\n\n\n\n\n\n\nImportant\n\n\n\n\n Prediction: it aims at predicting accurately the level of the variable of interest (the dependent variable) well based on explanatory variables.\n Causal Inference: it aims at predicting the change in the dependent variable when an explanatory variable of interest changes its value.\n\n\n\nExamples where  prediction matters:\n\nprediction of the future price of corn when the modeler is interested in using the predicted price to make money in the futures market\nprediction of the crop yield by field when the modeler is interested in using the field-level predicted crop yields as an dependent or explanatory variable in a regression analysis (e.g., the impact of weather on crop yield)\nprediction of what is in the vicinity of a self-driving car (the user)\n\nWhat is common among these examples is that the users wants to use the  level  or state of the dependent variable to drive their decisions.\nExamples where  causal inference matters:\n\nunderstand the impact of a micro-finance program on welfare in developing countries when the modelers is interested in whether they should implement such a program or not (does the benefit of implementing the program worth the cost?). The modelers do not care about what level of welfare people are gonna be at. They care about how much improvement (change) in welfare the program would make.\nunderstand the impact of water use limits for farmers on groundwater usage when the modeler (water managers) are interested in predicting how much water use reduction (change) they can expect.\nunderstand the impact of fertilizer on yield for when the modelers are interested in identifying the profit-maximizing fertilizer level. The modelers do not care about what the yield levels are going to be at different fertilizer levels. They care about how much yield improvement (change) can be achieved when more fertilizer is applied.\n\nWhat is common among these examples is that the users wants to use the information about the  change  in the dependent variable after changing the value of an explanatory variable (implementing a policy) in driving their decisions.\nNow, you may think that once you can predict the  level  of the dependent variable as a function of explanatory variables \\(X\\), say \\(\\hat{f}(X)\\), where \\(\\hat{f}(\\cdot)\\) is the trained model, then you can simply take the difference in the predicted values of the dependent variable evaluated at \\(X\\) before (\\(X_0\\)) and after (\\(X_1\\)) to find the change in the dependent variable caused by the change in \\(X\\).\n\\[\n\\begin{aligned}\n\\hat{f}(X_1) - \\hat{f}(X_0)\n\\end{aligned}\n\\]\nYou are indeed right and you can predict the change in the dependent variable when the value of an explanatory variable changes once the model is trained to predict the level of the dependent variable. However, this way of predicting the impact of \\(X\\) (the continuous treatment version of the so-called S-learner) is often biased. Instead, (most of) causal machine learning methods razor-focus on  directly  estimating the change in the dependent variable when the value of an explanatory variable changes and typically performs better.\nTraditionally, the vast majority of ML methods focused on prediction, rather than causal inference. It is only recently (I would say around 2015) that academics and practitioners in industry started to realize the limitation of prediction-oriented methods for many of the research and business problems they need to solve. In response, there are now an emerging tide of new kinds of machine learning methods called causal machine learning methods, which focus on causal identification of a treatment (e.g., pricing of a product, policy intervention, etc). The goal of this book is to learn such causal machine learning methods and add them to your econometric tool box for practical applications. This, however, does not mean we do not learn any prediction-oriented (traditional) machine learning methods. Indeed, it is essential to understand them because the prominent causal machine learning methods do use prediction-oriented ML methods in its process as we will see later. It is just that we do not use prediction-oriented ML methods by themselves for the task of identifying the causal impact of a treatment."
  },
  {
    "objectID": "preface.html#r-and-python",
    "href": "preface.html#r-and-python",
    "title": "Preface: Prediction v.s. Causal Inference",
    "section": "R and Python",
    "text": "R and Python\n\nR reticulate package: can call python functions from within R"
  },
  {
    "objectID": "B01-nonlinear.html",
    "href": "B01-nonlinear.html",
    "title": "1  Non-linear function estimation",
    "section": "",
    "text": "The purpose of this section is to introduce you to the idea of semi-parametric and non-parametric regression methods. We only scratch the surface by just looking at smoothing splines and K-nearest neighbor regression methods. The world of semi-parametric and non-parametric regression is much deeper. But, that’s out of the scope of this section. The primary goal of this section is to familiarize you with the concepts of over-fitting, regularization, hyper-parameters, and parameter tuning using smoothing splines and K-nearest neighbor regression methods as examples."
  },
  {
    "objectID": "B01-nonlinear.html#sec-functional-form",
    "href": "B01-nonlinear.html#sec-functional-form",
    "title": "1  Non-linear function estimation",
    "section": "1.1 Flexible functional form estimation",
    "text": "1.1 Flexible functional form estimation\n\n\nPackages to load for replications\n\nlibrary(data.table)\nlibrary(tidyverse)\nlibrary(mgcv)\nlibrary(parallel)\nlibrary(caret)\n\nThere is a clear limit to liner (in parameter) parametric models in flexibility to represent quantitative relationships between variables. For example, consider crop yield response to fertilizer. Typically, yield increases at the diminishing rate as fertilizer rate increases. However, at a high enough fertilizer rate, yield stops increasing (fertilizer is not a limiting factor at that point). This relationship is illustrated in the figure below.\n\nset.seed(83944)\n\n#=== generate data ===#\nN <- 300 # number of observations\nx <- seq(1, 250, length = N) \ny_det <- 240 * (1 - 0.4 * exp(- 0.03 * x))\ne <- 50 * runif(N) # error\ndata <- data.table(x = x, y = y_det + e, y_det = y_det)\n\n#=== plot ===#\n(\ng_base <- ggplot(data) +\n  geom_line(aes(y = y_det, x = x)) +\n  theme_bw()\n)\n\n\n\n\nLet’s try to fit this data using linear parametric models with \\(sqrt(x)\\), \\(log(x)\\), and \\(x + x^2\\), where the dependent variable is y_det, which is \\(E[y|x]\\) (no error added).\n\n#=== sqrt ===#\nlm_sq <- lm(y_det ~ sqrt(x), data = data)\ndata[, y_hat_sqrt := lm_sq$fit]\n\n#=== log ===#\nlm_log <- lm(y_det ~ log(x), data = data)\ndata[, y_hat_log := lm_log$fit]\n\n#=== quadratic ===#\nlm_quad <- lm(y_det ~ x + x^2, data = data)\ndata[, y_hat_quad := lm_quad$fit]\n\n\n\nCode\nplot_data <-\n  melt(data, id.var = \"x\") %>% \n  .[variable != \"y\", ] %>% \n  .[, fit_case := fcase(\n    variable == \"y_det\", \"True response\",\n    variable == \"y_hat_sqrt\", \"sqrt\",\n    variable == \"y_hat_log\", \"log\",\n    variable == \"y_hat_quad\", \"quadratic\"\n  )]\n\nggplot(plot_data) +\n  geom_line(aes(y = value, x = x, color = fit_case)) +\n  scale_color_discrete(name = \"\") +\n  theme_bw()\n\n\n\n\n\nNone of the specifications do quite well. Indeed, you cannot represent the relationship well using well-known popular functional forms. Let’s now look at methods that are flexible enough to capture the relationship. First, smoothing splines, and then K-nearest neighbor next."
  },
  {
    "objectID": "B01-nonlinear.html#smoothing-splines-semi-parametric",
    "href": "B01-nonlinear.html#smoothing-splines-semi-parametric",
    "title": "1  Non-linear function estimation",
    "section": "1.2 Smoothing Splines (semi-parametric)",
    "text": "1.2 Smoothing Splines (semi-parametric)\nDetailed discussion of smoothing splines is out of the scope of this book. Only its basic ideas will be presented in this chapter. See Wood (2006) for a fuller treatment of this topic.\nConsider a simple quantitative relationship of two variables \\(y\\) and \\(x\\): \\(y = f(x)\\).\n\\[\n\\begin{aligned}\ny = f(x)\n\\end{aligned}\n\\]\nIt is possible to characterize this function by using many functions in additive manner: \\(b_1(x), \\dots, b_K(x)\\).\n\\[\n\\begin{aligned}\ny = \\sum_{k=1}^K \\beta_k b_k(x)\n\\end{aligned}\n\\]\nwhere \\(\\beta_k\\) is the coefficient on \\(b_k(x)\\).\nHere are what \\(b_1(x), \\dots, b_K(x)\\) may look like (1 intercept and 9 cubic spline functions).\n\n\nCode\nbasis_data <-\n  gam(y_det ~ s(x, k = 10, bs = \"cr\"), data = data) %>% \n  predict(., type = \"lpmatrix\") %>% \n  data.table() %>% \n  .[, x := data[, x]] %>% \n  melt(id.var = \"x\")\n\nggplot(data = basis_data) +\n  geom_line(aes(y = value, x = x)) +\n  facet_grid(variable ~ .) +\n  theme_bw()\n\n\n\n\n\nBy assigning different values to \\(b_1(x), \\dots, b_K(x)\\), their summation can represent different functional relationships.\nHere is what \\(\\sum_{k=1}^K \\beta_k b_k(x)\\) looks like when \\(\\beta_1\\) through \\(\\beta_{10}\\) are all \\(200\\).\n\n\n\\[\ny = \\sum_{k=1}^{10} 200 b_k(x)\n\\]\n\n\nCode\ndata.table(\n  variable = unique(basis_data$variable),\n  coef = rep(200, 10)\n) %>% \n.[basis_data, on = \"variable\"] %>% \n.[, sum(coef * value), by = x] %>% \nggplot(data = .) +\n  geom_line(aes(y = V1, x = x)) +\n  theme_bw()\n\n\n\n\n\nHere is what \\(\\sum_{k=1}^K \\beta_k b_k(x)\\) looks like when \\(\\beta_1\\) through \\(\\beta_4\\) are all \\(50\\) and \\(\\beta_5\\) through \\(\\beta_9\\) are all \\(200\\).\n\n\n\\[\ny = \\sum_{k=1}^5 50 b_k(x) + \\sum_{k=6}^{10} 200 b_k(x)\n\\]\n\n\nCode\ndata.table(\n  variable = unique(basis_data$variable),\n  coef = c(rep(50, 5), rep(200, 5))\n) %>% \n.[basis_data, on = \"variable\"] %>% \n.[, sum(coef * value), by = x] %>% \nggplot(data = .) +\n  geom_line(aes(y = V1, x = x)) +\n  theme_bw()\n\n\n\n\n\nIn practice, we fit the model to a dataset to find coefficient estimates that fit the data well. Here, we use the gam() function from the mgcv package. Note that, we use \\(E[y|x]\\) (y_det) as the dependent variable to demonstrate the ability of smoothing splines to imitate the true function.\n\n\ngam stands for  Generalized Additive Model. It is a much wider class of model than our examples in this section. See Wood (2006) for more details.\n\ngam_fit <- gam(y_det ~ s(x, k = 10, bs = \"cr\"), data = data)\n\ns(x, k = 10, bs = \"cr\") in the regression formula tells gam() to use 10 knots, which results in an intercept and nine spline basis functions. bs = \"cr\" tells gam() to use cubic spline basis functions.\n\n\nThere are many other spline basis options offered by the mgcv package. Interested readers are referred to Wood (2006).\nHere are the coefficient estimates:\n\ngam_fit$coefficient\n\n(Intercept)      s(x).1      s(x).2      s(x).3      s(x).4      s(x).5 \n 227.421061   -1.486636   16.747228   28.309674   32.115070   34.173085 \n     s(x).6      s(x).7      s(x).8      s(x).9 \n  35.178339   34.541380   38.586536   21.979396 \n\n\nThis translate into the following fitted curve.\n\n\nCode\ndata.table(\n  variable = unique(basis_data$variable)[-1],\n  coef = gam_fit$coefficient[-1]\n) %>% \n.[basis_data, on = \"variable\"] %>% \n.[, .(y_no_int = sum(coef * value)), by = x] %>% \n.[, y_hat := gam_fit$coefficient[1] + y_no_int] %>% \nggplot(data = .) +\n  geom_line(aes(y = y_hat, x = x, color = \"gam-fitted\")) +\n  geom_line(data = data, aes(y = y_det, x = x, color = \"True\")) +\n  scale_color_manual(\n    name = \"\",\n    values = c(\"gam-fitted\" = \"red\", \"True\" = \"blue\")\n  ) +\n  ylab(\"y\") +\n  xlab(\"x\") +\n  theme_bw()\n\n\nWarning: Removed 300 row(s) containing missing values (geom_path).\n\n\n\n\n\nAs you can see, the trained model is almost perfect in representing the functional relationship of \\(y\\) and \\(x\\).\nNow, when gam() fits a model to a dataset, it penalizes the wiggliness (how wavy the curve is) of the estimated function to safe-guard against fitting the model too well to the data. Specifically, it finds coefficients that minimizes the sum of the squared residuals (for regression) plus an additional term that captures how wavy the resulting function is.\n\n\nHere is an example of wiggly (first) v.s. smooth (second) functions.\n\n\nCode\ngam_fit_wiggly <- gam(y ~ s(x, k = 40, bs = \"cr\", sp = 0), data = data)\nplot(gam_fit_wiggly, se = FALSE)\n\n\n\n\n\nCode\ngam_fit_smooth <- gam(y ~ s(x, k = 5, bs = \"cr\"), data = data)\nplot(gam_fit_smooth, se = FALSE)\n\n\n\n\n\n\\[\n\\begin{aligned}\nMin_{\\hat{f}(x)} \\sum_{i=1}^N(y_i - \\hat{f}(x_i))^2 + \\lambda \\Omega(\\hat{f}(x))\n\\end{aligned}\n\\]\nwhere \\(\\Omega(\\hat{f}(x)) > 0\\) is a function that captures how wavy the resulting function is. It takes a higher value when \\(\\hat{f}(x)\\) is more wiggly. \\(\\lambda > 0\\) is the penalization parameter. As \\(\\lambda\\) gets larger, a greater penalty on the wiggliness of \\(\\hat{f}(x)\\), thus resulting in a smoother curve.\nYou can specify \\(\\lambda\\) by sp parameter in gam(). When sp is not specified by the user, gam() finds the optimal value of sp internally using cross-validation (cross-validation will be introduce formally in Chapter 3). For now, just consider it as a method to find parameters that make the trained model a good representation of the underlying conditional mean function (\\(E[y|x]\\)).\n\n\nMore specifically, it uses generalized cross-validation (GCV). A special type of cross-validation that can be done when the model is linear in parameter.\nIf you do not pick the value of sp well, the estimated curve will be very wiggly. Let’s see an example by setting the value of sp to 0, meaning no punishment for being very wiggly. We also set the number of splines to \\(39\\) so that \\(\\sum_{k=1}^K \\beta_k b_k(x)\\) is  very flexible.\n\n#=== fit ===#\ngam_fit_wiggly <- gam(y ~ s(x, k = 40, bs = \"cr\", sp = 0), data = data)\n\n#=== assign the fitted values to a variable ===#\ndata[, y_hat := gam_fit_wiggly$fitted.values]\n\n#=== plot ===#\nggplot(data = data) +\n  geom_line(aes(y = y_hat, x = x)) +\n  geom_point(aes(y = y, x = x)) +\n  theme_bw()\n\n\n\n\nWe call this phenomenon over-fitting (of the data by the model). An over-fitted model does well in predicting \\(y\\) when applied to the data the model used to train itself. However, it would do a terrible job in prediction on the data it has never seen clearly because it is not predicting \\(E[y|x]\\) well.\n\n\n\n\n\n\nImportant\n\n\n\n\n Hyper-parameter: parameters that one has freedom to specify before fitting the model and affect the fitting process in ways that change the outcome of the fitting.\n Parameter tuning: process that attempts to find the optimal set of hyper-parameter values.\n\n\n\nIn mgcv::gam(), the hyper-parameters are the penalty parameter \\(\\lambda\\) (specified by. sp), the number of knots (specified by k)\\(^1\\), the type of splines (specified by bs). Coefficient estimates (\\(\\alpha\\), \\(\\beta_1, \\dots, \\beta_K\\)) change when the value of sp is altered. Here is what happens when k \\(= 3\\) (less flexible than the k \\(= 39\\) case above).\n\n\n\\(^1\\) or more precisely, how many knots and where to place them\n\n#=== fit ===#\ngam_fit_wiggly <- gam(y ~ s(x, k = 3, bs = \"cr\", sp = 0), data = data)\n\n#=== assign the fitted values to a variable ===#\ndata[, y_hat := gam_fit_wiggly$fitted.values]\n\n#=== plot ===#\nggplot(data = data) +\n  geom_line(aes(y = y_hat, x = x)) +\n  geom_point(aes(y = y, x = x)) +\n  theme_bw()\n\n\n\n\nHyper-parameters can significantly influence the outcome. Since the user gets to pick any numbers, it can potentially be used to twist the results in a way that favors the outcomes they want to have. Therefore, it is important to pick the values of hyper-parameters wisely. One way of achieving the goal is cross-validation, which is a data-driven way of finding the best value of hyper-parameters. We will discuss cross-validation in ?sec-cv in detail.\nHere is the fitted curve when the optimal value of sp is picked by gam() automatically given k = 40 and bs = \"cr\" using cross-validation.\n\n\nThat is, we are not tuning k and bs here. gam() uses generalized cross-validation, which we do not cover in this book.\n\n#=== fit ===#\ngam_fit_cv <- gam(y ~ s(x, k = 40, bs = \"cr\"), data = data)\n\n#=== assign the fitted values to a variable ===#\ndata[, y_hat := gam_fit_cv$fitted.values]\n\n#=== plot ===#\nggplot(data = data) +\n  geom_line(aes(y = y_hat, x = x)) +\n  geom_point(aes(y = y, x = x)) +\n  theme_bw()\n\n\n\n\nYou can see that the tuning of sp is successful and has resulted in a much better fitted curve compared to the case where sp was forced to be 0. As you will see, hyper-parameter tuning will be critical for many of the machine learning methods we will look at later."
  },
  {
    "objectID": "B01-nonlinear.html#local-regression",
    "href": "B01-nonlinear.html#local-regression",
    "title": "1  Non-linear function estimation",
    "section": "1.3 Local Regression",
    "text": "1.3 Local Regression\n\n\n\n\n\n\nNote\n\n\n\nUnderstanding what local regression helps you understand how generalized random forest works in Chapter 12.\n\n\n\n1.3.1 K-nearest neighbor\nThe idea of K-nearest neighbor (KNN) regression (a special case of kernel regression with a uniform kernel) is very simple. The prediction of \\(y\\) conditional on \\(x\\) is simply the average of \\(y\\) observed for the K closest (in terms of distance to \\(x\\)) observations in the data.\nLet’s illustrate the method using the data generated in Section 1.1. Suppose you are interested in estimating \\(E[y|x = 100]\\). Here is a visualization of the 10 closest data points to \\(x = 100\\). Blue points are the 10-closest observed data points. The red point is the 10-nearest neighbor estimate of \\(E[y|x = 100]\\).\n\n\nCode\nneighbors <-\n  copy(data)[, abs_dist_100 := abs(x -100)] %>% \n  .[order(abs_dist_100), ] %>%\n  .[1:10, ] \n\ny_estimate <- neighbors[, mean(y)]\n\nggplot() +\n  geom_point(data = data, aes(y = y, x = x), color = \"darkgray\") +\n  geom_point(data = neighbors, aes(y = y, x = x), color = \"blue\") +\n  geom_point(aes(y = y_estimate, x = 100), color = \"red\") +\n  geom_line(\n    data  = data.table(y = y_estimate, x = 0:100),\n    aes(y = y, x = x),\n    color = \"red\",\n    linetype = 2\n  )\n\n\n\n\n\nOne critical difference between KNN and smoothing splines is that KNN fits data locally while smoothing splines fit the data globally, meaning use all the data to fit a single curve at the same time.\nThe hyper-parameter for KNN regression is k (the number of neighbors). The choice of the value of \\(k\\) has a dramatic impacts on the fitted curve.\n\nplot_data <-\n  data.table(\n    k = c(1, 5, 10, 20)\n  ) %>% \n  rowwise() %>% \n  mutate(knn_fit = list(\n    knnreg(y ~ x, k = k, data = data)\n  )) %>% \n  mutate(eval_data = list(\n    data.table(x = seq(0, 250, length = 1000)) %>% \n    .[, y := predict(knn_fit, newdata = .)]\n  )) %>% \n  dplyr::select(k, eval_data) %>% \n  unnest() %>% \n  mutate(k_txt = paste0(\"k = \", k)) %>% \n  mutate(k_txt = factor(k_txt, levels = paste0(\"k = \", c(1, 5, 10, 20))))\n\nWarning: `cols` is now required when using unnest().\nPlease use `cols = c(eval_data)`\n\nggplot() +\n  geom_point(data = data, aes(y = y, x = x), color = \"darkgray\") +\n  geom_line(data = plot_data, aes(y = y, x = x), color = \"red\") +\n  facet_wrap(k_txt ~ ., nrow = 2)\n\n\n\n\nAs you can see, at k \\(= 1\\), the fitted curve fits perfectly with the observed data, and it is highly over-fitted. While increasing k to \\(5\\) alleviates the over-fitting problem, the fitted curve is still very much wiggly. Nobody uses KNN for any practical applications. However, KNN is great to illustrate the importance of the choice of an arbitrary parameter (hyper-parameter). As discussed above, we will later look at cross-validation as a way to tune hyper-parameters."
  },
  {
    "objectID": "B01-nonlinear.html#efficiency-parametric-vs-non-parametric-methods",
    "href": "B01-nonlinear.html#efficiency-parametric-vs-non-parametric-methods",
    "title": "1  Non-linear function estimation",
    "section": "1.4 Efficiency: Parametric vs Non-parametric Methods",
    "text": "1.4 Efficiency: Parametric vs Non-parametric Methods\nSemi-parametric and non-parametric approached are more flexible in representing relationships between the dependent and independent variables than linear-in-parameter models. So, why don’t we just use semi-parametric or non-parametric approaches instead for all the econometric tasks? One reason you might prefer linear-in-parameter models has something to do with efficiency.\n\n\n\n\n\n\nTip\n\n\n\n\nparametric:\n\npro: can be efficient than non-parametric approach when the underlying process is modeled well with parametric models\ncons: not robust to functional form mis-specifications\n\nsemi-, non-parametric:\n\npro: safe-guard against model mis-specification especially when you are modeling highly complex (non-linear in a way that is hard to capture with parametric models and multi-way interactions of variables)\ncons: possible loss in efficiency compared to parametric approach when the underlying model can be approximated well with parametric models\n\n\n\n\n\n1.4.1 Monte Carlo Simulation (parametric vs non-parametric)\nConsider the following data generating process:\n\\[\n\\begin{aligned}\ny = log(x) + v\n\\end{aligned}\n\\]\nYour objective is to understand the impact of treatment (increasing \\(x = 1\\) to \\(x = 2\\)). The true impact of the treatment is \\(TE[x=1 \\rightarrow x=2] = log(2) - log(1) = 0.6931472\\).\n\n\nCode\nmc_run <- function(i)\n{\n  print(i) # progress tracker\n\n  #=== set the number of observations (not really have to be inside the function..) ===#\n  N <- 1000\n\n  #=== generate the data ===#\n  x <- 3 * runif(N)\n  e <- 2 * rnorm(N)\n  y <- log(x) + e\n\n  data <-\n    data.table(\n      x = x,\n      y = y\n    )\n\n  eval_data <- data.table(x = c(1, 2))\n\n  #=== linear model with OLS ===#\n  lm_trained <- lm(y ~ log(x), data = data)\n  te_lm <- lm_trained$coefficient[\"log(x)\"] * log(2)\n\n  #=== gam ===#\n  gam_trained <- gam(y ~ s(x, k = 5), data = data)\n  y_hat_gam <- predict(gam_trained, newdata = eval_data)\n  te_gam <- y_hat_gam[2] - y_hat_gam[1]\n\n  #=== KNN ===#\n  knn_trained <- knnreg(y ~ x, k = 10, data = data)\n  y_hat_knn <- predict(knn_trained, newdata = eval_data)\n  te_knn <- y_hat_knn[2] - y_hat_knn[1]\n  \n  #=== combined the results ===#\n  return_data <-\n    data.table(\n      te = c(te_lm, te_gam, te_knn),\n      type = c(\"lm\", \"gam\", \"knn\")\n    )\n\n  return(return_data)\n}\n\nset.seed(5293)\n\nmc_results <-\n  mclapply(\n    1:500,\n    mc_run,\n    mc.cores = detectCores() * 3 / 4\n  ) %>% \n  rbindlist()\n\n# lapply(\n#   1:100,\n#   mc_run\n# )\n\n\nFigure 1.1 presents the density plots of treatment effect estimates by method. As you can see, the (correctly-specified) linear model performs better than the other methods. All the methods are unbiased, however they differ substantially in terms of efficiency. Note that there was no point in applying random forest at all as there is only a single explanatory variable and the none of the strong points of RF over linear model manifest in this simulation. Clearly, this simulation by no means is intended to claim that linear model is the best. It is merely showcasing  a scenario where (correctly-specified) linear model performs far better than the non-parametric models. This is also a reminder that no method works the best all the time. There are many cases where parametric models perform better (contextual knowledge is critical so that your parametric model can represent the true data generating process well). There are of course many cases non-parametric modeling work better than parametric modeling.\n\n\nCode\nggplot(data = mc_results) +\n  geom_density(aes(x = te, fill = type), alpha = 0.4) +\n  geom_vline(xintercept = log(2))\n\n\n\n\n\nFigure 1.1: Results of the MC simulations on the efficiency of various methods"
  },
  {
    "objectID": "B01-nonlinear.html#references",
    "href": "B01-nonlinear.html#references",
    "title": "1  Non-linear function estimation",
    "section": "References",
    "text": "References\n\n\nWood, Simon N. 2006. Generalized Additive Models: An Introduction with r. chapman; hall/CRC."
  },
  {
    "objectID": "B02-bias-variance-tradeoff.html",
    "href": "B02-bias-variance-tradeoff.html",
    "title": "2  Bias-variance Trade-off",
    "section": "",
    "text": "Packages to load for replication\n\nlibrary(data.table)\nlibrary(tidyverse)\nlibrary(mgcv)\nlibrary(caret)\nlibrary(parallel)\n\nSuppose you have a dataset: \\(D = \\{X_1, y_1\\}, \\{X_2, y_2\\}, \\dots, \\{X_N, y_N\\}\\), where \\(X_i\\) is a collection of features and \\(y_i\\) is the dependent variable for \\(i\\)th observation. Further, suppose you use \\(D\\) for training ML models and \\(\\hat{f}()\\) is a trained model.\nA most common measure of how good a model is mean squared error (MSE) defined as below:\n\\[\nMSE = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{f}(x_i))^2\n\\]\n\\(\\hat{f}(x_i)\\) is the value of \\(y\\) predicted by the trained model \\(\\hat{f}()\\), so \\(y_i - \\hat{f}(x_i)\\) is the residual (termed error more often).\nWhen you get the MSE of a trained model for the very data that is used to train the model, then we may call it training MSE.\nHowever, we are typically interested in how the trained model performs for the data that we have not seen. Let \\(D^{test} = \\{X^{test}_1, y^{test}_1\\}, \\{X^{test}_2, y^{test}_2\\}, \\dots, \\{X^{test}_M, y^{test}_M\\}\\) denote new data set with \\(M\\) data points. Then the test MSE would be:\n\\[\nMSE_{test} = \\frac{1}{M} \\sum_{i=1}^M (y^{test}_{i} - \\hat{f}(x^{test}_{i}))^2\n\\]\nTypically, we try different ML approaches (Random Forest, Support Vector Machine, Causal Forest, Boosted Regression Forest, Neural Network, etc). We also try different values of hyper-parameters for the same approach (e.g., tree depth and minimum observations per leaf for RF). Ideally, we would like to pick the model that has the smallest test \\(MSE\\) among all the models.\nSuppose you do not have a sufficiently large dataset to split to train and test datasets (often the case). So, you used all the available observations to train a model. That means you can get only training \\(MSE\\).\n\n\n\n\n\n\nImportant\n\n\n\nIn this case, can we trust the model that has the lowest training \\(MSE\\)?\n\n\nThe quick answer is no. The problem is that the model with the lowest training \\(MSE\\) does not necessarily achieve the lowest test \\(MSE\\).\nLet’s run some simulations to see this. The data generating process is as follows:\n\\[\ny  = (x - 2.5)^3 + \\mu\n\\]\nwhere \\(\\mu\\) is the error term.\n\nset.seed(943843)\n\n# define the data generating process\n# x fixed\ngen_data <- function(x) {\n  \n  ey <- (x - 2.5)^3 # E[y|x]\n  y <- ey + 3 * rnorm(length(x)) # y = E[y|x] + u\n  return_data <- data.table(y = y, x = x, ey = ey)\n\n  return(return_data)\n}\n\n## generate train data\ntrain_data <- gen_data(x = runif(100) * 5)\n\n  \n## generate test data\n# test data is large to stabilize test MSE \ntest_data <- gen_data(x = runif(10000) * 5)\n\nVisually, here is the relationship between \\(E[y]\\) and \\(x\\):\n\nggplot(data = train_data) +\n  geom_line(aes(y = ey, x = x)) +\n  theme_bw()\n\n\n\n\nNow, let’s define a function that runs regression with different levels of flexibility using a generalized additive model from the mgcv package, predict \\(y\\) for both the train and test datasets, and find train and test MSEs. Specifically, we vary the value of k (the number of knots) in gam() while intentionally setting sp to \\(0\\) so that the wiggliness of the fitted curve is not punished.\n\n\nA very brief introduction of generalized additive mode is available in Chapter 1\n\nest_gam <- function(k, train_data, test_data) \n{\n\n  # define train and test data\n  train_data <- copy(train_data)\n  test_data <- copy(test_data)\n\n  #--- train a model ---#  \n  trained_model <- gam(y ~ s(x, k = k), sp = 0, data = train_data)\n\n  #--- predict y for the train and test datasets ---#\n  train_data[, y_hat := predict(trained_model, newdata = train_data)] \n  train_data[, type := \"Train\"]\n  test_data[, y_hat := predict(trained_model, newdata = test_data)] \n  test_data[, type := \"Test\"]\n\n  #--- combine before returning ---#\n  return_data <- \n    rbind(train_data, test_data) %>%\n    .[, num_knots := k]\n\n  return(return_data)\n}\n\nWe now loop over the number of knots (num_knots).\n\nsim_results <- \n  lapply(1:50, function(x) est_gam(x, train_data, test_data)) %>%\n  rbindlist()\n\nFigure 2.1 presents the fitted regression lines for num_knots \\(= 1, 4, 5, 15, 25\\), and \\(50\\), along with the observed data points in the train dataset.\n\n\nCode\nggplot(sim_results[num_knots  %in% c(1, 4, 15, 25, 50), ]) +\n  geom_point(data = train_data, aes(y = y, x = x), color = \"grey\") +\n  geom_line(aes(y = y_hat, x = x, color = factor(num_knots))) +\n  theme_bw()\n\n\n\n\n\nFigure 2.1: Fitted curves by gam() with different numbers of knots\n\n\n\n\nWhen the number of knots is \\(1\\), gam is not flexible enough to capture the underlying cubic function. However, once the number of knots becomes \\(4\\), it is capable of capturing the underlying non-linearity. However, when you increase the number of knots to 15, you see that the fitted curve is very wiggly (sudden and large changes in \\(y\\) when \\(x\\) is changed slighly). When num_knots \\(= 50\\), the fitted curve looks crazy and does not resemble the underlying smooth cubic curve.\nNow, let’s check how train and test MSEs change as k changes. As you can see in Figure 2.2 below, train MSE goes down as k increases (the more complex the model is, the better fit you will get for the train data). However, test MSE is the lowest when num_knots \\(= 4\\), and it goes up afterward instead of going down. As we saw earlier, when model is made too flexible, it is trained to fit the trained data too well and lose generalizability (predict well for the dataset that has not been seen). This phenomenon is called over-fitting.\n\n\nCode\n#--- get train and test MSEs by k ---#\nsummary <- \n  sim_results[, .(mse = mean((y_hat - y)^2)), by = .(num_knots, type)]\n\nggplot(summary) +\n  geom_line(aes(y = mse, x = num_knots, color = type)) +\n  geom_point(aes(y = mse, x = num_knots, color = type)) +\n  xlab(\"Number of knots\") +\n  ylab(\"MSE\") +\n  scale_color_discrete(name = \"\") +\n  theme_bw()\n\n\n\n\n\nFigure 2.2: Train and test MSEs as a function of the number of knots\n\n\n\n\nIf we were to trust train MSE in picking the model, we would pick the model with k \\(= 50\\) in this particular instance. This clearly tells us that we should  NOT  use training MSE to pick the best model."
  },
  {
    "objectID": "B02-bias-variance-tradeoff.html#bias-variance-trade-off",
    "href": "B02-bias-variance-tradeoff.html#bias-variance-trade-off",
    "title": "2  Bias-variance Trade-off",
    "section": "2.2 Bias-variance trade-off",
    "text": "2.2 Bias-variance trade-off\nExpected test MSE at \\(x = x_0\\) can be written in general (no matter what the trained model is) as follows:\n\\[\nE[(y_0 - \\hat{f}(x_0))^2] = Var(\\hat{f}(x_0)) + Bias(\\hat{x}_0)^2 + Var(\\mu)\n\\]\n\n\\(Bias(\\hat{x}_0) = E[\\hat{f}(x_0)]-y_0\\)\n\\(Var(\\hat{f}(x_0)) = E[(E[\\hat{f}(x_0)]-\\hat{f}(x_0))^2]\\)\n\nThe first term is the variance of predicted value at \\(x_0\\), the second term is the squared bias of \\(\\hat{f}(x_0)\\) (how much \\(\\hat{f}(x_0)\\) differs from \\(E[y_0]\\) on average), and \\(Var(\\mu)\\) is the variance of the error term.\nTo illustrate this trade-off, we will run Monte Carlo simulations. We repeat the following steps 500 times.\n\nstep 1: generate train and test datasets\nstep 2: train gam with different values of \\(k\\) \\((1, 5, 15, 25, 50)\\) using the train dataset\nstep 3: predict \\(y\\) using the test dataset\n\nOnce all the iterations are completed, simulation results are summarized to estimate \\(Var(\\hat{f}(x_0))\\) and \\(E[y - \\hat{f}(x_0)]^2\\) for all the values of \\(x_0\\) (all the \\(x\\) values observed in the test dataset) by \\(k\\). We then average them to find the overall \\(Var(\\hat{f}(x_0))\\) and \\(E[y - \\hat{f}(x_0)]^2\\) by \\(k\\).\n\nx_train <- runif(100) * 5\n# x_test is fixed to make it easier to get average conditonal on a given value of x later  \nx_test <- runif(100) * 5\n\n# function that performs steps 1 ~ 3 (a single iteration) \nrun_mc <- function(i, x_train, x_test)\n{\n  print(i) # track progress\n  train_data <- gen_data(x_train) # generate data\n  test_data <- gen_data(x_test) # generate data\n  # run gam for K = 1, ..., 50\n  sim_results <- \n    lapply(\n      c(1, 5, 15, 25, 50), \n      function(x) est_gam(x, train_data, test_data)\n    ) %>%\n    rbindlist()\n\n  return(sim_results)\n}\n\n# runs run_mc 500 times\nmc_results <-\n  mclapply(\n    1:500,\n    function(x) run_mc(x, x_train, x_test),\n    mc.cores = 12\n  ) %>%\n  rbindlist(idcol = \"sim_id\") \n\nFigure 2.3 shows plots fitted curves for all the 500 simulations by \\(k\\) (grey lines). The blue line is the true \\(E[y|x]\\). The red line is \\(E[\\hat{f}(x)]\\)1. Figure Figure 2.4 plots the average2 \\(Var(\\hat{f}(x))\\) (red), \\(E[y - \\hat{f}(x)]^2\\) (blue), and test MSE (darkgreen) from the test datasets for different values of \\(k\\).\n\n\nCode\nmc_results_sum <- \n    mc_results %>%\n    .[type == \"Test\", ] %>% \n    .[, .(mean_y_hat = mean(y_hat)), by = .(x, ey, num_knots)]\n\nggplot() +\n    geom_line(data = mc_results[type == \"Test\", ], aes(y = y_hat, x = x, group = sim_id), color = \"gray\") +\n    geom_line(data = mc_results_sum, aes(y = mean_y_hat, x = x), color = \"red\") +\n    geom_line(data = mc_results_sum, aes(y = ey, x= x), color = \"blue\") +\n    facet_wrap(. ~ num_knots, ncol = 5) +\n    theme_bw()\n\n\n\n\n\nFigure 2.3: Bias-variance trade-off of GAM models with differing number of knots\n\n\n\n\nAs you can see in Figure 2.3, when \\(k = 1\\), it clearly has a significant bias in estimating \\(E[y|x]\\) except for several values of \\(x\\) at which \\(E[\\hat{f}(x)]\\) only happens to be unbiased. The model is simply too restrictive and suffers significant bias. However, the variance of \\(\\hat{f}(x)\\) is the smallest as shown in Figure 2.4. As we increase the value of \\(k\\) (making the model more flexible), bias dramatically reduces. However, the variance of \\(\\hat{f}(x)\\) slightly increases. Going from \\(k = 5\\) to \\(k = 15\\) further reduces bias. That is, even though individual fitted curves may look very bad, on average they perform well (as you know that what bias measures). However, the variance of \\(\\hat{f}(x)\\) dramatically increases (this is why individual fitted curves look terrible). Moving to a even higher value of \\(k\\) does not reduce bias, but increases the variance of \\(\\hat{f}(x)\\) even further. That is, increasing \\(k\\) from 15 to a higher value of \\(k\\) increases the variance of \\(\\hat{f}(x)\\) while not reducing bias at all.\nAccording to MSE presented in Figure 2.4, \\(k = 5\\) is the best model among all the models tried in this experiment. In this experiment, we had test datasets available. However, in practice, we need to pick the best model when test datasets are not available most of the time. For such a case, we would like a clever way to estimate test MSE even when test datasets are not available. We will later talk about cross-validation as a means to do so.\n\n\nCode\nsum_stat <- \n  mc_results %>%\n  .[type == \"Test\", ] %>% \n  .[\n    , \n    .(\n      var_hat = var(y_hat), # varianc of y_hat\n      bias_sq = mean(y_hat - ey)^2, # squared bias\n      mse = mean((y - y_hat)^2)\n    ),\n    by = .(x, num_knots)\n  ] %>%\n  .[\n    ,\n    .(\n      mean_var_hat = mean(var_hat),\n      mean_bias_sq = mean(bias_sq),\n      mean_mse = mean(mse)\n    ),\n    by = .(num_knots)\n  ]\n\nggplot(data = sum_stat) +\n  geom_line(aes(y = mean_var_hat, x = num_knots, color = \"Variance\")) +\n  geom_point(aes(y = mean_var_hat, x = num_knots, color = \"Variance\")) +\n  geom_line(aes(y = mean_bias_sq, x = num_knots, color = \"Bias\")) +\n  geom_point(aes(y = mean_bias_sq, x = num_knots, color = \"Bias\")) +\n  geom_line(aes(y = mean_mse, x = num_knots, color = \"test MSE\")) +\n  geom_point(aes(y = mean_mse, x = num_knots, color = \"test MSE\")) +\n  scale_color_manual(\n    values = c(\"Variance\" = \"red\", \"Bias\" = \"blue\", \"test MSE\" = \"darkgreen\"),\n    name = \"\"\n    ) +\n  ylab(\"\") +\n  xlab(\"Number of knots\") +\n  theme_bw()\n\n\n\n\n\nFigure 2.4: Expected variance of predicted values, bias, and test ME"
  },
  {
    "objectID": "B02-bias-variance-tradeoff.html#additional-example-k-nearest-neighbor-regression",
    "href": "B02-bias-variance-tradeoff.html#additional-example-k-nearest-neighbor-regression",
    "title": "2  Bias-variance Trade-off",
    "section": "2.3 Additional Example (K-nearest neighbor regression)",
    "text": "2.3 Additional Example (K-nearest neighbor regression)\nAnother example of bias-variance trade-off is presented using KNN as the regression method. Its hyper-parameter - the number of neighbors (k) - is varied to see its effect.\n\nfit_knn <- function(k, train_data, test_data) \n{\n\n  # define train and test data\n  train_data <- copy(train_data)\n  test_data <- copy(test_data)\n\n  #--- train a model ---#  \n  trained_model <- knnreg(y ~ x, k = k, data = train_data)\n\n  #--- predict y for the train and test datasets ---#\n  train_data[, y_hat := predict(trained_model, newdata = train_data)] \n  train_data[, type := \"Train\"]\n  test_data[, y_hat := predict(trained_model, newdata = test_data)] \n  test_data[, type := \"Test\"]\n\n  #--- combine before returning ---#\n  return_data <- \n    rbind(train_data, test_data) %>%\n    .[, num_nbs := k]\n\n  return(return_data)\n}\n\nWe now loop over the number of neighbors (k).\n\n## generate train data\ntrain_data <- gen_data(x = runif(1000) * 5)\n  \n## generate test data\ntest_data <- gen_data(x = runif(1000) * 5)\n\nsim_results <- \n  lapply(1:50, function(x) fit_knn(x, train_data, test_data)) %>%\n  rbindlist()\n\nFigure 2.5 presents the fitted regression lines for \\(k = 1, 5, 15, 25\\), and \\(50\\) using knnreg(), along with the observed data points in the train dataset.\n\n\nCode\nggplot(sim_results[num_nbs  %in% c(1, 5, 15, 25, 50), ]) +\n  geom_point(data = train_data, aes(y = y, x = x), color = \"gray\") +\n  geom_line(aes(y = y_hat, x = x), color = \"red\") +\n  facet_grid(num_nbs ~ .) +\n  theme_bw()\n\n\n\n\n\nFigure 2.5: Fitted curves by knnreg() with different numbers of neighbors\n\n\n\n\n\n\nCode\n#--- get train and test MSEs by k ---#\nsummary <- \n  sim_results[, .(mse = mean((y_hat - y)^2)), by = .(num_nbs, type)]\n\nggplot(summary) +\n  geom_line(aes(y = mse, x = num_nbs, color = type)) +\n  geom_point(aes(y = mse, x = num_nbs, color = type)) +\n  xlab(\"Number of neighbors\") +\n  ylab(\"MSE\") +\n  scale_color_discrete(name = \"\") +\n  theme_bw()\n\n\n\n\n\nFigure 2.6: Train and test MSEs as a function of the number of knots"
  },
  {
    "objectID": "B02-bias-variance-tradeoff.html#references",
    "href": "B02-bias-variance-tradeoff.html#references",
    "title": "2  Bias-variance Trade-off",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "B03-cross-validation.html",
    "href": "B03-cross-validation.html",
    "title": "3  Cross-validation",
    "section": "",
    "text": "No model works the best all the time, and searching for the best modeling approach and specifications is an essential part of modeling applications.\nFor example, we may consider five approaches with varying modeling specifications for each of the approaches:\n\nRandom Forest (RF)\n\nnumber of trees (1000, 2000)\nnumber of variables used in each tree (3, 5, 8)\nand many other hyper parameters\n\nLASSO\n\npenalty parameter (1, 2, 3, etc)\n\nGAM\n\nnumber of knots\npenalty parameter\n\nBoosted Regression Forest (BRF)\n\nnumber of trees (1000, 2000)\nnumber of variables used in each tree (3, 5, 8)\nand many other hyper parameters\n\nConvolutional Neural Network (CNN)\n\nconvolution matrix dimension\nthe order of convolution\nlearning rate\nand many other hyper parameters\n\n\nOur goal here is to find the model that would performs the best when applied to the data that has not been seen yet.\nWe saw earlier that training MSE is not appropriate for that purpose as picking the model with the lowest training MSE would very much likely to lead you to the over-fitted model. In this lecture, we consider a better way of selecting a model using only train data.\n\n\n\n\n\n\nImportant\n\n\n\nWhy CV? When the amount of data is limited and you cannot afford to split the data into training and test datasets, you can use a CV to estimate test MSE (by validating a trained model as if you have test data within the training data) for the purpose of picking the right model and hyper-parameter values.\n\n\nHere is a workflow of identifying the final model and come up with the final trained model.\n\nMake a list of models (e.g., RF, BRF, NN) with various hyper-parameter values to try for each of of the models (like above)\nConduct a CV to see which model-hyper-parameter combination minimizes MSE\nTrain the best model specification to the entire training dataset, which becomes your final trained model\n\n\n\n\n\n\n\nNote\n\n\n\nNote that you do not use any of the models trained during the CV process. They are ran purely for the purpose of finding the best model and hyper-parameter values."
  },
  {
    "objectID": "B03-cross-validation.html#leave-one-out-cross-validation-loocv",
    "href": "B03-cross-validation.html#leave-one-out-cross-validation-loocv",
    "title": "3  Cross-validation",
    "section": "3.2 Leave-One-Out Cross-Validation (LOOCV)",
    "text": "3.2 Leave-One-Out Cross-Validation (LOOCV)\n\n\nPackages to load for replication\n\nlibrary(data.table)\nlibrary(tidyverse)\nlibrary(mgcv)\nlibrary(rsample)\nlibrary(parallel)\n\nConsider a dataset: \\(D = \\{X_1, y_1\\}, \\{X_2, y_2\\}, \\dots, \\{X_N, y_N\\}\\), where \\(X_i\\) is a collection of features and \\(y_i\\) is the dependent variable for \\(i\\)th observation. Further, suppose you use \\(D\\) for training ML models and \\(\\hat{f}()\\) is a trained model.\nLOOCV leaves out a single observation (say \\(i\\)), and train a model (say, GAM with the number of knots of 10) using the all the other observations (-\\(i\\)), and then find MSE for the left-out observation. This process is repeated for all the observations, and then the average of the individual MSEs is calculated.\n\n3.2.1 R demonstration using mgcv::gam()\nLet’s demonstrate this using R. Here is the dataset we use.\n\nset.seed(943843)\n\n# define the data generating process\n# x fixed\ngen_data <- function(x) {\n  \n  ey <- (x - 2.5)^3 # E[y|x]\n  y <- ey + 3 * rnorm(length(x)) # y = E[y|x] + u\n  return_data <- data.table(y = y, x = x, ey = ey)\n\n  return(return_data)\n}\n\n## generate train data\ndata <- gen_data(x = runif(100) * 5)\n\nVisually, here is the relationship between \\(E[y]\\) and \\(x\\):\n\nggplot(data = data) +\n  geom_line(aes(y = ey, x = x))\n\n\n\n\nFor example, for the case where the first observation is left out for validation,\n\n# leave out the first observation\nleft_out_observation <- data[1, ]\n\n# all the rest\ntrain_data <- data[-1, ]\n\nNow we train a gam model using the train_data, predict \\(y\\) for the first observation, and find the MSE.\n\n#=== train the model ===#\nfitted <- gam(y ~ s(x, k = 10), sp = 0, data = train_data)\n\n#=== predict y for the first observation ===#\ny_fitted <- predict(fitted, newdata = left_out_observation)\n\n#=== get MSE ===#\nMSE <- (left_out_observation[, y] - y_fitted) ^ 2\n\nAs described above, LOOCV repeats this process for every single observation of the data. Now, let’s write a function that does the above process for any \\(i\\) you specify.\n\n#=== define the modeling approach ===#\ngam_k_10 <- function(train_data) \n{\n  gam(y ~ s(x, k = 30), sp = 0, data = train_data)\n}\n\n#=== define the process of getting MSE for ith observation ===#\nget_mse <- function(i, model)\n{\n  left_out_observation <- data[i, ]\n\n  # all the rest\n  train_data <- data[-i, ]\n\n  #=== train the model ===#\n  fitted <- model(train_data)\n\n  #=== predict y for the first observation ===#\n  y_fitted <- predict(fitted, newdata = left_out_observation)\n\n  #=== get MSE ===#\n  MSE <- (left_out_observation[, y] - y_fitted) ^ 2 \n\n  return(MSE)\n} \n\nFor example, this gets MSE for the 10th observation.\n\nget_mse(10, gam_k_10)\n\n       1 \n1.523446 \n\n\nLet’s now loop over \\(i = 1:100\\).\n\nmse_indiv <-\n  lapply(\n    1:100,\n    function(x) get_mse(x, gam_k_10)\n  ) %>% \n  #=== list to a vector ===#\n  unlist() \n\nHere is the distribution of MSEs.\n\nhist(mse_indiv)\n\n\n\n\nWe now get the average MSE.\n\nmean(mse_indiv)\n\n[1] 12.46016\n\n\n\n\n3.2.2 Selecting the best GAM specification: Illustration\nNow, let’s try to find the best (among the ones we try) GAM specification using LOOCV. We will try ten different GAM specifications which vary in penalization parameter. Penalization parameter can be set using the sp option for mgcv::gam(). A greater value of sp leads to a more smooth fitted curve.\n\nspecify_gam <- function(sp) {\n  function(train_data) {\n    gam(y ~ s(x, k = 30), sp = sp, data = train_data)\n  }\n}\n\nget_mse_by_sp <- function(sp)\n{\n\n  temp_gam <- specify_gam(sp)\n\n  mse_indiv <-\n    lapply(\n      1:100,\n      function(x) get_mse(x, temp_gam)\n    ) %>% \n    #=== list to a vector ===#\n    unlist() %>% \n    mean()\n\n  return_data <-\n    data.table(\n      mse = mse_indiv,\n      sp = sp\n    )\n\n  return(return_data)\n}\n\nFor example, the following code gets you the average MSE for sp \\(= 3\\).\n\nget_mse_by_sp(3)\n\n        mse sp\n1: 11.56747  3\n\n\nNow, let’s loop over ten values of sp: 0, 0.2, 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2.\n\n(\nmse_data <- \n  lapply(\n    seq(0, 2, by = 0.2),\n    function(x) get_mse_by_sp(x)\n  ) %>% \n  rbindlist()\n)\n\n          mse  sp\n 1: 12.460156 0.0\n 2:  9.909992 0.2\n 3:  9.957858 0.4\n 4: 10.049327 0.6\n 5: 10.164749 0.8\n 6: 10.293142 1.0\n 7: 10.427948 1.2\n 8: 10.565081 1.4\n 9: 10.701933 1.6\n10: 10.836829 1.8\n11: 10.968701 2.0\n\n\n\n\n\nSo, according to the LOOCV, we should pick sp \\(= 0.2\\) as the penalty parameter.\nNow, that we know sp \\(= 0.2\\) produces the lowest LOOCV MSE, we rerun gam() using the entire dataset (not leaving out any of the observations) and make it our final trained model.\n\nfinal_gam_spec <- specify_gam(sp = 1)\n\nfit_gam <- final_gam_spec(train_data)\n\nHere is what the fitted curve looks like:\n\nplot(fit_gam)\n\n\n\n\nLooks good. By the way, here are the fitted curves for some other sp values.\n\nfitted_curves <- \n  lapply(\n    c(0, 0.6, 1, 2),\n    function(x) {\n      temp_gam <- specify_gam(sp = x)\n      fit_gam <- temp_gam(train_data)   \n    }\n  )  \n\nfor (plot in fitted_curves) {\n  plot(plot)\n}\n\n\n\n\n\n\n\n(a) k = 0\n\n\n\n\n\n\n\n(b) k = 0.6\n\n\n\n\n\n\n\n\n\n(c) k = 1\n\n\n\n\n\n\n\n(d) k = 2\n\n\n\n\nFigure 3.1: Fitted curves at various penalization parameters\n\n\n\nLOOCV is perfectly general and can be applied to any statistical methods. However, it can be extremely computationally burdensome because you need to fit the same model for as many as the number of observations. So, if you have 10,000 observations, then you need to fit the model 10,000 times, which can take a long long time.\n\n\n3.2.3 Summary\n\n\n\n\n\n\nNote\n\n\n\nLOOCV is perfectly general and can be applied to any statistical methods.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nLOOCV can be highly computation-intensive when the dataset is large"
  },
  {
    "objectID": "B03-cross-validation.html#k-fold-cross-validation-kcv",
    "href": "B03-cross-validation.html#k-fold-cross-validation-kcv",
    "title": "3  Cross-validation",
    "section": "3.3 K-fold Cross-Validation (KCV)",
    "text": "3.3 K-fold Cross-Validation (KCV)\nKCV is a type of cross-validation that overcomes the LOOCV’s drawback of being computationally too intensive when the dataset is large. KCV first splits the entire dataset intro \\(K\\) folds (K groups) randomly. It then leaves out a chunk of observations that belongs to a fold (group), trains the model using the rest of the observations in the other folds, evaluate the trained model using the left-out group. It repeats this process for all the groups and average the MSEs obtained for each group.\nLet’s demonstrate this process using R.\n\nset.seed(89534)\ndata <- gen_data(x = runif(500) * 5)\n\nYou can use rsample::vfold_cv() to split the data into groups.\n\n#=== split into 5 groups ===#\n(\ndata_folds <- rsample::vfold_cv(data, v = 5)\n)\n\n#  5-fold cross-validation \n# A tibble: 5 × 2\n  splits            id   \n  <list>            <chr>\n1 <split [400/100]> Fold1\n2 <split [400/100]> Fold2\n3 <split [400/100]> Fold3\n4 <split [400/100]> Fold4\n5 <split [400/100]> Fold5\n\n\nAs you can see, rsample::vfold_cv() creates \\(v\\) (\\(=5\\) here) splits. And each split has both train and test datasets. <split [400/100]> means that \\(400\\) and \\(100\\) observations for the train and test datasets, respectively. Note that, the \\(100\\) observations in the first split (called Fold 1) are in the train datasets of the rest of the splits (Fold 2 through Fold 5).\nYou can extract the train and test datasets like below using the training() and testing() functions.\n\ntrain_data <- data_folds[1, ]$splits[[1]] %>% training()\ntest_data <- data_folds[1, ]$splits[[1]] %>% testing()\n\nNow, let’s get MSE for the first fold.\n\n#=== train the model ===#\nfitted_model <- gam(y ~ s(x, k = 30), sp = 0, data = train_data)\n\n#=== predict y for the test data ===#\ny_hat <- predict(fitted_model, test_data)\n\n#=== calculate MSE for the fold ===#\n(test_data[, y] - y_hat)^2 %>% mean()\n\n[1] 12.05604\n\n\nNow that we know how to get MSE for a single fold, let’s loop over folds and get MSE for each of the folds. We first create a function that gets us MSE for a single fold.\n\nget_mse_by_fold <- function(data, fold, model)\n{\n\n  test_data <- data_folds[fold, ]$splits[[1]] %>% testing()\n  train_data <- data_folds[fold, ]$splits[[1]] %>% training()\n\n  #=== train the model ===#\n  fitted_model <- model(train_data)\n\n  #=== predict y for the test data ===#\n  y_hat <- predict(fitted_model, test_data)\n\n  #=== calculate MSE for the fold ===#\n  mse <- (test_data[, y] - y_hat)^2 %>% mean() \n\n  return_data <- \n    data.table(\n      k = fold, \n      mse = mse\n    )\n\n  return(return_data)\n}\n\nThis will get you MSE for the third fold.\n\nget_mse_by_fold(data, 3, gam_k_10)\n\n   k      mse\n1: 3 10.65129\n\n\nNow, let’s loop over the row number of data_folds (loop over splits).\n\n(\nmse_all <-\n  lapply(\n    seq_len(nrow(data_folds)),\n    function(x) get_mse_by_fold(data, x, gam_k_10)\n  ) %>% \n  rbindlist()\n)\n\n   k       mse\n1: 1 12.056038\n2: 2  9.863496\n3: 3 10.651289\n4: 4 10.705704\n5: 5 10.166634\n\n\nBy averaging MSE values, we get\n\nmse_all[, mean(mse)]\n\n[1] 10.68863\n\n\n\n3.3.1 Selecting the best GAM specification: Illustration\nJust like we found the best gam specification (choice of penalization parameter) using LOOCV, we do the same now using KCV.\n\nget_mse_by_sp_kcv <- function(sp)\n{\n\n  temp_gam <- specify_gam(sp)\n\n  mse_by_k <-\n    lapply(\n      seq_len(nrow(data_folds)),\n      function(x) get_mse_by_fold(train_data, x, temp_gam)\n    ) %>% \n    rbindlist()\n\n  return_data <-\n    mse_by_k %>% \n    .[, sp := sp]\n\n  return(return_data[])\n}\n\nFor example, the following code gets you the MSE for all the folds for sp \\(= 3\\).\n\nget_mse_by_sp_kcv(3)\n\n   k      mse sp\n1: 1 13.56925  3\n2: 2 11.22831  3\n3: 3 11.45746  3\n4: 4 10.48461  3\n5: 5 11.29421  3\n\n\nNow, let’s loop over ten values of sp: 0, 0.2, 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2.\n\n(\nmse_results <- \n  lapply(\n    seq(0, 2, by = 0.2),\n    function(x) get_mse_by_sp_kcv(x)\n  ) %>% \n  rbindlist()\n)\n\n    k       mse  sp\n 1: 1 12.056038 0.0\n 2: 2  9.863496 0.0\n 3: 3 10.651289 0.0\n 4: 4 10.705704 0.0\n 5: 5 10.166634 0.0\n 6: 1 11.476153 0.2\n 7: 2  9.840434 0.2\n 8: 3  9.813155 0.2\n 9: 4 10.289085 0.2\n10: 5  9.895722 0.2\n11: 1 11.620416 0.4\n12: 2  9.882467 0.4\n13: 3  9.916178 0.4\n14: 4 10.239119 0.4\n15: 5  9.992636 0.4\n16: 1 11.785544 0.6\n17: 2  9.952820 0.6\n18: 3 10.033789 0.6\n19: 4 10.218321 0.6\n20: 5 10.090156 0.6\n21: 1 11.957640 0.8\n22: 2 10.040814 0.8\n23: 3 10.156511 0.8\n24: 4 10.211168 0.8\n25: 5 10.189377 0.8\n26: 1 12.130712 1.0\n27: 2 10.140277 1.0\n28: 3 10.281839 1.0\n29: 4 10.213564 1.0\n30: 5 10.290230 1.0\n31: 1 12.301318 1.2\n32: 2 10.246975 1.2\n33: 3 10.408152 1.2\n34: 4 10.223465 1.2\n35: 5 10.392306 1.2\n36: 1 12.467400 1.4\n37: 2 10.357879 1.4\n38: 3 10.534197 1.4\n39: 4 10.239448 1.4\n40: 5 10.495093 1.4\n41: 1 12.627767 1.6\n42: 2 10.470802 1.6\n43: 3 10.659006 1.6\n44: 4 10.260390 1.6\n45: 5 10.598085 1.6\n46: 1 12.781780 1.8\n47: 2 10.584159 1.8\n48: 3 10.781853 1.8\n49: 4 10.285370 1.8\n50: 5 10.700824 1.8\n51: 1 12.929162 2.0\n52: 2 10.696811 2.0\n53: 3 10.902210 2.0\n54: 4 10.313617 2.0\n55: 5 10.802917 2.0\n    k       mse  sp\n\n\nLet’s now get the average MSE by sp:\n\n(\nmean_mse_data <- mse_results[, .(mean_mse = mean(mse)), by = sp]\n)\n\n     sp mean_mse\n 1: 0.0 10.68863\n 2: 0.2 10.26291\n 3: 0.4 10.33016\n 4: 0.6 10.41613\n 5: 0.8 10.51110\n 6: 1.0 10.61132\n 7: 1.2 10.71444\n 8: 1.4 10.81880\n 9: 1.6 10.92321\n10: 1.8 11.02680\n11: 2.0 11.12894\n\n\n\n\n\nSo, according to the KCV, we should pick sp \\(= 0.2\\) as the penalty parameter.\nBy the way, here is what MSE values look like for each fold based on the value of sp by fold.\n\nggplot(data = mse_results) +\n  geom_line(aes(y = mse, x = sp, color = factor(k))) +\n  scale_color_discrete(name = \"Fold\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nEven though we compared different specification of the same approach (GAM), we can compare across different models as well. For example, you can find KCV for an RF model with a particular specifications of its hyper-parameters and compare the KCV with those of the GAM model specifications and see what comes at the top."
  },
  {
    "objectID": "B03-cross-validation.html#repeated-k-fold-cross-validation-kcv",
    "href": "B03-cross-validation.html#repeated-k-fold-cross-validation-kcv",
    "title": "3  Cross-validation",
    "section": "3.4 Repeated K-fold Cross-Validation (KCV)",
    "text": "3.4 Repeated K-fold Cross-Validation (KCV)\nAs its name suggest, repeated KCV repeats the process of KCV multiple times. Each KCV iteration splits the original data into k-fold in a different way. A single KCV may not be reliable as the original data was split into such a way that favors one parameter set of or model class over the others. However, if we repeat KCV multiple times, then we can safe-guard against this randomness in a KCV procedure. Repeated KCV is preferred over a single KCV.\nYou can use rsample::vfold_cv() to create repeated k-fold datasets by using the repeats argument.\n\n#=== split into 5 groups ===#\n(\ndata_folds <- rsample::vfold_cv(data, v = 5, repeats = 5)\n)\n\n#  5-fold cross-validation repeated 5 times \n# A tibble: 25 × 3\n   splits            id      id2  \n   <list>            <chr>   <chr>\n 1 <split [400/100]> Repeat1 Fold1\n 2 <split [400/100]> Repeat1 Fold2\n 3 <split [400/100]> Repeat1 Fold3\n 4 <split [400/100]> Repeat1 Fold4\n 5 <split [400/100]> Repeat1 Fold5\n 6 <split [400/100]> Repeat2 Fold1\n 7 <split [400/100]> Repeat2 Fold2\n 8 <split [400/100]> Repeat2 Fold3\n 9 <split [400/100]> Repeat2 Fold4\n10 <split [400/100]> Repeat2 Fold5\n# … with 15 more rows\n\n\nThe output has 5 (number of folds) times 5 (number of repeats) splits. It also has an additional column that indicates which repeat each row is in (id). You can apply get_mse_by_fold() (this function is defined above and calculate MSE) to each row (split) one by one and calculate MSE just like we did above.\n\n(\nmean_mse <-\n  lapply(\n    seq_len(nrow(data_folds)),\n    function(x) get_mse_by_fold(data, x, gam_k_10)\n  ) %>% \n  rbindlist() %>% \n  .[, mean(mse)]\n)\n\n[1] 10.65908"
  },
  {
    "objectID": "B03-cross-validation.html#does-kcv-really-work",
    "href": "B03-cross-validation.html#does-kcv-really-work",
    "title": "3  Cross-validation",
    "section": "3.5 Does KCV really work?",
    "text": "3.5 Does KCV really work?\nLOOCV and KCV use the train data to estimate test MSE. But, does it really work? In other words, does it let us pick the parameter that minimizes the test MSE? We will run a simple MC simulations to test this. We continue to use the same data generating process and gam models for this simulation as well.\nNow, since we know the data generating process, we can actually use the following metric instead of MSE.\n\\[\n\\sum_{i=1}^N(\\hat{f}(x_i) - E[y|x_i])^2\n\\]\nThis measure removes the influence of the error term that appears in the test MSE. Your objective is to minimize this measure. Of course, you cannot do this in practice because you do not observe \\(E[y|x]\\). Let’s call this “pure” MSE.\nFirst, we define a function that gets you MSE from KCV and MSE using the test data as a function of sp.\n\nget_mse_by_sp <- function(sp)\n{\n\n  #=== generate train data and test data ===#\n  train_data <- gen_data(x = runif(500) * 5)\n  test_data <- gen_data(x = runif(500) * 5)\n\n  #/*----------------------------------*/\n  #' ## MSE from KCV \n  #/*----------------------------------*/\n  #=== split train_data into 5 groups ===#\n  data_folds <- rsample::vfold_cv(train_data, v = 5)\n  \n  #=== specify the model ===#\n  temp_gam <- specify_gam(sp)\n\n  #=== get MSE by fold ===#\n  mse_by_k <-\n    lapply(\n      seq_len(nrow(data_folds)),\n      function(x) get_mse_by_fold(train_data, x, temp_gam)\n    ) %>% \n    rbindlist()\n\n  #=== find the average MSE (over folds) ===#\n  mse_kcv <-\n    mse_by_k %>% \n    .[, .(mse = mean(mse))] %>% \n    .[, sp := sp] %>% \n    .[, type := \"KCV\"]\n\n  #/*----------------------------------*/\n  #' ## pure MSE from the test data  \n  #/*----------------------------------*/\n  #=== train using the entire train dataset ===#\n  fitted <- temp_gam(train_data)\n\n  #=== find the average MSE (over observations) ===#\n  mse_test <- \n    test_data %>% \n    #=== predict y ===#\n    .[, y_hat := predict(fitted, newdata = .)] %>% \n    .[, .(mse = mean((ey - y_hat)^2))] %>% \n    .[, sp := sp] %>% \n    .[, type := \"Pure\"]\n\n  #/*----------------------------------*/\n  #' ## Combine and return\n  #/*----------------------------------*/\n  return_data <- rbind(mse_kcv, mse_test)\n\n  return(return_data)\n}\n\n\n\n\n\n\n\nWarning\n\n\n\nNote that you do not have to use an independent test data to obtain pure MSE above even though the code does it so. You could just use the train_data in getting pure MSE and the results would be essentially the same.\n\n\nFor example, this will give you MSE from KCV and MSE using the test data for sp \\(= 2\\).\n\nget_mse_by_sp(2)\n\n          mse sp type\n1: 11.0072347  2  KCV\n2:  0.7131384  2 Pure\n\n\nNow, we define a function that loops over all the sp values we test.\n\nsp_seq <- seq(0, 2, by = 0.2)\n\nget_mse <- function(i)\n{ \n  print(i) # progress tracker\n\n  lapply(\n    sp_seq,\n    function(x) get_mse_by_sp(x)\n  ) %>% \n  rbindlist()\n}\n\nFor example, the following gives you MSE values for all the sp values for a single iteration.\n\nget_mse(1)\n\n[1] 1\n\n\n            mse  sp type\n 1: 10.49305386 0.0  KCV\n 2:  0.46854532 0.0 Pure\n 3: 10.07060222 0.2  KCV\n 4:  0.06385132 0.2 Pure\n 5: 10.14490012 0.4  KCV\n 6:  0.28102534 0.4 Pure\n 7: 10.23670256 0.6  KCV\n 8:  0.29782934 0.6 Pure\n 9: 10.33852029 0.8  KCV\n10:  0.46728439 0.8 Pure\n11: 10.44652013 1.0  KCV\n12:  0.78308220 1.0 Pure\n13: 10.55803026 1.2  KCV\n14:  0.15087488 1.2 Pure\n15: 10.67109831 1.4  KCV\n16:  0.64433189 1.4 Pure\n17: 10.78429633 1.6  KCV\n18:  0.59293569 1.6 Pure\n19: 10.89658887 1.8  KCV\n20:  0.86996493 1.8 Pure\n21: 11.00723472 2.0  KCV\n22:  0.92992710 2.0 Pure\n            mse  sp type\n\n\nFinally, we run get_mse() 500 times.\n\nmse_results <-\n  mclapply(\n    1:500,\n    get_mse,\n    mc.cores = 12\n  ) %>% \n  rbindlist(idcol = \"sim_id\")\n\n#=== use this if you are a Windows user ===#\n# mse_results <-\n#   lapply(\n#     1:500,\n#     get_mse\n#   )\n\nFor each simulation round, let’s find the best sp using KCV and pure MSE.\n\n(\nwhich_sp_optimal <-\n  mse_results %>% \n  .[, .SD[which.min(mse), ], by = .(type, sim_id)] %>% \n  #=== drop mse ===#\n  .[, mse := NULL] %>% \n  dcast(sim_id ~ type, value.var = \"sp\") %>% \n  .[, num_comb := .N, by = .(KCV, Pure)]\n)\n\nFor example, for the first simulation, sp that would have minimized pure MSE (least error relative to the true \\(E[y|x]\\) across varying values of \\(x\\)) was 0.2. On the other hand, if you relied on KCV, you would have chosen 0.2.\nThe following figure shows the relationship between KCV-based and pure MSE-based sp values.\n\n#=== plot the frequency of sp chosen by KCV ===#\nggplot(data = which_sp_optimal) +\n  geom_point(aes(y = Pure, x = KCV, size = num_comb)) +\n  scale_y_continuous(breaks = sp_seq) +\n  scale_x_continuous(breaks = sp_seq) \n\n\n\n\nAs you can see KCV gives you sp that is close to the sp based on pure MSE in many cases. But, you can also see that KCV can suggest you a very different number as well. KCV is not perfect, which is kind of obvious."
  },
  {
    "objectID": "B03-cross-validation.html#references",
    "href": "B03-cross-validation.html#references",
    "title": "3  Cross-validation",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "B04-regularization.html",
    "href": "B04-regularization.html",
    "title": "4  Regression Shrinkage Methods",
    "section": "",
    "text": "We have talked about variance-bias trade-off. When you “shrink” coefficients towards zero, you may be able to achieve lower variance of \\(\\hat{f}(x)\\) while increasing bias, which can result in a lower MSE.\nConsider the following generic linear model:\n\\[\ny = X\\beta + \\mu\n\\]\n\n\\(y\\): dependent variable\n\\(X\\): a collection of explanatory variables (\\(K\\) variables)\n\\(\\beta\\): a collection of coefficients on the explanatory variables \\(X\\)\n\\(\\mu\\): error term\n\nBorrowing from the documentation of the glmnet package(), the minimization problem shrinkage methods solve to estimate coefficients for a linear model can be written as follows:\n\\[\nMin_{\\beta} \\sum_{i=1}^N (y_i - X_i\\beta)^2 + \\lambda \\huge[\\normalsize(1-\\alpha)||\\beta||^2_2/2 + \\alpha ||\\beta||_1\\huge]\n\\tag{4.1}\\]\n\n\\(||\\beta||_1 = |\\beta_1| + |\\beta_2| + \\dots+ |\\beta_K|\\) (called L1 norm)\n\\(||\\beta||_2 = (|\\beta_1|^2 + |\\beta_2|^2 + \\dots+ |\\beta_K|^2)^{\\frac{1}{2}}\\) (called L2 norm)\n\n\\(\\lambda (> 0)\\) is the penalization parameter that governs how much coefficients shrinkage happens (more details later).\nThe shrinkage method is called Lasso when \\(\\alpha = 1\\), Ridge regression when \\(\\alpha = 0\\), and elastic net when \\(\\alpha \\in (0, 1)\\).\n\n\n\n\n\n\nNote\n\n\n\n\nLasso : \\(\\alpha = 1\\)\nRidge : \\(\\alpha = 0\\)\nElastic net : \\(0 < \\alpha < 1\\)\n\n\n\nRidge regression and elastic net are rarely used. So, we are going to cover only Lasso here."
  },
  {
    "objectID": "B04-regularization.html#lasso",
    "href": "B04-regularization.html#lasso",
    "title": "4  Regression Shrinkage Methods",
    "section": "4.2 Lasso",
    "text": "4.2 Lasso\n\n\nPackages to load for replication\n\nlibrary(data.table)\nlibrary(tidyverse)\nlibrary(glmnet)\n\nWhen there are many potential variables to include, it is hard to know which ones to include. Lasso can be used to select variables to build a more parsimonious model, which may help reducing MSE.\nAs mentioned above, Lasso is a special case of shrinkage methods where \\(\\alpha = 1\\) in Equation 4.1. So, the optimization problem of Lasso is\n\\[\nMin_{\\beta} \\sum_{i=1}^N (y_i - X_i\\beta)^2 + \\lambda \\sum_{k=1}^K |\\beta_k|\n\\tag{4.2}\\]\n, where \\(\\lambda\\) is the penalization parameter.\nAlternatively, we can also write the optimization problem as the constrained minimization problem as follows1:\n\\[\n\\begin{aligned}\nMin_{\\beta} & \\sum_{i=1}^N (y_i - X_i\\beta)^2 \\\\\n\\mbox{s.t. } & \\sum_{k=1}^K |\\beta_k| \\leq t\n\\end{aligned}\n\\tag{4.3}\\]\nA graphical representation of the minimization problem is highly illustrative on what Lasso does. Consider the following data generating process:\n\\[\ny  = 0.2 x_1 + 2 * x_2 + \\mu\n\\]\nWhen \\(t\\) is set to 1 in Equation 4.3, Lasso tries to estimate the coefficient on \\(x_1\\) and \\(x_2\\) by solving the following problem:\n\\[\n\\begin{align}\nMin_{\\beta} & \\sum_{i=1}^N (y_i - \\beta_1 x_1 - \\beta_2 x_2)^2 \\\\\n\\mbox{s.t. } & \\sum_{k=1}^K |\\beta_k| < \\textcolor{red}{1}    \n\\end{align}\n\\]\nThis means that, we need to look for the combinations of \\(\\beta_1\\) and \\(\\beta_2\\) such that the sum of their absolute values is less than 1. Graphically, here is what the constraint looks like:\n\n\nCode\nggplot() +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  geom_segment(aes(x = 1, y = 0, xend = 0, yend = 1), color = \"red\", size = 1.2) +\n  geom_segment(aes(x = 1, y = 0, xend = 0, yend = -1), color = \"red\", size = 1.2) +\n  geom_segment(aes(x = -1, y = 0, xend = 0, yend = 1), color = \"red\", size = 1.2) +\n  geom_segment(aes(x = -1, y = 0, xend = 0, yend = -1), color = \"red\", size = 1.2) +\n  coord_equal() +\n  xlab(\"beta_1\") +\n  ylab(\"beta_2\") +\n  theme_minimal()\n\n\n\n\n\nNow, let’s calculate what value the objective function takes at different values of \\(\\beta_1\\) and \\(\\beta_2\\).\nWe first generate data.\n\nN <- 1000 # number of observations\nx_1 <- rnorm(N)\nx_2 <- rnorm(N)\nmu <- rnorm(N) # error term\ny <- 2 * x_1 + 0.2 * x_2 + mu\n\ndata <-\n  data.table(\n    y = y,\n    x_1 = x_1,\n    x_2 = x_2\n  )\n\nWithout the constraint, here is the combination of \\(\\beta_1\\) and \\(\\beta_2\\) that minimizes the objective function of Equation 4.3, which is the same as OLS estimates.\n\n(\nols_coefs_1 <- lm(y ~ x_1 + x_2, data = data)$coefficient\n)\n\n(Intercept)         x_1         x_2 \n 0.05558374  1.99119966  0.23249147 \n\n\nWe now calculate the value of the objective functions at different values of \\(\\beta_1\\) and \\(\\beta_2\\). Here is the set of \\(\\{\\beta_1, \\beta_2\\}\\) combinations we look at.\n\n(\nbeta_table <- \n  data.table::CJ(\n    beta_1 = seq(-2, 2, length = 50),\n    beta_2 = seq(-1, 1, length = 50) \n  )\n)\n\n      beta_1     beta_2\n   1:     -2 -1.0000000\n   2:     -2 -0.9591837\n   3:     -2 -0.9183673\n   4:     -2 -0.8775510\n   5:     -2 -0.8367347\n  ---                  \n2496:      2  0.8367347\n2497:      2  0.8775510\n2498:      2  0.9183673\n2499:      2  0.9591837\n2500:      2  1.0000000\n\n\n\n\n\n\n\n\nNote\n\n\n\ndata.table::CJ() takes more than one set of vectors and find the complete combinations the values of the vectors. Trying\n\ndata.table::CJ(x1 = c(1, 2, 3), x2 = c(4, 5, 6))\n\nwill help you understand exactly what it does.\n\n\nLoop over the row numbers of beta_table to find SSE for all the rows (all the combinations of \\(\\beta_1\\) and \\(\\beta_2\\)).\n\n#=== define the function to get SSE ===#\nget_sse <- function(i, data)\n{\n  #=== extract beta_1 and beta_2 for ith observation  ===#\n  betas <- beta_table[i, ]\n\n  #=== calculate SSE ===#\n  sse <-\n    copy(data) %>% \n    .[, y_hat := x_1 * betas[, beta_1] + x_2 * betas[, beta_2]] %>% \n    .[, se := (y - y_hat)^2] %>% \n    .[, sum(se)]\n\n  return(sse)\n}\n\n#=== calculate SSE for each row of beta_table ===#\nsse_all <-\n  lapply(\n    1:nrow(beta_table),\n    function(x) get_sse(x, data)\n  ) %>% \n  unlist()\n\n#=== assign the calculated sse values as a variable ===#\n(\nbeta_table[, sse_1 := sse_all]\n)\n\nHere is the contour map of SSE as a function of \\(\\beta_1\\) and \\(\\beta_2\\). The solution to the unconstrained problem (OLS estimates) is represented by the red point. Since Lasso needs to find a point within the red square, the solution would be \\(\\beta_1 = 1\\) and \\(\\beta_2 = 0\\) (yellow point). Lasso did not give anything to \\(\\beta_2\\) as \\(x_1\\) is a much bigger contributor of the two included variables. Lasso tends to give the coefficient of \\(0\\) to some of the variables when the constraint is harsh, effectively eliminating them from the model. For this reason, Lasso is often used as a variable selection method.\n\n\nCode\nggplot() +\n  stat_contour(\n    data = beta_table, \n    aes(x = beta_1, y = beta_2, z = sse_1, color = ..level..),\n    size = 1.2,\n    breaks = \n      round(\n        quantile(beta_table$sse_1, seq(0, 1, 0.05)),\n        0\n      )\n    ) +\n  scale_color_viridis_c(name = \"SSE\") +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  geom_segment(aes(x = 1, y = 0, xend = 0, yend = 1), color = \"red\", size = 1.2) +\n  geom_segment(aes(x = 1, y = 0, xend = 0, yend = -1), color = \"red\", size = 1.2) +\n  geom_segment(aes(x = -1, y = 0, xend = 0, yend = 1), color = \"red\", size = 1.2) +\n  geom_segment(aes(x = -1, y = 0, xend = 0, yend = -1), color = \"red\", size = 1.2) +\n  #=== OLS point estimates (solutions without the constraint) ===#\n  geom_point(\n    aes(x = ols_coefs_1[\"x_1\"], y = ols_coefs_1[\"x_2\"]),\n    color = \"red\",\n    size = 3\n  ) +\n  geom_point(\n    aes(x = 1, y = 0),\n    color = \"yellow\",\n    size = 3\n  ) +\n  coord_equal() +\n  theme_minimal()\n\n\n\n\n\nLet’s consider a different data generating process: \\(y = x_1 + x_2 + \\mu\\). Here, \\(x_1\\) and \\(x_2\\) are equally important unlike the previous case. Here is what happens:\n\n\nCode\nN <- 1000 # number of observations\nx_1 <- rnorm(N)\nx_2 <- rnorm(N)\nmu <- rnorm(N) # error term\ny <- x_1 + x_2 + mu\n\ndata <-\n  data.table(\n    y = y,\n    x_1 = x_1,\n    x_2 = x_2\n  )\n\nols_coefs_2 <- lm(y ~ x_1 + x_2, data = data)$coefficient\n\n#=== calculate sse for each row of beta_table ===#\nsse_all <-\n  lapply(\n    1:nrow(beta_table),\n    function(x) {\n      betas <- beta_table[x, ]\n      sse <-\n        copy(data) %>% \n        .[, y_hat := x_1 * betas[, beta_1] + x_2 * betas[, beta_2]] %>% \n        .[, se := (y - y_hat)^2] %>% \n        .[, sum(se)]\n      return(sse)\n    }\n  ) %>% \n  unlist()\n\n#=== assign the calculated sse values as a variable ===#\nbeta_table[, sse_2 := sse_all]\n\n#=== visualize ===#\nggplot() +\n  stat_contour(\n    data = beta_table, \n    aes(x = beta_1, y = beta_2, z = sse_2, color = ..level..),\n    size = 1.2,\n    breaks = \n      round(\n        quantile(beta_table$sse_2, seq(0, 1, 0.05)),\n        0\n      )\n    ) +\n  scale_color_viridis_c(name = \"SSE\") +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  geom_segment(aes(x = 1, y = 0, xend = 0, yend = 1), color = \"red\", size = 1.2) +\n  geom_segment(aes(x = 1, y = 0, xend = 0, yend = -1), color = \"red\", size = 1.2) +\n  geom_segment(aes(x = -1, y = 0, xend = 0, yend = 1), color = \"red\", size = 1.2) +\n  geom_segment(aes(x = -1, y = 0, xend = 0, yend = -1), color = \"red\", size = 1.2) +\n  #=== OLS point estimates (solutions without the constraint) ===#\n  geom_point(\n    aes(x = ols_coefs_2[\"x_1\"], y = ols_coefs_2[\"x_2\"]),\n    color = \"red\",\n    size = 3\n  ) +\n  geom_point(\n    aes(x = 0.5, y = 0.5),\n    color = \"yellow\",\n    size = 3\n  ) +\n  coord_equal() +\n  theme_minimal()\n\n\n\n\n\nIn this case, the solution would be (very close to) \\(\\{\\beta_1 = 0.5, \\beta_2 = 0.5\\}\\), with neither of them sent to zero. This is because \\(x_1\\) and \\(x_2\\) are equally important in explaining \\(y\\)."
  },
  {
    "objectID": "B04-regularization.html#ridge-and-elastic-net-regression",
    "href": "B04-regularization.html#ridge-and-elastic-net-regression",
    "title": "4  Regression Shrinkage Methods",
    "section": "4.3 Ridge and Elastic Net regression",
    "text": "4.3 Ridge and Elastic Net regression\nRidge regression uses L2 norm for regularization and solves the following minimization problem:\n\\[\n\\begin{aligned}\nMin_{\\beta} & \\sum_{i=1}^N (y_i - X_i\\beta)^2 \\\\\n\\mbox{s.t. } & \\sum_{k=1}^K \\beta_k^2 \\leq t\n\\end{aligned}\n\\tag{4.4}\\]\nFigure 4.1 shows the constraint when \\(t=1\\) (red circle) and the contour of SSE for the first model we considered (\\(E[y|x] = 2 \\times x_1 + 0.2 \\times x_2\\)). Unlike Lasso, the constraint is a circle (since it is two-dimensional), and you can expect that Ridge coefficient estimates do not generally become 0. Therefore, Ridge regression cannot be used for variable selection.\n\n\nCode\nggplot() +\n  stat_contour(\n    data = beta_table, \n    aes(x = beta_1, y = beta_2, z = sse_1, color = ..level..),\n    size = 1.2,\n    breaks = \n      round(\n        quantile(beta_table$sse_1, seq(0.033, 1, 0.05)),\n        0\n      )\n    ) +\n  scale_color_viridis_c(name = \"SSE\") +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  ggforce::geom_circle(aes(x0 = 0, y0 = 0, r = 1), color = \"red\") +\n  #=== OLS point estimates (solutions without the constraint) ===#\n  geom_point(\n    aes(x = ols_coefs_1[\"x_1\"], y = ols_coefs_1[\"x_2\"]),\n    color = \"red\",\n    size = 3\n  ) +\n  geom_point(\n    aes(x = 0.99, y = 0.1),\n    color = \"orange\",\n    size = 3\n  ) +\n  coord_equal() +\n  theme_minimal()\n\n\n\n\n\nFigure 4.1: Illustration of Ridge Regression\n\n\n\n\nElastic net is at somewhere between Lasso and Ridge with \\(0 < \\alpha < 1\\) in Equation 4.1 and solves the following minimization problem:\n\\[\n\\begin{aligned}\nMin_{\\beta} & \\sum_{i=1}^N (y_i - X_i\\beta)^2 \\\\\n\\mbox{s.t. } & \\frac{1-\\alpha}{2}\\sqrt{\\sum_{k=1}^K \\beta_k^2} + \\alpha\\sum_{k=1}^K |\\beta_k| < t\n\\end{aligned}\n\\tag{4.5}\\]\nFigure 4.2 shows the constraint when \\(t=1\\) (red circle) and the contour of SSE for the first model we considered (\\(E[y|x] = 2 \\times x_1 + 0.2 \\times x_2\\)). Its constraint is a mix of that of Lasso and Ridge regression. It has four pointy points at the points where either one of \\(\\beta_1\\) and \\(\\beta_2\\) just like Lasso. But, the curves that connect those points are not straight. Elastic net can eliminate variables (setting coefficients to 0), but not as strongly as Lasso does.\n\n\nCode\nget_const <- function(x1, x2, alpha){\n  (1 - alpha) * sqrt(x1^2 + x2^2) / 2 + alpha * (abs(x1) + abs(x2))\n}\n\nalpha <- 0.5\n\nen_const_data <-\n  CJ(\n    x1 = seq(-2, 2, length = 1000),\n    x2 = seq(-2, 2, length = 1000)\n  ) %>% \n  .[, c := abs(get_const(x1, x2, alpha)-1)] %>% \n  .[, x2_sign := x2 < 0] %>% \n  .[, .SD[which.min(c), ], by = .(x1, x2_sign)] %>% \n  .[abs(x1) <= 2/(1+alpha), ]\n\nggplot() +\n  stat_contour(\n    data = beta_table, \n    aes(x = beta_1, y = beta_2, z = sse_1, color = ..level..),\n    size = 1.2,\n    breaks = \n      round(\n        quantile(beta_table$sse_1, seq(0, 1, 0.05)),\n        0\n      )\n    ) +\n  scale_color_viridis_c(name = \"SSE\") +\n  geom_hline(yintercept = 0) +\n  geom_vline(xintercept = 0) +\n  #=== elastic net constraint ===#\n  geom_line(data = en_const_data[x1 > 0 & x2 > 0], aes(x=x1, y = x2), color = \"red\") +\n  geom_line(data = en_const_data[x1 < 0 & x2 > 0], aes(x=x1, y = x2), color = \"red\") +\n  geom_line(data = en_const_data[x1 > 0 & x2 < 0], aes(x=x1, y = x2), color = \"red\") +\n  geom_line(data = en_const_data[x1 < 0 & x2 < 0], aes(x=x1, y = x2), color = \"red\") +\n  #=== OLS point estimates (solutions without the constraint) ===#\n  geom_point(\n    aes(x = ols_coefs_1[\"x_1\"], y = ols_coefs_1[\"x_2\"]),\n    color = \"red\",\n    size = 3\n  ) +\n  geom_point(\n    aes(x = 1.33, y = 0),\n    color = \"orange\",\n    size = 3\n  ) +\n  coord_equal() +\n  theme_minimal()\n\n\n\n\n\nFigure 4.2: Illustration of Ridge Regression\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhile Lasso is frequently used. Ridge regression or elastic net is nowhere near as popular."
  },
  {
    "objectID": "B04-regularization.html#lasso-implementation-r",
    "href": "B04-regularization.html#lasso-implementation-r",
    "title": "4  Regression Shrinkage Methods",
    "section": "4.4 Lasso implementation: R",
    "text": "4.4 Lasso implementation: R\nYou can use the glmnet() from the glmnet package to run Lasso. For demonstration, we use the QuickStartExample data.\n\n#=== get the data ===#\ndata(QuickStartExample)\n\n#=== see the structure ===#\nstr(QuickStartExample)\n\nList of 2\n $ x: num [1:100, 1:20] 0.274 2.245 -0.125 -0.544 -1.459 ...\n $ y: num [1:100, 1] -1.275 1.843 0.459 0.564 1.873 ...\n\n\nAs you can see, QuickStartExample is a list of two elements. First one (x) is a matrix of dimension 100 by 20, which is the data of explanatory variables. Second one (y) is a matrix of dimension 100 by 1, which is the data for the dependent variable.\n\n\n\n\n\n\nNote\n\n\n\nIf you are used to running regressions in R, you should have specified a model using formula (e.g., y ~ x). However, most of the machine learning functions in R accept the dependent variable and explanatory variables in a matrix form (or data.frame). This is almost always the case for ML methods in Python as well.\n\n\nBy default, alpha parameter for glmnet() (\\(\\alpha\\) in Equation 4.1) is set to 1. So, to run Lasso, you can simply do the following:\n\n#=== extract X and y ===#\nX <- QuickStartExample$x\ny <- QuickStartExample$y\n\n#=== run Lasso ===#\nlasso <- glmnet(X, y)\n\nBy looking at the output below, you can see that glmnet() tried many different values of \\(\\lambda\\).\n\nlasso\n\n\nCall:  glmnet(x = X, y = y) \n\n   Df  %Dev  Lambda\n1   0  0.00 1.63100\n2   2  5.53 1.48600\n3   2 14.59 1.35400\n4   2 22.11 1.23400\n5   2 28.36 1.12400\n6   2 33.54 1.02400\n7   4 39.04 0.93320\n8   5 45.60 0.85030\n9   5 51.54 0.77470\n10  6 57.35 0.70590\n11  6 62.55 0.64320\n12  6 66.87 0.58610\n13  6 70.46 0.53400\n14  6 73.44 0.48660\n15  7 76.21 0.44330\n16  7 78.57 0.40400\n17  7 80.53 0.36810\n18  7 82.15 0.33540\n19  7 83.50 0.30560\n20  7 84.62 0.27840\n21  7 85.55 0.25370\n22  7 86.33 0.23120\n23  8 87.06 0.21060\n24  8 87.69 0.19190\n25  8 88.21 0.17490\n26  8 88.65 0.15930\n27  8 89.01 0.14520\n28  8 89.31 0.13230\n29  8 89.56 0.12050\n30  8 89.76 0.10980\n31  9 89.94 0.10010\n32  9 90.10 0.09117\n33  9 90.23 0.08307\n34  9 90.34 0.07569\n35 10 90.43 0.06897\n36 11 90.53 0.06284\n37 11 90.62 0.05726\n38 12 90.70 0.05217\n39 15 90.78 0.04754\n40 16 90.86 0.04331\n41 16 90.93 0.03947\n42 16 90.98 0.03596\n43 17 91.03 0.03277\n44 17 91.07 0.02985\n45 18 91.11 0.02720\n46 18 91.14 0.02479\n47 19 91.17 0.02258\n48 19 91.20 0.02058\n49 19 91.22 0.01875\n50 19 91.24 0.01708\n51 19 91.25 0.01557\n52 19 91.26 0.01418\n53 19 91.27 0.01292\n54 19 91.28 0.01178\n55 19 91.29 0.01073\n56 19 91.29 0.00978\n57 19 91.30 0.00891\n58 19 91.30 0.00812\n59 19 91.31 0.00739\n60 19 91.31 0.00674\n61 19 91.31 0.00614\n62 20 91.31 0.00559\n63 20 91.31 0.00510\n64 20 91.31 0.00464\n65 20 91.32 0.00423\n66 20 91.32 0.00386\n67 20 91.32 0.00351\n\n\nYou can access the coefficients for each value of lambda by applying coef() method to lasso.\n\n#=== get coefficient estimates ===#\ncoef_lasso <- coef(lasso)\n\n#=== check the dimension ===#\ndim(coef_lasso)\n\n[1] 21 67\n\n#=== take a look at the first and last three ===#\ncoef_lasso[, c(1:3, 65:67)]\n\n21 x 6 sparse Matrix of class \"dgCMatrix\"\n                   s0           s1         s2          s64          s65\n(Intercept) 0.6607581  0.631235043  0.5874616  0.111208836  0.111018972\nV1          .          0.139264992  0.2698292  1.378068980  1.378335220\nV2          .          .            .          0.023067319  0.023240539\nV3          .          .            .          0.762792114  0.763209604\nV4          .          .            .          0.059619334  0.060253956\nV5          .          .            .         -0.901460720 -0.901862151\nV6          .          .            .          0.613661389  0.614081490\nV7          .          .            .          0.117323876  0.117960550\nV8          .          .            .          0.396890604  0.397260052\nV9          .          .            .         -0.030538991 -0.031073136\nV10         .          .            .          0.127412702  0.128222375\nV11         .          .            .          0.246801359  0.247227761\nV12         .          .            .         -0.063941712 -0.064471794\nV13         .          .            .         -0.045935249 -0.046242852\nV14         .         -0.005878595 -0.1299063 -1.158552963 -1.159038292\nV15         .          .            .         -0.137103471 -0.138012175\nV16         .          .            .         -0.045085698 -0.045661882\nV17         .          .            .         -0.047272446 -0.048039238\nV18         .          .            .          0.051702567  0.052180547\nV19         .          .            .         -0.001791685 -0.002203174\nV20         .          .            .         -1.144262012 -1.144641845\n                     s66\n(Intercept)  0.110845721\nV1           1.378578220\nV2           0.023398270\nV3           0.763589908\nV4           0.060832496\nV5          -0.902227796\nV6           0.614464085\nV7           0.118540773\nV8           0.397596878\nV9          -0.031560145\nV10          0.128960349\nV11          0.247615990\nV12         -0.064955124\nV13         -0.046522983\nV14         -1.159480668\nV15         -0.138840304\nV16         -0.046186890\nV17         -0.048737920\nV18          0.052615915\nV19         -0.002578088\nV20         -1.144987654\n\n\nApplying plot() method gets you how the coefficient estimates change as the value of \\(\\lambda\\) changes:\n\nplot(lasso)\n\n\n\n\nA high L1 Norm is associated with a “lower” value of \\(\\lambda\\) (weaker shrinkage). You can see that as \\(\\lambda\\) increases (L1 Norm decreases), coefficients on more and more variables are set to 0.\nNow, the obvious question is which \\(\\lambda\\) should we pick? One way to select a \\(\\lambda\\) is K-fold cross-validation (KCV), which we covered in section. We can implement KCV using the cv.glmnet() function. You can set the number of folds using the nfolds option (the default is 10). Here, let’s 5-fold CV.\n\ncv_lasso <- cv.glmnet(X, y, nfolds = 5)\n\nThe results of KCV can be readily visualized by applying the plot() method:\n\nplot(cv_lasso)\n\n\n\n\nThere are two vertical dotted lines. The left one indicates the value of \\(\\lambda\\) where CV MSE is minimized (called lambda.min). The right one indicates the  highest  (most regularized) value of \\(\\lambda\\) such that the CV error is within one standard error of the minimum (called lambda.1se).\nYou can access the MSE-minimizing \\(\\lambda\\) as follows:\n\ncv_lasso$lambda.min\n\n[1] 0.09117281\n\n\nYou can access the coefficient estimates when \\(\\lambda\\) is lambda.min as follows\n\ncoef(cv_lasso, s = \"lambda.min\")\n\n21 x 1 sparse Matrix of class \"dgCMatrix\"\n                    s1\n(Intercept)  0.1501130\nV1           1.3253733\nV2           .        \nV3           0.6833693\nV4           .        \nV5          -0.8246020\nV6           0.5293940\nV7           0.0127738\nV8           0.3259468\nV9           .        \nV10          .        \nV11          0.1528706\nV12          .        \nV13          .        \nV14         -1.0656291\nV15          .        \nV16          .        \nV17          .        \nV18          .        \nV19          .        \nV20         -1.0330999\n\n\nThe following code gives you the coefficient estimates when \\(\\lambda\\) is lambda.1se\n\ncoef(cv_lasso, s = \"lambda.1se\")\n\n21 x 1 sparse Matrix of class \"dgCMatrix\"\n                    s1\n(Intercept)  0.1586314\nV1           1.2670788\nV2           .        \nV3           0.5805823\nV4           .        \nV5          -0.7365962\nV6           0.4454661\nV7           .        \nV8           0.2474957\nV9           .        \nV10          .        \nV11          0.0386113\nV12          .        \nV13          .        \nV14         -1.0040712\nV15          .        \nV16          .        \nV17          .        \nV18          .        \nV19          .        \nV20         -0.8995116\n\n\n\n\n\n\n\n\nNote\n\n\n\nglmnet() can be used to much broader class of models (e.g., Logistic regression, Poisson regression, Cox regression, etc). As the name suggests it’s elastic  net  methods for  generalized  linear  model."
  },
  {
    "objectID": "B04-regularization.html#lasso-implementation-python",
    "href": "B04-regularization.html#lasso-implementation-python",
    "title": "4  Regression Shrinkage Methods",
    "section": "4.5 Lasso implementation: Python",
    "text": "4.5 Lasso implementation: Python\nComing later."
  },
  {
    "objectID": "B04-regularization.html#scaling",
    "href": "B04-regularization.html#scaling",
    "title": "4  Regression Shrinkage Methods",
    "section": "4.6 Scaling",
    "text": "4.6 Scaling\nUnlike linear model estimation without shrinkage (regularization), shrinkage method is sensitive to the scaling of independent variables. Scaling of a variable has basically no consequence in linear model without regularization. It simply changes the interpretation of the scaled variable and the coefficient estimates on all the other variables remain unaffected. However, scaling of a single variable has a ripple effect to the other variables in shrinkage methods. This is because the penalization term: \\(\\lambda \\huge[\\normalsize(1-\\alpha)||\\beta||^2_2/2 + \\alpha ||\\beta||_1\\huge]\\). As you can see, \\(\\lambda\\) is applied universally to all the coefficients without any consideration of the scale of the variables.\nLet’s scale the first variable in X (this variable is influential as it survived even when \\(\\lambda\\) is very low) by 1/1000 and see what happens. Now, by default, the standardize option is set to TRUE. So, we need to set it to FALSE explicitly to see the effect.\nHere is before scaling:\n\ncv.glmnet(X, y, nfolds = 5, standardize = FALSE) %>% \n  coef(s = \"lambda.min\")\n\n21 x 1 sparse Matrix of class \"dgCMatrix\"\n                     s1\n(Intercept)  0.14894850\nV1           1.33450311\nV2           .         \nV3           0.68637683\nV4           .         \nV5          -0.81775106\nV6           0.53834467\nV7           0.01605235\nV8           0.33462300\nV9           .         \nV10          .         \nV11          0.15558443\nV12          .         \nV13          .         \nV14         -1.07590955\nV15          .         \nV16          .         \nV17          .         \nV18          .         \nV19          .         \nV20         -1.02266841\n\n\nHere is after scaling:\n\n#=== scale the first variable ===#\nX_scaled <- X\nX_scaled[, 1] <- X_scaled[, 1] / 1000\n\ncv.glmnet(X_scaled, y, nfolds = 5, standardize = FALSE) %>% \n  coef(s = \"lambda.min\")\n\n21 x 1 sparse Matrix of class \"dgCMatrix\"\n                     s1\n(Intercept)  0.52017552\nV1           .         \nV2           .         \nV3           0.64440275\nV4          -0.06209527\nV5          -0.83276204\nV6           0.79746836\nV7           0.09714251\nV8           0.11654042\nV9           .         \nV10          .         \nV11          0.55308320\nV12          .         \nV13          .         \nV14         -1.04121517\nV15          .         \nV16          .         \nV17          .         \nV18          0.18179432\nV19          .         \nV20         -1.37767767\n\n\nAs you can see, the coefficient on the first variable is 0 after scaling. Setting standardize = TRUE (or not doing anything with this option) gives you very similar results whether the data is scaled or not.\n\n#=== not scaled ===#\ncv.glmnet(X, y, nfolds = 5, standardize = TRUE) %>% \n  coef(s = \"lambda.min\")\n\n21 x 1 sparse Matrix of class \"dgCMatrix\"\n                      s1\n(Intercept)  0.147927056\nV1           1.337393911\nV2           .          \nV3           0.704086481\nV4           .          \nV5          -0.842853150\nV6           0.549403480\nV7           0.032703914\nV8           0.342430521\nV9           .          \nV10          0.001206608\nV11          0.178995989\nV12          .          \nV13          .          \nV14         -1.079993473\nV15          .          \nV16          .          \nV17          .          \nV18          .          \nV19          .          \nV20         -1.061382444\n\n#=== scaled ===#\ncv.glmnet(X_scaled, y, nfolds = 5, standardize = TRUE) %>% \n  coef(s = \"lambda.min\")\n\n21 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s1\n(Intercept)    0.14936467\nV1          1329.75266851\nV2             .         \nV3             0.69096092\nV4             .         \nV5            -0.83122558\nV6             0.53669611\nV7             0.02005438\nV8             0.33193760\nV9             .         \nV10            .         \nV11            0.16239419\nV12            .         \nV13            .         \nV14           -1.07081121\nV15            .         \nV16            .         \nV17            .         \nV18            .         \nV19            .         \nV20           -1.04340741\n\n\nWhile you do not have to worry about scaling issues as long as you are using glmnet(), this is something worth remembering."
  },
  {
    "objectID": "B04-regularization.html#references",
    "href": "B04-regularization.html#references",
    "title": "4  Regression Shrinkage Methods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "B05-bootstrap.html",
    "href": "B05-bootstrap.html",
    "title": "5  Bootstrap",
    "section": "",
    "text": "Bootstrap can be used to quantify the uncertainty associated with an estimator. For example, you can use it to estimate the standard error (SE) of a coefficient of a linear model. Since there are closed-form solutions for that, bootstrap is not really bringing any benefits to this case. However, the power of bootstrap comes in handy when you do NOT have a closed form solution. We will first demonstrate how bootstrap works using a linear model, and then apply it to a case where no-closed form solution is available."
  },
  {
    "objectID": "B05-bootstrap.html#how-it-works",
    "href": "B05-bootstrap.html#how-it-works",
    "title": "5  Bootstrap",
    "section": "5.2 How it works",
    "text": "5.2 How it works\n\n\nPackages to load for replication\n\nlibrary(data.table)\nlibrary(tidyverse)\nlibrary(mgcv)\nlibrary(caret)\nlibrary(parallel)\nlibrary(fixest)\nlibrary(tidymodels)\nlibrary(ranger)\n\nHere are the general steps of a bootstrap:\n\nStep 1: Sample the data with replacement (You can sample the same observations more than one times. You draw a ball and then you put it back in the box.)\nStep 2: Run a statistical analysis to estimate whatever quantity you are interested in estimating\nRepeat Steps 1 and 2 many times and store the estimates\nDerive uncertainty measures from the collection of estimates obtained above\n\nLet’s demonstrate this using a very simple linear regression example.\nHere is the data generating process:\n\nset.seed(89343)\nN <- 100\nx <- rnorm(N)\ny <- 2 * x + 2 * rnorm(N)\n\nWe would like to estimate the coefficient on \\(x\\) by applying OLS to the following model:\n\\[\ny = \\alpha + \\beta_x + \\mu\n\\]\nWe know from the econometric theory class that the SE of \\(\\hat{\\beta}_{OLS}\\) is \\(\\frac{\\sigma}{\\sqrt{SST_x}}\\), where \\(\\sigma^2\\) is the variance of the error term (\\(\\mu\\)) and \\(SST_x = \\sum_{i=1}^N (x_i - \\bar{x})^2\\) (\\(\\bar{x}\\) is the mean of \\(x\\)).\n\nmean_x <- mean(x)\nsst_x <- ((x-mean(x))^2) %>% sum()\n(\nse_bhat <- sqrt(4 / sst_x)\n)\n\n[1] 0.217933\n\n\nSo, we know that the true SE of \\(\\hat{\\beta}_{OLS}\\) is 0.217933. There is not really any point in using bootstrap in this case, but this is a good example to see if bootstrap works or not.\nLet’s implement a single iteration of the entire bootstrap steps (Steps 1 and 2).\n\n#=== set up a dataset ===#\ndata <-\n  data.table(\n    y = y,\n    x = x\n  )\n\nNow, draw observations with replacement so the resulting dataset has the same number of observations as the original dataset.\n\nnum_obs <- nrow(data)\n\n#=== draw row numbers ===#\n(\nrow_indices <- sample(seq_len(num_obs), num_obs, replace = TRUE)\n)\n\n  [1] 29 56 39 83 52 66 70 19  4 34 28 34 81 32  9 95 99 86  4 79 30 15 41 97 43\n [26] 89 60 41 16 19 66 96 34 91 86 67 75 28 74 50 71 95 74 87 58 27  9 65 80 41\n [51] 71 64 21 47 45 77 97 94 72 50 23 10 33 45 14 17 82 56 33 75 70 63 78 81 64\n [76] 16 84 90  2 17  5 46 53 37 93 85 72 63 10 35 42 20 70 49 74 32 25 73 76 32\n\n\nUse the sampled indices to create a bootstrapped dataset:\n\n\nYou could also use bootstraps() from the rsample package.\n\ntemp_data <- data[row_indices, ]\n\nNow, apply OLS to get a coefficient estimate on \\(x\\) using the bootstrapped dataset.\n\nlm(y ~ x, data = temp_data)$coefficient[\"x\"]\n\n       x \n2.040957 \n\n\nThis is the end of Steps 1 and 2. Now, let’s repeat this step 1000 times. First, we define a function that implements Steps 1 and 2.\n\nget_beta <- function(i, data)\n{\n  num_obs <- nrow(data)\n\n  #=== sample row numbers ===#\n  row_indices <- sample(seq_len(num_obs), num_obs, replace = TRUE)\n\n  #=== bootstrapped data ===#\n  temp_data <- data[row_indices, ]\n\n  #=== get coefficient ===#\n  beta_hat <- lm(y ~ x, data = temp_data)$coefficient[\"x\"]\n\n  return(beta_hat)\n}\n\nNow repeat get_beta() many times:\n\nbeta_store <-\n  lapply(\n    1:1000,\n    function(x) get_beta(x, data)\n  ) %>% \n  unlist()\n\nCalculate standard deviation of \\(\\hat{\\beta}_{OLS}\\),\n\nsd(beta_store)\n\n[1] 0.2090611\n\n\nNot, bad. What if we make the number of observations to 1000 instead of 100?\n\nset.seed(67343)\n\n#=== generate data ===#\nN <- 1000\nx <- rnorm(N)\ny <- 2 * x + 2 * rnorm(N)\n\n#=== set up a dataset ===#\ndata <-\n  data.table(\n    y = y,\n    x = x\n  )\n\n#=== true SE ===#\nmean_x <- mean(x)\nsst_x <- sum(((x-mean(x))^2))\n(\nse_bhat <- sqrt(4 / sst_x)\n)\n\n[1] 0.06243842\n\n\n\n#=== bootstrap-estimated SE ===#\nbeta_store <-\n  lapply(\n    1:1000,\n    function(x) get_beta(x, data)\n  ) %>% \n  unlist()\n\nsd(beta_store)\n\n[1] 0.06147708\n\n\nThis is just a single simulation. So, we cannot say bootstrap works better when the number of sample size is larger only from these experiments. But, it is generally true that bootstrap indeed works better when the number of sample size is larger."
  },
  {
    "objectID": "B05-bootstrap.html#sec-boot-comp",
    "href": "B05-bootstrap.html#sec-boot-comp",
    "title": "5  Bootstrap",
    "section": "5.3 A more complicated example",
    "text": "5.3 A more complicated example\nConsider a simple production function (e.g., yield response functions for agronomists):\n\\[\ny = \\beta_1 x + \\beta_2 x^2 + \\mu\n\\]\n\n\\(y\\): output\n\\(x\\): input\n\\(\\mu\\): error\n\nThe price of \\(y\\) is 5 and the cost of \\(x\\) is 2. Your objective is to identify the amount of input that maximizes profit. You do not know \\(\\beta_1\\) and \\(\\beta_2\\), and will be estimating them using the data you have collected. Letting \\(\\hat{\\beta_1}\\) and \\(\\hat{\\beta_2}\\) denote the estimates of \\(\\beta_1\\) and \\(\\beta_2\\), respectively, the mathematical expression of the optimization problem is:\n\\[\nMax_x 5(\\hat{\\beta}_1 x + \\hat{\\beta}_2 x^2) - 2 x\n\\]\nThe F.O.C is\n\\[\n5\\hat{\\beta}_1 + 10 \\hat{\\beta}_2 x - 2 = 0\n\\]\nSo, the estimated profit-maximizing input level is \\(\\hat{x}^* = \\frac{2-5\\hat{\\beta}_1}{10\\hat{\\beta}_2}\\). What we are interested in knowing is the SE of \\(x^*\\). As you can see, it is a non-linear function of the coefficients, which makes it slightly harder than simply getting the SE of \\(\\hat{\\beta_1}\\) or \\(\\hat{\\beta_2}\\). However, bootstrap can easily get us an estimate of the SE of \\(\\hat{x}^*\\). The bootstrap process will be very much the same as the first bootstrap example except that we will estimate \\(x^*\\) in each iteration instead of stopping at estimating just coefficients. Let’s work on a single iteration first.\n\n\nAlternatively, you could use the delta method, which lets you find an estimate of the SE of a statistics that is a non-linear function of estimated coefficients.\nHere is the data generating process:\n\nset.seed(894334)\n\nN <-  1000\nx <-  runif(N) * 3\ney <- 6 * x - 2 * x^2\nmu <- 2 * rnorm(N)\ny <- ey + mu\n\ndata <- \n  data.table(\n    x = x,\n    y = y,\n    ey = ey\n  )\n\nUnder the data generating process, here is what the production function looks like:\n\nggplot(data = data) +\n  geom_line(aes(y = ey, x = x)) +\n  theme_bw()\n\n\n\n\n\nnum_obs <- nrow(data)\nrow_indices <- sample(seq_len(num_obs), num_obs, replace = TRUE)\nboot_data <- data[row_indices, ]\nreg <- lm(y ~ x + I(x^2), data = boot_data)\n\nNow that we have estimated \\(\\beta_1\\) and \\(\\beta_2\\), we can easily estimate \\(x^*\\) using its analytical formula.\n\n(\nx_star <- (2 - 5 * reg$coef[\"x\"])/ (10 * reg$coef[\"I(x^2)\"])\n)\n\n      x \n1.40625 \n\n\nWe can repeat this many times to get a collection of \\(x^*\\) estimates and calculate the standard deviation.\n\nget_x_star <- function(i)\n{\n  row_indices <- sample(seq_len(num_obs), num_obs, replace = TRUE)\n  boot_data <- data[row_indices, ]\n  reg <- lm(y ~ x + I(x^2), data = boot_data)\n  x_star <- (2 - 5 * reg$coef[\"x\"])/ (10 * reg$coef[\"I(x^2)\"])\n}\n\n\nx_stars <- \n  lapply(\n    1:1000,\n    get_x_star\n  ) %>%\n  unlist()\n\nHere is the histogram:\n\nhist(x_stars, breaks = 30)\n\n\n\n\nSo, it seems to follow a normal distribution. You can get standard deviation of x_stars as an estimate of the SE of \\(\\hat{x}^*\\).\n\nsd(x_stars)\n\n[1] 0.01783721\n\n\nYou can get the 95% confidence interval (CI) like below:\n\nquantile(x_stars, prob = c(0.025, 0.975))\n\n    2.5%    97.5% \n1.364099 1.430915"
  },
  {
    "objectID": "B05-bootstrap.html#one-more-example-with-a-non-parametric-model",
    "href": "B05-bootstrap.html#one-more-example-with-a-non-parametric-model",
    "title": "5  Bootstrap",
    "section": "5.4 One more example with a non-parametric model",
    "text": "5.4 One more example with a non-parametric model\nWe now demonstrate how we can use bootstrap to get an estimate of the SE of \\(\\hat{x}^*\\) when we use random forest (RF) as our regression method instead of OLS. When RF is used, we do not have any coefficients like the OLS case above. Even then, bootstrap allows us to estimate the SE of \\(\\hat{x}^*\\).\n\n\nRF will be covered in Section 6.2. Please take a quick glance at what RF is to confirm this point. No deep understanding of RF is necessary to understand the significance of bootstrap in this example. Please also note that there is no benefit in using RF in this example because we have only one variable and semi-parametric approach like gam() will certainly do better. RF is used here only because it does not give you coefficient estimates (there are no coefficients in the first place for RF) like linear models.\nThe procedure is exactly the same except that we use RF to estimate the production function and also that we need to conduct numerical optimization as no analytical formula is available unlike the case above.\nWe first implement a single iteration.\n\n#=== get bootstrapped data ===#\nrow_indices <- sample(seq_len(num_obs), num_obs, replace = TRUE)\nboot_data <- data[row_indices, ]\n\n#=== train RF ===#\nreg_rf <- ranger(y ~ x, data = boot_data)\n\nOnce you train RF, we can predict yield at a range of values of \\(x\\), calculate profit, and then pick the value of \\(x\\) that maximizes the estimated profit. Here is what the estimated production function looks like:\n\n#=== create series of x values at which yield will be predicted ===#\neval_data <- data.table(x = seq(0, 3, length = 1000))\n\n#=== predict yield based on the trained RF ===#\neval_data[, y_hat := predict(reg_rf, eval_data)$predictions]\n\n#=== plot ===#\nggplot(data = eval_data) +\n  geom_line(aes(y = y_hat, x = x)) +\n  theme_bw()\n\n\n\n\nWell, it is very spiky (we need to tune hyper-parameters using KCV. But, more on this later. The quality of RF estimation has nothing to do with the goal of this section).\nWe can now predict profit at each value of \\(x\\).\n\n#=== calculate profit ===#\neval_data[, profit_hat := 5 * y_hat - 2 * x]\n\nhead(eval_data)\n\n             x      y_hat profit_hat\n1: 0.000000000 -0.4807528  -2.403764\n2: 0.003003003 -0.4807528  -2.409770\n3: 0.006006006 -0.4807528  -2.415776\n4: 0.009009009  1.0193781   5.078872\n5: 0.012012012  1.0193781   5.072866\n6: 0.015015015  1.2466564   6.203252\n\n\nThe only thing left for us to do is find the \\(x\\) value that maximizes profit.\n\neval_data[which.max(profit_hat), ]\n\n          x    y_hat profit_hat\n1: 1.273273 8.675033   40.82862\n\n\nOkay, so 1.2732733 is the \\(\\hat{x}^*\\) from this iteration.\nAs you might have guessed already, we can just repeat this step to get an estimate of the SE of \\(\\hat{x}^*_{RF}\\).\n\nget_x_star_rf <- function(i)\n{\n  print(i) # progress tracker\n  \n  #=== get bootstrapped data ===#\n  row_indices <- sample(seq_len(num_obs), num_obs, replace = TRUE)\n  boot_data <- data[row_indices, ]\n\n  #=== train RF ===#\n  reg_rf <- ranger(y ~ x, data = boot_data)\n\n  #=== create series of x values at which yield will be predicted ===#\n  eval_data <- data.table(x = seq(0, 3, length = 1000))\n\n  #=== predict yield based on the trained RF ===#\n  eval_data[, y_hat := predict(reg_rf, eval_data)$predictions]\n  \n  #=== calculate profit ===#\n  eval_data[, profit_hat := 5 * y_hat - 2 * x]\n  \n  #=== find x_star_hat ===#\n  x_star_hat <- eval_data[which.max(profit_hat), x]\n  \n  return(x_star_hat)\n}\n\n\nx_stars_rf <- \n  mclapply(\n    1:1000,\n    get_x_star_rf,\n    mc.cores = 12\n  ) %>%\n  unlist()\n\n#=== Windows user ===#\n# library(future.apply)\n# plan(\"multisession\", workers = detectCores() - 2)\n# x_stars_rf <- \n#   future_lapply(\n#     1:1000,\n#     get_x_star_rf\n#   ) %>%\n#   unlist()\n\nHere are the estimate of the SE of \\(\\hat{x}^*_{RF}\\) and 95% CI.\n\nsd(x_stars_rf)\n\n[1] 0.3964915\n\nquantile(x_stars_rf, prob = c(0.025, 0.975))\n\n     2.5%     97.5% \n0.5465465 2.0570571 \n\n\nAs you can see, the estimation of \\(x^*\\) is much more inaccurate than the previous OLS approach. This is likely due to the fact that we are not doing a good job of tuning the hyper-parameters of RF (but, again, more on this later).\nThis conclude the illustration of the power of using bootstrap to estimate the uncertainty of the statistics of interest (\\(x^*\\) here) when the analytical formula of the statistics is non-linear or not even known."
  },
  {
    "objectID": "B05-bootstrap.html#bag-of-little-bootstraps",
    "href": "B05-bootstrap.html#bag-of-little-bootstraps",
    "title": "5  Bootstrap",
    "section": "5.5 Bag of Little Bootstraps",
    "text": "5.5 Bag of Little Bootstraps\nBag of little bootstraps (BLB) is a variant of bootstrap that has similar desirable statistical properties as bootstrap, but is more (computer) memory friendly (Kleiner et al. 2014).\nWhen the sample size is large (say 1GB) bootstrap is computationally costly both in terms of time and memory because the data of the same size is bootstrapped for many times (say, 1000 times). BLB overcomes this problem.\nSuppose you are interested in \\(\\eta\\) (e.g., standard error, confidence interval) of the estimator \\(\\hat{\\theta}\\). Further, let \\(N\\) denoted the sample size of the training data.\nBLB works as follows:\n\nSplit the dataset into \\(S\\) sets of subsamples of size \\(B\\) so that \\(s \\times B = N\\)  without replacement.\nFor each of the subsample, bootstrap \\(N\\) ( not \\(B\\) ) samples  with replacement  \\(M\\) times, and find \\(\\eta\\) (call it \\(\\eta_s\\)) for each of the subsample sets.\nAverage \\(\\hat{\\eta}_s\\) to get \\(\\hat{eta}\\).\n\n\n5.5.1 Demonstrations\nLet’s go back to the example discussed in Section 5.3, where the goal is to identify the confidence interval (\\(\\eta\\)) of the economically optimal input rate (\\(\\theta\\)).\n Step 1: \n\n\nStep 1: Split the dataset into \\(S\\) sets of subsamples of size \\(B\\) so that \\(s \\times B = N\\)  without replacement\nHere, we set \\(S\\) at 5, so \\(B = 200\\).\n\nN <- nrow(data)\nS <- 5\n\n(\nsubsamples <-\n  data[sample(seq_len(N), N), ] %>%\n  .[, subgroup := rep(1:S, each = N/S)] %>% \n  split(.$subgroup)\n)\n\n$`1`\n             x           y       ey subgroup\n  1: 2.3733159  5.30861102 2.974639        1\n  2: 1.6275776  3.09436504 4.467448        1\n  3: 0.2254504 -0.00443899 1.251047        1\n  4: 1.9988901  3.51795123 4.002217        1\n  5: 2.6819931  2.65018050 1.705785        1\n ---                                        \n196: 0.6356858  1.70404729 3.005922        1\n197: 0.5894847  5.41369203 2.841924        1\n198: 1.1667444  5.82214614 4.277881        1\n199: 0.9570687  4.01145812 3.910451        1\n200: 2.6627629  5.73397413 1.795965        1\n\n$`2`\n             x           y        ey subgroup\n  1: 1.8809192  4.57934497 4.2098011        2\n  2: 0.1314569 -0.61561180 0.7541794        2\n  3: 2.5253701  1.49742398 2.3972324        2\n  4: 0.8415726  5.29003218 3.6329468        2\n  5: 0.4067221  3.22079460 2.1094867        2\n ---                                         \n196: 0.3281349  5.33737092 1.7534644        2\n197: 0.1399768  3.43767708 0.8006737        2\n198: 2.6292985 -0.06800098 1.9493698        2\n199: 2.3410270  3.73951071 3.0853472        2\n200: 2.8142845 -2.75691735 1.0453126        2\n\n$`3`\n             x          y          ey subgroup\n  1: 1.2296299  5.0020342 4.353800036        3\n  2: 0.8974602  5.2290199 3.773891666        3\n  3: 1.0734530  1.0161772 4.136115258        3\n  4: 2.8013991  2.7505559 1.112720709        3\n  5: 1.5841919  4.9322236 4.485823441        3\n ---                                          \n196: 0.1508607  1.6273472 0.859646069        3\n197: 0.6124024  1.8253166 2.924341101        3\n198: 2.9990628 -0.4053550 0.005621654        3\n199: 2.8691630 -0.9073811 0.750785549        3\n200: 1.0579288  6.2074265 4.109146183        3\n\n$`4`\n             x          y       ey subgroup\n  1: 2.3998924 -0.8807641 2.880388        4\n  2: 1.0257974  6.6735166 4.050264        4\n  3: 1.1611026  3.5404186 4.270297        4\n  4: 1.0894566  2.5421898 4.162908        4\n  5: 0.1975354  0.3857778 1.107172        4\n ---                                       \n196: 0.7921199  7.0038608 3.497812        4\n197: 2.6279086  1.9564617 1.955645        4\n198: 1.4766912  2.8731862 4.498913        4\n199: 0.6217361  2.1170408 2.957305        4\n200: 2.0381772  1.8095725 3.920731        4\n\n$`5`\n             x          y          ey subgroup\n  1: 0.5453160 10.3885051 2.677156938        5\n  2: 1.2077153  1.2218954 4.329139357        5\n  3: 1.1022868  1.8370493 4.183648383        5\n  4: 2.9998229 -0.5902619 0.001062293        5\n  5: 1.8845175  0.8490326 4.204292516        5\n ---                                          \n196: 1.5471997  4.8499174 4.495544367        5\n197: 0.5813285  3.7268747 2.812085354        5\n198: 1.5839438  7.7635746 4.485906861        5\n199: 0.2343757  1.8121762 1.296390509        5\n200: 1.9727159  1.9638589 4.053079371        5\n\n\n\n\nYou could alternatively do this:\n\nfold_data <- vfold_cv(data, v = 5)\n\nsubsamples <-\n  lapply(\n    1:5,\n    function(x) fold_data[x, ]$splits[[1]] %>% assessment()\n  )\n\n Steps 2: \n\n\n\nStep 2: For each of the subsample, bootstrap \\(N\\) ( not \\(B\\) ) samples  with replacement  \\(M\\) times, and find \\(\\eta\\) (call it \\(\\eta_s\\)) for each of the subsample sets.\n\nInstead of creating bootstrapped samples \\(M\\) times, only one bootstrapped sample from the first subsample is created for now, and then the estimate of \\(x^*\\) is obtained.\n\n(\nboot_data <-\n  sample(\n    seq_len(nrow(subsamples[[1]])),\n    N,\n    replace = TRUE\n  ) %>% \n  subsamples[[1]][., ]\n)\n\n               x          y          ey\n   1: 2.99982294 -0.5902619 0.001062293\n   2: 0.36841088  0.6414166 1.939012145\n   3: 0.36841088  0.6414166 1.939012145\n   4: 1.52659488  4.1079431 4.498585425\n   5: 2.68124652  1.4721212 1.709313320\n  ---                                  \n 996: 2.20473789  2.7760518 3.506689002\n 997: 0.67635816  6.1153578 3.143228227\n 998: 1.89649565  6.3148116 4.185582404\n 999: 0.03379467 -0.2925077 0.200483835\n1000: 2.51539783  1.7992266 2.437934475\n\n\nWe can now train a linear model using the bootstrapped data and calculate \\(\\hat{x}^*\\) based on it.\n\nreg <- lm(y ~ x + I(x^2), data = boot_data)\n\n(\nx_star <- (2 - 5 * reg$coef[\"x\"])/ (10 * reg$coef[\"I(x^2)\"])\n)\n\n       x \n1.367197 \n\n\nWe can loop this process over all the \\(100\\) (\\(M = 100\\)) bootstrapped data from the fist subsample set. Before that, let’s define a function that conducts a single implementation of bootstrapping and estimating \\(x^*\\) from a single subsample set.\n\nget_xstar_boot <- function(subsample)\n{\n  boot_data <-\n    sample(\n      seq_len(nrow(subsample)),\n      N,\n      replace = TRUE\n    ) %>% \n    subsample[., ]\n  reg <- lm(y ~ x + I(x^2), data = boot_data)\n\n  x_star <- (2 - 5 * reg$coef[\"x\"])/ (10 * reg$coef[\"I(x^2)\"])\n\n  return(x_star)\n}\n\nThis is a single iteration.\n\nget_xstar_boot(subsamples[[1]])\n\n       x \n1.400198 \n\n\nNow, let’s repeat this 100 (\\(M = 100\\)) times.\n\nM <- 100\n\nxstar_hats <-\n  lapply(\n    1:M,\n    function(x) get_xstar_boot(subsamples[[1]])\n  ) %>% \n  unlist()\n\nWe can now get the 95% confidence interval of \\(\\hat{x}^*\\).\n\ncf_lower <- quantile(xstar_hats, prob = 0.025) \ncf_upper <- quantile(xstar_hats, prob = 0.975) \n\nSo, [1.36848876286897,1.44320748221065] is the 95% confidence interval estimate from the first subsample set. We repeat this for the other subsample sets and average the 95% CI from them (get_CI() defined on the side).\n\n\nThis function find the 95% confidence interval (\\(\\eta\\)) from a single subsample set.\n\nget_CI <- function(subsample){\n  xstar_hats <-\n    lapply(\n      1:M,\n      function(x) get_xstar_boot(subsamples[[1]])\n    ) %>% \n    unlist() \n\n  cf_lower <- quantile(xstar_hats, prob = 0.025) \n  cf_upper <- quantile(xstar_hats, prob = 0.975) \n\n  return_data <-\n    data.table(\n      cf_lower = cf_lower,\n      cf_upper = cf_upper\n    )\n\n  return(return_data)\n}\n\n\n(\nci_data <-\n  lapply(\n    subsamples,\n    function(x) get_CI(x)\n  ) %>% \n  rbindlist()\n)\n\n   cf_lower cf_upper\n1: 1.374328 1.443085\n2: 1.368850 1.443649\n3: 1.365456 1.444492\n4: 1.368362 1.440309\n5: 1.362626 1.445573\n\n\n Steps 3: \nBy averaging the lower and upper bounds, we get our estimate of the lower and upper bound of the 95% CI of \\(\\hat{x}^*\\).\n\nci_data[, .(mean(cf_lower), mean(cf_upper))]\n\n         V1       V2\n1: 1.367924 1.443422\n\n\nNote the estimated CI is very much similar to that from a regular bootstrap obtained in Section 5.3.\n\n\n\n\n\n\nTip\n\n\n\n\nBag of little bootstraps (BLB) is a variant of bootstrap that has similar desirable statistical properties as bootstrap, but is more computation and memory friendly."
  },
  {
    "objectID": "B05-bootstrap.html#references",
    "href": "B05-bootstrap.html#references",
    "title": "5  Bootstrap",
    "section": "References",
    "text": "References\n\n\nKleiner, Ariel, Ameet Talwalkar, Purnamrita Sarkar, and Michael I. Jordan. 2014. “A Scalable Bootstrap for Massive Data.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 76 (4): 795–816. https://doi.org/10.1111/rssb.12050."
  },
  {
    "objectID": "P01-random-forest.html",
    "href": "P01-random-forest.html",
    "title": "6  Random Forest",
    "section": "",
    "text": "Packages to load for replication\n\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(rpart)\nlibrary(rattle)\nlibrary(wooldridge)\nlibrary(gganimate)\n\n\n\nHere is an example of regression tree to explain logged salary (lsalary) using the mlb1 data from the wooldridge package.\n\n\nCode\n#=== get mlb1 data (from wooldridge) ===#\ndata(mlb1)\n\n#=== build a simple tree ===#\nsimple_tree <-\n  rpart(\n    lsalary ~ hits + runsyr, \n    data = mlb1, \n    control = rpart.control(minsplit = 200)\n  )\n\nfancyRpartPlot(simple_tree)\n\n\n\n\n\nHere is how you read the figure. At the first node, all the observations belong to it (\\(n=353\\)) and the estimate of lsalary is 13. Now, the whole datasets are split into two based on the criteria of whether hits is less than 262 or not. If yes, then such observations will be grouped into the node with “2” on top (the leftmost node), and the estimated lsalary for all the observations in that group (\\(n = 132\\)) is 12. If no, then such observations will be grouped into the node with “3” on top, and the estimated lsalary for all the observations in that group (\\(n = 221\\)) is 14. This node is further split into two groups based on whether runsyr is less than 44 or not. For those observations with runsyr \\(< 44\\) (second node a the bottom), estimated lsalary is 14. For those with runsyr \\(>= 44\\) (rightmost node at the bottom), estimated lsalary is 15. The nodes that do not have any further bifurcations below are called terminal nodes or leafs.\nAs illustrated in the figure above, a regression tree splits the data into groups based on the value of explanatory variables, and all the observations in the same group will be assigned the same estimate (the sample average of the dependent variable of the group).\n\n\n\n\n\n\nTip\n\n\n\nThe estimated value of \\(Y\\) (the dependent variable) for \\(X = x\\) is the average of \\(Y\\) from all the observations that belong to the same terminal node (leaf) as \\(X = x\\).\n\n\nAnother way of illustrating this grouping is shown below:\n\n\nCode\nggplot(mlb1) +\n  geom_point(aes(y = hits, x = runsyr, color = lsalary)) +\n  scale_color_viridis_c() +\n  geom_hline(yintercept = 262) +\n  geom_line(\n    data = data.table(x = 44, y = seq(262, max(mlb1$hits), length = 100)), \n    aes(y = y, x = x)\n  ) +\n  annotate(\n    \"text\", \n    x = 44, y = 111, \n    label = \"Region 2\", \n    color = \"red\"\n  ) +\n  annotate(\n    \"text\", \n    x = 22, y = 1500, \n    label = \"Region 6\", \n    color = \"red\"\n  ) +\n  annotate(\n    \"text\", \n    x = 75, y = 1500, \n    label = \"Region 7\", \n    color = \"red\"\n  )\n\n\n\n\n\nThe mechanism called recursive binary splitting is used to split the predictor space like the example above. Suppose you have K explanatory variables (\\(X_1, \\dots, X_K\\)). Further, let \\(c\\) denote the threshold that splits the sample into two regions such that {\\(X|X_k < c\\)} and {\\(X|X_k \\geq c\\)}.\n\n\n{\\(X|X_k < c\\)} means observations that satisfy the condition stated right to the vertical bar (|). Here, it means all the observations for which its \\(X_k\\) value is less than \\(c\\).\n\nStep 1: For each of the explanatory variables (\\(X_1\\) through \\(X_K\\)), find all the threshold values that result in unique splits.\nStep 2: For each of the explanatory variables (\\(X_1\\) through \\(X_K\\)), go through all the threshold values and then find the threshold value that leads to the lowest sum of the squared residuals.\nStep 3: Among all the splits (as many as the number of explanatory variables), pick the variable-threshold combination that leads to the lowest sum of the squared residuals.\n\nThe data is then split according to the chosen criteria and then the same process is repeated for each of the branches, ad infinitum until the user-specified stopping criteria is met. This way of splitting is called a greedy (or I would call it myopic or shortsighted) approach because the split happens to minimize the immediate RSS without considering the implication of the split for the later splits.\nLet’s try to write this process (an inefficient version) for the first split from the beginning node using the mlb1 data as an illustration based on a simple grid search to find the optimal thresholds (Step 1).\n\n#=== get data ===#\nlibrary(wooldridge)\ndata(mlb1)\n\nmlb1_dt <- \n  mlb1 %>% \n  data.table() %>% # turn into data.table \n  .[, salary := NULL] %>% # remove salary (use lsalary instead)\n  na.omit() # remove observations with NA in any of the variables\n\nLet’s work on splitting based on hruns. One way to find the all the threshold values is simply find the mean of two consecutive numbers from the ordered unique hruns values.\n\nvalue_seq <- \n  #=== order hruns values and find the unique values ===#\n  mlb1_dt[order(hruns), unique(hruns)] %>%\n  #=== get the rolling mean ===#\n  frollmean(2) %>% \n  .[-1]\n\nFigure 6.1 shows all the threshold values (blue lines) stored in value_seq that result in unique splits.\n\n\nCode\nthreshold_data <- \n  data.table(thre = value_seq) %>% \n  .[, id := 1:.N]\n\nggplot(mlb1_dt) +\n  geom_point(aes(y = lsalary, x = hruns)) +\n  geom_vline(\n    data = threshold_data, \n    aes(xintercept = thre), \n    color = \"blue\", \n    size = 0.2\n  ) +\n  geom_vline(xintercept = value_seq[[50]], color = \"red\") +\n  geom_vline(xintercept = value_seq[[70]], color = \"orange\")\n\n\n\n\n\nFigure 6.1: All the threshold values when splitting based on hruns\n\n\n\n\nFor each value in value_seq, we find the RSS. For example, for the 50th value in value_seq (the red line in Figure 6.1),\n\ncopy(mlb1_dt) %>% \n  #=== find the mean of lsalary by whether hruns is less than the threshold or not ===#\n  .[, y_hat := mean(lsalary), by = (hruns < value_seq[50])] %>% \n  #=== get squared residuals ===#\n  .[, (lsalary - y_hat)^2] %>% \n  #=== get RSS ===#\n  sum()\n\n[1] 291.1279\n\n\nHow about 70th value in value_seq (the orange line in Figure 6.1)?\n\ncopy(mlb1_dt) %>% \n  #=== find the mean of lsalary by whether hruns is less than the threshold or not ===#\n  .[, y_hat := mean(lsalary), by = (hruns < value_seq[70])] %>% \n  #=== get squared residuals ===#\n  .[, (lsalary - y_hat)^2] %>% \n  #=== get RSS ===#\n  sum()\n\n[1] 322.3688\n\n\nThis means value_seq[70] (79.5) is a better threshold than value_seq[50] (49.5).\nOkay, let’s consider all the candidate values, not just 50th and 70th, and then pick the best.\n\nget_rss <- function(i, var_name, value_seq, data)\n{\n  rss <-\n    copy(data) %>% \n    setnames(var_name, \"var\") %>% \n    .[, y_hat := mean(lsalary), by = (var < value_seq[i])] %>% \n    .[, (lsalary - y_hat)^2] %>% \n    sum()\n\n  return_data <-\n    data.table(\n      rss = rss,\n      var_name = var_name,\n      var_value = value_seq[i]\n    )\n\n  return(return_data)\n}\n\nHere are RSS values at every value in value_seq.\n\nrss_value <-\n  lapply(\n    seq_len(length(value_seq)),\n    function(x) get_rss(x, \"hruns\", value_seq, mlb1_dt) \n  ) %>% \n  rbindlist()\n\nhead(rss_value)\n\n        rss var_name var_value\n1: 396.8287    hruns       0.5\n2: 383.2393    hruns       1.5\n3: 362.5779    hruns       2.5\n4: 337.0760    hruns       3.5\n5: 325.4865    hruns       4.5\n6: 309.2780    hruns       5.5\n\ntail(rss_value)\n\n        rss var_name var_value\n1: 437.6865    hruns     279.0\n2: 440.0806    hruns     291.5\n3: 441.9307    hruns     348.0\n4: 437.6143    hruns     398.5\n5: 441.0689    hruns     406.5\n6: 443.3740    hruns     423.0\n\n\nFinding the threshold value that minimizes RSS,\n\nrss_value[which.min(rss), ]\n\n        rss var_name var_value\n1: 259.7855    hruns      27.5\n\n\nOkay, so, the best threshold for hruns is 27.5\nSuppose we are considering only five explanatory variables in building a regression tree: hruns, years, rbisyr, allstar, runsyr, hits, and bavg. We do the same operation we did for hruns for all the variables.\n\nget_rss_by_var <- function(var_name, data)\n{\n\n  temp_data <- \n    copy(data) %>% \n    setnames(var_name, \"temp_var\")\n\n  #=== define a sequence of values of hruns ===#\n  value_seq <-\n    temp_data[order(temp_var), unique(temp_var)] %>%\n    #=== get the rolling mean ===#\n    frollmean(2) %>% \n    .[-1]\n\n  setnames(temp_data, \"temp_var\", var_name)\n\n  #=== get RSS ===#\n  rss_value <-\n    lapply(\n      seq_len(length(value_seq)),\n      function(x) get_rss(x, var_name, value_seq, temp_data) \n    ) %>% \n    rbindlist() %>% \n    .[which.min(rss),]\n\n  return(rss_value)\n}\n\nLooping over the set of variables,\n\n(\nmin_rss_by_var <-\n  lapply(\n    c(\"hruns\", \"years\", \"rbisyr\", \"allstar\", \"runsyr\", \"hits\", \"bavg\"),\n    function(x) get_rss_by_var(x, mlb1_dt)\n  ) %>% \n  rbindlist()\n)\n\n        rss var_name  var_value\n1: 259.7855    hruns  27.500000\n2: 249.9090    years   3.500000\n3: 261.1537   rbisyr  31.794642\n4: 277.3388  allstar   7.417582\n5: 249.5106   runsyr  37.666666\n6: 205.1488     hits 356.500000\n7: 375.0281     bavg 252.499992\n\n\nSo, the variable-threshold combination that minimizes RSS is hits - 356.5. We now have the first split. This tree is developed further by splitting nodes like this.\n\n\n\nYou can fit a regression tree using rpart() from the rpart package. Its syntax is similar to that of lm() for a quick fitting.\n\nrpart(\n  formula,\n  data\n)\n\nUsing mlb1, let’s fit a regression tree where lsalary is the dependent variable and hruns, years, rbisyr, allstar, runsyr, hits, and bavg are the explanatory variables.\n\n#=== fit a tree ===#\nfitted_tree <-\n  rpart(\n    lsalary ~ hruns + years + rbisyr + allstar + runsyr + hits + bavg, \n    data = mlb1_dt\n  )\n\nHere is the visualization of the fitted tree using fancyRpartPlot() from the rattle package.\n\nfancyRpartPlot(fitted_tree)\n\n\n\n\nNow, you may wonder why rpart() is not building a tree that has as many leaves as the number of observations so that we have a perfect prediction for the train data (mlb1). If we are simply implementing recursive binary splitting, then it should not have stopped where it stopped. This is because rpart() sets parameter values that control the development of a tree by default. Those default parameters can be seen below:\n\nrpart.control()\n\n$minsplit\n[1] 20\n\n$minbucket\n[1] 7\n\n$cp\n[1] 0.01\n\n$maxcompete\n[1] 4\n\n$maxsurrogate\n[1] 5\n\n$usesurrogate\n[1] 2\n\n$surrogatestyle\n[1] 0\n\n$maxdepth\n[1] 30\n\n$xval\n[1] 10\n\n\nFor example, minsplit is the minimum number of observations that must exist in a node in order for a split to be attempted. cp refers to the complexity parameter. For a given value of cp, a tree is build to minimize the following:\n\\[\n\\sum_{t=1}^T\\sum_{x_i\\in R_t} (y_i - \\hat{y}_{R_t})^2 + cp\\cdot T\n\\]\nwhere \\(R_t\\) is the \\(t\\)th region and \\(\\hat{y_{R_t}}\\) is the estimate of \\(y\\) for all the observations that reside in \\(R_t\\). So, the first term is RSS. The objective function has a penalization term (the second term) just like shrinkage methods we saw in Section 4.1. A higher value of cp leads to a less complex tree with less leaves.\nIf you want to build a much deeper tree that has many leaves, then you can do so using the control option like below.\n\nfull_tree <-\n  rpart(\n    lsalary ~ hruns + years + rbisyr + allstar + runsyr + hits + bavg, # formula\n    data = mlb1_dt, # data\n    control = # control of the hyper parameters\n      rpart.control(\n        minsplit = 2, \n        cp = 0 # complexity parameter\n      )\n  )\n\nLet’s see how amazing this tree is by comparing the observed and fitted lsalary values.\n\n#=== get fitted values ===#\nmlb1_dt[, y_hat := predict(full_tree, newdata = mlb1_dt)]\n\n#=== visualize the fit ===#\nggplot(data = mlb1_dt) +\n  geom_point(aes(y = lsalary, x = y_hat)) +\n  geom_abline(slope = 1, color = \"red\")\n\n\n\n\nYes, perfect prediction accuracy! At least for the train data anyway. But, we all know we want nothing to do with this kind of model. It is clearly over-fitting the train data.\nIn order to find a reasonable model, we can use KCV over cp. Fortunately, when we run rpart(), it automatically builds multiple trees at different values of cp that controls the number of leaves and conduct KCV. You can visualize this using plotcp().\n\nplotcp(fitted_tree)\n\n\n\n\nMSE and cp are presented on the y- and x-axis, respectively. According to the KCV results, cp \\(= 0.018\\) provides the tree with the smallest number of leaves (the most simple) where the MSE value is within one standard deviation from the lowest MSE. You can access the tree built under cp \\(= 0.018\\) like below.\n\n#=== get the best tree ===#\nbest_tree <- prune(full_tree, cp = 0.018)\n\n#=== visualize it ===#\nfancyRpartPlot(best_tree)\n\n\n\n\nEven though how a regression tree is build in R. In practice, you never use a regression tree itself as the final model for your research as its performance is rather poor and tend to over-fit compared to other competitive methods. But, understanding how building a regression tree is important to understand its derivatives like random forest, boosted regression forest."
  },
  {
    "objectID": "P01-random-forest.html#sec-rf",
    "href": "P01-random-forest.html#sec-rf",
    "title": "6  Random Forest",
    "section": "6.2 Random Forest (RF)",
    "text": "6.2 Random Forest (RF)\nRegression tree approach is often not robust and suffers from high variance. Here, we look at the process called  bagging  and how it can be used to train RF model, which is much more robust than a regression tree.\n\n6.2.1 Bagging (Bootstrap Averaging)\nBefore talking about how RF is trained. Let’s first talk about the concept of bagging. Let \\(\\theta(X)\\) denote the statistics of interest you would like to estimate from the data. Bagging (Bootstrap averaging) works like this:\n\nBootstrap the data many times\nEstimate \\(\\theta(X)\\) for each of the bootstrapped datasets (\\(\\hat{\\theta}_1, \\dots, \\hat{\\theta}_B\\))\nAverage the estimates and use it as the final estimate of \\(\\theta(X)\\).\n\n\\[\n\\hat{\\theta}(X) = \\frac{\\hat{\\theta}_1(X) + \\dots + \\hat{\\theta}_B(X)}{B}\n\\]\nTo understand the power of bagging, we need to understand the power of averaging and when it is most effective using a very simple example of estimating the expected value of a random variable.\nConsider two random variables \\(x_1\\) and \\(x_2\\) from the identical distribution, where \\(E[x_i] = \\alpha\\) and \\(Var(x_i) = \\sigma^2\\). You are interested in estimating \\(E[x_i]\\). We have two options:\n\nOption 1: Use \\(x_1\\) as the estimate of \\(E[x_i]\\).\nOption 2: Use the mean of \\(x_1\\) and \\(x_2\\) as the estimate of \\(E[x_i]\\)\n\nThe variance of the first estimator is of course simply the variance of \\(x_1\\), so \\(\\sigma^2\\).\nFor option 2, we know the following relationship holds in general:\n\\[\n\\begin{aligned}\nVar(\\frac{x_1 + x_2}{2}) & = \\frac{Var(x_1)}{4} + \\frac{Var(x_2)}{4} + \\frac{Cov(x_1, x_2)}{2} \\\\\n& = \\frac{\\sigma^2}{2} + \\frac{Cov(x_1, x_2)}{2}\n\\end{aligned}\n\\]\nSo, how good the mean of \\(x_1\\) and \\(x_2\\) as an estimator depends on \\(Cov(x_1, x_2)\\). When they are perfectly positively correlated, then \\(Cov(x_1, x_2) = Var(x_1) = \\sigma^2\\). So, \\(Var(\\frac{x_1 + x_2}{2})\\) is \\(\\sigma^2\\), which is no better than option 1.\n\n\nThis makes sense because adding information from one more variable that is perfectly correlated with the other variable does nothing because they are the same values.\nHowever, as long as \\(x_1\\) and \\(x_2\\) are not perfectly correlated, option 2 is better. The benefit of averaging is greater when the value of \\(Cov(x_1, x_2)\\) is smaller.\nLet’s do a little experiment to see this. We consider three cases:\n\nApproach 0: use \\(x_1\\) as the estimator\nApproach 1: use \\((x_1 + x_2)/2\\) as the estimator (\\(x_1\\) and \\(x_2\\) are independent)\nApproach 2: use \\((x_1 + x_2)/2\\) as the estimator (\\(x_1\\) and \\(x_2\\) are positively correlated)\nApproach 3: use \\((x_1 + x_2)/2\\) as the estimator (\\(x_1\\) and \\(x_2\\) are negatively correlated)\n\n\n#=== set the number of observations to 1000 ===#\nN <- 1000\n\n\n#=== first approach (no correlation) ===#\nx_1 <- rnorm(N)\nx_2 <- rnorm(N)\n\ncor(x_1, x_2)\n\n[1] -0.001429999\n\n#=== second approach (positively correlated) ===#\nx_1 <- rnorm(N)\nx_2 <- 0.5 * x_1 + sqrt(1-(0.5)^2) * rnorm(N)\n\ncor(x_1, x_2)\n\n[1] 0.5049434\n\n#=== third approach (negatively correlated) ===#\nx_1 <- rnorm(N)\nx_2 <- - 0.8 * x_1 - sqrt(1-(0.8)^2) * rnorm(N)\n\ncor(x_1, x_2)\n\n[1] -0.7822314\n\n\nThe following function runs a single iteration of estimating \\(E[x]\\) using the four approaches.\n\nget_alpha <- function(i)\n{\n  #=== approach 0 ===#\n  alpha_hat_0 <- rnorm(1)\n\n  #=== approach 1 (no correlation) ===#\n  x_1 <- rnorm(1)\n  x_2 <- rnorm(1)\n\n  alpha_hat_1 <- (x_1 + x_2) / 2\n\n  #=== approach 2 (positively correlated) ===#\n  x_1 <- rnorm(1)\n  x_2 <- 0.5 * x_1 + sqrt(1-(0.5)^2) * rnorm(1)\n\n  alpha_hat_2 <- (x_1 + x_2) / 2\n\n  #=== approach 3 (negatively correlated) ===#\n  x_1 <- rnorm(1)\n  x_2 <- - 0.8 * x_1 - sqrt(1-(0.8)^2) * rnorm(1)\n\n  alpha_hat_3 <- (x_1 + x_2) / 2\n\n  return_data <-\n    data.table(\n      alpha_hat_0 = alpha_hat_0,\n      alpha_hat_1 = alpha_hat_1,\n      alpha_hat_2 = alpha_hat_2,\n      alpha_hat_3 = alpha_hat_3\n    )\n\n  return(return_data)\n\n} \n\nHere is the results of single iteration.\n\nget_alpha(1)\n\n   alpha_hat_0 alpha_hat_1 alpha_hat_2 alpha_hat_3\n1:   -1.005648  -0.0580865  0.06391733  -0.5243365\n\n\nRepeating this many times,\n\nset.seed(234934)\n\nsim_results <-\n  lapply(\n    1:1000,\n    get_alpha\n  ) %>% \n  rbindlist() %>% \n  melt()\n\nWarning in melt.data.table(.): id.vars and measure.vars are internally\nguessed when both are 'NULL'. All non-numeric/integer/logical type columns are\nconsidered id.vars, which in this case are columns []. Consider providing at\nleast one of 'id' or 'measure' vars in future.\n\n\nFigure 6.2 shows the density plot of the estimates from the four approaches.\n\nsim_results %>% \n  .[, label := gsub(\"alpha_hat_\", \"Approach \", variable)] %>% \n  ggplot(data = .) +\n    geom_density(\n      aes(x = value, fill = label), \n      alpha = 0.5\n    ) +\n  scale_fill_discrete(name = \"\") +\n  theme_bw()\n\n\n\n\nFigure 6.2: Results of simple MC simulations to estimate the expected value of x\n\n\n\n\nAs you can see, they are all pretty much unbiased. However, all the cases that averaged two values (cases 1, 2, and 3) outperformed the base case that relied on a single value each iteration. You can see that when the random variables are negatively correlated, the power of averaging is greater compared to when they are independent or positively correlated. The independent case (case 1) is better than the positive correlation case (case 2).\nNow that we understand the power of bagging, let’s apply this to a regression problem using a tree. The statistics of interest is \\(E[y|X]\\), which is denoted as \\(f(x)\\).\n\nBootstrap the data \\(B\\) times\nTrain a regression tree to each of the bootstrapped dataset, which results in \\(B\\) distinctive trees\nTo predict \\(E[y|x]\\), average the estimate from all the trees (\\(\\hat{f}_1(x), \\dots, \\hat{f}_B(x)\\)) and use it as the final estimate.\n\n\\[\n\\hat{f}(x) = \\frac{\\hat{f}_1(X) + \\dots + \\hat{f}_B(X)}{B}\n\\]\nLet’s implement this for \\(B = 10\\) using mlb1_dt. First, define a function that bootstrap data, fit a regression tree, and then return the fitted values (a single iteration).\n\ntrain_a_tree <- function(i, data)\n{\n  #=== number of observations ===#\n  N <- nrow(data)\n\n  #=== bootstrapped data ===#\n  boot_data <- data[sample(1:N, N, replace = TRUE), ]\n\n  #=== train a regression tree ===#\n  rpart <-\n    rpart(\n      lsalary ~ hruns + years + rbisyr + allstar + runsyr + hits + bavg, \n      data = boot_data\n    )\n\n  #=== predict ===#\n  return_data <-\n    copy(data) %>% \n    .[, y_hat := predict(rpart, newdata = data)] %>% \n    .[, .(id, y_hat)] %>% \n    .[, tree := i] \n\n  return(return_data)\n}\n\nWe now repeat train_a_tree() 10 times.\n\n#=== create observation id for later group-by averaging ===#\nmlb1_dt[, id := 1:.N]\n\n(\ny_estimates <-\n  lapply(\n    1:10,\n    function(x) train_a_tree(x, mlb1_dt) \n  ) %>% \n  rbindlist() %>% \n  .[order(id),]\n)\n\n       id    y_hat tree\n   1:   1 15.15792    1\n   2:   1 15.04446    2\n   3:   1 15.21238    3\n   4:   1 15.07311    4\n   5:   1 14.92840    5\n  ---                  \n3296: 330 11.98177    6\n3297: 330 12.09743    7\n3298: 330 12.02678    8\n3299: 330 12.02287    9\n3300: 330 12.02302   10\n\n\nBy averaging \\(y\\) estimates by id, we can get bagging estimates.\n\ny_estimates[, mean(y_hat), by = id]\n\n      id       V1\n  1:   1 15.09137\n  2:   2 14.66143\n  3:   3 14.61425\n  4:   4 14.31035\n  5:   5 13.74852\n ---             \n326: 326 13.04455\n327: 327 12.79717\n328: 328 12.79717\n329: 329 13.67179\n330: 330 12.02051\n\n\nThis is bagging of many regression trees.\n\n\n6.2.2 Random Forest (RF)\nNow, let’s take a look at the individual estimates of \\(y\\) for the first observation from the bagging process we just implemented.\n\ny_estimates[id == 1, ]\n\n    id    y_hat tree\n 1:  1 15.15792    1\n 2:  1 15.04446    2\n 3:  1 15.21238    3\n 4:  1 15.07311    4\n 5:  1 14.92840    5\n 6:  1 14.98571    6\n 7:  1 15.12060    7\n 8:  1 15.14064    8\n 9:  1 15.05676    9\n10:  1 15.19375   10\n\n\nHmm, the estimates look very similar. Actually, that is not just of the observations with id == 1. This is because the trained trees are very similar for many reasons, and the trees are highly “positively” correlated with each other. From our very simple experiment above, we know that the power of bagging is not very high when that is the case.\nRF introduces additional uncertainty to the process to make trees less correlated with each other (decorrelate trees). Specifically, for any leave of any tree, they consider only a randomly select subset of the explanatory variables when deciding how to split a leave. A typical choice of the number of variables considered at each split is \\(\\sqrt{K}\\), where \\(K\\) is the number of the explanatory variables specified by the user. In the naive example above, all \\(K\\) variables are considered for all the split decisions of all the trees. Some variables are more influential than others and they get to be picked as the splitting variable at similar places, which can result in highly correlated trees. Instead, RF gives other variables a chance, which helps decorrelate the trees. This means that the tree we build in RF is not deterministic. Depending on which variables are selected for consideration in splitting, the tree will be different even if you use the same bootstrapped dataset.\nLet’s code the process of building a tree for RF. We first bootstrap a dataset.\n\nn_obs <- nrow(mlb1_dt)\n\nboot_data <- mlb1_dt[sample(1:n_obs, n_obs, replace = TRUE), ]\n\nLet’s now build a tree using boot_data. We first split the entire dataset into two.\nWe can use get_rss_by_var() we wrote earlier, which gets us RSS-minimizing threshold and the minimized RSS value for a single variable. Earlier when we build a regression tree, we looped over all the explanatory variables, which are hruns, years, rbisyr, allstar, runsyr, hits, and bavg. But, when building a tree in RF, you can choose to select just a subset of the variables. Here, let’s randomly select \\(\\sqrt{K}\\) variables. So, rounding \\(\\sqrt{7}\\), we have three.\n\nvar_list <- c(\"hruns\", \"years\", \"rbisyr\", \"allstar\", \"runsyr\", \"hits\", \"bavg\")\nK <- length(var_list)\nK_for_split <- sqrt(K) %>% round()\n\nWe randomly select three variables among the list of variables (var_list).\n\n(\nvars_for_split <- sample(var_list, K_for_split, replace = FALSE)\n)\n\n[1] \"years\"   \"bavg\"    \"allstar\"\n\n\nYou only consider these 3 variables in this splitting process.\n\n(\nmin_rss_by_var <-\n  lapply(\n    vars_for_split,\n    function(x) get_rss_by_var(x, mlb1_dt)\n  ) %>% \n  rbindlist()\n)\n\n        rss var_name  var_value\n1: 249.9090    years   3.500000\n2: 375.0281     bavg 252.499992\n3: 277.3388  allstar   7.417582\n\n\nSo, our choice of split criteria is\n\nmin_rss_by_var[which.min(rss), ]\n\n       rss var_name var_value\n1: 249.909    years       3.5\n\n\nYou will repeat this process for any splits you consider until you met the stopping criteria.\nBy the way, if we were to use all the variables instead of just these three, then we would have use the following criteria.\n\n(\n  lapply(\n    var_list,\n    function(x) get_rss_by_var(x, mlb1_dt)\n  ) %>% \n  rbindlist() %>% \n  .[which.min(rss), ]\n)\n\n        rss var_name var_value\n1: 205.1488     hits     356.5\n\n\n\n\n6.2.3 Implementation\nWe can use ranger() from the ranger package to train an RF model.\n\n\nAnother compelling R package for RF is the randomForest package.\nThe ranger() function has many options you can specify that determine how trees are built. Here are some of the important ones (see here for the complete description of the hyper-parameters.):\n\nmtry: the number of variables considered in each split (default is the square root of the total numbers of explanatory variables rounded down.)\nnum.trees: the number of tree to be built (default is 500)\nmin.node.size: minimum number of observations in each node (default varies based on the the type of analysis)\nreplace: where sample with or without replacement when bootstrapping samples (default is TRUE)\nsample.fraction: the fraction of the entire observations that are used in each tree (default is 1 if sampling with replacement, 0.632 if sampling without replacement)\n\nLet’s try fitting an RF with ranger() with the default parameters.\n\n#=== load the package ===#\nlibrary(ranger)\n\n\nAttaching package: 'ranger'\n\n\nThe following object is masked from 'package:rattle':\n\n    importance\n\n#=== fit and RF ===#\n(\nrf_fit <- \n  ranger(\n    lsalary ~ hruns + years + rbisyr + allstar + runsyr + hits + bavg, \n    data = mlb1_dt\n  )\n)\n\nRanger result\n\nCall:\n ranger(lsalary ~ hruns + years + rbisyr + allstar + runsyr +      hits + bavg, data = mlb1_dt) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      330 \nNumber of independent variables:  7 \nMtry:                             2 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       0.3640505 \nR squared (OOB):                  0.7308852 \n\n\n\n\nSince we have many trees, it is no longer possible to have a nice graphical representation of the trained RF model like we did with a regression tree.\nIn the output, you can see OOB prediction error (MSE). OOB stands for  out-of-bag. When bootstrapping, some of the train data will not be used to build a tree.\n\n#=== bootstrapped data ===#\nboot_data <- mlb1_dt[sample(1:n_obs, n_obs, replace = TRUE), ]\n\n#=== which rows (observations) from the original datasets are missing? ===#\nmlb1_dt[, id %in% unique(boot_data$id)] %>% mean()\n\n[1] 0.630303\n\n\nSo, only \\(65\\%\\) of the rows from the original data (mlb1_dt) in this bootstrapped sample (many duplicates of the original observations). The observations that are NOT included in the bootstrapped sample is called out-of-bag observations. This provides a great opportunity to estimate test MSE while training an RF model! For a given regression tree, you can apply it to the out-of-bag samples to calculate MSE. You can repeat this for all the trees and average the MSEs, effectively conducting cross-validation. When the number of trees is large enough, OOB MSE is almost equivalent to MSE from LOOCV (James et al., n.d.). This means that we can tune hyper-parameters by comparing OOB MSEs under different sets of hyper-parameter values.\nYou can use a simple grid-search to find the best hyper-parameter values. Grid-search is simply a brute-force optimization methods that goes through all the combinations of hyper-parameters and see which combination comes at the top. The computational intensity of grid-search depends on how many hyper-parameters you want to vary and how many values you would like to look at for each of the hyper-parameters. Here, let’s tune mtry, min.node.size, and sample.fraction.\n\n#=== define set of values you want to look at ===#\nmtry_seq <- c(2, 4, 7)\nmin_node_size_seq <- c(2, 5, 10)\nsample_fraction_seq <- c(0.5, 0.75, 1)\n\n#=== create a complete combinations of the three parameters ===#\n(\nparameters <-\n  data.table::CJ(\n    mtry = mtry_seq,\n    min_node_size = min_node_size_seq,\n    sample_fraction = sample_fraction_seq\n  )\n)\n\n    mtry min_node_size sample_fraction\n 1:    2             2            0.50\n 2:    2             2            0.75\n 3:    2             2            1.00\n 4:    2             5            0.50\n 5:    2             5            0.75\n 6:    2             5            1.00\n 7:    2            10            0.50\n 8:    2            10            0.75\n 9:    2            10            1.00\n10:    4             2            0.50\n11:    4             2            0.75\n12:    4             2            1.00\n13:    4             5            0.50\n14:    4             5            0.75\n15:    4             5            1.00\n16:    4            10            0.50\n17:    4            10            0.75\n18:    4            10            1.00\n19:    7             2            0.50\n20:    7             2            0.75\n21:    7             2            1.00\n22:    7             5            0.50\n23:    7             5            0.75\n24:    7             5            1.00\n25:    7            10            0.50\n26:    7            10            0.75\n27:    7            10            1.00\n    mtry min_node_size sample_fraction\n\n\nIn total, we have 27 (\\(3 \\times 3 \\times 3\\)) cases. You can see how quickly the number of cases increases as you increase the number of parameters to tune and the values of each parameter. We can now loop over the rows of this parameter data (parameters) and get OOB MSE for each of them.\n\noob_mse_all <-\n  lapply(\n    seq_len(nrow(parameters)),\n    function(x) {\n\n      #=== Fit the mode ===#\n      rf_fit <- \n        ranger(\n          lsalary ~ hruns + years + rbisyr + allstar + runsyr + hits + bavg, \n          data = mlb1_dt,\n          num.trees = 1000,\n          mtry = parameters[x, mtry],\n          min.node.size = parameters[x, min_node_size],\n          sample.fraction = parameters[x, sample_fraction]\n        )\n\n      #=== return OOB SME ===#\n      return(rf_fit$prediction.error)\n      \n    }\n  ) %>% \n  unlist()\n\n#=== assign OOB MSE to the parameters data ===#\nparameters[, oob_mse := oob_mse_all]\n\n#=== take a look ===#\nparameters\n\n    mtry min_node_size sample_fraction   oob_mse\n 1:    2             2            0.50 0.3611884\n 2:    2             2            0.75 0.3622574\n 3:    2             2            1.00 0.3701546\n 4:    2             5            0.50 0.3601237\n 5:    2             5            0.75 0.3634168\n 6:    2             5            1.00 0.3665383\n 7:    2            10            0.50 0.3618498\n 8:    2            10            0.75 0.3621112\n 9:    2            10            1.00 0.3662482\n10:    4             2            0.50 0.3626452\n11:    4             2            0.75 0.3668244\n12:    4             2            1.00 0.3735296\n13:    4             5            0.50 0.3630609\n14:    4             5            0.75 0.3705019\n15:    4             5            1.00 0.3742656\n16:    4            10            0.50 0.3610209\n17:    4            10            0.75 0.3623941\n18:    4            10            1.00 0.3661138\n19:    7             2            0.50 0.3737602\n20:    7             2            0.75 0.3766916\n21:    7             2            1.00 0.3827657\n22:    7             5            0.50 0.3697100\n23:    7             5            0.75 0.3725411\n24:    7             5            1.00 0.3822502\n25:    7            10            0.50 0.3695385\n26:    7            10            0.75 0.3699830\n27:    7            10            1.00 0.3800858\n    mtry min_node_size sample_fraction   oob_mse\n\n\nSo, the best choice among the ones tried is:\n\nparameters[which.min(oob_mse), ]\n\n   mtry min_node_size sample_fraction   oob_mse\n1:    2             5             0.5 0.3601237"
  },
  {
    "objectID": "P01-random-forest.html#over-fitting",
    "href": "P01-random-forest.html#over-fitting",
    "title": "6  Random Forest",
    "section": "6.3 Over-fitting",
    "text": "6.3 Over-fitting"
  },
  {
    "objectID": "P01-random-forest.html#resources",
    "href": "P01-random-forest.html#resources",
    "title": "6  Random Forest",
    "section": "6.4 Resources",
    "text": "6.4 Resources\n\nGradient Boosting with Scikit-Learn, XGBoost, LightGBM, and CatBoost by Jason Brownlee\nA Gentle Introduction to XGBoost for Applied Machine Learning\n\n\n\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. n.d. An Introduction to Statistical Learning. Vol. 112. Springer."
  },
  {
    "objectID": "P02-boosted-regression-forest.html",
    "href": "P02-boosted-regression-forest.html",
    "title": "7  Boosted Regression Forest",
    "section": "",
    "text": "Packages to load for replication\n\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(rpart)\nlibrary(rattle)\nlibrary(wooldridge)\n\nDataset for replication\n\n#=== get data ===#\ndata(mlb1)\n\nmlb1_dt <- \n  mlb1 %>% \n  data.table() %>% # turn into data.table \n  .[, salary := NULL] %>% # remove salary (use lsalary instead)\n  na.omit() # remove observations with NA in any of the variables\n\nIn training RF that uses the idea of bagging, the original data is used to generate many bootstrapped datasets, a regression tree is trained on each of them  independently , and then they are averaged when prediction. Boosting is similar to bagging (bootstrap aggregation) in that it trains many statistical models and then combine them. However, instead of training models independently, it trains models  sequentially  in a manner that improves prediction step by step.\nWhile there are many variants of boosting methods (see Chapter 10 of Hastie et al. (2009)), we will look at gradient boosting using trees for regression in particular (Algorithm 10.3 in Hastie et al. (2009) presents the generic gradient tree boosting algorithm), where squared error is used as the loss function.\n\nSet \\(f_0(X_i) = \\frac{\\sum_{i=1}^N y_i}{N}\\) for all \\(i = 1, \\dots, N\\)\nFor b = 1 to B,\n\n\nFor \\(i = 1, \\dots, N\\), calculate \\[\n    r_{i,b} =  (y_i - f_{b-1}(X_i))\n    \\]\nFit a regression tree to \\(r_{i, b}\\), which generates terminal regions \\(R_{j,b}\\), \\(j = 1, \\dots, J\\), and denote the predicted value of region \\(R_{j,b}\\) as \\(\\gamma_{j,b}\\).\nSet \\(f_b(X_i) = f_{b-1}(X_i) + \\lambda \\cdot \\sum_{j=1}^J\\gamma_{j, b}\\cdot I(X_i \\in R_{j,b})\\)\n\n\nFinally, \\(\\hat{f}(X_i) = f_B(X_i)\\)\n\nLet’s try to go through this algorithm a bit to have it sink in for you.\n Step 1 \nStep 1 finds the mean of the dependent variable. This quantity is used as the starting estimate for the dependent variable.\n\n(\nf_0 <- mean(mlb1_dt$lsalary)\n)\n\n[1] 13.51172\n\n\n Step 2\n\\(b = 1\\)\nNow, we get residuals:\n\nmlb1_dt[, resid_1 := lsalary - f_0]\n\nThe residuals contain information in lsalary that was left unexplained by simply using the mean of lsalary. By training a regression tree using the residuals as the dependent variable, we are finding a tree that can explain the unexplained parts of lsalary using the explanatory variables.\n\ntree_fit_b1 <- \n  rpart(\n    resid_1 ~ ., # . means all variables\n    data = mlb1_dt \n  )\n\nHere is the fitted value of the residuals (\\(\\sum_{j=1}^J\\gamma_{j, b}\\cdot I(X_i \\in R_{j,b})\\))\n\nresid_1_hat <- predict(tree_fit_b1, newdata = mlb1_dt)\nhead(resid_1_hat)\n\n         1          2          3          4          5          6 \n 1.7134881  1.7134881  1.2414996  1.2414996  0.5054178 -0.1851016 \n\n\nNow, we update our prediction according to \\(f_b(X_i) = f_{b-1}(X_i) + \\lambda \\cdot \\sum_{j=1}^J\\gamma_{j, b}\\cdot I(X_i \\in R_{j,b})\\). We set \\(\\lambda\\) to be \\(0.2\\) in this illustration.\n\nlambda <- 0.2\nf_1 <- f_0 + lambda * resid_1_hat\nhead(f_1)\n\n       1        2        3        4        5        6 \n13.85441 13.85441 13.76002 13.76002 13.61280 13.47470 \n\n\nDid we actually improve prediction accuracy? Let’s compare f_0 and f_1.\n\nsum((mlb1_dt$lsalary - f_0)^2)\n\n[1] 445.0615\n\nsum((mlb1_dt$lsalary - f_1)^2)\n\n[1] 288.3205\n\n\nGreat. Let’s move on to \\(b = 2\\).\n\n#=== get negative of the residuals ===#\nmlb1_dt[, resid_2 := lsalary - f_1]\n\n#=== fit a regression tree ===#\ntree_fit_b2 <- \n  rpart(\n    resid_2 ~ ., # . means all variables\n    data = mlb1_dt \n  )\n\n#=== get predicted values ===#\nresid_2_hat <- predict(tree_fit_b2, newdata = mlb1_dt)\n\n#=== update ===#\nf_2 <- f_1 + lambda * resid_2_hat\n\n\nsum((mlb1_dt$lsalary - f_1)^2)\n\n[1] 288.3205\n\nsum((mlb1_dt$lsalary - f_2)^2)\n\n[1] 186.9229\n\n\nWe further improved our predictions. We repeat this process until certain user-specified stopping criteria is met.\nAs you probably have noticed, there are several key parameters in the process above that controls the performance of gradient boosting forest. \\(\\lambda\\) controls the speed of learning. The lower \\(\\lambda\\) is, slower the learning speed is. \\(B\\) (the number of trees) determines how many times we want to make small improvements to the original prediction. When you increase the value of \\(\\lambda\\), you should decrease the value of \\(B\\). Too high values of \\(\\lambda\\) and \\(B\\) can lead to over-fitting.\nYou may have been wondering why this algorithm is called Gradient boosting. Gradient boosting is much more general than the one described here particularly for gradient tree boosting for regression. It can be applied to both regression and classification1. In general, Step 2.a can be written as follows:\n\\[\nr_{i,b} = - \\huge[\\normalsize\\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)}\\huge]\\normalsize_{f = f_{b-1}}\n\\]\nwhere \\(L(y_i, f(x_i))\\) is the loss function. For regression, the loss function is almost always squared error: \\((y_i - f(x_i))^2\\). For, \\(L(y_i, f(x_i)) = (y_i - f(x_i))^2\\), the negative of the derivative of the loss function with respect to \\(f(x_i)\\) is\n\\[\n- \\huge[\\normalsize\\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)}\\huge]\\normalsize_{f = f_{b-1}} = - (- 2 (y_i - f(x_i))) = 2 (y_i - f(x_i))\n\\]\nThis is why we have \\(r_{i,b} = (y_i - f_{b-1}(X_i))\\) at Step 2.a. And, as you just saw, we are using the gradient of the loss function for model updating, which is why it is called  gradient  boosting. Note that it does not really matter whether you have \\(2\\) in front of the residuals or not the fitted residuals is multiplied (scaled) by \\(\\lambda\\) to when updating the model. You can always find the same \\(\\lambda\\) that would result in the same results as when just non-scaled residuals are used.\nMost R and python packages allow you to use a fraction of the train sample that are randomly selected and/or to use a subset of the included variables in building a tree within Step 2. This generate randomness in the algorithm and they are referred to as  stochastic gradient boosting."
  },
  {
    "objectID": "P02-boosted-regression-forest.html#implementation",
    "href": "P02-boosted-regression-forest.html#implementation",
    "title": "7  Boosted Regression Forest",
    "section": "7.2 Implementation",
    "text": "7.2 Implementation\nWe can use the gbm package to train a gradient boosting regression. Just like ranger(), gbm takes formula and data like below.\n\nlibrary(gbm)\n\nLoaded gbm 2.1.8\n\n#=== fit a gbm model ===#\ngbm_fit <- \n  gbm(\n    lsalary ~ hruns + years + rbisyr + allstar + runsyr + hits + bavg, \n    data = mlb1_dt \n  )\n\nDistribution not specified, assuming gaussian ...\n\n\nHere is the list of some parameters to be aware of:\n\nn.trees: Number of trees (\\(B\\)). Default is \\(100\\).\ninteraction.depth: 1 implies an additive model without interactions between included variables2, 2 implies a model with 2-way interactions. Default is 1.\nn.minobsinnode: Minimum number of observations in a terminal node (leaf).\nshrinkage: Learning rate (\\(\\lambda\\)). Default is 0.1.\nbag.fraction: The fraction of the train data observations that are select randomly in building a tree. Default is 0.5.\ncv.folds: The number of folds in conducting KCV\n\nBy specifying cv.folds, gbm() automatically conducts cross-validation for you.\n\n#=== gbm fit with CV ===#\ngbm_fit <- \n  gbm(\n    lsalary ~ hruns + years + rbisyr + allstar + runsyr + hits + bavg, # . means all variables\n    data = mlb1_dt,\n    cv.folds = 5,\n  )\n\nDistribution not specified, assuming gaussian ...\n\n#=== see the MSE history ===#  \ngbm_fit$cv.error\n\n  [1] 1.2253902 1.1116156 1.0205298 0.9433985 0.8753336 0.8117850 0.7593895\n  [8] 0.7155764 0.6749629 0.6405197 0.6118801 0.5826483 0.5540288 0.5337625\n [15] 0.5177188 0.5007340 0.4878714 0.4747264 0.4663002 0.4583888 0.4503843\n [22] 0.4436678 0.4382948 0.4307027 0.4249498 0.4213937 0.4168023 0.4129313\n [29] 0.4095522 0.4078445 0.4061228 0.4019636 0.3991148 0.3961212 0.3944443\n [36] 0.3933995 0.3927365 0.3916205 0.3909225 0.3905965 0.3876704 0.3875484\n [43] 0.3857054 0.3856561 0.3843438 0.3845230 0.3834490 0.3820814 0.3799813\n [50] 0.3787884 0.3772388 0.3766243 0.3780853 0.3774009 0.3772533 0.3754301\n [57] 0.3748004 0.3727322 0.3717758 0.3704500 0.3697340 0.3690566 0.3700647\n [64] 0.3693901 0.3699901 0.3689645 0.3679994 0.3679133 0.3664746 0.3670761\n [71] 0.3656355 0.3664515 0.3662979 0.3662695 0.3648815 0.3648221 0.3649502\n [78] 0.3650628 0.3647017 0.3643149 0.3650453 0.3658924 0.3661806 0.3655060\n [85] 0.3644879 0.3642080 0.3645590 0.3635509 0.3638712 0.3628455 0.3619427\n [92] 0.3623487 0.3623673 0.3620047 0.3620308 0.3620440 0.3620364 0.3609548\n [99] 0.3603481 0.3612251\n\n\nYou can visualize the CV results using gbm.perf().\n\ngbm.perf(gbm_fit)\n\n\n\n\n[1] 99\n\n\nNote that it will tell you what the optimal number of trees is  given  the values of the other hyper-parameters (here default values). If you want to tune other parameters as well, you need to program it yourself.\n\n\n\n\nHastie, Trevor, Robert Tibshirani, Jerome H Friedman, and Jerome H Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Vol. 2. Springer."
  },
  {
    "objectID": "P03-xgb.html",
    "href": "P03-xgb.html",
    "title": "8  Extreme Gradient Boosting",
    "section": "",
    "text": "Extreme gradient boosting (XGB) is a variant of gradient boosting that has been extremely popular due to its superb performance. The basic concept is the same as the gradient boosting algorithm described above, however, it has its own way of building a tree, which is more mindful of avoiding over-fitting trees."
  },
  {
    "objectID": "P03-xgb.html#tree-updating-in-xgb-general",
    "href": "P03-xgb.html#tree-updating-in-xgb-general",
    "title": "8  Extreme Gradient Boosting",
    "section": "8.1 Tree updating in XGB (general)",
    "text": "8.1 Tree updating in XGB (general)\nLet \\(f_{i,b}(x_i)\\) be the prediction for the \\(i\\)th observation at the \\(b\\)-th iteration. Further, let \\(w_t(x_i)\\) is the term that is added to \\(f_{i,b}(x_i)\\) to obtain \\(f_{i,b+1}(x_i)\\). In XGB, \\(w_t(x_i)\\) is such that it minimizes the following objective:\n\\[\n\\Psi_t = \\sum_{i=1}^N [L(y_i, f_{i,b}(x_i) + w_t(x_i))] + \\Omega(w_t)\n\\tag{8.1}\\]\nwhere \\(L()\\) is the user-specified loss-function that is differentiable and \\(\\Omega(w_t)\\) is the regularization term. Instead of Equation 8.1, XGB uses the second order Taylor expansion of \\(L()\\) about \\(w\\)1.\n\\[\n\\tilde{\\Psi}_t = \\sum_{i=1}^N [L(y_i, f_{i,b}(x_i)) + g_i w_t(x_i) + \\frac{1}{2}h_i w_t(x_i)^2] + \\Omega(w_t)\n\\tag{8.2}\\]\nwhere \\(g_i = \\frac{\\partial L(y_i, p_i)}{\\partial p_i}\\) (first-order derivative) and \\(h_i = \\frac{\\partial^2 L(y_i, p_i)}{\\partial p_i^2}\\) (second-order derivative). Since \\(L(y_i, f_{i,b}(x_i))\\) is just a constant, we can safely remove it from the objective function, which leads to\n\\[\n\\tilde{\\Psi}_t = \\sum_{i=1}^N [g_i w_t(x_i) + \\frac{1}{2}h_i w_t(x_i)^2] + \\Omega(w_t)\n\\tag{8.3}\\]\nLet \\(I_j\\) denote a set of observations that belong to leaf \\(j\\) (\\(j = 1, \\dots, J\\)). Then, Equation 8.3 is written as follows:\n\\[\n\\tilde{\\Psi}_t = \\sum_{j=1}^J\\huge[\\normalsize (\\sum_{i\\in I_j}g_i)w_j + \\frac{1}{2}(\\sum_{i\\in I_j}h_i + \\lambda)w_j^2 \\huge]\\normalsize + \\gamma J\n\\tag{8.4}\\]\n\n\nRemember that all the observations in the same leaf shares the same prediction. So, for all \\(i\\)s that belong to leaf \\(j\\), the prediction is denoted as \\(w_j\\) in Equation 8.4. That is, \\(w_t(x_i)\\) that belongs to leaf \\(j\\) is \\(w_j\\).\nFor a given tree structure (denoted as \\(q(x)\\)), the leaves can be treated independently in minimizing this objective.\nTaking the derivative of \\(\\tilde{\\Psi}_t\\) w.r.t \\(w_j\\),\n\\[\n\\begin{aligned}\n(\\sum_{i\\in I_j}g_i) + (\\sum_{i\\in I_j}h_i + \\lambda)w_j = 0 \\\\\n\\Rightarrow w_j^* = \\frac{-\\sum_{i\\in I_j}g_i}{\\sum_{i\\in I_j}h_i + \\lambda}\n\\end{aligned}\n\\tag{8.5}\\]\nThe minimized value of \\(\\tilde{\\Psi}_t\\) is then (obtained by plugging \\(w_j^*\\) into Equation 8.4),\n\\[\n\\begin{aligned}\n\\tilde{\\Psi}_t(q)^* & = \\sum_{j=1}^J\\huge[\\normalsize (\\sum_{i\\in I_j}g_i)\\frac{-\\sum_{i\\in I_j}g_i}{\\sum_{i\\in I_j}h_i + \\lambda} + \\frac{1}{2}(\\sum_{i\\in I_j}h_i + \\lambda)(\\frac{-\\sum_{i\\in I_j}g_i}{\\sum_{i\\in I_j}h_i + \\lambda})^2 \\huge]\\normalsize + \\gamma J \\\\\n& = \\sum_{j=1}^J\\huge[\\normalsize \\frac{-(\\sum_{i\\in I_j}g_i)^2}{\\sum_{i\\in I_j}h_i + \\lambda} + \\frac{1}{2}\\frac{(\\sum_{i\\in I_j}g_i)^2}{\\sum_{i\\in I_j}h_i + \\lambda} \\huge]\\normalsize + \\gamma J \\\\\n& = -\\frac{1}{2} \\sum_{j=1}^J \\huge[\\normalsize\\frac{(\\sum_{i\\in I_j}g_i)^2}{\\sum_{i\\in I_j}h_i + \\lambda}\\huge]\\normalsize + \\gamma J\n\\end{aligned}\n\\tag{8.6}\\]\nFor rotational convenience, we call \\(\\frac{(\\sum_{i\\in I_j}g_i)^2}{\\sum_{i\\in I_j}h_i + \\lambda}\\) quality score and denote it by \\(Q_j\\) ( Quality score for leaf \\(j\\)).\nWe could find the best tree structure by finding \\(w_j^*(q)\\) according to Equation 8.4 and calculate \\(\\tilde{\\Psi}_t(q)^*\\) according to Equation 8.6 for each of all the possible tree structures, and then pick the tree structure q(x) that has the lowest \\(\\tilde{\\Psi}_t(q)^*\\).\nHowever, it is impossible to consider all possible tree structures practically. So, a greedy (myopic) approach that starts from a single leaf and iteratively splits leaves is used instead.\nConsider splitting an existing leaf \\(s\\) (where in the tree it may be located) into two leaves \\(L\\) and \\(R\\) when there are \\(J\\) existing leaves. Then, we find \\(w_j^*\\) and calculate \\(\\tilde{\\Psi}_t(q)^*\\) for each leaf, and the resulting minimized objective is\n\\[\n-\\frac{1}{2} \\huge[\\normalsize Q_L + Q_R + \\Gamma \\huge]\\normalsize + \\gamma(J+1)\n\\]\nwhere \\(\\Gamma\\) is the sum of quality scores for all the leaves except \\(L\\) and \\(R\\).\n\n\n\\[\n\\Gamma = \\sum_{j\\ne \\{L, R\\}}^J Q_j\n\\]\nThe minimized objective before splitting is\n\\[\n-\\frac{1}{2} \\huge[\\normalsize Q_s + \\Gamma \\huge]\\normalsize + \\gamma J\n\\]\nSo, the reduction  in loss after the split is\n\\[\nG(s, L, R) = \\frac{1}{2} \\huge[\\normalsize Q_L + Q_R - Q_s \\huge]\\normalsize - \\gamma\n\\]\nLet’s call \\(G(s, L, R)\\) simply a gain (of the split).\n\n\nA more positive value of gain (\\(G(s, L, R)\\)) means a more successful split.\nWe can try many different patterns of \\(I_L\\) and \\(I_R\\) (how to split tree \\(s\\)), calculate the gain for each of them and pick the split that has the highest gain.\n\n\nDifferent patterns of \\(I_L\\) and \\(I_R\\) arise from different variable-cutpoint combinations\nIf the highest gain is negative, then the leaf under consideration for splitting is not split.\nOnce the best tree is chosen (the tree that has the highest gain among the ones investigated), then we update our prediction based on \\(w^*\\) of the tree. For observation \\(i\\) that belongs to leaf \\(j\\) of the tree,\n\\[\n\\begin{aligned}\nf_{i,b} = f_{i,b-1} + \\eta \\cdot w_j^*\n\\end{aligned}\n\\tag{8.7}\\]\nwhere \\(\\eta\\) is the learning rate."
  },
  {
    "objectID": "P03-xgb.html#tree-updating-in-xgb-regression",
    "href": "P03-xgb.html#tree-updating-in-xgb-regression",
    "title": "8  Extreme Gradient Boosting",
    "section": "8.2 Tree updating in XGB (regression)",
    "text": "8.2 Tree updating in XGB (regression)\nWe now make the general tree updating algorithm specific to regression problems, where the loss function is squared error: \\(L(y_i, p_i) = \\frac{1}{2}(y_i - p_i)^2\\), where \\(p_i\\) is the predicted value for \\(i\\).\n\nFirst, let’s find \\(g_i\\) and \\(h_i\\) for \\(L(y_i, p_i) = \\frac{1}{2}(y_i - p_i)^2\\).\n\\[\n\\begin{aligned}\ng_i = \\frac{\\partial L(y_i, p_i)}{\\partial p_i}  = -(y_i - p_i)\\\\\nh_i = \\frac{\\partial^2 L(y_i, p_i)}{\\partial p_i^2} = 1 \\\\\n\\end{aligned}\n\\]\nSo, \\(g_i\\) is simply the negative of the residual for \\(i\\).\nNow, suppose you are at iteration \\(b\\) and the predicted value for \\(i\\) is denoted as \\(f_{i,b}(x_i)\\). Further, let \\(r_{i,b}\\) denote the residual (\\(y_i - f_{i,b}(x_i)\\)).\nPlugging these into Equation 8.5,\n\\[\n\\begin{aligned}\nw_j^* & = \\frac{\\sum_{i\\in I_j}r_{i,b}}{\\sum_{i\\in I_j}1 + \\lambda} \\\\\n      & = \\frac{\\sum_{i\\in I_j}r_{i,b}}{N_j + \\lambda}\n\\end{aligned}\n\\tag{8.8}\\]\nThat is, for a given leaf \\(j\\), the optimal predicted value (\\(w_j^*\\)) is the sum of the residuals of all the observations in leaf \\(j\\) divided by the number of observations in leaf \\(j\\) plus \\(\\lambda\\). When \\(\\lambda = 0\\), the optimal predicted value (\\(w_j^*\\)) is simply the mean of the residuals.\nThe quality score for leaf \\(j\\) is then,\n\\[\nQ_j = \\frac{(\\sum_{i\\in I_j}r_{i,b})^2}{N_j + \\lambda}\n\\tag{8.9}\\]"
  },
  {
    "objectID": "P03-xgb.html#illustration-of-xgb-for-regression",
    "href": "P03-xgb.html#illustration-of-xgb-for-regression",
    "title": "8  Extreme Gradient Boosting",
    "section": "8.3 Illustration of XGB for regression",
    "text": "8.3 Illustration of XGB for regression\n\n\nPackages to load for replication\n\nlibrary(tidyverse)\nlibrary(data.table)\n\nIn order to further our understanding of the entire XGB algorithm, let’s take a look at a simple regression problem as an illustration. We consider a four-observation data as follows:\n\n(\ndata <-\n  data.table(\n    y = c(-3, 7, 8, 12),\n    x = c(1, 4, 6, 8)\n  )\n)\n\n    y x\n1: -3 1\n2:  7 4\n3:  8 6\n4: 12 8\n\n\n\n(\ng_0 <-\n  ggplot(data) +\n  geom_point(aes(y = y, x = x))\n)\n\n\n\n\nFirst step (\\(b = 0\\)) is to make an initial prediction. This can be any number, but let’s use the mean of y and set it as the predicted value for all the observations.\n\n(\nf_0 <- mean(data$y) # f_0: the predicted value for all the observations\n)\n\n[1] 6\n\n\nLet’s set \\(\\gamma\\), \\(\\lambda\\), and \\(\\eta\\) to \\(10\\), \\(1\\), and \\(0.3\\), respectively.\n\ngamma <- 10\nlambda <- 1\neta <- 0.3\n\nWe have a single-leaf tree at the moment. And the quality score for this leaf is\n\n\nquality score for leaf \\(j\\) is \\(\\frac{(\\sum_{i\\in I_j}r_{i,b})^2}{N_j + \\lambda}\\)\n\n#=== get residuals ===#\ndata[, resid := y - f_0]\n\n#=== get quality score ===#\n(\nq_0 <- (sum(data$resid))^2/(nrow(data) + lambda)\n)\n\n[1] 0\n\n\nQuality score of the leaf is 0.\n\n\nSince we are using the mean of \\(y\\) as the prediction, of course, the sum of the residuals is zero, which then means that the quality score is zero.\nNow, we have three potential to split patterns: {x, 2}, {x, 5}, {x, 7}.\n\n\n{x, 2} means the leaf is split into two leaves: \\({x | x <2}\\) and \\({x | x >= 2}\\). Note that any number between \\(1\\) and \\(4\\) will result in the same split results.\nLet’s consider them one by one.\n\n8.3.0.1  Split: {x, 2} \nHere is the graphical representations of the split:\n\n\nCode\ng_0 +\n  geom_vline(xintercept = 2, color = \"red\") +\n  annotate(\"text\", x = 1.25, y = 6, label = \"leaf L\", color = \"red\") +\n  annotate(\"text\", x = 5, y = 6, label = \"leaf R\", color = \"red\")\n\n\n\n\n\n\n\n\n\nCode\nDiagrammeR::grViz(\n\"\ndigraph {\n  graph [ranksep = 0.2]\n  node [shape = box]\n    T1R [label = 'L: -9']\n    T1L [label = 'R: 1 , 2 , 6']\n    T0 [label = '-9, 1 , 2 , 6']\n  edge [minlen = 2]\n    T0->T1L\n    T0->T1R\n  { rank = same; T1R; T1L}\n}\n\"\n)\n\n\n\n\n\n\nLet’s split the data.\n\n#=== leaf L ===#\n(\ndata_L_1 <- data[x < 2, ]\n)\n\n    y x resid\n1: -3 1    -9\n\n#=== leaf R ===#\n(\ndata_R_1 <- data[x >= 2, ]\n)\n\n    y x resid\n1:  7 4     1\n2:  8 6     2\n3: 12 8     6\n\n\nUsing Equation 8.8,\n\n\n\\(w_j^* = \\frac{\\sum_{i\\in I_j}r_{i,b}}{N_j + \\lambda}\\)\n\nw_L <- (sum(data_L_1$resid))/(nrow(data_L_1) + lambda)\nw_R <- (sum(data_R_1$resid))/(nrow(data_R_1) + lambda)\n\n\\[\n\\begin{aligned}\nw_L^* & = -9 / (1 + 1) = -4.5 \\\\\nw_R^* & = 1 + 2 + 6 / (3 + 1) = 2.25\n\\end{aligned}\n\\]\nUsing Equation 8.9, the quality scores for the leaves are\n\n\n\\(Q_j = \\frac{(\\sum_{i\\in I_j}r_{i,b})^2}{N_j + \\lambda}\\)\n\nq_L <- (sum(data_L_1$resid))^2/(nrow(data_L_1) + lambda)\nq_R <- (sum(data_R_1$resid))^2/(nrow(data_R_1) + lambda)\n\n\n\n\n\nCode\nDiagrammeR::grViz(\n  paste0(\n  \"\n  digraph {\n    graph [ranksep = 0.2]\n    node [shape = box]\n      T1R [label = 'L: -9 \\n Q score = \", round(q_L, digits = 2), \"']\n      T1L [label = 'R: 1 , 2 , 6 \\n Q score = \", round(q_R, digits = 2), \"']\n      T0 [label = '-9, 1 , 2 , 6']\n    edge [minlen = 2]\n      T0->T1L\n      T0->T1R\n    { rank = same; T1R; T1L}\n  }\n  \"\n  )\n\n)\n\n\n\n\n\n\n\\[\n\\begin{aligned}\nq_L^* & = (-9)^2 / (1 + 1) = 40.5 \\\\\nq_R^* & = (1 + 2 + 6)^2 / (3 + 1) = 20.25\n\\end{aligned}\n\\]\nNotice that residuals are first summed and then squared in the denominator of the quality score (the higher, the better). This means that if the prediction is off in the same direction (meaning they are similar) among the observations within the leaf, then the quality score is higher. On the other hand, if the prediction is off in both directions (meaning they are not similar), then the residuals cancel each other out, resulting in a lower quality score. Since we would like to create leaves consisting of similar observations, a more successful split has a higher quality score.\nFinally, the gain of this split is\n\n\n\\[\nG(s, L, R) = \\frac{1}{2} \\huge[\\normalsize Q_L + Q_R - Q_s \\huge]\\normalsize - \\gamma\n\\]\nwhere \\(s\\) is the leaf before split, \\(L\\) and \\(R\\) are leaves after the split of leaf \\(s\\).\n\ngain_1 <- (q_L + q_R - q_0)/2 - gamma\n\n\\[\nG_1 = \\frac{40.5 + 20.25 - 0}{2} - 10 = 20.375\n\\]\nNow that we have gone through the process of finding update value (\\(w\\)), quality score (\\(q\\)), and gain (\\(G\\)) for a given split structure, let’s write a function that returns the values of these measures by feeding the cutpoint before moving onto the next split candidate.\n\nget_info <- function(data, cutpoint, lambda, gamma)\n{\n  q_0 <- (sum(data$resid))^2/(nrow(data) + lambda)\n\n  data_L <- data[x < cutpoint, ]\n  data_R <- data[x >= cutpoint, ]\n\n  w_L <- (sum(data_L$resid))/(nrow(data_L) + lambda)\n  w_R <- (sum(data_R$resid))/(nrow(data_R) + lambda)\n\n  q_L <- (sum(data_L$resid))^2/(nrow(data_L) + lambda)\n  q_R <- (sum(data_R$resid))^2/(nrow(data_R) + lambda)\n\n  gain <- (q_L + q_R - q_0)/2 - gamma\n\n  return(list(\n    w_L = w_L, \n    w_R = w_R, \n    q_L = q_L, \n    q_R = q_R, \n    gain = gain \n  ))\n}\n\n\n\n8.3.0.2  Split: {x, 5} \n\nmeasures_2 <- get_info(data, 5, lambda, gamma)\n\n\n\nCode\ng_0 +\n  geom_vline(xintercept = 5, color = \"red\") +\n  annotate(\"text\", x = 3, y = 6, label = \"leaf L\", color = \"red\") +\n  annotate(\"text\", x = 7, y = 6, label = \"leaf R\", color = \"red\")\n\n\n\n\n\n\n\n\n\nCode\nDiagrammeR::grViz(\n  paste0(\n    \"\n    digraph {\n      graph [ranksep = 0.2]\n      node [shape = box]\n        T1R [label = 'L: -9, 1 \\n Q score = \", round(measures_2$q_L, digits = 2), \"']\n        T1L [label = 'R: 2 , 6 \\n Q score = \", round(measures_2$q_R, digits = 2), \"']\n        T0 [label = '-9, 1 , 2 , 6']\n      edge [minlen = 2]\n        T0->T1L\n        T0->T1R\n      { rank = same; T1R; T1L}\n    }\n    \"\n  )\n)\n\n\n\n\n\n\n\\[\n\\begin{aligned}\nq_L^* & = (-9)^2 / (2 + 1) = 21.33 \\\\\nq_R^* & = (1 + 2 + 6)^2 / (2 + 1) = 21.33\n\\end{aligned}\n\\]\n\\[\nG_2 = \\frac{21.33 + 21.33 - 0}{2} - 10 = 11.3333333\n\\]\n\n\n8.3.0.3  Split: {x, 7} \n\nmeasures_3 <- get_info(data, 7, lambda, gamma)\n\n\n\nCode\ng_0 +\n  geom_vline(xintercept = 7, color = \"red\") +\n  annotate(\"text\", x = 4, y = 6, label = \"leaf L\", color = \"red\") +\n  annotate(\"text\", x = 8, y = 6, label = \"leaf R\", color = \"red\")\n\n\n\n\n\n\n\n\n\nCode\nDiagrammeR::grViz(\n  paste0(\n    \"\n    digraph {\n      graph [ranksep = 0.2]\n      node [shape = box]\n        T1R [label = 'L: -9, 1, 2 \\n Q score = \", round(measures_3$q_L, digits = 2), \"']\n        T1L [label = 'R: 6 \\n Q score = \", round(measures_3$q_R, digits = 2), \"']\n        T0 [label = '-9, 1 , 2 , 6']\n      edge [minlen = 2]\n        T0->T1L\n        T0->T1R\n      { rank = same; T1R; T1L}\n    }\n    \"\n  )\n)\n\n\n\n\n\n\n\\[\n\\begin{aligned}\nq_L^* & = (-9)^2 / (1 + 1) = 9 \\\\\nq_R^* & = (1 + 2 + 6)^2 / (3 + 1) = 18\n\\end{aligned}\n\\]\n\\[\nG_3 = \\frac{9 + 18 - 0}{2} - 10 = 3.5\n\\]\nAmong all the splits we considered, the first case (Split: {x, 2}) has the highest score. This is easy to confirm visually and shows picking a split based on the gain measure indeed makes sense.\nNow we consider how to split leaf R (leaf L cannot be split further as it has only one observation). We have two split candidates: {x, 5} and {x, 7}. Let’s get the gain measures using get_info().\n\n#=== first split ===#\nget_info(data_R_1, 5, lambda, gamma)$gain \n\n[1] -9.208333\n\n#=== second split ===#\nget_info(data_R_1, 7, lambda, gamma)$gain\n\n[1] -9.625\n\n\nSo, neither of the splits has a positive gain value. Therefore, we do not adopt either of the splits. For this iteration (\\(b=1\\)), this is the end of tree building.\n\n\n\n\n\n\nNote\n\n\n\nIf the value of \\(\\gamma\\) is lower (say, 0), then we would have adopted the second split.\n\nget_info(data_R_1, 5, lambda, 0)$gain # first split\n\n[1] 0.7916667\n\nget_info(data_R_1, 7, lambda, 0)$gain # second split\n\n[1] 0.375\n\n\nAs you can see, a higher value of \\(\\gamma\\) leads to a more aggressive tree pruning.\n\n\nSo, the final tree for this iteration (\\(b = 1\\)) is\n\n\nCode\nDiagrammeR::grViz(\n  paste0(\n  \"\n  digraph {\n    graph [ranksep = 0.2]\n    node [shape = box, width = 0.3, height = 0.15, fontsize = 3, fixedsize = TRUE, penwidth = 0.2]\n      T1R [label = 'L: -9 \\n w* = \", round(w_L, digits = 2), \"']\n      T1L [label = 'R: 1 , 2 , 6 \\n w* = \", round(w_R, digits = 2), \"']\n      T0 [label = '-9, 1 , 2 , 6']\n    edge [penwidth = 0.2, arrowsize = 0.3, len = 0.3]\n      T0->T1L\n      T0->T1R\n    { rank = same; T1R; T1L}\n  }\n  \"\n  )\n\n)\n\n\n\n\n\n\nWe now use \\(w^*\\) from this tree to update our prediction according to Equation 8.7.\n\n\n\\(f_{i,b} = f_{i,b-1} + \\eta \\cdot w_j^*\\)\n\nmeasures_1 <- get_info(data, 2, lambda, gamma)\n\nSince the first observation is in \\(L\\),\n\\[\nf_{i = 1,b = 1} = 6 + 0.3 \\times -4.5 = 4.65\n\\]\nSince the second, third, and fourth observations are in \\(R\\),\n\\[\n\\begin{aligned}\nf_{i = 2,b = 1} = 6 + 0.3 \\times 2.25 = 6.68 \\\\\nf_{i = 3,b = 1} = 6 + 0.3 \\times 2.25  = 6.68\\\\\nf_{i = 4,b = 1} = 6 + 0.3 \\times 2.25 = 6.68\n\\end{aligned}\n\\]\n\ndata %>% \n  .[, f_0 := f_0] %>% \n  .[1, f_1 := f_0 + measures_1$w_L * eta] %>%\n  .[2:4, f_1 := f_0 + measures_1$w_R * eta]\n\nThe prediction updates can be seen below. Though small, we made small improvements in our prediction.\n\n\nCode\nggplot(data = data) +\n  geom_point(aes(y = y, x = x, color = \"observed\")) +\n  geom_point(aes(y = f_1, x = x, color = \"after (f1)\")) +\n  geom_point(aes(y = f_0, x = x, color = \"before (f0)\")) +\n  scale_color_manual(\n    values = \n      c(\n        \"before (f0)\" = \"blue\", \n        \"after (f1)\" = \"red\",\n        \"observed\" = \"black\"\n      ), \n    name = \"\"\n  ) +\n  geom_segment(\n    aes(y = f_0, x = x, yend = f_1, xend = x), \n    color = \"blue\",\n    arrow = arrow(length = unit(0.1, \"cm\"))\n  ) +\n  theme_bw()\n\n\n\n\n\nNow, we move on to \\(b=2\\). We first update residuals:\n\ndata[, resid := y - f_1]\n\ndata\n\n    y x  resid f_0   f_1\n1: -3 1 -7.650   6 4.650\n2:  7 4  0.325   6 6.675\n3:  8 6  1.325   6 6.675\n4: 12 8  5.325   6 6.675\n\n\nJust like at \\(b=1\\), all the possible splits are {x, 2}, {x, 5}, {x, 7}. Let’s find the gain for each split.\n\nlapply(\n  c(2, 5, 7),\n  function(x) get_info(data, x, lambda, gamma)$gain\n)\n\n[[1]]\n[1] 10.66639\n\n[[2]]\n[1] 6.267458\n\n[[3]]\n[1] 1.543344\n\n\nSo, the first split is again the best split. Should we split the right leaf, which has the observations except the first one?\n\nlapply(\n  c(5, 7),\n  function(x) get_info(data[2:3, ], x, lambda, gamma)$gain\n)\n\n[[1]]\n[1] -9.988437\n\n[[2]]\n[1] -10\n\n\nAll the splits have negative gains. So, we do not split this leaf just like at \\(b=1\\).\nSo, the final tree for this iteration (\\(b = 1\\)) is\n\nmeasures_b2 <- get_info(data, 2, lambda, gamma)\n\n#| code-fold: true\n#| fig-height: 2\n#| fig-width: 4\n\nDiagrammeR::grViz(\n  paste0(\n  \"\n  digraph {\n    graph [ranksep = 0.2]\n    node [shape = box, width = 0.4, height = 0.15, fontsize = 3, fixedsize = TRUE, penwidth = 0.2]\n      T1R [label = 'L: -8.18 \\n w* = \", round(measures_b2$w_L, digits = 2), \"']\n      T1L [label = 'R: 0.71 , 1.71 , 5.71 \\n w* = \", round(measures_b2$w_R, digits = 2), \"']\n      T0 [label = '-8.18, 0.71 , 1.71 , 5.71']\n    edge [penwidth = 0.2, arrowsize = 0.3, len = 0.3]\n      T0->T1L\n      T0->T1R\n    { rank = same; T1R; T1L}\n  }\n  \"\n  )\n\n)\n\n\n\n\n\nLet’s now update our predictions.\n\ndata %>% \n  .[1, f_2 := f_1 + measures_b2$w_L * eta] %>%  \n  .[2:4, f_2 := f_1 + measures_b2$w_R * eta] \n\n\n\nCode\nggplot(data = data) +\n  geom_point(aes(y = y, x = x, color = \"observed\")) +\n  geom_point(aes(y = f_2, x = x, color = \"f2\")) +\n  geom_point(aes(y = f_1, x = x, color = \"f1\")) +\n  geom_point(aes(y = f_0, x = x, color = \"f0\")) +\n  scale_color_manual(\n    values = \n      c(\n        \"f0\" = \"blue\", \n        \"f1\" = \"red\",\n        \"f2\" = \"red\",\n        \"observed\" = \"black\"\n      ), \n    name = \"\"\n  ) +\n  geom_segment(\n    aes(y = f_0, x = x, yend = f_1, xend = x), \n    color = \"blue\",\n    arrow = arrow(length = unit(0.1, \"cm\"))\n  ) +\n  geom_segment(\n    aes(y = f_1, x = x, yend = f_2, xend = x), \n    color = \"blue\",\n    arrow = arrow(length = unit(0.1, \"cm\"))\n  ) +\n  theme_bw()\n\n\n\n\n\nAgain, we made small improvements in our predictions. This process continues until user-specified stopping criteria is met.\n\n\n\n\n\n\nTip\n\n\n\n\n\\(\\lambda\\):\n\nA higher value of \\(\\lambda\\) leads to a lower value of prediction updates (\\(w^*\\)).\nA higher value of \\(\\lambda\\) leads to a lower value of quality score (\\(Q\\)), thus leading to a lower value of gain (\\(G\\)), which then leads to more aggressive pruning for a given value of \\(\\gamma\\).\n\n\\(\\gamma\\):\n\nA higher value of \\(\\gamma\\) leads to more aggressive pruning.\n\n\\(\\eta\\):\n\nA higher value of \\(\\eta\\) leads to faster learning."
  },
  {
    "objectID": "P03-xgb.html#implementation",
    "href": "P03-xgb.html#implementation",
    "title": "8  Extreme Gradient Boosting",
    "section": "8.4 Implementation",
    "text": "8.4 Implementation\n\nRPython\n\n\nYou can use the xgboost package to implement XGB modeling.\n\nlibrary(xgboost)\n\n\nAttaching package: 'xgboost'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\n\nThe first task is to create a class of matrix called xgb.DMatrix using the xgb.DMatrix() function. You provide the explanatory variable data matrix to the data option and the dependent variable matrix (vector) to the label option in xgb.DMatrix() like below.\nLet’s get the mlb1 data from the wooldridge package for demonstration.\n\nlibrary(wooldridge)\ndata(mlb1)\n\nmlb1_dt <- \n  mlb1 %>% \n  data.table() %>% # turn into data.table \n  .[, salary := NULL] %>% # remove salary (use lsalary instead)\n  na.omit() # remove observations with NA in any of the variables\n\n\nmlb1_dm_X <- \n  xgb.DMatrix(\n    data = as.matrix(mlb1_dt[, .(hruns, years, rbisyr, allstar, runsyr, hits, bavg)]),\n    label = as.matrix(mlb1_dt[, lsalary])\n  )\n\nWe can then use xgb.train() to train a model using the XGB algorithm.\n\nxgb_fit <-\n  xgb.train(\n    data = mlb1_dm_X, # independent variable\n    nrounds = 100, # number of iterations (trees to add)\n    eta = 1, # learning rate\n    objective = \"reg:squarederror\" # objective function\n  )"
  },
  {
    "objectID": "C00-causal-ml.html",
    "href": "C00-causal-ml.html",
    "title": "Causal Machine Learning (CML) Methods",
    "section": "",
    "text": "Unlike prediction-oriented machine learning (POML) methods, the focus of causal machine learning (CML) methods is to identify the heterogenous treatment effects of a treatment (or small number of distinct treatments).\n\\[\nTE(X) = \\theta(X)\\cdot T\n\\]\n\\(\\theta(X)\\) is the impact of the treatment when \\(T\\) is binary and marginal impact of the treatment when \\(T\\) is continuous. \\(\\theta(X)\\) is a function of attributes (\\(X\\)), meaning that the impact of the treatment varies (heterogeneous) based on the value of the attributes.\n\n\n\\(T\\) is 0 if not treated, 1 if treated.\nCML considers the following model (following the documentation of the econml Python package)\n\\[\n\\begin{aligned}\nY & = \\theta(X)\\cdot T + g(X, W) + \\varepsilon \\\\\nT & = f(X, W) + \\eta\n\\end{aligned}\n\\]\n\\(W\\) are the collection of attributes that affect \\(Y\\) along with \\(X\\) (represented by \\(g(X, W)\\)), but not as drivers of the heterogeneity in the impact of the treatment. \\(X\\) not just affects \\(Y\\) as drivers of the heterogeneity in the impact of the treatment (\\(\\theta(X)\\cdot T\\)), but also directly along with \\(W\\).\nBoth \\(X\\) and \\(W\\) are potential confounders. While we do control for them (eliminating their influence) by partialing out \\(f(X, W)\\) and \\(g(X, W)\\), the sole focus is on the estimation of \\(\\theta(X)\\). This is in stark contrast to the focus of the ML methods we have seen in earlier sections, which primarily focuses on the accurate prediction of the  level of the dependent variable, rather than how the level of the dependent variable  changes  when treated like CML methods.\n guidance on how this chapeter looks \n\nfirst DML with \\(\\theta(X)\\) is just \\(\\theta\\)\nR-leaner heterogeneous\nDoubly-robust\nCausal forest\northogonal forest"
  },
  {
    "objectID": "C01-dml.html",
    "href": "C01-dml.html",
    "title": "9  Double Machine Learning",
    "section": "",
    "text": "The most important ideas of the recent development of causal machine learning (CML) methods originate from Chernozhukov et al. (2018), which proposed Double/Debiased ML methods. In this section, we go over those key ideas that are the heart of many other important CML methods we will learn later."
  },
  {
    "objectID": "C01-dml.html#problem-setting",
    "href": "C01-dml.html#problem-setting",
    "title": "9  Double Machine Learning",
    "section": "9.1 Problem Setting",
    "text": "9.1 Problem Setting\nWe are interested in the estimating the following econometric model\n\n\nWe follow the notations of Chernozhukov et al. (2018).\n\\[\n\\begin{aligned}\ny = \\theta d + g_0(X) + \\mu \\\\\nT = m_0(X) + \\eta\n\\end{aligned}\n\\]\nYour sole interest is in estimating \\(\\theta\\): the impact of the treatment (\\(d\\)). \\(g_0(X)\\) is the impact of a collection of variables \\(X\\). \\(m_0(X)\\) expresses how \\(X\\) affects the treatment status, \\(d\\). \\(d\\) may be binary or continuous. The key assumptions here are \\(E[\\mu|X]\\) and \\(E[\\eta|X]\\)……. That is…\nNote that the treatment effect is assumed to be constant irrespective of the value of \\(X\\). So, the treatment effect is not heterogeneous. We will cover heterogeneous treatment effect estimation later.\n\n\n\n\n\n\nTip\n\n\n\n\\(g_0(X)\\) and \\(m_0(X)\\) are called nuisance functions because we are not interested in understanding them. We are only interested in controlling for them to estimate \\(\\theta\\) accurately."
  },
  {
    "objectID": "C01-dml.html#a-naive-approach-and-regularization-bias",
    "href": "C01-dml.html#a-naive-approach-and-regularization-bias",
    "title": "9  Double Machine Learning",
    "section": "9.2 A naive approach and regularization bias",
    "text": "9.2 A naive approach and regularization bias\n\n\nPackages to load for replication\n\nlibrary(data.table)\nlibrary(magick)\nlibrary(fixest)\nlibrary(DoubleML)\nlibrary(tidyverse)\nlibrary(mlr3)\nlibrary(parallel)\nlibrary(mlr3learners)\nlibrary(ggbrace)\nlibrary(rsample)\nlibrary(MASS)\nlibrary(ranger)\n\nOne way to estimate \\(\\theta\\) follows the following steps:\n\nStep 1: Estimate \\(g_0(X)\\) and then subtract the fitted value of \\(g_0(X)\\) from \\(y\\).\n\nStep 1.1: Regress \\(y\\) on \\(X\\) to estimate \\(E[y|X]\\) and call it \\(\\hat{l}_0(x)\\)\nStep 1.2: Regress \\(d\\) on \\(X\\) to estimate \\(E[d|X]\\) (\\(m_0(X)\\)), call it \\(\\hat{m}_0(x)\\), and calculate \\(\\tilde{d} = d - \\hat{m}_0(X)\\).\nStep 1.3: Get an initial estimate of \\(\\theta\\) using \\[\n\\begin{aligned}\n\\hat{\\theta}_{init} = (\\frac{1}{n}\\sum_{i=1}^N \\tilde{d}_i \\tilde{d}_i)^{-1}\\frac{1}{n}\\sum_{i=1}^N \\tilde{d}_i (y_i - \\hat{l}_0(X_i))\n\\end{aligned}\n\\tag{9.1}\\]\nStep 1.4: Regress \\(y_i - \\hat{\\theta}_{init}d\\) on \\(X\\) to estimate \\(g_0(X)\\) and call it \\(\\hat{g}_0(X)\\).\n\nStep 2: Regress \\(y - \\hat{g}_0(X)\\) on \\(d\\). Or equivalently, use the following formula \\[\n\\begin{aligned}\n  \\hat{\\theta} = (\\frac{1}{n}\\sum_{i=1}^N d_i d_i)^{-1}\\frac{1}{n}\\sum_{i=1}^N d_i (y_i - \\hat{g}_0(X_i))\n\\end{aligned}\n\\tag{9.2}\\]\n\nAs it will turn out, this procedure suffers from the so-called regularization and over-fitting bias(Chernozhukov et al. 2018).\nTo demonstrate the bias problem, we work on the following data generating process used in the user guide for the DoubleML package.\n\\[\n\\begin{aligned}\ny_i = 0.5 d_i + x_{i,1} + \\frac{1}{4}\\cdot\\frac{exp(x_{i,3})}{1 + exp(x_{i,3})} + \\mu_i \\\\\nd_i = \\frac{exp(x_{i,1})}{1 + exp(x_{i,1})} + \\frac{1}{4}\\cdot x_{i,3}+ \\eta_i\n\\end{aligned}\n\\]\nwhere \\(\\mu_i \\sim N(0, 1)\\) and \\(\\eta_i \\sim N(0, 1)\\) (So, no endogeneity problem). In this data generating process, \\(d\\) is not binary and its effect on \\(y\\) is assumed to be linear.\nWe use the gen_data() function (defined on the right), which is a slightly generalized version of the make_plr_CCDDHNR2018() function from the DoubleML package.\n\n\ngen_data() allows you to specify \\(g_0(X)\\) and \\(m_0(X)\\) unlike make_plr_CCDDHNR2018().\n\ngen_data <- function(\n  g_formula = formula(~ I(exp(x1)/(1+exp(x1))) + I(x3/4)), # formula that defines g(x)\n  m_formula = formula(~ x1 + I(exp(x3)/(1+exp(x3))/4)), # formula that defines m(x)\n  te_formula = formula(~ I(0.5*d)), # formula that defines theta(x) * t\n  n_obs = 500, \n  n_vars = 20, \n  mu_x = 0, \n  vcov_x = NULL,\n  sigma = 1 # sd of the error term in the y equation\n)\n{\n\n  if (is.null(vcov_x)) {\n    vcov_x <- matrix(rep(0, n_vars^2), nrow = n_vars)\n    for (i in seq_len(n_vars)) {\n      vcov_x[i, ] <- 0.7^abs(i - seq_len(n_vars)) \n    }\n  }\n\n  #=== draw from multivariate normal ===#\n  data <- \n    mvrnorm(n_obs, mu = rep(0, n_vars), Sigma = vcov_x) %>% \n    data.table() %>% \n    setnames(names(.), paste0(\"x\", 1:n_vars))  \n\n  #=== generate d ===#\n  if (m_formula == \"independent\") {\n    data[, d := rnorm(n_obs)]\n  } else {\n    data[, d := model.frame(m_formula, data = data) %>% rowSums() + rnorm(n_obs)]\n  }\n\n  #=== generate y ===#\n  data[, g := model.frame(g_formula, data = data) %>% rowSums()]\n\n  #=== generate treatment effect ===#\n  data[, te := model.frame(te_formula, data = data) %>% rowSums()]\n\n  #=== generate y ===#\n  data[, y := te + g + rnorm(n_obs, sd = sigma)]\n\n  return(data[])\n\n}\n\n\nset.seed(2893434)\n\ntraining_data <- gen_data()\n\nIt has 20 x variables (for \\(X\\)) along with d (treatment) and y (dependent variable). Only x1 and x3 are the relevant variables and the rest of \\(X\\) are irrelevant.\n\nstr(training_data)\n\nClasses 'data.table' and 'data.frame':  500 obs. of  24 variables:\n $ x1 : num  0.867 0.435 -0.454 0.282 0.361 ...\n $ x2 : num  0.338 1.197 0.461 0.977 0.111 ...\n $ x3 : num  0.302 1.872 -0.202 1.189 0.584 ...\n $ x4 : num  0.7643 0.93275 -0.08757 0.44677 0.00945 ...\n $ x5 : num  0.641 1.793 0.294 0.257 0.54 ...\n $ x6 : num  0.403 1.028 1.272 -0.104 -0.327 ...\n $ x7 : num  0.335 2.096 0.631 0.428 -1.161 ...\n $ x8 : num  -0.732 3.224 0.81 1.915 -1.882 ...\n $ x9 : num  -1.172 2.838 0.186 2.378 -1.156 ...\n $ x10: num  -1.216 3.058 -1.299 1.938 -0.075 ...\n $ x11: num  -0.8736 1.2757 -1.2655 2.0184 -0.0257 ...\n $ x12: num  0.841 -0.294 -0.537 1.463 -0.547 ...\n $ x13: num  0.805 -0.335 -0.834 1.875 -0.82 ...\n $ x14: num  0.336 0.851 -0.903 1.648 -1.375 ...\n $ x15: num  -0.175 -1.104 -0.196 1.174 -1.435 ...\n $ x16: num  -1.3271 -0.6673 -0.1669 0.0313 -0.8977 ...\n $ x17: num  -0.518 -0.49 -0.755 0.226 0.248 ...\n $ x18: num  0.335 -0.592 -0.433 0.486 -0.446 ...\n $ x19: num  0.839 -1.647 -1.352 -0.152 -0.446 ...\n $ x20: num  -0.0674 -2.1706 -2.148 -0.8532 -0.5553 ...\n $ d  : num  1.242 -0.103 -0.227 1.69 1.047 ...\n $ g  : num  0.78 1.075 0.338 0.867 0.735 ...\n $ te : num  0.621 -0.0513 -0.1135 0.8449 0.5234 ...\n $ y  : num  3.215 0.027 -0.669 3.583 1.922 ...\n - attr(*, \".internal.selfref\")=<externalptr> \n\n\n\n9.2.1 Step 1\nLet’s now work on Step 1. We estimate \\(g_0(X)\\) using random forest (RF). As described above, this is a four-step process.\n\n\nIt does not have to be RF. Indeed, you can use any statistical methods in this step.\nStep 1.1: Estimate \\(l_0(X)\\) by regressing \\(y\\) on \\(X\\).\n\n#--------------------------\n# Step 1.1\n#--------------------------\nrf_fitted_l0 <-\n  ranger(\n    y ~ .,\n    data = dplyr::select(training_data, c(\"y\", starts_with(\"x\"))),\n    mtry = 5,\n    num.trees = 132,\n    max.depth = 5,\n    min.node.size = 1\n  )\n\n#=== fitted values ===#\nl0_hat <- predict(rf_fitted_l0, data = training_data)$predictions\n\n#=== create y - l0_hat ===#\ntraining_data[, y_less_l := y - l0_hat]\n\nStep 1.2: Estimate \\(m_0(X)\\) by regressing \\(d\\) on \\(X\\).\n\n#--------------------------\n# Step 1.2\n#--------------------------\nrf_fitted_m0 <-\n  ranger(\n    d ~ .,\n    data = dplyr::select(training_data, c(\"d\", starts_with(\"x\"))),\n    mtry = 5,\n    num.trees = 378,\n    max.depth = 3,\n    min.node.size = 6\n  )\n\n#=== fitted values ===#\nm0_hat <- predict(rf_fitted_m0, data = training_data)$predictions\n\n#=== create y - m0_hat ===#\ntraining_data[, d_less_m := d - m0_hat]\n\n\n\nFigure of d (treatment variable) plotted against m0_hat (\\(\\hat{m}_0(X)\\)).\n\n\nCode\nggplot(training_data) +\n  geom_point(aes(y = d, x = m0_hat)) +\n  geom_abline(slope = 1, color = \"red\") +\n  theme_bw()\n\n\n\n\n\nStep 1.3: Get an initial estimate of \\(\\theta\\) using Equation 9.1.\n\n#--------------------------\n# Step 1.2\n#--------------------------\ntheta_init <- training_data[, sum(d_less_m * y_less_l) / sum(d_less_m * d_less_m) ]\n\nStep 1.4: Regress \\(y - \\theta_{init}d\\) on \\(X\\) to fit \\(g_0(X)\\).\n\n#--------------------------\n# Step 1.3\n#--------------------------\n#=== define y - treatment effect ===#\ntraining_data[, y_less_te := y - theta_init * d]\n\n#=== fit rf ===#\nrf_fitted_g0 <-\n  ranger(\n    y_less_te ~ .,\n    data = dplyr::select(training_data, c(\"y_less_te\", starts_with(\"x\"))),\n    mtry = 5,\n    num.trees = 132,\n    max.depth = 5,\n    min.node.size = 1\n  )\n\n#=== fitted values ===#\ng0_hat <- predict(rf_fitted_g0, data = training_data)$predictions\n\n#=== create y - g0 ===#\ntraining_data[, y_less_g := y - g0_hat]\n\nFigure 9.1 plots true \\(g_0(X)\\) (g) against \\(\\hat{g}_0(X)\\) (g0_hat). As you can see, \\(\\hat{g}_0(X)\\) is a bit biased.\n\n\nCode\nggplot(training_data) +\n  geom_point(aes(y = g, x = g0_hat)) +\n  geom_abline(slope = 1, color = \"red\") +\n  theme_bw() +\n  coord_equal()\n\n\n\n\n\nFigure 9.1: ?(caption)\n\n\n\n\n\n\n9.2.2 Step 2\nFinally, we regress \\(y - \\hat{g}_0(X)\\) on \\(d\\) (or equivalently using Equation 9.2).\n\n(\ntheta_hat <- lm(y_less_g ~ d, data = training_data)$coefficient[\"d\"]\n)\n\n        d \n0.4944072 \n\n\nSo, in this instance, we get an estimate of \\(\\theta\\) that is a bit lower than the true value of \\(\\theta\\). Let’s repeat this process many times to see how this procedure performs on average.\n\n\nCode\nfit_m0 <- function(training_data, mtry = 10) {\n\n  rf_fitted_m0 <-\n    ranger(\n      d ~ .,\n      data = dplyr::select(training_data, c(\"d\", starts_with(\"x\"))),\n      # mtry = 5,\n      mtry = mtry,\n      # num.trees = 378,\n      num.trees = 500,\n      max.depth = 3,\n      # min.node.size = 6\n      min.node.size = 10\n    )\n\n  return(rf_fitted_m0)\n\n}\n\nfit_l0 <- function(training_data, mtry = 12)\n{\n  rf_fitted_l0 <-\n    ranger(\n      y ~ .,\n      data = dplyr::select(training_data, c(\"y\", starts_with(\"x\"))),\n      mtry = mtry,\n      # num.trees = 132,\n      num.trees = 500,\n      max.depth = 5,\n      # min.node.size = 1\n      min.node.size = 10\n    )\n\n  return(rf_fitted_l0)\n}\n\n#===================================\n# Define a function that will get you g0_hat\n#===================================\n# this function will be used later\n\nfit_g0 <- function(training_data, rf_fitted_m0, mtry_l = 12, mtry_g = 12) {\n  #--------------------------\n  # Step 1.1\n  #--------------------------\n  rf_fitted_l0 <- fit_l0(training_data, mtry_l)\n\n  #=== fitted values ===#\n  l0_hat <- predict(rf_fitted_l0, data = training_data)$predictions\n\n  #=== create y - l0_hat ===#\n  training_data[, y_less_l := y - l0_hat]\n\n  #--------------------------\n  # Step 1.2\n  #--------------------------\n  #=== fitted values ===#\n  m0_hat <- predict(rf_fitted_m0, data = training_data)$predictions\n\n  #=== create y - m0_hat ===#\n  training_data[, d_less_m := d - m0_hat]\n\n  #--------------------------\n  # Step 1.2\n  #--------------------------\n  theta_init <- training_data[, sum(d_less_m * y_less_l) / sum(d_less_m * d_less_m)]\n\n  #--------------------------\n  # Step 1.3\n  #--------------------------\n  #=== define y - treatment effect ===#\n  training_data[, y_less_te := y - theta_init * d]\n\n  #=== fit rf ===#\n  rf_fitted_g0 <-\n    ranger(\n      y_less_te ~ .,\n      data = dplyr::select(training_data, c(\"y_less_te\", starts_with(\"x\"))),\n      mtry = mtry_g,\n      # num.trees = 132,\n      num.trees = 500,\n      max.depth = 5,\n      # min.node.size = 1\n      min.node.size = 10\n    )\n\n  return(rf_fitted_g0)\n\n}\n\n#===================================\n# Define a function that runs a single simulation and gets you theta_hat\n#===================================\nrun_sim_naive <- function(i){\n\n  # training_data <- data[[i]] %>% data.table()\n  training_data <- gen_data()\n\n  rf_fitted_m0 <- fit_m0(training_data)\n  rf_fitted_g0 <- fit_g0(training_data, rf_fitted_m0)\n  g0_hat <- predict(rf_fitted_g0, data = training_data)$predictions\n\n  #=== create y - g0 ===#\n  training_data[, y_less_g := y - g0_hat]\n\n  theta_hat <- lm(y_less_g ~ d, data = training_data)$coefficient[\"d\"]\n\n  # theta_hat <- training_data[, sum(d * y_less_g) / sum(d * d)]\n\n  return(theta_hat)\n\n}\n\n#===================================\n# Repeat MC simulations 500 times\n#===================================\ntheta_hats_apr1 <-\n  mclapply(\n    1:500,\n    run_sim_naive,\n    mc.cores = detectCores() / 4 * 3\n  ) %>% \n  unlist() %>% \n  data.table(theta = .)\n\n#===================================\n# Plot the results\n#===================================\nggplot(theta_hats_apr1) +\n  geom_histogram(aes(x = theta), color = \"white\") +\n  theme_bw() + \n  geom_vline(aes(xintercept = 0.5,color = \"True Value\")) +\n  geom_vline(aes(xintercept = theta_hats_apr1[, mean(theta)], color = \"Mean of the Estimates\")) +\n  scale_color_manual(\n    values = c(\"True Value\" = \"red\", \"Mean of the Estimates\" = \"blue\"),\n    name = \"\"\n  ) +\n  guides(color = guide_legend(nrow = 1, byrow = TRUE)) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 9.2: Simulation results of the naive procedure\n\n\n\n\nFigure 9.2 shows the histogram of \\(\\hat{\\theta}\\) from 500 simulations. You can see that this procedure has led to consistent underestimation of the treatment effect. There are two sources of bias in this approach: regularization and over-fitting bias.\n\n\n\n\n\n\nNote\n\n\n\n\nRegularization bias: the bias coming from bias in estimating \\(g_0(X)\\)\nOver-fitting bias: the bias coming from over-fitting \\(g_0(X)\\) and \\(m_0(X)\\) due to the fact that the same sample is used for \\(g_0(X)\\) and \\(m_0(X)\\) estimation and \\(\\theta\\) estimation\n\n\n\nRegularization bias is termed so because bias in estimating \\(g_0(X)\\) can occur when some form of regularization is implemented (e.g., lasso). However, its name is slightly misleading because \\(g_0(X)\\) cannot be estimated without bias even without any regularization in general. This is because the estimation of initial \\(\\theta\\) (in Step 1.3) is biased, which comes from the fact that \\(m_0(X)\\) is correlated with \\(g_0(X)\\) through \\(X\\).\n\n\nAnother (unofficial) implementation of \\(g_0(X)\\) estimation is to just use \\(\\hat{l}_0(X_i)\\) as \\(\\hat{g}_0(X_i)\\) (a blog post) and then use the following formula.\n\\[\n\\begin{aligned}\n  \\hat{\\theta} = (\\frac{1}{n}\\sum_{i=1}^N d_i d_i)^{-1}\\frac{1}{n}\\sum_{i=1}^N d_i (y_i - \\hat{l}_0(X_i))\n\\end{aligned}\n\\]\n\\(\\hat{\\theta}\\) is biased because \\(\\hat{l}_0(X_i)\\) is a biased estimator of \\(\\hat{g}_0(X_i)\\) when \\(m_0(X)\\) and \\(g_0(X)\\) are correlated. The point here is that, \\(g_0(X)\\) is hard to estimate without bias irrespective of whether any regularization happens or not.\nHowever, if the treatment is independent, then, \\(g_0(X)\\) can be estimated well and this approach works well except it still suffers from over-fitting bias. Figure 9.3 shows that the distribution of \\(\\hat{\\theta}\\) when \\(m_0(X)\\) is independent of \\(g_0(X)\\) and the RF with the same hyper-parameters are used. While regularization can lead to bias in the estimation of \\(g_0(X)\\), regularization is not the only source of bias.\n\n\nCode\nrun_sim_naive <- function(i){\n\n  training_data <- gen_data(m_formula = \"independent\")\n\n  rf_fitted_m0 <- fit_m0(training_data)\n  rf_fitted_g0 <- fit_g0(training_data, rf_fitted_m0)\n  # rf_fitted_g0 <- fit_l0(training_data)\n\n  g0_hat <- predict(rf_fitted_g0, data = training_data)$predictions\n\n  #=== create y - g0 ===#\n  training_data[, y_less_g := y - g0_hat]\n\n  theta_hat <- lm(y_less_g ~ d, data = training_data)$coefficient[\"d\"]\n\n  # theta_hat <- training_data[, sum(d * y_less_g) / sum(d * d)]\n\n  return(theta_hat)\n\n}\n\n#===================================\n# Repeat MC simulations 500 times\n#===================================\ntheta_hats_apr1_indep <-\n  mclapply(\n    1:500,\n    run_sim_naive,\n    mc.cores = detectCores() / 4 * 3\n  ) %>% \n  unlist() %>% \n  data.table(theta = .)\n\n#===================================\n# Plot the results\n#===================================\nggplot(theta_hats_apr1_indep) +\n  geom_histogram(aes(x = theta), color = \"white\") +\n  theme_bw() + \n  geom_vline(aes(xintercept = 0.5,color = \"True Value\")) +\n  geom_vline(aes(xintercept = theta_hats_apr1_indep[, mean(theta)], color = \"Mean of the Estimates\")) +\n  scale_color_manual(\n    values = c(\"True Value\" = \"red\", \"Mean of the Estimates\" = \"blue\"),\n    name = \"\"\n  ) +\n  guides(color = guide_legend(nrow = 1, byrow = TRUE)) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 9.3: Performance of approach 1 when the treatment status is independent"
  },
  {
    "objectID": "C01-dml.html#overcoming-the-regularization-bias",
    "href": "C01-dml.html#overcoming-the-regularization-bias",
    "title": "9  Double Machine Learning",
    "section": "9.3 Overcoming the regularization bias",
    "text": "9.3 Overcoming the regularization bias\nRegularization bias can be overcome by double-debiasing (orthogonalizing both \\(d\\) and \\(y\\)). Specifically,\n\nStep 1: Estimate \\(g_0(X)\\) and then subtract the fitted value of \\(g_0(X)\\) from \\(y\\)\nStep 2: Subtract \\(\\hat{m}_0(X)\\) from \\(d\\) (\\(\\tilde{d} = d - \\hat{m}_0(x)\\))\nStep 3: Calculate \\(\\hat{\\theta}\\) based on the following formula\n\n\\[\n\\begin{aligned}\n\\hat{\\theta} = (\\frac{1}{n}\\sum_{i=1}^N \\tilde{d}_i d_i)^{-1}\\frac{1}{n}\\sum_{i=1}^N \\tilde{d}_i (y_i - \\hat{g}_0(X_i))\n\\end{aligned}\n\\]\nThe key difference from the previous approach is that this approach uses IV-like formula, where \\(\\tilde{d}\\) is acting like an instrument.\n\n\nFor \\(y = X\\beta + \\mu\\) with instruments \\(Z\\), the IV estimator is\n\\[\n\\begin{aligned}\n\\hat{\\beta} = (Z'X)^{-1}Z'y\n\\end{aligned}\n\\]\nWe have done Steps 1 and 2 already in the previous approach. So,\n\n#--------------------------\n# Step 3\n#--------------------------\n(\ntheta_hat <- training_data[, sum(d_less_m * y_less_g) / sum(d_less_m * d)]\n)\n\n[1] 0.4933098\n\n\nNow, let’s repeat this 500 times.\n\n\nCode\nrun_sim_dereg <- function(i)\n{\n  training_data <- gen_data()\n  # training_data <- data[[i]] %>% data.table\n\n  rf_fitted_m0 <- fit_m0(training_data)\n  m0_hat <- predict(rf_fitted_m0, data = training_data)$predictions\n\n  #=== create d - m0_hat ===#\n  training_data[, d_less_m := d - m0_hat]\n\n  #=== get g0_hat ===#\n  rf_fitted_g0 <- fit_g0(training_data, rf_fitted_m0)\n  g0_hat <- predict(rf_fitted_g0, data = training_data)$predictions\n\n  #=== create y - g0 ===#\n  training_data[, y_less_g := y - g0_hat]\n\n  theta_hat <- training_data[, sum(d_less_m * y_less_g) / sum(d_less_m * d)]\n\n  return(theta_hat)\n}\n\ntheta_hats_apr2 <-\n  mclapply(\n    1:500,\n    function(x) run_sim_dereg(x),\n    mc.cores = detectCores() / 4 * 3\n  ) %>% \n  unlist() %>% \n  data.table(theta = .)\n\nggplot(theta_hats_apr2) +\n  geom_histogram(aes(x = theta), color = \"white\") +\n  theme_bw() + \n  geom_vline(aes(xintercept = 0.5,color = \"True Value\")) +\n  geom_vline(aes(xintercept = theta_hats_apr2[, mean(theta)], color = \"Mean of the Estimates\")) +\n  scale_color_manual(\n    values = c(\"True Value\" = \"red\", \"Mean of the Estimates\" = \"blue\"),\n    name = \"\"\n  ) +\n  guides(color = guide_legend(nrow = 1, byrow = TRUE)) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 9.4: Simulation results of double-debiased approach\n\n\n\n\nFigure 9.4 shows the distribution of \\(\\hat{\\theta}\\), which is centered about \\(0.48\\). The current approach still suffers from the so-called over-fitting bias(Chernozhukov et al. 2018). Let’s look at how we can overcome this bias next."
  },
  {
    "objectID": "C01-dml.html#sec-cf",
    "href": "C01-dml.html#sec-cf",
    "title": "9  Double Machine Learning",
    "section": "9.4 Overcoming the over-fitting bias",
    "text": "9.4 Overcoming the over-fitting bias\nOver-fitting bias can be overcome by cross-fitting. First, the training data is split into \\(K\\)-folds just like K-fold cross-validation. Let’s denote them as \\(I_1, \\dots, I_k\\). For example, for \\(I_1\\), the following steps are taken (Figure 9.5 provides a visual illustration):\n\nStep 1: Estimate \\(\\hat{g}_0(x)\\) and \\(\\hat{m}_0(x)\\) using the data from the other folds (\\(I_2, \\dots, I_K\\)).\nStep 2: Estimate \\(\\hat{g}_0(x_i)\\) and \\(\\hat{m}_0(x_i)\\) for each \\(i \\in I_1\\) and calculate \\(\\tilde{y}_i = y_i - \\hat{g}_0(x_i)\\) and \\(\\tilde{d}_i = d_i - \\hat{m}_0(x_i)\\).\nStep 3: Use the following formula to obtain \\(\\hat{\\theta}\\).\n\n\\[\n\\begin{aligned}\n\\hat{\\theta} = (\\frac{1}{n}\\sum_{i=1}^N \\tilde{d}_i d_i)^{-1}\\frac{1}{n}\\sum_{i=1}^N \\tilde{d}_i (y_i - \\hat{g}_0(X_i))\n\\end{aligned}\n\\tag{9.3}\\]\nThis process is repeated for all the \\(K\\) folds, and then the the final estimate of \\(\\hat{\\theta}\\) is obtained as the average of \\(\\hat{\\theta}\\)s.\n\n\nYou can implement repeated K-fold cross-fitting using the DoublML package, which is not demonstrated here as it is very much similar in concept to repeated K-fold CV explained in Chapter 3.\n\n\nCode\nggplot() +\n  #=== fold 1 ===#\n  geom_rect(\n    aes(xmin = 0, xmax = 10, ymin = 0, ymax = 2),\n    fill = \"grey\",\n    color = \"black\",\n    size = 1.2\n  ) +\n  geom_rect(\n    aes(xmin = 0, xmax = 2.5, ymin = 0, ymax = 2),\n    fill = \"black\"\n  ) +\n  #=== fold 2 ===#\n  geom_rect(\n    aes(xmin = 0, xmax = 10, ymin = 2.2, ymax = 4.2),\n    fill = \"grey\",\n    color = \"black\",\n    size = 1.2\n  ) +\n  geom_rect(\n    aes(xmin = 2.5, xmax = 5, ymin = 2.2, ymax = 4.2),\n    fill = \"black\"\n  ) +\n  #=== fold 3 ===#\n  geom_rect(\n    aes(xmin = 0, xmax = 10, ymin = 4.4, ymax = 6.4),\n    fill = \"grey\",\n    color = \"black\",\n    size = 1.2\n  ) +\n  geom_rect(\n    aes(xmin = 5, xmax = 7.5, ymin = 4.4, ymax = 6.4),\n    fill = \"black\"\n  ) +\n  #=== fold 4 ===#\n  geom_rect(\n    aes(xmin = 0, xmax = 10, ymin = 6.6, ymax = 8.6),\n    fill = \"grey\",\n    color = \"black\",\n    size = 1.2\n  ) +\n  geom_rect(\n    aes(xmin = 7.5, xmax = 10, ymin = 6.6, ymax = 8.6),\n    fill = \"black\"\n  ) +\n  geom_brace(aes(c(0, 7.4), c(8.7, 9.2)), inherit.data=F) +\n  annotate(\n    \"text\", x = 3.5, y = 9.7, parse = TRUE,\n    label = \"'Find ' * hat(g)[0](x) * ' and ' * hat(m)[0](x) * ' from this data.'\",\n    size = 6\n  ) +\n  geom_curve(\n    aes(x = 2.2, xend = 8.5, y = 10.3, yend = 8.8),\n    arrow = arrow(length = unit(0.03, \"npc\")),\n    curvature = -0.3\n  ) +\n  geom_curve(\n    aes(x = 3.4, xend = 8, y = 10.3, yend = 8.8),\n    arrow = arrow(length = unit(0.03, \"npc\")),\n    curvature = -0.3\n  ) +\n  ylim(NA, 11) +\n  # coord_equal() +\n  theme_void()\n\n\n\n\n\nFigure 9.5: Illustration of cross-fitting\n\n\n\n\nLet’s code the cross-fitting procedure. We first split the data into 2 folds (\\(K = 2\\)).\n\n\n\\(K\\) does not have to be 2.\n\n(\ndata_folds <- rsample::vfold_cv(training_data, v = 2)\n)\n\n#  2-fold cross-validation \n# A tibble: 2 × 2\n  splits            id   \n  <list>            <chr>\n1 <split [250/250]> Fold1\n2 <split [250/250]> Fold2\n\n\nLet’s cross-fit for fold 1.\n\nsplit_1 <- data_folds[1, ]\n\n#=== data for estimating g_0  and m_0 ===#\ndata_train <- analysis(split_1$splits[[1]]) \n\n#=== data for which g_0(x_i) and m_0(x_i) are calculated ===#\ndata_target <- assessment(split_1$splits[[1]]) \n\nFirst, we fit \\(\\hat{g}_0(x)\\) and \\(\\hat{m}_0(x)\\) using the data from the other folds.\n\n#=== m0 ===#\nm_rf_fit <- fit_m0(data_train)\n\n#=== g0 ===#\ng_rf_fit <- fit_g0(data_train, m_rf_fit)\n\nNext, we predict \\(\\hat{g}_0(x_i)\\) and \\(\\hat{m}_0(x_i)\\) for each \\(i\\) of fold 1 (the target dataset) and calculate \\(\\tilde{y}_i = y_i - \\hat{g}_0(x_i)\\) and \\(\\tilde{d}_i = d_i - \\hat{m}_0(x_i)\\).\n\ndata_orth <-\n  data_target %>% \n  #=== prediction of g_0(x_i) ===#\n  .[, g_0_hat := predict(g_rf_fit, data = .)$predictions] %>% \n  #=== orthogonalize y ===#\n  .[, y_tilde := y - g_0_hat] %>% \n  #=== prediction of m_0(x_i) ===#\n  .[, m_0_hat := predict(m_rf_fit, data = .)$predictions] %>% \n  #=== orthogonalize d ===#\n  .[, d_tilde := d - m_0_hat]\n\nThen, \\(\\hat{\\theta}\\) is obtained for this fold using Equation 9.3.\n\n(\ntheta_hat <- data_orth[, sum(d_tilde * y_tilde) / sum(d_tilde * d)]\n)\n\n[1] 0.5172923\n\n\nWe can repeat this for all the folds (cross_fit() which finds \\(\\hat{\\theta}\\) for a particular fold is defined on the side).\n\n\n\ncross_fit <- function(i, data_folds, mtry_l = 12, mtry_m = 10, mtry_g = 12)\n{\n\n  #--------------------------\n  # Prepare data\n  #--------------------------\n  #=== ith split ===#\n  working_split <- data_folds[i, ]\n\n  #=== data for estimating g_0  and m_0 ===#\n  data_train <- analysis(working_split$splits[[1]]) \n\n  #=== data for which g_0(x_i) and m_0(x_i) are calculated ===#\n  data_target <- assessment(working_split$splits[[1]]) \n\n  #--------------------------\n  # Fit g0 and m0\n  #--------------------------\n  #=== m0 ===#\n  m_rf_fit <- fit_m0(data_train, mtry_m)\n\n  #=== g0 ===#\n  g_rf_fit <- fit_g0(data_train, m_rf_fit, mtry_l, mtry_g)\n\n  #--------------------------\n  # Get y_tilde and d_tilde\n  #--------------------------\n  data_orth <-\n    data_target %>% \n    #=== prediction of g_0(x_i) ===#\n    .[, g_0_hat := predict(g_rf_fit, data = .)$predictions] %>% \n    #=== orthogonalize y ===#\n    .[, y_tilde := y - g_0_hat] %>% \n    #=== prediction of m_0(x_i) ===#\n    .[, m_0_hat := predict(m_rf_fit, data = .)$predictions] %>% \n    #=== orthogonalize d ===#\n    .[, d_tilde := d - m_0_hat] %>% \n    .[, .(y_tilde, d_tilde, d)]\n\n  theta_cf <- data_orth[, sum(d_tilde * y_tilde) / sum(d_tilde * d)]\n\n  return(theta_cf)\n}\n\n\n(\ntheta_hat <- \n  lapply(\n    seq_len(nrow(data_folds)), # loop over folds\n    function(x) cross_fit(x, data_folds) # get theta_hat\n  ) %>% \n  unlist() %>%\n  mean() # average them\n)\n\n[1] 0.5055226\n\n\nOkay, now that we understand the steps of this approach, let’s repeat this many times (get_theta_cf() that finds \\(\\hat{\\theta}\\) by cross-fitting is defined on the side).\n\n\n\nget_theta_cf <- function(data_folds, mtry_l = 12, mtry_m = 10, mtry_g = 12){\n\n  theta_hat <- \n    lapply(\n      seq_len(nrow(data_folds)),\n      function(x) cross_fit(x, data_folds, mtry_l, mtry_m, mtry_g)\n    ) %>% \n    unlist() %>% \n    mean()\n\n  return(theta_hat)\n}\n\n\n\nCode\ntheta_hats_cf <-\n  mclapply(\n    1:500,\n    function(x) {\n      print(x)\n      training_data <- gen_data()\n      data_folds <- rsample::vfold_cv(training_data, v = 2)\n      theta_hat <-  get_theta_cf(data_folds)\n      return(theta_hat)\n    },\n    mc.cores = detectCores() * 3 / 4\n  ) %>% \n  unlist()\n\n#=== visualize the results ===#\nggplot() +\n  geom_histogram(aes(x = theta_hats_cf), color = \"white\") +\n  theme_bw() + \n  geom_vline(aes(xintercept = 0.5,color = \"True Value\")) +\n  geom_vline(aes(xintercept = mean(theta_hats_cf), color = \"Mean of the Estimates\")) +\n  scale_color_manual(\n    values = c(\"True Value\" = \"red\", \"Mean of the Estimates\" = \"blue\"),\n    name = \"\"\n  ) +\n  guides(color = guide_legend(nrow = 1, byrow = TRUE)) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 9.6: The distribution of treatment effect estimated by the double-debiased approach with cross-fitting\n\n\n\n\nFigure 9.6 shows the distribution of \\(\\hat{\\theta}\\) with double-deabiasing and cross-fitting. It is slightly biased in this instance (mean is 0.501), but the average \\(\\hat{\\theta}\\) is very close to the true parameter.\n\n\n\n\n\n\nImportant\n\n\n\n\nDouble-debiasing (double orthogonalization) can help overcome the bias in \\(\\hat{\\theta}\\) that comes from the bias in estimating \\(g_0(X)\\).\nCross-fitting can help overcome the bias from estimating all of \\(g_0(X)\\), \\(m_0(X)\\), and \\(\\theta\\) using the same data."
  },
  {
    "objectID": "C01-dml.html#implementation",
    "href": "C01-dml.html#implementation",
    "title": "9  Double Machine Learning",
    "section": "9.5 Implementation",
    "text": "9.5 Implementation\n\n\n\n\nChernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. 2018. “Double/debiased machine learning for treatment and structural parameters.” The Econometrics Journal 21 (1): C1–68. https://doi.org/10.1111/ectj.12097."
  },
  {
    "objectID": "C02-het-te.html",
    "href": "C02-het-te.html",
    "title": "10  R-learner",
    "section": "",
    "text": "In Chapter 9, the basic idea of double machine learning (DML) methods was introduced when the treatment effect is homogeneous. We now turn our focus to the task of estimating heterogeneous treatment effects: the impact of a treatment varies based on observed attributes of the subjects. Heterogeneous treatment effect is also referred to as  conditional  average treatment effect (CATE).\n\n\n Conditional  on observed attributes."
  },
  {
    "objectID": "C02-het-te.html#motivation",
    "href": "C02-het-te.html#motivation",
    "title": "10  R-learner",
    "section": "10.2 Motivation",
    "text": "10.2 Motivation\nUnderstanding how treatment effects vary can be highly valuable in many circumstances.\n Example 1:  If we come to know a particular drug is effective on elderly people but detrimental to kids, then doctors can make a smart decision of prescribing the drug to elderly people, but not to kids.\n\n\nIn this example, the heterogeneity driver is age.\n Example 2:  If we come to know that fertilizer is more effective in increasing corn yield in soil type A than B, then farmers can apply more fertilizer on the parts of the field where soil type is A but less on where soil type is B.\n\n\nIn this example, the heterogeneity driver is soil type.\nAs you can see in these examples, knowledge on the heterogeneity of the treatment effect and its drivers can help decision makers smart-target treatments."
  },
  {
    "objectID": "C02-het-te.html#modeling-framework",
    "href": "C02-het-te.html#modeling-framework",
    "title": "10  R-learner",
    "section": "10.3 Modeling Framework",
    "text": "10.3 Modeling Framework\nThe model of interest in a general form here is as follows:\n\\[\n\\begin{aligned}\nY & = \\theta(X)\\cdot T + g(X, W) + \\varepsilon \\\\\nT & = f(X, W) + \\eta\n\\end{aligned}\n\\]\n\n\\(Y\\): dependent variable\n\\(T\\): treatment variable (can be either binary dummy or continuous)\n\\(X\\): collection of variables that affect Y indirectly through the treatment (\\(\\theta(X)\\cdot T\\)) and directly (\\(g(X, W)\\)) independent of the treatment\n\\(W\\): collection of variables that affect directly (\\(g(X, W)\\)) independent of the treatment, but not through the treatment\n\nHere are the key assumptions:\n\n\\(E[\\varepsilon|X, W] = 0\\)\n\\(E[\\eta|X, W] = 0\\)\n\\(E[\\eta\\cdot\\varepsilon|X, W] = 0\\)\n\nOur objective is to estimate the  constant  marginal CATE \\(\\theta(X)\\). (constant in the sense marginal CATE is the same irrespective of the value of the treatment)"
  },
  {
    "objectID": "C02-het-te.html#r-learner",
    "href": "C02-het-te.html#r-learner",
    "title": "10  R-learner",
    "section": "10.4 R-learner",
    "text": "10.4 R-learner\n\n10.4.1 Theoretical background\nUnder the assumptions,\n\\[\n\\begin{aligned}\nE[Y|X, W] = \\theta(X)\\cdot E[T|X,W] + g(X,W)\n\\end{aligned}\n\\]\nThus,\n\\[\n\\begin{aligned}\nY & = \\theta(X)\\cdot T + g(X,W) + \\varepsilon \\\\\n\\Rightarrow Y - E[Y|X, W] & = \\theta(X)\\cdot T + g(X,W) + \\varepsilon - \\theta(X)\\cdot E[T|X,W] - g(X,W) \\\\\n\\Rightarrow Y - E[Y|X, W] & = \\theta(X)\\cdot (T - E[T|X,W]) + \\varepsilon \\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\nY - E[Y|X, W] & = \\theta(X)\\cdot (T - E[T|X,W]) + \\varepsilon\n\\end{aligned}\n\\]\nSuppose we know \\(E[Y|X, W]\\) and \\(E[T|X,W]\\), then we can construct the following new variables:\n\n\\(\\tilde{Y} = Y - E[Y|X, W]\\)\n\\(\\tilde{T} = T - E[T|X, W] = \\eta\\)\n\nThen, the problem of identifying \\(\\theta(X)\\) reduces to estimating the following model:\n\\[\n\\begin{aligned}\n\\tilde{Y} = \\theta(X)\\cdot \\tilde{T} + \\varepsilon\n\\end{aligned}\n\\]\nSince \\(E[\\eta\\cdot\\varepsilon|X] = 0\\) by assumption, we can regress \\(\\tilde{Y}\\) on \\(X\\) and \\(\\tilde{T}\\),\n\\[\n\\begin{aligned}\n\\hat{\\theta} = argmin_{\\theta} \\;\\; E[(\\tilde{Y} - \\theta(X)\\cdot \\tilde{T})^2]\n\\end{aligned}\n\\tag{10.1}\\]\n\n\n10.4.2 Estimation steps\nIn practice, we of course do not observe \\(E[Y|X, W]\\) and \\(E[T|X, W]\\). So, we first need to estimate them using the data at hand to construct \\(\\hat{\\tilde{Y}}\\) and \\(\\hat{\\tilde{T}}\\). You can use any suitable statistical methods to estimate \\(E[Y|X, W]\\) and \\(E[T|X, W]\\). Some machine learning methods allow you to estimate them without assuming any functional form or structural assumptions. If you believe they are linear functions of \\(X\\) and \\(W\\), may could alternatively use lasso or other linear models. It is important to keep in mind that the estimation of \\(E[Y|X, W]\\) and \\(E[T|X, W]\\) is done by cross-fitting (see Section 9.4) to avoid over-fitting bias. Let, \\(f(X, W)\\) and \\(g(X,W)\\) denote \\(\\tilde{Y}\\) and \\(\\tilde{T}\\), respectively. Further, let \\(I_{-i}\\) denote all the observations that belong to the folds that \\(i\\) does  not  belong to. Finally, let \\(\\hat{f}(X_i, W_i)^{I_{-i}}\\) and \\(\\hat{g}(X_i, W_i)^{I_{-i}}\\) denote \\(\\tilde{Y}\\) and \\(\\tilde{T}\\) estimated using \\(I_{-i}\\).\n\n\nJust like the DML approach discussed in Chapter 9, both \\(Y\\) and \\(T\\) are orthogonalized.\nThen the quality of fit (explaining the heterogeneity in the impact of treatment) can be expressed as follows, which is the empirical version of Equation 10.1:\n\\[\n\\begin{aligned}\n\\sum_{i=1}^N [Y_i - \\hat{f}(X_i,W_i)^{I_{-i}} - \\theta(X)\\cdot (T_i - \\hat{g}(X_i,W_i)^{I_{-i}})]^2\n\\end{aligned}\n\\]\nThis is called  R-score, and it can be used for causal model selection, which is covered later.\nThe final stage of the R-learner is to estimate \\(\\theta(X)\\) by minimizing the R-score plus the regularization term (if desirable).\n\\[\n\\begin{aligned}\n\\hat{\\theta}(X) = argmin_{\\theta(X)}\\;\\;\\sum_{i=1}^N [Y_i - \\hat{f}(X_i,W_i)^{I_{-i}} - \\theta(X)\\cdot (T_i - \\hat{g}(X_i,W_i)^{I_{-i}})]^2 + \\Lambda(\\theta(X))\n\\end{aligned}\n\\]\nwhere \\(\\Lambda(\\theta(X))\\) is the penalty on the complexity of \\(\\theta(X)\\). For example, if you choose to use lasso, then \\(\\Lambda(\\theta(X))\\) is the L1 norm. You have lots of freedom as to what model you use in the final stage. The econml package offers several off-the-shelf choices of R-learner (DML) approaches that differ in the model used at the final stage, including causal forest, lasso, etc."
  },
  {
    "objectID": "C02-het-te.html#implementation-example",
    "href": "C02-het-te.html#implementation-example",
    "title": "10  R-learner",
    "section": "10.5 Implementation Example",
    "text": "10.5 Implementation Example\n\n\nPackages to load for replication\n\nlibrary(data.table)\nlibrary(magick)\nlibrary(fixest)\nlibrary(officer)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(reticulate)\nlibrary(DoubleML)\nlibrary(MASS)\n\nWe use the python econml pacakge (in conjunction with the R reticulate package), which offers one of the most comprehensive sets of off-the-shelf R-leaner (DML) methods (Keith Battocchi 2019).\nLet’s first take a look at the DML class, which implements DML where the final stage has to be a linear model. That is,\n\n\nDML is a child class of _Rlearner, which is a private class. The DML class has several child classes: LinearDML, SpatseLinearDML, NonParamDML, and CausalForestDML.\n\\[\n\\begin{aligned}\n\\theta(x) = \\beta_1 + x_1 + \\dots + \\beta_k + x_k\n\\end{aligned}\n\\]\nAs we saw above, we need to specify three models.\n\nmodel_y: model for estimating \\(E[Y|X,W]\\)\nmodel_t: model for estimating \\(E[T|X,W]\\)\nmodel_final: model for estimating \\(\\theta(X)\\)\n\nIn this example, let’s use gradient boosting regression for both model_y and model_t and use lasso with cross-validation for model_final.\nFirst import all the functions we will need.\n\nlibrary(reticulate)\nuse_virtualenv(\"ml-learning\")\n\n#=== econml.dml ===#\nem_dml <- import(\"econml.dml\")\n\n#=== sklearn ===#\nsl <- import(\"sklearn\") \ngbr <- sl$ensemble$GradientBoostingRegressor\nlassocv <- sl$linear_model$LassoCV\n\nLet’s generates data according to the following data generating process:\n\\[\n\\begin{aligned}\ny_i = exp(x_{i,1}) d_i + x_{i,1} + \\frac{1}{4}\\cdot\\frac{exp(x_{i,3})}{1 + exp(x_{i,3})} + \\mu_i \\\\\nd_i = \\frac{exp(x_{i,1})}{1 + exp(x_{i,1})} + \\frac{1}{4}\\cdot x_{i,3}+ \\eta_i\n\\end{aligned}\n\\]\n\n#=== sample size ===#\nN <- 1000 \n\n#=== generate data ===#\nsynth_data <-\n  gen_data(\n    te_formula = formula(~ I(exp(x1)*d)),\n    n_obs = N *2\n  )\n\nWe now split the data into training and test datasets.\n\nrow_indices <- sample(1:(2*N), N, replace = FALSE)\n\n#=== train data ===#\ntrain_data <- synth_data[row_indices,]\n\nX_train <- \n  dplyr::select(train_data, starts_with(\"x\")) %>% \n  as.matrix()\ny_train <- train_data[, y]\nd_train <- train_data[, d]\n\n#=== test data ===#\ntest_data <- synth_data[-row_indices,]\n\nX_test <- \n  dplyr::select(test_data, starts_with(\"x\")) %>% \n  as.matrix()\ny_test <- test_data[, y]\nd_test <- test_data[, d]\n\nBefore we train a DML model, we first set up a DML estimation framework like below.\n\n#=== set up DML estimation ===#\nest <-\n  em_dml$DML(\n    model_y = gbr(),\n    model_t = gbr(),\n    model_final = lassocv(fit_intercept=FALSE) \n  )\n\nHere, training has not happened yet. We simply created a recipe. Once we provide ingredients (data), we can cook (train) with the fit() method.\n\nest$fit(\n  y_train, \n  d_train, \n  X = X_train, \n  W = X_train\n)\n\nOnce, the training is done. We can use the effect() method to predict \\(\\theta(X)\\).\n\n#=== calculate theta_hat(X) ===#\ntheta_pred <- est$effect(X_test)\n\n#=== assign the predicted theta_hat to a variable ===#\ntest_data[, theta_hat := theta_pred]\n\nFigure 10.1 presents the estimated and true marginal treatment effect (\\(\\theta(X)\\)) as a function of x1.\n\nggplot(test_data) +\n  geom_point(aes(y = theta_hat, x = x1)) +\n  geom_line(aes(y = exp(x1), x = x1), color = \"blue\") +\n  theme_bw()\n\n\n# g_het_te <-\n#   ggplot(test_data) +\n#     geom_point(aes(y = theta_hat, x = x1)) +\n#     geom_line(aes(y = exp(x1), x = x1), color = \"blue\") +\n#     theme_bw()\n\n# saveRDS(g_het_te, \"LectureNotes/g_hte_te.rds\")\n\ng_het_te <- readRDS(\"g_hte_te.rds\")\ng_het_te\n\n\n\n\nFigure 10.1: Estimated and true marginal treatment effects\n\n\n\n\nSince we forced \\(\\theta(X)\\) to be linear in x1, it is not surprising that the estimated MTE looks linear in x1 even though the true MTE is an exponential function of x1.\n\n\n\n\n\n\nTip\n\n\n\nI recommend going through examples presented here for DML\n\n\n\n\n\n\nKeith Battocchi, Maggie Hei, Eleanor Dillon. 2019. “EconML: A Python Package for ML-Based Heterogeneous Treatment Effects Estimation.” https://github.com/microsoft/EconML."
  },
  {
    "objectID": "E01-spatial-cv.html",
    "href": "E01-spatial-cv.html",
    "title": "11  Spatial Cross-validation",
    "section": "",
    "text": "K-fold and repeated K-fold cross-validation methods can under-estimate test MSE (that is how good the trained model is in predicting \\(y\\) in a new dataset) when the observations in the train data are not independent with each other. A typical example would be spatial correlation of the error term. In many applications including environmental and agricultural events, spatially correlated error is commonly observed. In order to combat the problem of test MSE underestimation by regular K-fold cross-validation (hereafter, simply KCV), you can use spatial K-fold cross-validation (SKCV) instead.\nFor this section, we will consider the following data generating process.\n\\[\n\\begin{aligned}\ny = \\alpha + \\beta_1 x + \\beta_2 x^2 + u\n\\end{aligned}\n\\]\nwhere \\(x\\) and \\(u\\) are spatially correlated (which makes \\(y\\) spatially correlated).\nHere is the data we are going to work with (see the side note for the code to define gen_data, which takes a seed value and generate a spatial dataset):\nWe have three main variables, y (dependent variable), x (explanatory variable), and e (error). Figure 11.1 shows how they are spatially distributed. It also shows that all of them are spatially positively correlated.\nWe are going to use gam() with k \\(= 30\\) and sp \\(= 0\\) as the model in conducting KCV and spatial KCV. Let’s first create folds for KCV and SKCV. First, here is KCV folds.\nFigure 11.2 is the visualization of the spatial distribution of training and test datasets for each of the five folds for KCV.\nNow, let’s create a five spatially clustered folds using the spatialsample package for SKCV.\nFigure 11.3 presents the spatial distribution of training and test datasets for each of the five folds for SKCV.\nLet’s now implement KCV and SKCV. Since we observe the true generating process, we can calculate how good the fitted curve is compared to true \\(E[y|X]\\) in addition to observed \\(y\\) for the left-out samples in each fold.\nYou can see that MSE values (mse_obs) are mostly greater and also more variable for SKCV. By averaging MSE over folds by CV type,\nSo, indeed KCV provides lower estimate of test MSE than SKCV. Now, it is important to recognize the fundamental difference in what is measured by KCV and SKCV. KCV measures the accuracy of the trained model applied to the new data points that are located inside the area where the train data covers geographically. SKCV, on the other hand, measures the accuracy of the trained model applied to the new data points that are  outside  the area where the train data covers geographically. In other words, it measures the modeling accuracy when the trained model is applied to a new region. So, KCV does overstate the accuracy of the model if your interest is applying the model to a new region.\nLet’s look into the SKCV results a bit more. Looking at MSE values by fold for KCV and SKCV, you can notice that MSE is much more variable for SKCV. Figure 11.4 plots data points in the test data and the curve fitted on the training data by cross-validation type and fold (Note that Fold1 for KCV and SKCV have nothing to do with each other). Since KCV uses randomly split data, all the KCV test datasets are scattered all across the area and they look similar to each other. On the other hand, SKCV test datasets look quite different from each other as they are sampled in a spatially clustered manner. If the original data has no spatial correlation, then SKCV and KCV would have resulted in very simple test datasets. However, since the original data is spatially positively correlated, one packet of a field may look very different from another pocket of the field. By looking at Figure 11.4, you can easily see that FOLD2 and FOLD5 have very high MSE for SKCV. This observation is confirmed with the MSE value by fold presented above.\nThis is primarily because of the data imbalance between the training and test datasets, in particular the error term in this case. Figure 11.5 compares the distribution of the error term for the training and test datasets by fold for SKCV. As you can see, their distributions are very different for FOLD2 and FOLD5. In FOLD2, the average of the error term for the training data is much higher than that for the test data. The opposite is true for FOLD5. These differences of course result in large differences in the average value of the dependent variable. This, then, further results in consistent under or over-estimation of the level of the dependent variable. Since squared error is defined as \\([y_i -\\hat{f}(x_i)]^2\\), the consistent bias (you can consider this as bias in intercept in the linear-in-parameter modeling setting) will naturally lead to a higher MSE. That is exactly what is observed for FOLD2 and FOLD5.\nWhile MSE values for FOLD2 and FOLD5 are high in SKCV, notice that their fitted curves are very much similar to the other folds and trace the true \\(E[y|x]\\) quite well. The culprit of their high MSEs is the consistent difference in the average value of \\(y\\) between the training and test datasets. This means that if your interest is in understanding the causal impact of \\(x\\) on \\(y\\), then the fitted curves for FOLD2 and FOLD5 are just as good as those for the other folds despite the fact that their MSE values are much higher than the rest. This illustrates well that MSE is not really a good criteria to rely on if you ultimate interest is in estimating the causal impact of a treatment even though it is a good measure if your ultimate interest in prediction (predicting the  level  of the dependent variable).\nFinally, whether KCV is over-optimistic in estimating test MSE than SKCV or not is of little importance for those who are interested in causal inference because how well the trained model predicts the  level  of the dependent variable (which MSE measures) is irrelevant. Rather, more critical question is whether the use of KCV leads us to choose sub-optimal hyper parameter values compared to SKCV. This could have a real consequence when we select a model and its hyper-parameter values for the first-stage ML applications in Double Machine Learning methods."
  },
  {
    "objectID": "E01-spatial-cv.html#hyper-parameter-tuning",
    "href": "E01-spatial-cv.html#hyper-parameter-tuning",
    "title": "11  Spatial Cross-validation",
    "section": "11.1 Hyper-parameter tuning",
    "text": "11.1 Hyper-parameter tuning\nNow, let’s see whether KCV and SKCV lead to different tuning results using gam(). Here, we fix k at \\(50\\) and vary sp to find the best sp value. The following function takes an sp value and return MSE values from both KCV and SKCV.\n\nget_mse <- function(sp, kcv_folds, skcv_folds) {\n  cv_results <-\n    rbind(kcv_folds, skcv_folds) %>% \n    #=== make it possible to apply function row by row ===#\n    rowwise() %>% \n    #=== get mse ===#\n    mutate(mse_data = list(\n      assessment(splits) %>% \n      data.table() %>% \n      .[, y_hat := predict( # predict y based on the fitted model\n        #=== train the model ===#\n        gam(y ~ s(x, k = 50, sp = sp), data = analysis(splits)), \n        newdata = .)\n      ] %>% \n      .[, .(\n        mse_obs = mean((y - y_hat)^2)\n      )]\n    )) %>% \n    dplyr::select(id, type, mse_data) %>% \n    unnest() %>% \n    data.table() %>% \n    .[, sp_v := sp] %>%\n    .[]\n\n  return(cv_results)\n\n}\n\nHere is an example at sp \\(= 0.5\\).\n\nget_mse(0.5, kcv_folds, skcv_folds)\n\n       id type   mse_obs sp_v\n 1: Fold1  KCV  771.1552  0.5\n 2: Fold2  KCV  757.1101  0.5\n 3: Fold3  KCV  699.6830  0.5\n 4: Fold4  KCV  749.1340  0.5\n 5: Fold5  KCV  904.7470  0.5\n 6: Fold6  KCV  865.9606  0.5\n 7: Fold1 SKCV  324.7865  0.5\n 8: Fold2 SKCV 2149.3093  0.5\n 9: Fold3 SKCV  646.5064  0.5\n10: Fold4 SKCV  462.3724  0.5\n11: Fold5 SKCV 2350.4966  0.5\n12: Fold6 SKCV  257.4204  0.5\n\n\nLet’s try sp values ranging from 0 to 5.\n\nsp_seq <- c(0.05, seq(0.5, 5, by = 0.25))\n\ncv_results <-\n  mclapply(\n    sp_seq,\n    function(x) get_mse(x, kcv_folds, skcv_folds),\n    mc.cores = detectCores() / 4 * 3\n  ) %>% \n  rbindlist() %>% \n  .[, .(mse = mean(mse_obs)), by = .(type, sp_v)]\n\nNow, let’s see what value of sp would have been the best to estimate \\(E[y|x]\\) (this is what we really want to know, but can’t in practice because you do not observe the true data generating process unlike the simulation we are running here).\nWe will fit gam() at the same sequence of sp values used just above using the entire training dataset and check what sp value minimizes the prediction error from true \\(E[y|x]\\).\n\n\nNote that since we are testing against the true \\(E[y|x]\\), we can simply use the training dataset instead of using independent test datasets. Of course, you cannot do this in practice.\n\n(\nmse_e <-\n  data.table(\n    sp_v = sp_seq\n  ) %>% \n  rowwise() %>% \n  mutate(fitted_values = list(\n    gam(y ~ s(x, k = 50), sp = sp_v, data = train_data)$fitted.value\n  )) %>% \n  mutate(mse = mean((train_data$y_det - fitted_values)^2)) %>%\n  data.table() %>% \n  .[, .(sp_v, mse)] %>% \n  .[, type := \"Test\"]\n)\n\nLet’s combine all the results and see how they are different.\n\n\nCode\nall_mse <- rbind(mse_e, cv_results)\n\nggplot() +\n  geom_line(data = all_mse, aes(y = mse, x = sp_v)) +\n  geom_point(data = all_mse, aes(y = mse, x = sp_v)) +\n  geom_point(\n    data = all_mse[, .SD[which.min(mse), ], by = type], \n    aes(y = mse, x = sp_v),\n    color = \"red\",\n    size = 2\n  ) +\n  facet_grid(type ~ ., scale = \"free_y\") +\n  theme_bw()\n\n\n\n\n\nFigure 11.6: Cross-validation results\n\n\n\n\nSo, the sp value that would produce the fitted curve that is closest to the true \\(E[y|x]\\) is 0.5. SKCV suggests that sp value of 2 is optimal. Finally, KCV suggests that sp value of 2.5 is optimal. But, of course this is just a single instance. Let’s run more of the same simulations to see if our guess is correct or not. Figure 11.7 presents the optimal sp values estimated by KCV and SKCV plotted against the actual optimal sp value from 100 simulations. Points around the red one-to-one line indicate good predictions by KCV and SKCV.\n\n\nCode\nrun_cv_sim <- function(i)\n{\n\n  print(i)\n\n  #=== create the training data ===#\n  training_data  <- gen_data(seed = runif(1) * 1000)\n\n  #--------------------------\n  # prepare data for CV\n  #--------------------------\n  kcv_folds <- \n    rsample::vfold_cv(training_data, v = 6) %>% \n    mutate(type := \"KCV\")\n\n  skcv_folds <- \n    spatial_clustering_cv(training_data, v = 6) %>% \n    mutate(type := \"SKCV\")\n\n  #--------------------------\n  # run KCV and SKCV\n  #--------------------------  \n  cv_results <-\n    lapply(\n      sp_seq,\n      function(x) get_mse(x, kcv_folds, skcv_folds)\n    ) %>% \n    rbindlist() %>% \n    .[, .(mse = mean(mse_obs)), by = .(type, sp_v)] %>% \n    .[, .SD[which.min(mse)], by = .(type)]\n\n  #--------------------------\n  # gam fits on the entire train dataset\n  #--------------------------\n  gam_fits <-\n    data.table(\n      sp_v = sp_seq\n    ) %>% \n    rowwise() %>% \n    mutate(gam_fit = list(\n      gam(y ~ s(x, k = 50), sp = sp_v, data = training_data)\n    )) %>% \n    mutate(mse = mean((training_data$y_det - gam_fit$fitted.value)^2)) \n\n  #--------------------------\n  #  Level 2 comment out\n  #--------------------------\n  opt_x_data <-\n    left_join(cv_results, gam_fits, by = \"sp_v\") %>% \n    rowwise() %>% \n    mutate(opt_x = \n      data.table(\n        x = seq(min(training_data$x), max(training_data$x), length = 100)\n      ) %>% \n      .[, y_hat := predict(gam_fit, newdata = .)] %>% \n      .[, pi_hat := y_hat - 8 * x] %>% \n      .[which.max(pi_hat), x]\n    ) %>% \n    data.table() %>% \n    .[, .(type, opt_x)]\n\n  #--------------------------\n  # best sp\n  #--------------------------\n  best_sp <-\n    gam_fits %>% \n    data.table() %>% \n    .[which.min(mse), sp_v]\n\n  return_results <- \n    cv_results %>% \n    .[, sp_best := best_sp] %>% \n    opt_x_data[., on = \"type\"]\n\n  return(return_results)\n}\n\nsim_results <-\n  mclapply(\n    1:100,\n    function(x) run_cv_sim(x),\n    mc.cores = detectCores() * 3 / 4\n  ) %>% \n  rbindlist(idcol = \"sim\")\n\nggplot(data = sim_results[, .N, by = .(sp_v, type, sp_best)]) +\n  geom_point(aes(x = sp_v, y = sp_best, size = N)) +\n  geom_abline(slope = 1, color = \"red\") +\n  facet_grid(type ~ .) +\n  theme_bw()\n\n\n\n\n\nFigure 11.7: Optimal penalization parameter estimated by KCV and SKCV compared against the actual optimal parameter.\n\n\n\n\nKCV seems to underestimate the optimal value of sp. While SKCV seems to be less biased, it is not doing a great job just like KCV.\nThis was an interesting experiment. However, how consequential is it in terms of treatment effect estimation? When the actual optimal sp value is 5 and KCV is suggesting sp value of 0.05 (in many cases), using sp = 0 as the final model leads to a model that is terrible at treatment effect estimation? To see this, let’s consider a simple economic problem. Suppose \\(y\\) and \\(x\\) are an output and input, respectively. Further suppose that the price of \\(y\\) is 1 and the cost of \\(x\\) is 8. We would like to solve the following profit maximization problem:\n\\[\n\\begin{aligned}\n& Max_{x} 4 \\times y - 2 \\times x \\\\\n\\Rightarrow & Max_{x} 1 \\times (10 + 48 \\times x - 4 \\times x^2) - 8 \\times x\n\\end{aligned}\n\\]\nHere, \\(1 \\times (10 + 48 \\times x - 4 \\times x^2)\\) is the revenue and \\(8 \\times x\\) is the cost (the difference is the profit).\nTaking the derivative of the objective function with respect \\(x\\),\n\\[\n1 \\times 48 - 1 \\times 4 \\times 2 \\times x - 8 = 0\n\\tag{11.1}\\]\nHere, \\(1 \\times 48 - 1 \\times 4 \\times 2 \\times x\\) represents the marginal treatment effect of \\(x\\). Now, note that the intercept (\\(10\\)) goes away and it plays no role in determining the optimal value of the input (\\(x\\)). That is, it is perfectly fine to get the intercept wrong as long as we are estimating the marginal treatment effect of \\(x\\) accurately (as we discussed earlier, MSE is not a good measure for this as it punishes a bias in intercept).\nAccording to Equation 11.1, optimal \\(x\\) is \\(5\\). Figure 11.8 shows the density plot of estimated optimal \\(x\\) by KCV and SKCV. It seems like KCV and SKCV provide similar results and it does not really matter which one to use for sp parameter tuning at least in this particular instance. It is important to note that this result clearly does not generalize. We looked at a very simply toy example. This section was not intended to give you a general advise on which KCV or SKCV you should use. It would be interesting to examine the difference between KCV and SKCV under more complex data generating process using other ML methods.\n\n\nCode\nggplot(sim_results) +\n  geom_density(aes(x = opt_x, fill = type), alpha = 0.4) +\n  scale_fill_discrete(name = \"\") +\n  theme_bw()\n\n\n\n\n\nFigure 11.8: Accuracy of estimating optimal value of the input (x) by CV type"
  },
  {
    "objectID": "E02-grf.html",
    "href": "E02-grf.html",
    "title": "12  Generalized Random Forest",
    "section": "",
    "text": "Athey, Tibshirani, and Wager (2019)"
  },
  {
    "objectID": "E02-grf.html#grf-in-a-nutshell",
    "href": "E02-grf.html#grf-in-a-nutshell",
    "title": "12  Generalized Random Forest",
    "section": "12.1 GRF in a nutshell",
    "text": "12.1 GRF in a nutshell\nHere, we follow the notations used in Athey, Tibshirani, and Wager (2019) as much as possible. Let, \\(O_i\\) denote the entire data available.\n\n\nFor random forest, \\(O_i\\) is {\\(Y_i\\), \\(X_i\\)} where \\(Y_i\\) is the dependent variable and \\(X_i\\) is a collection of independent variables. For causal forest, \\(O_i\\) is {\\(Y_i\\), \\(W_i\\), \\(X_i\\)}, where \\(W_i\\) is the treatment variable.\nLet \\(\\theta(X)\\) denote the statistics of interest (e.g., CATE for causal forest, conditional quantile for for quantile forest) and \\(\\nu(X)\\) denote any nuisance (you are not interested in it) statistics. Generalized random forest (GRF) solves the following problem to find the estimate of \\(\\theta\\) conditional on \\(X_i= x\\):\n\\[\n\\begin{aligned}\n\\theta(x),\\nu(x) = argmin_{\\theta,\\nu} \\sum_{i=1}^n \\alpha_i(x)\\Psi_{\\theta, \\nu}(O_i)^2\n\\end{aligned}\n\\tag{12.1}\\]\nwhere \\(\\Psi_{\\theta, \\nu}(O_i)\\) is a score function, and \\(\\alpha_i(x)\\) is the weight given to \\(i\\)th observation.\n\n\n\n\n\n\nNote\n\n\n\nWhy is it called  generalized  random forest?\n\n\nThis is because depending on how the score function (\\(\\Psi_{\\theta, \\nu}(O_i)\\)) is defined, you can estimate  a wide range of statistics using different approaches under the  same estimation framework.\n\nConditional expectation (\\(E[Y|X]\\))\n\nRegression Forest (Random forest for regression)\nBoosted Regression Forest\n\nConditional average treatment effect (CATE)\n\nCausal Forest\nInstrumental Forest\n\nConditional quantile\n\nQuantile Forest\n\n\nHow can Equation 12.1 represents so many (very) different statistical approaches? It all boils down to how \\(\\Psi_{\\theta, \\nu}(O_i)\\) is specified. Here are some examples:\n\n\\(\\Psi_{\\theta, \\nu}(Y_i, X_i) = Y_i - \\theta(X)\\): traditional random forest\n\\(\\Psi_{\\theta, \\nu}(Y_i, X_i, T_i) = (Y_i - E[Y|X])- \\theta(X)(T_i - E[T|X])\\): causal forest\n\\(\\Psi_{\\theta, \\nu}(Y_i) = qI\\{Y_i > \\theta\\} - (1-q)I\\{Y_i \\leq \\theta\\}\\): quantile forest\n\n\n\n\\(I\\{\\}\\) is an indicator function that takes 1 if the condition inside the curly brackets and 0 otherwise.\n\n\n\n\n\n\nNote\n\n\n\nWhy is it called generalized  random forest?\n\n\n GRF uses random forest to find the weights \\(\\alpha_i(x)\\). Specifically, it trains a random forest in which the dependent variable is  pseudo outcome (\\(\\rho_i\\)) derived from the score function that is specific to the type of regression you are running. Based on the trees build, then \\(\\alpha_i(x)\\) is calculated as the proportion of the number of times observation \\(i\\) ended up in the same terminal node (leaf) relative to the total number of observations that \\(X = x\\) share leaves with for all the trees.\nSuppose you build \\(T\\) trees using RF on the pseudo outcomes. Each of the tree has its own splitting rules, and you can identify which leaf \\(X=x\\) belongs to for each of the \\(T\\) trees. Now, let \\(\\eta_{i,t}(X)\\) is 1 if observation \\(i\\) belongs to the same leaf as \\(X=x\\) in tree \\(t\\). Then the weight given to observation \\(i\\) is\n\\[\n\\begin{aligned}\n\\alpha_i(x) = \\frac{1}{T}\\sum_{t=1}^T\\frac{\\eta_{i,t}(x)}{\\sum_{i=1}^N\\eta_{i,t}(x)}\n\\end{aligned}\n\\tag{12.2}\\]\n\n\\(\\sum_{i=1}^{N}\\eta_{i,t}(x)\\): the number of observations in the same terminal node as \\(X=x\\) in tree \\(t\\)\n\n\n\nNote that some trees do not even have observation \\(i\\) as bootstrapped samples are used to build trees.\nNote that trees are build only once in GRF and it is used repeatedly when predicting \\(\\theta(X)\\) at different values of \\(X\\). So, the trained RF is applied  globally, but the weights obtained based on the forest are  local to the point of evaluation (\\(X_i = x\\)). As you can see from Equation 12.2, depending on the value of \\(X\\) (\\(x\\)), individual weights are adjusted according to how similar the observations are to the point of evaluation (\\(X=x\\)). For a given value of \\(X\\), the weights are plugged into Equation 12.1 and the minimization problem is solve to identify \\(\\theta(x)\\).\n\n\nOrthogonal random forest (a forest-based heterogeneous treatment effect estimator like causal forest), on the other hand, build trees every time when predicting treatment effect \\(\\theta(X)\\) at particular values of \\(X\\), which is why orthogonal random forest takes a very long time especially when there are many evaluation points.\nYou probably have noticed the similarity in idea between GRF and generalized method moments (GMM). Indeed, GRF can also be considered as local GMM (see ?sec-local-reg to get a sense of what a local regression is like).\n\n\n\n\n\n\nTip\n\n\n\nGRF procedure\n\nStep 1: train random forest\n\nSpecify the score function that is appropriate for the statistics of interest and the data generating process\nDerive pseudo outcome from the score function\nTrain random forest using the pseudo outcomes as the dependent variable\n\nStep 2: estimate (predict) \\(\\theta(x)\\)\n\nFind the weight for each observation based on the trained random forest according to Equation 12.2 and then solve the weighted minimization problem (Equation 12.1)."
  },
  {
    "objectID": "E02-grf.html#random-forest-as-a-grf",
    "href": "E02-grf.html#random-forest-as-a-grf",
    "title": "12  Generalized Random Forest",
    "section": "12.2 Random forest as a GRF",
    "text": "12.2 Random forest as a GRF\nHere, we take a look at RF as a GRF as an illustration to understand the general GRF procedure better.\n\n12.2.1 Forest building (train an RF on pseudo outcome)\nWhen \\(\\Psi_{\\theta, \\nu}(Y_i, X_i)\\) is set to \\(Y_i - \\theta(X)\\), then GRF is simply RF. By plugging \\(Y_i - \\theta(X)\\) into Equation 12.1, the minimization problem to predict \\(\\theta(x)\\) (\\(E[Y|X=x]\\)) for this GRF is then,\n\\[\n\\begin{aligned}\n\\theta(x) = argmin_{\\theta} \\sum_{i=1}^n \\alpha_i(x)[Y_i - \\theta(X)]^2\n\\end{aligned}\n\\tag{12.3}\\]\n\n\nNo nuisance parameters (\\(\\nu(X)\\)) here.\nNow, let’s consider building a forest to find \\(\\alpha_i(x)\\) in Equation 12.3. For a given bootstrapped sample and set of variables randomly selected, GRF starts with solving the unweighted version of Equation 12.3.\n\\[\n\\begin{aligned}\n\\theta(x) = argmin_{\\theta} \\sum_{i=1}^n [Y_i - \\theta]^2\n\\end{aligned}\n\\tag{12.4}\\]\nThe solution to this problem is simply the mean of \\(Y\\), which will be denoted as \\(\\bar{Y}_P\\), where \\(P\\) represents the parent node. Here, the parent node include all the data points as this is the first split.\nThen the pseudo outcome (\\(\\rho_i\\)) that is used in splitting is\n\\[\n\\begin{aligned}\n\\rho_i = Y_i - \\bar{Y}_P\n\\end{aligned}\n\\]\n\n\nIn general,\n\\[\n\\begin{aligned}\n\\rho_i = - \\xi^T A_P^{-1}\\Psi_{\\hat{\\theta}_P, \\hat{\\nu}_P}(O_i)\n\\end{aligned}\n\\]\nwhere \\(A_P = \\frac{1}{N_P} \\sum_{i=1}^{N_P} \\nabla \\Psi_{\\hat{\\theta}_P, \\hat{\\nu}_P}(O_i)\\)\nHere,\n\n\\(\\xi^T = 1\\)\n\\(\\Psi_{\\hat{\\theta}_P, \\hat{\\nu}_P}(O_i) = Y_i - \\theta_P\\)\n\nTherefore,\n\\[\n\\begin{aligned}\nA_P = \\frac{1}{N_P} \\sum_{i=1}^{N_P} \\times (-1) = -1\n\\end{aligned}\n\\]\nThus,\n\\[\n\\begin{aligned}\n\\rho_i = -1(-1)(Y_i - \\theta_P) = Y_i - \\bar{Y}_P\n\\end{aligned}\n\\]\nsince \\(\\theta_P = \\bar{Y}_P\\).\nNow, a standard CART regression split is applied on the pseudo outcomes. That is, the variable-cutpoint combination that maximizes the following criteria is found in a greedy manner (see Section 6.1 for how a CART is build):\n\\[\n\\begin{aligned}\n\\tilde{\\Delta}(C_1, C_2) = \\frac{(\\sum_{i \\in C_1} \\rho_i)^2}{N_{C_1}} + \\frac{(\\sum_{i \\in C_2} \\rho_i)^2}{N_{C_2}}\n\\end{aligned}\n\\]\nwhere \\(C_1\\) and \\(C_2\\) represent two child node candidates for a given split. This is exactly the same as how the traditional RF builds trees.\nNote that the pseudo outcomes are first summed and then squared in \\((\\sum_{i \\in C_1} \\rho_i)^2\\). This is a similarity score. If the pseudo outcomes are similar to one another, then they do not cancel each other out, which leads to a higher similarity score. Maximizing the weighted sum of similarity scores from the two child node candidates means that you are trying to find a split so that each of the group have similar pseudo outcomes  within the group (which in turn means larger heterogeneity in pseudo outcomes  between the child nodes).\nOnce the best split is identified, each of the new child nodes is split following the exactly the same procedure. Splitting continues until one of the user-specified condition prevent a further splitting.\nMany trees from bootstrapped samples are created (just like the regular random forest) and they form a random forest.\n\n\n12.2.2 Prediction\nTo predict \\(E[Y|X=x]\\), solve Equation 12.3 with the weights. The first order condition is then\n\\[\n\\begin{aligned}\n\\sum_{i=1}^N \\alpha_i(X)(Y_i-\\theta) = 0\n\\end{aligned}\n\\]\nSo,\n\\[\n\\begin{aligned}\n\\theta(x) & = \\frac{\\sum_{i=1}^N \\alpha_i(x)Y_i}{\\sum_{i=1}^N \\alpha_i(x)}\\\\\n& = \\sum_{i=1}^N \\alpha_i(x)Y_i \\;\\; \\mbox{(since } \\sum_{i=1}^N \\alpha_i(x) = 1\\mbox{)} \\\\\n& = \\sum_{i=1}^N \\huge[\\normalsize \\frac{1}{T}\\cdot\\sum_{t=1}^T\\frac{\\eta_{i,t}(x)}{\\sum_{i=1}^N\\eta_{i,t}(x)}\\cdot y_i\\huge]\\\\\n& = \\frac{1}{T}  \\cdot\\sum_{t=1}^T\\sum_{i=1}^N \\frac{\\eta_{i,t}(x)}{\\sum_{i=1}^N\\eta_{i,t}(x)}\\cdot y_i \\;\\; \\mbox{(changing the order of the summations)} \\\\\n& = \\frac{1}{T} \\cdot\\sum_{t=1}^T \\bar{Y}_t\n\\end{aligned}\n\\]\nSo, \\(\\theta(x)\\) from GRF is the average of tree-specific predictions, which is exactly how RF predicts \\(E[Y|X=x]\\) as well."
  },
  {
    "objectID": "E02-grf.html#honesty",
    "href": "E02-grf.html#honesty",
    "title": "12  Generalized Random Forest",
    "section": "12.3 Honesty",
    "text": "12.3 Honesty\nGRF applies  honesty when it trains forests. Specifically, when building a tree, the bootstrapped sample is first split into two groups: subsamples for  splitting and prediction. Then, the a tree is trained on the subsample for splitting and then generate the splitting rules. However, when predicting (say \\(E[Y|X]\\) at \\(X=x\\)), the value of \\(Y\\) from the subsamples for splitting are not used. Rather, only the splitting rules are taken from the trained tree and then they are applied to the subsamples for prediction (Figure 12.1 illustrates this process).\n\n\nPackages to load for replication\n\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(grf)\nlibrary(rpart)\nlibrary(rattle)\nlibrary(wooldridge)\n\n\n\nCode\nDiagrammeR::grViz(\n\"\ndigraph {\n  graph [ranksep = 0.2, fontsize = 4]\n  node [shape = box]\n    SS [label = 'Subsamples for splitting']\n    SP [label = 'Subsamples for predicting']\n    BD [label = 'Bootstrapped Data']\n    TT [label = 'Trained tree']\n    PV [label = 'Predicted value']\n  edge [minlen = 2]\n    BD->SP\n    BD->SS\n    SS->TT\n    SP->PV\n    TT->SP [label='apply the splitting rules']\n  { rank = same; SS; SP}\n  { rank = same; TT}\n  { rank = same; PV}\n}\n\"\n)\n\n\n\n\n\nFigure 12.1: Illustration of an honest tree\n\n\n\nLet’s demonstrate this using a very simple regression tree with two terminal nodes using the mlb data from the wooldridge package.\n\ndata(mlb1)\nmlb1_dt <- data.table(mlb1)\n\nWe would like to train a RF using this data where the dependent variable is logged salary (lsalary). We will illustrate the honesty rule by working on building a single tree within the process of building a forest.\nWe first bootstrap data.\n\nset.seed(89232)\nnum_obs <- nrow(mlb1_dt)\nrow_indices <- sample(seq_len(num_obs), num_obs, replace = TRUE)\nboot_mlb1_dt <- mlb1_dt[row_indices, ]\n\nWe now split the bootstrapped data into two groups: for splitting and prediction.\n\nrows_split <- sample(seq_len(num_obs), num_obs / 2, replace = FALSE)\n\n#=== data for splitting ===#\nsplit_data <- boot_mlb1_dt[rows_split, ]\n\n#=== data for prediction ===#\neval_data <- boot_mlb1_dt[-rows_split, ]\n\nWe then train a tree using the data for splitting (split_data):\n\n#=== build a simple tree ===#\ntree_trained <-\n  rpart(\n    lsalary ~ hits + runsyr, \n    data = split_data, \n    control = rpart.control(minsplit = 120)\n  )\n\nfancyRpartPlot(tree_trained, digits = 4)\n\n\n\n\nFigure 12.2: A simple regression tree using the subsamples for splitting\n\n\n\n\nSo the splitting rule is hits < 356 as shown in Figure 12.2. At the terminal nodes, you see the prediction of lsalary: \\(12.47\\) for the left and \\(14.23\\) for the right. These predictions are NOT honest. They are obtained from the observed values of lsalary within the node using the splitting data (the data the tree is trained for). Instead of using these prediction values, an honest prediction applied the splitting rules (hits < 356) to the data reserved for prediction.s\n\n(\nhonest_pred <- eval_data[, mean(lsalary), by = hits < 356]\n)\n\n    hits       V1\n1: FALSE 14.43151\n2:  TRUE 12.61746\n\n\nSo, instead of \\(12.47\\) and \\(14.23\\), the predicted values from the honest tree are \\(12.62\\) and \\(14.43\\) for the left and right nodes, respectively. Trees are built in this manner many times to form a forest.\nMore generally, in GRF, honesty is applied by using the evaluation data to solve Equation 12.1 based on the weight \\(\\alpha_i(x)\\) derived from the trained forest using the splitting data. Honesty is required for the GRF estimator to be consistent and asymptotically normal (Athey, Tibshirani, and Wager 2019). However, the application of honesty can do more damage than help when the sample size is small.\n\n\n\n\nAthey, Susan, Julie Tibshirani, and Stefan Wager. 2019. “Generalized Random Forests.” The Annals of Statistics 47 (2): 1148–78."
  },
  {
    "objectID": "PROG-01-reticulate.html",
    "href": "PROG-01-reticulate.html",
    "title": "13  Running Python from R",
    "section": "",
    "text": "In this chapter, we learn how to run Python codes from within R using the reticulate package. Its package website is very helpful and cover more topics than this chapter does.\nWe will run mainly the following python codes in different ways on R."
  },
  {
    "objectID": "PROG-01-reticulate.html#set-up-a-python-virtual-environment",
    "href": "PROG-01-reticulate.html#set-up-a-python-virtual-environment",
    "title": "13  Running Python from R",
    "section": "13.1 Set up a Python virtual environment",
    "text": "13.1 Set up a Python virtual environment\nIt is recommended that you have a python virtual environment (VE) specific for each of your projects. A VE is independent of the rest of the Python environment on your computer and avoid breaking your the codes from your old projects that may depend on the specific versions of Python packages. It also made it easier for others to replicate your work.\nLet’s first load the reticulate package.\n\nlibrary(reticulate)\n\nFor inexperienced Python users, the reticulate package offers an easy way to set up and manage a Python VE. First, you can create a VE using virtualenv_create(). The following code for example create a VE called ml-learning.\n\nvirtualenv_create(\"ml-learning\")\n\nThe newly created VE have only minimal packages. Here is the list of packages I have in this new VE.\n\n\n\n\n\nThis is despite all the packages that I have installed in my main Python installation.\nYou can install packages to a VE using virtualenv_install(). For example, the code below attempts to install the econml package to the VE called ml-learning.\n\nvirtualenv_install(\"ml-learning\", \"econml\")\n\nIn order to use a VE, you use use_virtualenv() after you load the reticulate package.\n\nuse_virtualenv(\"ml-learning\")\n\nAfter this, all your python codes will be run on the VE when you are running them through the reticulate package. We will now look at how we actually run Python codes from R."
  },
  {
    "objectID": "PROG-01-reticulate.html#sec-r-python-repl",
    "href": "PROG-01-reticulate.html#sec-r-python-repl",
    "title": "13  Running Python from R",
    "section": "13.2 R and Python REPL simultaneously",
    "text": "13.2 R and Python REPL simultaneously\n\n\nRPEL stands for read-evaluate-print-loop.\nWithin your R session, you can start a Python session that is embedded in it by running repl_python() from R. This will allow you to go back and forth between R and Python. Objects created in Python can be accessed from R using the py object as described above. This provides an ideal environment where you can test the interactions of your R and Python codes in a seamless manner.\nYou initiate a Python environment using repl_python(). If you are using RStudio, this is what it looks like.\n\nrepl_python()\n\n\n\n\n\n\nAs you can see on the console tab, a python session has been initiated, which is connected your R session. As long as you are using RStudio, you do not have to worry about switching between R and Python manually. RStudio is smart enough to know where the codes are coming from and execute codes where they should be executed. That is, if you run codes from a python (R) file, they will be sent to the python (R) console.\nPicture below shows what the console looks like after I executed library(data.table) while I am still on the python console. You can see >>> quit (exit from python) before library(data.table) was executed on R.\n\n\n\n\n\nLet’s now define an object called a in python.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFrom R, you can access objects defined on Python by using py$ prefix to the object name on python. This is like you are accessing an element of a list on R. py has a list of objects defined on Python.\n\n\nLet’s now confirm this by accessing a on R.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhen you access R objects in Python, you can use r. in front of the object name. We first run b <- 1:10 and then confirm r.b can get you the Python equivalent of b in R.\n\n\n\n\n\n\n\nGreat. This seamless integration is really nice especially when you are writing an R-centric program that include pythons codes and are tying to debug.\nWhenever R and Python exchange objects, they are automatically converted to their counterparts in the receiving program. For example, a vector with multiple elements in R will be converted to a list in Python, and vice versa. See here for more examples."
  },
  {
    "objectID": "PROG-01-reticulate.html#importing-python-modules",
    "href": "PROG-01-reticulate.html#importing-python-modules",
    "title": "13  Running Python from R",
    "section": "13.3 Importing Python modules",
    "text": "13.3 Importing Python modules\nYou can import Python modules and use them as if they are R functions. This way of interacting with Python involves the least direct interactions with Python among all the options that the reticulate package provides us with. Unlike the approach above (Section 13.2), you do not see outcomes of your Python codes on the Python console. Rather, you will see the outcomes translated to R object on R.\nYou can import a python module using import() like below, which imports the Python os package.\n\n(\nos <- import(\"os\")\n)\n\nModule(os)\n\n\nThe imported module is assigned to os on R. You can access functions in a module using $ instead of . as done in Python.\nFor example, the following code use listdir() from the os module.\n\nos$listdir(\".\") %>% head()\n\n[1] \"B03-cross-validation_files\"             \n[2] \"C01-dml.qmd\"                            \n[3] \".Rhistory\"                              \n[4] \"P03-xgb.html\"                           \n[5] \"L00-prediction-vs-causal-inference.html\"\n[6] \"P01-random-forest.html\"                 \n\n\nLet’s rewrite the above sample Python code using this method. First, we import all the modules (not the functions) we need.\n\nsk_ds <- import(\"sklearn.datasets\")\nsk_ms <- import(\"sklearn.model_selection\")\nsk_ensemble <- import(\"sklearn.ensemble\")\nnp <- import(\"numpy\")\n\nIf you try this, you will get a complaint.\n\nmake_regression <- import(\"sklearn.datasets.make_regression\")\n\nError in py_module_import(module, convert = convert): ModuleNotFoundError: No module named 'sklearn.datasets.make_regression'\n\n\nNow, let’s define parameters used in make_regression().\n\nn_samples <- as.integer(2000)\nn_features <- as.integer(20)\nn_informative <- as.integer(15)\nnoise <- 2\nrng <- np$random$RandomState(as.integer(123))\n\nNote that the values of n_sample, n_features, and n_informative are made integers explicitly. This is because Python accepts only integers for those parameters (see here). noise is a float and you can be loose about what type of numeric value you provide.\nNow, in the Python code below, make_regression create a tuple (like a list on R) of tuple of length 2: the first one is an array of 2000 by 20 (assigned to X) and the second one is an array of 2000 by 1 (assigned to y). Python has a convenient way of assigning the elements of a tuple to new objects as shown below.\n\nX, y = make_regression(\n  n_samples, \n  n_features, \n  n_informative = n_informative, \n  noise = noise, \n  random_state = rng\n)\n\nR is a bit clumsy on this. So, we can just assign the list of arrays into a single object as a list like below and then extract its elements separately later.\n\nsynth_data <-\n  sk_ds$make_regression(\n    n_samples, \n    n_features, \n    n_informative = n_informative, \n    noise = noise, \n    random_state = rng\n  )\n\nHere is the structure of the data created.\n\nstr(synth_data)\n\nList of 2\n $ :[[ 1.4649944  -0.17713836  0.88586352 ...  0.05907441  1.09976599\n  -0.40625161]\n [-0.89462401 -1.4348529   1.0520037  ...  0.34328588 -0.04932831\n  -1.54376141]\n [ 1.12908372  1.01140982 -0.13517644 ...  0.60035475 -0.65366504\n  -0.28248023]\n ...\n [ 0.86846283  0.55731802 -0.47741222 ...  1.80427412 -0.81294912\n  -0.83775918]\n [-1.14917653 -0.1635096   0.5295092  ...  1.42499691 -1.5558915\n   2.03518107]\n [-0.45903111 -0.36568454 -0.59911521 ...  0.55501302 -0.46528405\n  -1.74544894]]\n $ :[ 168.28301984  -95.31783719  163.00132715 ...    8.48171647 -137.40988386\n  132.03129657]\n\n\nWe can then generate \\(X\\) and \\(y\\) like below.\n\nX <- synth_data[[1]]\ny <- synth_data[[2]]\n\nWe now split the dataset into the train and test datasets using train_test_split().\n\ntrain_test_ls <- sk_ms$train_test_split(X, y, random_state = rng)\n\nAssign each element of train_test_ls to R objects with appropriate names.\n\nX_train <- train_test_ls[[1]]\nX_test <- train_test_ls[[2]]\ny_train <- train_test_ls[[3]]\ny_test <- train_test_ls[[4]]\n\nThen, train RF on the train data, and then test the fit.\n\n#=== set up an RF ===#\nreg_rf <- sk_ensemble$RandomForestRegressor(max_features = \"sqrt\")\n\n#=== train an RF on the train data ===#\nreg_rf$fit(X_train, y_train)\n\nRandomForestRegressor(max_features='sqrt')\n\n#=== test the fit ===#\nreg_rf$score(X_test, y_test)\n\n0.6946956103101651"
  },
  {
    "objectID": "PROG-01-reticulate.html#sourcing-python-scripts",
    "href": "PROG-01-reticulate.html#sourcing-python-scripts",
    "title": "13  Running Python from R",
    "section": "13.4 Sourcing Python scripts",
    "text": "13.4 Sourcing Python scripts\nAnother way to run Python codes from R is to source a python script to make python functions and modules available.\nHere is the Python code in the file called import_modules.py.\n\n# | eval: false\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_regression\nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy\n\nYou can source this file using source_python().\n\nsource_python(\"import_modules.py\")\n\n\n\nimport_modules.py is included in the github repository.\nThe first three lines are importing functions (train_test_split, make_regression, and RandomForestRegressor), while the last line is importing a module (numpy). When functions are imported in this manner, they can be used without py$ as if they are R functions.\n\nsynth_data <-\n  make_regression(\n    n_samples, \n    n_features, \n    n_informative = n_informative, \n    noise = noise, \n    random_state = rng\n  )\n\nlength(synth_data)\n\n[1] 2\n\n\nHowever, a module imported in this manner cannot be used without py$. So, you cannot do this.\n\nnumpy$random$RandomState(as.integer(123))\n\nError in eval(expr, envir, enclos): object 'numpy' not found\n\n\nRather, you need to do this:\n\npy$numpy$random$RandomState(as.integer(123))\n\nRandomState(MT19937) at 0x2B42A5940\n\n\nYou can source user-defined functions on Python as well. Here is the python code in the file called make_data.py. train_test_RF() takes some of the arguments for make_regression() as its arguments, split the data, train RF on the train data, evaluate the fit, and then return the test score.\n\ndef train_test_RF(n_samples, n_features, n_informative, noise, rng):\n\n  from sklearn.model_selection import train_test_split\n  from sklearn.datasets import make_regression\n  from sklearn.ensemble import RandomForestRegressor\n  import numpy as np\n  \n  X, y = make_regression(\n    n_samples, \n    n_features, \n    n_informative = n_informative, \n    noise = noise, \n    random_state = rng\n  )\n  \n  X_train, X_test, y_train, y_test = train_test_split(\n      X, y, random_state = rng\n  )\n\n  reg_rf = RandomForestRegressor(max_features = \"sqrt\")\n\n  reg_rf.fit(X_train, y_train)\n\n  test_score = reg_rf.score(X_test, y_test)\n\n  return test_score\n\nSourcing this file using source_python(),\n\nsource_python(\"run_rf.py\")\n\nOnce this is done, train_test_RF() is now available to use on R.\n\ntrain_test_RF(\n  n_samples, \n  n_features, \n  n_informative = n_informative, \n  noise = noise, \n  rng = rng\n)\n\n0.6995202942939165"
  },
  {
    "objectID": "PROG-01-reticulate.html#python-in-rmarkdown-or-quarto",
    "href": "PROG-01-reticulate.html#python-in-rmarkdown-or-quarto",
    "title": "13  Running Python from R",
    "section": "13.5 Python in Rmarkdown or Quarto",
    "text": "13.5 Python in Rmarkdown or Quarto\nUsing R and Python in an Rmarkdown or Quarto file is extremely easy. You can have both R and Python codes in the same Rmd file. When you write R (python) codes, you use R (python) code chunks indicated by {R} ({python}) like below.\n\n\n\n\n\nFirst code chunk is an R code chunk and loads libraries including reticulate.\n\n\n\n\n\n\nTip\n\n\n\nIf you do not load the reticulate package, python code chunks still run, but the objects defined on python would not be available on R.\n\n\nThe second code chunk is a python code chunk and generate synthetic data using make_regression from sklearn.datasets.\nThe third code chunk is an R code chunk. It uses X and y generated on Python to train an RF using ranger()."
  },
  {
    "objectID": "A01-mc-simulation.html",
    "href": "A01-mc-simulation.html",
    "title": "Appendix A — Monte Carlo (MC) Simulation",
    "section": "",
    "text": "Monte Carlo (MC) simulation is an important tool to test econometric hypothesis  numerically and it is highly desirable that you can conduct your own MC simulations that fit your need. Suppose you are interested in learning whether the OSL estimator of a simple linear-in-parameter model is unbiased when the error term is correlated with one of the explanatory variables. Well, it has been theoretically proven that the OLS estimator is biased under such a data generating process. So, we do not really have to show this numerically. But, what if you are facing with a more complex econometric task for which the answer is not clear for you? For example, what is the impact of over-fitting the first stage estimations in a double machine learning approach to the bias and efficiency of the estimation of treatment effect in the second stage? We can partially answer to this question (though not generalizable unlike theoretical expositions) using MC simulations. Indeed, this book uses MC simulations often to get insights into econometric problems for which the answers are not clear or to just confirm if econometric theories are correct.\nIt is important to first recognize that it is  impossible  to test econometric theory using real-wold data. That is simply because you never know the underlying data generating process of real-world data. In MC simulations, we generate data according to the data generating process we specify. This allows us to check if the econometric outcome is consistent with the data generating process or not. This is the reason every journal article with newly developed statistical procedures published in an econometric journal has MC simulations to check the new econometric theories are indeed correct (e.g., whether the new estimator is unbiased or not).\nHere, we learn how to program MC simulations using very simple econometric examples."
  },
  {
    "objectID": "A01-mc-simulation.html#random-number-generator",
    "href": "A01-mc-simulation.html#random-number-generator",
    "title": "Appendix A — Monte Carlo (MC) Simulation",
    "section": "A.1 Random number generator",
    "text": "A.1 Random number generator\nTo create a dateset, we use pseudo random number generators. In most cases, runif() and rnorm() are sufficient.\n\n#=== uniform ===#\nx_u <- runif(1000) \n\nhead(x_u)\n\n[1] 0.87942703 0.58791133 0.87123737 0.14090788 0.28020296 0.08995099\n\nhist(x_u)\n\n\n\n\n\n#=== normal ===#\nx_n <- rnorm(1000, mean = 0, sd = 1)\n\nhead(x_n)\n\n[1] -1.2181307 -1.0867873  1.0407114 -0.6873026  0.6656139 -1.1782411\n\nhist(x_n)\n\n\n\n\nWe can use runif() to draw from the Bernouli distribution, which can be useful in generating a treatment variable.\n\n#=== Bernouli (0.7) ===#\nrunif(30) > 0.3\n\n [1]  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n[13]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE\n[25]  TRUE  TRUE FALSE  TRUE  TRUE FALSE\n\n\nThey are called  pseudo  random number generators because they are not truly random. What sequence of numbers you get is determined by  seed . In R, you can use set.seed() to set seed.\n\nset.seed(43230)\nrunif(10)\n\n [1] 0.78412711 0.06118740 0.02052893 0.80489633 0.69706142 0.65549966\n [7] 0.18618487 0.87417016 0.39325872 0.06729849\n\n\nIf you run the code on your computer, then you would get exactly the same set of numbers. So, pseudo random generators generate random-looking numbers, but it is not truly random. You are simply drawing from a pre-determined sequence of number that  act like random numbers. This is a very important feature of pseudo random number generators. The fact that  anybody can generate the same sequence of numbers mean that any results based on pseudo random number generators can be reproducible. When you use MC simulations, you  must set a seed so that your results are reproducible."
  },
  {
    "objectID": "A01-mc-simulation.html#mc-simulation-steps",
    "href": "A01-mc-simulation.html#mc-simulation-steps",
    "title": "Appendix A — Monte Carlo (MC) Simulation",
    "section": "A.2 MC simulation steps",
    "text": "A.2 MC simulation steps\n\nStep 1: Generate data based on the data generating process  you  specify\nStep 2: Get an estimate based on the generated data (e.g. OLS, mean)\nStep 3: Repeat Steps 1 and 2 many times (e.g., 1000)\nStep 4: Compare your estimates with the true parameter specified in Step 1\n\nGoing though Steps 1 and 2 only once is not going to give you an idea of how the estimator of interest performs. So, you repeat Steps 1 and 2 many times to see what you can expect form the estimator on average.\nLet’s use a very simple example to better understand the MC steps. The statistical question of interest here is whether sample mean is an unbiased estimator of the expected value: \\(E[\\frac{1}{n}\\sum_{i=1}^n x_i] = E[x]\\), where \\(x_i\\) is an independent random draw from the same distribution.\n\n\nOf course, \\(x_i\\) does not have to be independent. But, just making things as simple as possible.\nHere is Step 1.\n\nx <- runif(100) \n\nHere, \\(x\\) follows \\(Unif(0, 1)\\) and \\(E[x] = 0.5\\). This is the data generating process. And, data (x) has been generated using x <- runif(100).\nStep 2 is the estimation of \\(E[x]\\). The estimator is the mean of the observed values of x.\n\n(\nmean_x <- mean(x)\n)\n\n[1] 0.5226597\n\n\nOkay, pretty close. But, remember this is just a single realization of the estimator. Let’s move on to Step 3 (repeating the above many times). Let’s write a function that does Steps 1 and 2.\n\nget_estimate <- function()\n{\n  x <- runif(100) \n  mean_x <- mean(x)\n  return(mean_x)\n}\n\nYou can now repeat get_estimate() many times. There are numerous ways to do this in R. But, let’s use lapply() here.\n\nestimates <-\n  lapply(\n    1:1000,\n    function(x) get_estimate()\n  ) %>% \n  unlist()\n\nHere is the mean of the estimates (the estimate of \\(E[\\frac{1}{n}\\sum_{i=1}^n x_i]\\)).\n\nmean(estimates)\n\n[1] 0.4991132\n\n\nVery close. Of course, you will not get the exact number you are hoping to get, which is \\(0.5\\) in this case as MC simulation is a random process.\nWhile this example may seem excessively simple, no matter what you are trying to test, the basic steps will be exactly the same."
  },
  {
    "objectID": "A01-mc-simulation.html#another-example",
    "href": "A01-mc-simulation.html#another-example",
    "title": "Appendix A — Monte Carlo (MC) Simulation",
    "section": "A.3 Another Example",
    "text": "A.3 Another Example\nLet’s work on a slightly more complex MC simulations. We are interested in understanding what happens to \\(\\beta_1\\) if \\(E[u|x]\\ne 0\\) when estimating \\(y=\\beta_0+\\beta_1 x + u\\) (classic endogeneity problem).\nLet’s set some parameters first.\n\nB <- 1000 # the number of iterations\nN <- 100 # sample size\n\nLet’s write a code to generate data for a single iteration (Step 1).\n\nmu <- rnorm(N) # the common term shared by both x and u\nx <- rnorm(N) + mu # independent variable\nu <- rnorm(N) + mu # error\ny <- 1 + x + u # dependent variable\ndata <- data.frame(y = y, x = x)\n\nSo, the target parameter (\\(\\beta_1\\)) is 1 in this data generating process. x and u are correlated because they share the common term mu.\n\ncor(x, u)\n\n[1] 0.5222601\n\n\nThis code gets the OLS estimate of \\(\\beta_1\\) (Step 2).\n\nlm(y ~ x, data = data)$coefficient[\"x\"]\n\n       x \n1.518179 \n\n\nOkay, things are not looking good for OLS already.\nLet’s repeat Steps 1 and 2 many times (Step 3).\n\nget_ols_estimate <- function()\n{\n  mu <- rnorm(N) # the common term shared by both x and u\n  x <- rnorm(N) + mu # independent variable\n  u <- rnorm(N) + mu # error\n  y <- 1 + x + u # dependent variable\n  data <- data.frame(y = y, x = x)\n\n  beta_hat <- lm(y ~ x, data = data)$coefficient[\"x\"]\n\n  return(beta_hat)\n}\n\nestimates <-\n  lapply(\n    1:1000,\n    function(x) get_ols_estimate()\n  ) %>% \n  unlist()\n\nYes, the OLS estimator of \\(\\beta_1\\) is biased as we expected.\n\nmean(estimates)\n\n[1] 1.50085\n\nhist(estimates)"
  }
]