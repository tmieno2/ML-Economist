<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-0.9.629">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Introduction to Machine Learning for Economists (Under Construction) - 17&nbsp; Generalized Random Forest</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./PROG-R-01-mlr3.html" rel="next">
<link href="./E01-spatial-cv.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<link href="style.css" rel="stylesheet" type="text/css">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-45VKT2HZSL"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'G-45VKT2HZSL');
</script>
<script src="site_libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="site_libs/viz-1.8.2/viz.js"></script>
<link href="site_libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet">
<script src="site_libs/grViz-binding-1.0.9/grViz.js"></script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Generalized Random Forest</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Introduction to Machine Learning for Economists (Under Construction)</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/tmieno2/ML-Economist" title="Source Code" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
    <a href="" title="Share" id="sidebar-tool-dropdown-0" class="sidebar-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi bi-share"></i></a>
    <ul class="dropdown-menu" aria-labelledby="sidebar-tool-dropdown-0">
        <li>
          <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=|url|">
            <i class="bi bi-bi-twitter pe-1"></i>
          Twitter
          </a>
        </li>
        <li>
          <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
            <i class="bi bi-bi-facebook pe-1"></i>
          Facebook
          </a>
        </li>
        <li>
          <a class="dropdown-item sidebar-tools-main-item" href="https://www.linkedin.com/sharing/share-offsite/?url=|url|">
            <i class="bi bi-bi-linkedin pe-1"></i>
          LinkedIn
          </a>
        </li>
    </ul>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Welcome</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./H00-preface.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
    <div class="sidebar-item-container"> 
        <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Basics</a>
      <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
        <i class="bi bi-chevron-right ms-2"></i>
      </a>
    </div>
    <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./B01-nonlinear.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Non-linear function estimation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./B02-bias-variance-tradeoff.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Bias-variance Trade-off</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./B03-cross-validation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Cross-validation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./B04-regularization.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Regression Shrinkage Methods</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./B05-bootstrap.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Bootstrap</span></a>
  </div>
</li>
    </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
    <div class="sidebar-item-container"> 
        <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">Prediction ML</a>
      <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
        <i class="bi bi-chevron-right ms-2"></i>
      </a>
    </div>
    <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P01-random-forest.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Random Forest</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P02-boosted-regression-forest.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Boosted Regression Forest</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P03-xgb.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Extreme Gradient Boosting</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./P04-local-linear-forest.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Local Linear Forest</span></a>
  </div>
</li>
    </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
    <div class="sidebar-item-container"> 
        <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">C0P-causal-ml.qmd</a>
      <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
        <i class="bi bi-chevron-right ms-2"></i>
      </a>
    </div>
    <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C00-why-not-this.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Why can’t we just do this?</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C01-dml.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Double Machine Learning</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C02-xstr-learner.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">S-, X-, T-, and R-learner</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C03-cf-orf.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Forest-based CATE Estimators</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C04-cf-extension.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Extensions of Causal Forest</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./C05-causal-model-selection.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Causal Model Selection</span></a>
  </div>
</li>
    </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
    <div class="sidebar-item-container"> 
        <a href="./E0P-extensions.html" class="sidebar-item-text sidebar-link">Extensions</a>
      <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
        <i class="bi bi-chevron-right ms-2"></i>
      </a>
    </div>
    <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./E01-spatial-cv.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Spatial Cross-validation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./E02-grf.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Generalized Random Forest</span></a>
  </div>
</li>
    </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
    <div class="sidebar-item-container"> 
        <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">Programming: R</a>
      <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
        <i class="bi bi-chevron-right ms-2"></i>
      </a>
    </div>
    <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./PROG-R-01-mlr3.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Machine Learning with <code>mlr3</code></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./PROG-R-02-reticulate.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Running Python from R</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./PROG-R-03-model-selection-prediction.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Model Selection (Prediction)</span></a>
  </div>
</li>
    </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
    <div class="sidebar-item-container"> 
        <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">Programming: Python</a>
      <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
        <i class="bi bi-chevron-right ms-2"></i>
      </a>
    </div>
    <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./PROG-P-01-scikitlearn.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Prediction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./PROG-P-02-CATE-econml.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Treatment Effect Estimation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./PROG-P-03-model-selection.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Model selection</span></a>
  </div>
</li>
    </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
    <div class="sidebar-item-container"> 
        <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">Appendices</a>
      <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">
        <i class="bi bi-chevron-right ms-2"></i>
      </a>
    </div>
    <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A01-mc-simulation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Monte Carlo (MC) Simulation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./A02-method-of-moment.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Primer on method of moment</span></a>
  </div>
</li>
    </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#random-forest-as-a-local-constant-regression" id="toc-random-forest-as-a-local-constant-regression" class="nav-link active" data-scroll-target="#random-forest-as-a-local-constant-regression"> <span class="header-section-number">17.1</span> Random forest as a local constant regression</a></li>
  <li><a href="#grf" id="toc-grf" class="nav-link" data-scroll-target="#grf"> <span class="header-section-number">17.2</span> GRF</a></li>
  <li><a href="#examples-of-grf" id="toc-examples-of-grf" class="nav-link" data-scroll-target="#examples-of-grf"> <span class="header-section-number">17.3</span> Examples of GRF</a>
  <ul class="collapse">
  <li><a href="#random-forest-as-a-grf" id="toc-random-forest-as-a-grf" class="nav-link" data-scroll-target="#random-forest-as-a-grf"> <span class="header-section-number">17.3.1</span> Random forest as a GRF</a></li>
  <li><a href="#causal-forest-as-a-grf" id="toc-causal-forest-as-a-grf" class="nav-link" data-scroll-target="#causal-forest-as-a-grf"> <span class="header-section-number">17.3.2</span> Causal forest as a GRF</a></li>
  </ul></li>
  <li><a href="#sec-grf-honest" id="toc-sec-grf-honest" class="nav-link" data-scroll-target="#sec-grf-honest"> <span class="header-section-number">17.4</span> Honesty</a></li>
  <li><a href="#understanding-grf-better-by-example-sec-understand-grf-example" id="toc-understanding-grf-better-by-example-sec-understand-grf-example" class="nav-link" data-scroll-target="#understanding-grf-better-by-example-sec-understand-grf-example"> <span class="header-section-number">17.5</span> Understanding GRF better by example (#sec-understand-grf-example)</a>
  <ul class="collapse">
  <li><a href="#orthogonalization-of-y-and-t" id="toc-orthogonalization-of-y-and-t" class="nav-link" data-scroll-target="#orthogonalization-of-y-and-t"> <span class="header-section-number">17.5.1</span> Orthogonalization of <span class="math inline">\(Y\)</span> and <span class="math inline">\(T\)</span></a></li>
  <li><a href="#sec-build-trees" id="toc-sec-build-trees" class="nav-link" data-scroll-target="#sec-build-trees"> <span class="header-section-number">17.5.2</span> Building trees</a></li>
  <li><a href="#prediction" id="toc-prediction" class="nav-link" data-scroll-target="#prediction"> <span class="header-section-number">17.5.3</span> Prediction</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/tmieno2/ML-Economist/edit/master/E02-grf.qmd" class="toc-action">Edit this page</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-grf" class="quarto-section-identifier d-none d-lg-block"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Generalized Random Forest</span></span></h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<div class="callout-important callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
What you will learn
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>What GRF is</li>
<li>How GRF was motivated</li>
<li>Random forest is a special case of GRF</li>
<li>Honesty rule</li>
</ul>
</div>
</div>
<div class="callout-tip callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Background knowledge
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>(necessary) random forest (<a href="P01-random-forest.html#sec-rf"><span>Section&nbsp;6.2</span></a>)</li>
<li>(necessary) local regression (<a href="B01-nonlinear.html#sec-local"><span>Section&nbsp;1.3</span></a>)</li>
</ul>
</div>
</div>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Packages to load for replication
</div>
</div>
<div class="callout-body-container callout-body">
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(data.table)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ranger)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(grf)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<section id="random-forest-as-a-local-constant-regression" class="level2" data-number="17.1">
<h2 data-number="17.1" class="anchored" data-anchor-id="random-forest-as-a-local-constant-regression"><span class="header-section-number">17.1</span> Random forest as a local constant regression</h2>
<p>Suppose you are interested in estimating <span class="math inline">\(E[y|X]\)</span> using a dataset and you have trained a random forest model with <span class="math inline">\(T\)</span> tress. Now, let <span class="math inline">\(\eta_{i,t}(X)\)</span> takes <span class="math inline">\(1\)</span> if observation <span class="math inline">\(i\)</span> belongs to the same leaf as <span class="math inline">\(X\)</span> in tree <span class="math inline">\(t\)</span>, where <span class="math inline">\(X\)</span> is a vector of covariates (<span class="math inline">\(K\)</span> variables). Then, the RF’s predicted value of <span class="math inline">\(y\)</span> conditional on a particular value of <span class="math inline">\(X\)</span> (say, <span class="math inline">\(X_0\)</span>) can be written as follows:</p>
<p><span class="math display">\[
\begin{aligned}
\hat{y}(X_0) = \frac{1}{T} \cdot\sum_{t=1}^T\sum_{i=1}^N \frac{\eta_{i,t}(X_0)}{\sum_{i=1}^N\eta_{i,t}(X_0)}\cdot y_i
\end{aligned}
\]</span></p>
<p>Note that <span class="math inline">\(\sum_{i=1}^N\eta_{i,t}(X_0)\)</span> represents the number of observations in the same leaf as <span class="math inline">\(X_0\)</span>. Therefore, <span class="math inline">\(\sum_{i=1}^N \frac{\eta_{i,t}(X_0)}{\sum_{i=1}^N\eta_{i,t}(X_0)}\cdot y_i\)</span> is the average value of <span class="math inline">\(y\)</span> of the leaf that <span class="math inline">\(X_0\)</span> belongs to. So, while looking slightly complicated, it is the average value of <span class="math inline">\(y\)</span> of the tree <span class="math inline">\(X_0\)</span> belongs to averaged across the trees, which we know is how RF predicts <span class="math inline">\(y\)</span> at <span class="math inline">\(X_0\)</span>.</p>
<p>We can switch the summations like this,</p>
<p><span class="math display">\[
\begin{aligned}
\hat{y}(X_0) = \sum_{i=1}^N \cdot\frac{1}{T}\sum_{t=1}^T\frac{\eta_{i,t}(X_0)}{\sum_{i=1}^N\eta_{i,t}(X_0)}\cdot y_i
\end{aligned}
\]</span></p>
<p>Let <span class="math inline">\(\alpha(X_i, X_0)\)</span> denote <span class="math inline">\(\frac{1}{T}\sum_{t=1}^T\frac{\eta_{i,t}(X_0)}{\sum_{i=1}^N\eta_{i,t}(X_0)}\)</span>. Then, we can rewrite the above equation as</p>
<p><span class="math display">\[
\begin{aligned}
\hat{y}(X_0) = \sum_{i=1}^N \alpha(X_i,X_0) \cdot y_i
\end{aligned}
\]</span></p>
<p>Now, it is easy to show that <span class="math inline">\(\hat{y}(X_0)\)</span> is a solution to the following minimization problem.</p>
<p><span id="eq-ll-constant"><span class="math display">\[
\begin{aligned}
Min_{\theta} \sum_{i=1}^N \alpha(X_i,X_0)\cdot[y_i -\theta]^2
\end{aligned}
\tag{17.1}\]</span></span></p>
<p>In this formulation of the problem, <span class="math inline">\(\alpha(X_i,X_0)\)</span> can be considered the weight given to observation <span class="math inline">\(i\)</span>.</p>
<p>By definition,</p>
<ul>
<li><span class="math inline">\(0 \leq \alpha(X_i,X_0) \leq 1\)</span></li>
<li><span class="math inline">\(\sum_{i=1}^N \alpha(X_i,X_0) = 1\)</span></li>
</ul>
<p>You may notice that <a href="#eq-ll-constant">Equation&nbsp;<span>17.1</span></a> is actually a special case of <span style="color:blue">local constant regression</span> where the individual weights are <span class="math inline">\(\alpha(X_i, X_0)\)</span>. Roughly speaking, <span class="math inline">\(\alpha(X_i, X_0)\)</span> measures how often observation <span class="math inline">\(i\)</span> share the same leaves as the evaluation point (<span class="math inline">\(X_0\)</span>) across <span class="math inline">\(T\)</span> trees. So, it measures how similar <span class="math inline">\(X_i\)</span> is to <span class="math inline">\(X_0\)</span> in the RF way, but not based on euclidean distance (which is subject to curse of dimensionality). So, RF is actually a local constant regression with a special way of distributing weights to the individual observations.</p>
</section>
<section id="grf" class="level2 page-columns page-full" data-number="17.2">
<h2 data-number="17.2" class="anchored" data-anchor-id="grf"><span class="header-section-number">17.2</span> GRF</h2>
<p>Interpretation of RF as a local regression led <span class="citation" data-cites="athey2019generalized">Athey, Tibshirani, and Wager (<a href="#ref-athey2019generalized" role="doc-biblioref">2019</a>)</span> to conceive GRF, under which various statistics (e.g., conditional average treatment effect, conditional quantile) can be estimated under the unified framework.</p>
<p>You can consider prediction of <span class="math inline">\(y\)</span> at <span class="math inline">\(X = X_0\)</span> using RF as a three-step process.</p>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Predicting <span class="math inline">\(y\)</span> at <span class="math inline">\(X=X_0\)</span> using RF
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li>Grow trees (find splitting rules for each tree)</li>
<li>For <span class="math inline">\(X = X_0\)</span>, find the weights for individual observations based on the trees grown<br>
</li>
<li>Solve <a href="#eq-ll-constant">Equation&nbsp;<span>17.1</span></a> based on the weights</li>
</ol>
</div>
</div>
<p>Step 1 is done only once. Every time you make a prediction at a different value of <span class="math inline">\(X\)</span>, you go over steps 2 and 3.</p>
<p>GRF follows exactly the same steps except that it <span style="color:red">adjusts the way trees are grown </span>in step 1 and <span style="color:red">adjusts the way the local minimization problem</span> is solved at step 3 (they are actually interrelated) depending on what you would like to estimate.</p>
<p>Here, we follow the notations used in <span class="citation" data-cites="athey2019generalized">Athey, Tibshirani, and Wager (<a href="#ref-athey2019generalized" role="doc-biblioref">2019</a>)</span> as much as possible. Here are the list of notations:</p>
<ul>
<li><span class="math inline">\(O_i\)</span>: data for observation <span class="math inline">\(i\)</span></li>
<li><span class="math inline">\(\theta\)</span>: the statistics of interest (e.g., CATE for causal forest, conditional quantile for for quantile forest)</li>
<li><span class="math inline">\(\nu\)</span>: nuisance statistics (you are not interested in estimating this).</li>
<li><span class="math inline">\(\Psi_{\theta, \nu}(O)\)</span>: score function</li>
<li><span class="math inline">\(\alpha_i(x)\)</span>: weight given to observation <span class="math inline">\(i\)</span> when predicting at <span class="math inline">\(X=x\)</span>.</li>
</ul>
<p>In general, GRF solves the following problem to find the estimate of <span class="math inline">\(\theta\)</span> conditional on <span class="math inline">\(X_i= x\)</span>:</p>
<p><span id="eq-opt"><span class="math display">\[
\begin{aligned}
\sum_{i=1}^n \alpha_i(x)\Psi_{\theta, \nu}(O_i) = 0
\end{aligned}
\tag{17.2}\]</span></span></p>
<p>As we saw earlier, GRF is RF when <span class="math inline">\(\Psi_{\theta, \nu}(O_i) = Y_i-\theta\)</span>. There is no nuisance statistics, <span class="math inline">\(\nu(x)\)</span>, in the RF case. By changing how the score function (<span class="math inline">\(\Psi_{\theta, \nu}(O_i)\)</span>) is defined, you can estimate <span style="color:blue"> a wide range of statistics using different approaches </span>under the <span style="color:blue"> same</span> estimation framework (this is why it is called <span style="color:red">generalized </span>random forest). Here are some of the statistics and estimation approaches that are under the GRF framework.</p>
<ul>
<li>Conditional expectation (<span class="math inline">\(E[Y|X]\)</span>)
<ul>
<li>Regression forest (Random forest for regression)</li>
<li>Local linear forest</li>
<li>Boosted regression forest</li>
</ul></li>
<li>Conditional average treatment effect (CATE)
<ul>
<li>Causal forest</li>
<li>Instrumental forest</li>
</ul></li>
<li>Conditional quantile
<ul>
<li>Quantile forest</li>
</ul></li>
</ul>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Score function examples
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><p>Random forest: <span class="math inline">\(\Psi_{\theta, \nu}(O_i) = Y_i - \theta\)</span></p></li>
<li><p>Causal forest: <span class="math inline">\(\Psi_{\theta, \nu}(O_i) = (\tilde{Y}_i- \theta\tilde{T}_i)\cdot\tilde{T}_i\)</span>, where <span class="math inline">\(\tilde{v_i} = v_i - E[v_i|X_i]\)</span></p></li>
<li><p>Quantile forest: <span class="math inline">\(\Psi_{\theta, \nu}(O_i) = qI\{Y_i &gt; \theta\} - (1-q)I\{Y_i \leq \theta\}\)</span></p></li>
</ul>
</div>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<p><span class="math inline">\(I\{\}\)</span> is an indicator function that takes 1 if the condition inside the curly brackets and 0 otherwise.</p>
</div></div><hr>
<p>So far, we have only talked about score functions so far, but not the weights. Do all the approaches listed above use the weights derived from the trees grown by the traditional RF in solving <a href="#eq-opt">Equation&nbsp;<span>17.2</span></a>? You could (you are free to use any weights), but that would not be wise. As mentioned earlier, GRF adjusts the way trees are grown (thus weights) as well according to the score function so that weights are optimized for your objective. This makes sense. Right <span style="color:blue"> neighbors </span>should be different based on what you are interesting in estimating.</p>
<p>Specifically, GRF uses the random forest <span style="color:blue">algorithm</span> to grow trees based on <span style="color:blue"> pseudo outcome (<span class="math inline">\(\rho_i\)</span>) </span> derived from the score function that is specific to the type of regression you are running, but not on <span class="math inline">\(Y\)</span>. Basically, you are using exactly the same algorithm as the traditional RF we saw in <a href="P01-random-forest.html#sec-rf"><span>Section&nbsp;6.2</span></a> except that <span class="math inline">\(Y\)</span> is swapped with the pseudo outcome.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>See <span class="citation" data-cites="athey2019generalized">Athey, Tibshirani, and Wager (<a href="#ref-athey2019generalized" role="doc-biblioref">2019</a>)</span> for how the pseudo outcome is derived from a score function in general.</p>
</div></div><div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Example pseudo outcomes
</div>
</div>
<div class="callout-body-container callout-body">
<p><span style="color:blue">Random forest</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\rho_i = Y_i - \hat{\theta}_P
\end{aligned}
\]</span></p>
<p><span style="color:blue"> Causal forest</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\rho_i = (\tilde{Y}_i- \hat{\theta}_P \tilde{T}_i)\cdot\tilde{T}_i
\end{aligned}
\]</span></p>
<p><span style="color:blue"> Quantile forest</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\rho_i = I\{Y_i &gt; \hat{\theta}_P\}
\end{aligned}
\]</span></p>
</div>
</div>
<p>Note that <span class="math inline">\(\hat{\theta}_P\)</span> in the pseudo outcomes presented above is the solution to <a href="#eq-opt">Equation&nbsp;<span>17.2</span></a> with their respective score functions using the data in the parent node. For example, <span class="math inline">\(\hat{\theta}_P = \bar{Y}_p\)</span> for RF, which is the average value of <span class="math inline">\(Y\)</span> in the parent node. In quantile forest, <span class="math inline">\(\hat{\theta}_P\)</span> is the <span class="math inline">\(q\)</span>th quantile of the parent node if you are estimating the <span class="math inline">\(q\)</span>the conditional quantile.</p>
<hr>
<p>Finally, here are the conceptual steps of GRF:</p>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
GRF: conceptual steps
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Training (Forest Growing) Step:
<ol type="1">
<li>Determine what statistics you are interested in (<span class="math inline">\(\theta(X)\)</span>, e.g., CATE, conditional quantile)</li>
<li>Define the appropriate score function according to the statistics of interest</li>
<li>Derive the pseudo outcome based on the score function</li>
<li>Grow trees using the RF algorithm based on the pseudo outcome</li>
</ol></li>
<li>Prediction Step:
<ol type="1">
<li><p>Determine what value of <span class="math inline">\(X\)</span> you would like to predict <span class="math inline">\(\theta(X)\)</span> (call it <span class="math inline">\(X_0\)</span>)</p></li>
<li><p>Find the individual weights <span class="math inline">\(\alpha(X_i, X_0)\)</span> according to <span class="math display">\[
\begin{aligned}
\alpha(X_i, X_0) = \frac{1}{T}\sum_{t=1}^T\frac{\eta_{i,t}(X_0)}{\sum_{i=1}^N\eta_{i,t}(X_0)}
\end{aligned}
\]</span> , which measures how often observation <span class="math inline">\(i\)</span> belongs to the same node as the evaluation point <span class="math inline">\(X_0\)</span>.</p></li>
<li><p>Solve <a href="#eq-opt">Equation&nbsp;<span>17.2</span></a> with the weights obtained above and the score function specified earlier</p></li>
</ol></li>
</ul>
</div>
</div>
<p>The training step is done only once (trees are built only once). But, whenever you predict <span class="math inline">\(\theta(X)\)</span> at different values of <span class="math inline">\(X\)</span>, you go through the prediction step.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>Orthogonal random forest (a forest-based heterogeneous treatment effect estimator like causal forest), on the other hand, build trees every time when predicting treatment effect <span class="math inline">\(\theta(X)\)</span> at particular values of <span class="math inline">\(X\)</span>, which is why orthogonal random forest takes a very long time especially when there are many evaluation points. Orthogonal random forest is covered in <a href="C03-cf-orf.html"><span>Chapter&nbsp;13</span></a>.</p>
</div></div></section>
<section id="examples-of-grf" class="level2" data-number="17.3">
<h2 data-number="17.3" class="anchored" data-anchor-id="examples-of-grf"><span class="header-section-number">17.3</span> Examples of GRF</h2>
<section id="random-forest-as-a-grf" class="level3" data-number="17.3.1">
<h3 data-number="17.3.1" class="anchored" data-anchor-id="random-forest-as-a-grf"><span class="header-section-number">17.3.1</span> Random forest as a GRF</h3>
<p>Here, we take a look at RF as a GRF as an illustration to understand the general GRF procedure better. When <span class="math inline">\(\Psi_{\theta, \nu}(Y_i)\)</span> is set to <span class="math inline">\(Y_i - \theta\)</span>, then GRF is equivalent to the traditional RF. By plugging <span class="math inline">\(Y_i - \theta\)</span> into <a href="#eq-opt">Equation&nbsp;<span>17.2</span></a>, the estimate of <span class="math inline">\(E[Y|X=X_0]\)</span>, <span class="math inline">\(\theta(X_0)\)</span>, is identified by solving</p>
<p><span id="eq-rf"><span class="math display">\[
\begin{aligned}
\sum_{i=1}^n \alpha_i(X_0)(Y_i - \theta) = 0
\end{aligned}
\tag{17.3}\]</span></span></p>
<p>The weights <span class="math inline">\(\alpha_i(X_i, X_0)\)</span> are defined as follows:</p>
<p><span class="math display">\[
\begin{aligned}
\alpha_i(X_i, X_0) = \frac{1}{T}\cdot\sum_{t=1}^T\frac{\eta_{i,t}(X_0)}{\sum_{i=1}^N\eta_{i,t}(X_0)}
\end{aligned}
\]</span></p>
<p>, where <span class="math inline">\(\eta_{i,t}(X_0)\)</span> is 1 if observation <span class="math inline">\(i\)</span> which has feature values <span class="math inline">\(X_i\)</span> belongs to the same leaf as <span class="math inline">\(X_0\)</span>.</p>
<hr>
<p><span style="color:blue"> Step 1: Grow trees</span>:</p>
<p>Now, let’s consider growing trees to find <span class="math inline">\(\alpha_i(X_0)\)</span> in <a href="#eq-rf">Equation&nbsp;<span>17.3</span></a>. For a given sample and set of variables randomly selected, GRF starts with solving the unweighted version of <a href="#eq-rf">Equation&nbsp;<span>17.3</span></a>.</p>
<p><span id="eq-rf-initial"><span class="math display">\[
\begin{aligned}
\sum_{i=1}^n Y_i - \theta = 0
\end{aligned}
\tag{17.4}\]</span></span></p>
<p>The solution to this problem is simply the mean of <span class="math inline">\(Y\)</span>, which will be denoted as <span class="math inline">\(\bar{Y}_P\)</span>, where <span class="math inline">\(P\)</span> represents the parent node. Here, the parent node include all the data points as this is the first split.</p>
<p>Then the pseudo outcome (<span class="math inline">\(\rho_i\)</span>) that is used in splitting is</p>
<p><span class="math display">\[
\begin{aligned}
\rho_i = Y_i - \bar{Y}_P
\end{aligned}
\]</span></p>
<p>Now, a standard CART regression split is applied on the pseudo outcomes. That is, the variable-threshold combination that maximizes the following criteria is found:</p>
<p><span id="eq-criteria"><span class="math display">\[
\begin{aligned}
\tilde{\Delta}(C_1, C_2) = \frac{(\sum_{i \in C_1} \rho_i)^2}{N_{C_1}} + \frac{(\sum_{i \in C_2} \rho_i)^2}{N_{C_2}}
\end{aligned}
\tag{17.5}\]</span></span></p>
<p>where <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span> represent two child node candidates for a given split. Since <span class="math inline">\(\bar{Y}_P\)</span> is just a constant, it is equivalent to splitting on <span class="math inline">\(Y_i\)</span>. So, this is exactly the same as how the traditional RF builds trees (see <a href="P01-random-forest.html#sec-split-sim"><span>Section&nbsp;6.1.2</span></a> for the rationale behind maximizing the criteria presented in <a href="#eq-criteria">Equation&nbsp;<span>17.5</span></a>).</p>
<p>Once the best split is identified, each of the new child nodes is split following the exactly the same procedure. Splitting continues until one of the user-specified condition prevent a further splitting. Many trees from bootstrapped samples are created (just like the regular random forest) and they form a forest. This shows that the GRF with <span class="math inline">\(\Psi_{\theta, \nu}(Y_i, X_i) = Y_i - \theta\)</span> grows trees in the same manner as the traditional RF. RF in GRF is implemented by <code>regression_forest()</code>. But, note that running <code>ranger()</code> and <code>regression_forest()</code> will not result in the same forest because of the randomness involved in resampling and random selection of variables. Only their algorithms are equivalent</p>
<hr>
<p><span style="color:blue"> Step 2: Predict</span>:</p>
<p>To predict <span class="math inline">\(E[Y|X=X_0]\)</span>, <a href="#eq-rf">Equation&nbsp;<span>17.3</span></a> is solved</p>
<p><span class="math display">\[
\begin{aligned}
\sum_{i=1}^N \alpha_i(X_i, X_0)(Y_i-\theta) = 0
\end{aligned}
\]</span></p>
<p>So,</p>
<p><span class="math display">\[
\begin{aligned}
\theta(X_0) &amp; = \frac{\sum_{i=1}^N \alpha_i(X_0)Y_i}{\sum_{i=1}^N \alpha_i(X_0)}\\
&amp; = \sum_{i=1}^N \alpha_i(X_0)Y_i \;\; \mbox{(since } \sum_{i=1}^N \alpha_i(X_0) = 1\mbox{)} \\
&amp; = \sum_{i=1}^N \huge[\normalsize \frac{1}{T}\cdot\sum_{t=1}^T\frac{\eta_{i,t}(X_0)}{\sum_{i=1}^N\eta_{i,t}(X_0)}\cdot y_i\huge]\\
&amp; = \frac{1}{T}  \cdot\sum_{t=1}^T\sum_{i=1}^N \frac{\eta_{i,t}(X_0)}{\sum_{i=1}^N\eta_{i,t}(X_0)}\cdot y_i \;\; \mbox{(changing the order of the summations)} \\
&amp; = \frac{1}{T} \cdot\sum_{t=1}^T \bar{Y}_t
\end{aligned}
\]</span></p>
<p>So, <span class="math inline">\(\theta(X_0)\)</span> from GRF is the average of tree-specific predictions for <span class="math inline">\(X_0\)</span>, which is exactly how RF predicts <span class="math inline">\(E[Y|X=X_0]\)</span> as well.</p>
<p>So, it has been shown that GRF with <span class="math inline">\(\Psi_{\theta, \nu}(Y_i)=Y_i - \theta\)</span> grows trees in the same manner as the traditional RF and also that GRF predicts <span class="math inline">\(E[Y|X=X_0]\)</span> in the same manner as the traditional RF. So, RF is a special case of GRF, where <span class="math inline">\(\Psi_{\theta, \nu}(Y_i)=Y_i - \theta\)</span>.</p>
</section>
<section id="causal-forest-as-a-grf" class="level3" data-number="17.3.2">
<h3 data-number="17.3.2" class="anchored" data-anchor-id="causal-forest-as-a-grf"><span class="header-section-number">17.3.2</span> Causal forest as a GRF</h3>
<p>Causal forest (with a single treatment variable) as an R-learner is a GRF with</p>
<p><span class="math display">\[
\begin{aligned}
\Psi_{\theta, \nu}(O_i) = [(Y_i - E[Y|X_i])- \theta(X_i)(T_i - E[T|X_i])](T_i - E[T|X_i])
\end{aligned}
\]</span></p>
<p>In practice <span class="math inline">\(E[Y|X_i]\)</span> and <span class="math inline">\(E[T|X_i]\)</span> are first estimated using appropriate machine learning methods (e.g., lasso, random forest, gradient boosted forest) in a cross-fitting manner and then the estimation of <span class="math inline">\(Y_i - E[Y|X_i]\)</span> and <span class="math inline">\(T_i - E[T|X_i]\)</span> are constructed. Let’s denote them by <span class="math inline">\(\tilde{Y}_i\)</span> and <span class="math inline">\(\tilde{T}_i\)</span>. Then the score function is written as</p>
<p><span id="eq-cf-score"><span class="math display">\[
\begin{aligned}
\Psi_{\theta} = (\tilde{Y}_i- \theta\tilde{T}_i)\tilde{T}_i
\end{aligned}
\tag{17.6}\]</span></span></p>
<p>So, the conditional treatment effect at <span class="math inline">\(X=X_0\)</span>, <span class="math inline">\(\theta(X_0)\)</span>, is estimated by solving the following problem:</p>
<p><span id="eq-cf-solve"><span class="math display">\[
\begin{aligned}
\hat{\theta}(X_0) = \sum_{i=1}^N \alpha_i(x)(\tilde{Y_i} - \theta\cdot \tilde{T_i})\tilde{T_i} = 0
\end{aligned}
\tag{17.7}\]</span></span></p>
<p>where <span class="math inline">\(\alpha_i(x)\)</span> is the weight obtained from the trees built using random forest on the pseudo outcomes that are derived from the score function (<a href="#eq-cf-score">Equation&nbsp;<span>17.6</span></a>).</p>
<p>For a given parent node <span class="math inline">\(p\)</span>, the pseudo outcomes are defined as</p>
<p><span class="math display">\[
\begin{aligned}
\rho_i = (\tilde{Y}_i- \hat{\theta}_P\tilde{T}_i)\tilde{T}_i
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\hat{\theta}_P\)</span> is the solution of the following problem using the observations in the parent node:</p>
<p><span class="math display">\[
\begin{aligned}
\sum_{i=1}^N \tilde{T_i}(\tilde{Y_i}-\theta \tilde{T_i}) = 0
\end{aligned}
\]</span></p>
<p>, which is the unweighted version of <a href="#eq-cf-solve">Equation&nbsp;<span>17.7</span></a>.</p>
<p>The standard CART splitting algorithm is applied on the pseudo outcomes to build trees.</p>
</section>
</section>
<section id="sec-grf-honest" class="level2 page-columns page-full" data-number="17.4">
<h2 data-number="17.4" class="anchored" data-anchor-id="sec-grf-honest"><span class="header-section-number">17.4</span> Honesty</h2>
<p>GRF applies <span style="color:blue"> honesty </span>when it trains forests. Specifically, when building a tree, the bootstrapped sample is first split into two groups: subsamples for <span style="color:blue"> splitting</span> and <span style="color:blue">prediction</span>. Then, the a tree is trained on the subsample for splitting and then generate the splitting rules. However, when predicting (say <span class="math inline">\(E[Y|X]\)</span> at <span class="math inline">\(X=x\)</span>), the value of <span class="math inline">\(Y\)</span> from the subsamples for splitting are not used. Rather, only the splitting rules are taken from the trained tree and then they are applied to the subsamples for prediction (<a href="#fig-honest">Figure&nbsp;<span>17.1</span></a> illustrates this process).</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><strong>Packages to load for replication</strong></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(data.table)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(grf)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rattle)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(wooldridge)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div></div><div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>DiagrammeR<span class="sc">::</span><span class="fu">grViz</span>(</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="st">"</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="st">digraph {</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="st">  graph [ranksep = 0.2, fontsize = 4]</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="st">  node [shape = box]</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="st">    SS [label = 'Subsamples for splitting']</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="st">    SP [label = 'Subsamples for predicting']</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="st">    BD [label = 'Bootstrapped Data']</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="st">    TT [label = 'Trained tree']</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="st">    PV [label = 'Predicted value']</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="st">  edge [minlen = 2]</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="st">    BD-&gt;SP</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="st">    BD-&gt;SS</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="st">    SS-&gt;TT</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a><span class="st">    SP-&gt;PV</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="st">    TT-&gt;SP [label='apply the splitting rules']</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="st">  { rank = same; SS; SP}</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="st">  { rank = same; TT}</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a><span class="st">  { rank = same; PV}</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="st">"</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-honest" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<div id="htmlwidget-d352795399bb0c96dbae" style="width:100%;height:325px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-d352795399bb0c96dbae">{"x":{"diagram":"\ndigraph {\n  graph [ranksep = 0.2, fontsize = 4]\n  node [shape = box]\n    SS [label = \"Subsamples for splitting\"]\n    SP [label = \"Subsamples for predicting\"]\n    BD [label = \"Bootstrapped Data\"]\n    TT [label = \"Trained tree\"]\n    PV [label = \"Predicted value\"]\n  edge [minlen = 2]\n    BD->SP\n    BD->SS\n    SS->TT\n    SP->PV\n    TT->SP [label=\"apply the splitting rules\"]\n  { rank = same; SS; SP}\n  { rank = same; TT}\n  { rank = same; PV}\n}\n","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
<p></p><figcaption aria-hidden="true" class="figure-caption">Figure 17.1: Illustration of an honest tree</figcaption><p></p>
</figure>
</div>
</div>
<p>Let’s demonstrate this using a very simple regression tree with two terminal nodes using the <code>mlb</code> data from the <code>wooldridge</code> package.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(mlb1)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>mlb1_dt <span class="ot">&lt;-</span> <span class="fu">data.table</span>(mlb1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We would like to train a RF using this data where the dependent variable is logged salary (<code>lsalary</code>). We will illustrate the honesty rule by working on building a single tree within the process of building a forest.</p>
<p>We first bootstrap data.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">358349</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>num_obs <span class="ot">&lt;-</span> <span class="fu">nrow</span>(mlb1_dt)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>row_indices <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">seq_len</span>(num_obs), num_obs, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>boot_mlb1_dt <span class="ot">&lt;-</span> mlb1_dt[row_indices, ]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We now split the bootstrapped data into two groups: for splitting and prediction.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>rows_split <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">seq_len</span>(num_obs), num_obs <span class="sc">/</span> <span class="dv">2</span>, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co">#=== data for splitting ===#</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>split_data <span class="ot">&lt;-</span> boot_mlb1_dt[rows_split, ]</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co">#=== data for prediction ===#</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>eval_data <span class="ot">&lt;-</span> boot_mlb1_dt[<span class="sc">-</span>rows_split, ]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We then train a tree using the data for splitting (<code>split_data</code>):</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co">#=== build a simple tree ===#</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>tree_trained <span class="ot">&lt;-</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rpart</span>(</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    lsalary <span class="sc">~</span> hits <span class="sc">+</span> runsyr, </span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> split_data, </span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">control =</span> <span class="fu">rpart.control</span>(<span class="at">minsplit =</span> <span class="dv">120</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="fu">fancyRpartPlot</span>(tree_trained, <span class="at">digits =</span> <span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-tree-sub" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="E02-grf_files/figure-html/fig-tree-sub-1.png" class="img-fluid figure-img" width="672"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Figure 17.2: A simple regression tree using the subsamples for splitting</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>So the splitting rule is <code>hits &lt; 356</code> as shown in <a href="#fig-tree-sub">Figure&nbsp;<span>17.2</span></a>. At the terminal nodes, you see the prediction of <code>lsalary</code>: <span class="math inline">\(12.47\)</span> for the left and <span class="math inline">\(14.23\)</span> for the right. These predictions are NOT honest. They are obtained from the observed values of <code>lsalary</code> within the node using the splitting data (the data the tree is trained for). Instead of using these prediction values, an honest prediction applied the splitting rules (<code>hits &lt; 356</code>) to the data reserved for prediction.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>honest_pred <span class="ot">&lt;-</span> eval_data[, <span class="fu">mean</span>(lsalary), <span class="at">by =</span> hits <span class="sc">&lt;</span> <span class="dv">356</span>]</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    hits       V1
1: FALSE 14.18867
2:  TRUE 12.49114</code></pre>
</div>
</div>
<p>So, instead of <span class="math inline">\(12.47\)</span> and <span class="math inline">\(14.23\)</span>, the predicted values from the honest tree are <span class="math inline">\(12.49\)</span> and <span class="math inline">\(14.19\)</span> for the left and right nodes, respectively. Trees are built in this manner many times to form a forest.</p>
<p>More generally, in GRF, honesty is applied by using the evaluation data to solve <a href="#eq-opt">Equation&nbsp;<span>17.2</span></a> based on the weight <span class="math inline">\(\alpha_i(x)\)</span> derived from the trained forest using the splitting data. Honesty is required for the GRF estimator to be consistent and asymptotically normal <span class="citation" data-cites="athey2019generalized">(<a href="#ref-athey2019generalized" role="doc-biblioref">Athey, Tibshirani, and Wager 2019</a>)</span>. However, the application of honesty can do more damage than help when the sample size is small.</p>
</section>
<section id="understanding-grf-better-by-example-sec-understand-grf-example" class="level2 page-columns page-full" data-number="17.5">
<h2 data-number="17.5" class="anchored" data-anchor-id="understanding-grf-better-by-example-sec-understand-grf-example"><span class="header-section-number">17.5</span> Understanding GRF better by example (#sec-understand-grf-example)</h2>
<p>This section goes through CF model training and CATE prediction step by step with codes to enhance our understanding of how GRF works. In the process, we also learn the role of many of the hyper-parameters. They include those that are common across all the GRF methods and those that are specific to CF.</p>
<p>While the process explained here is for CF, the vast majority of the process is exactly the same for other models under GRF. Here are some differences:</p>
<ul>
<li>Orthogonalization of the data at the beginning for CF</li>
<li>The definition of the pseudo outcome (but, the way they are used in the algorithm is identical once they are defined)</li>
<li>The role of <code>min.node.size</code>, <code>alpha</code>, and <code>imbalance.penalty</code> used for safeguarding against extreme splits</li>
</ul>

<div class="no-row-height column-margin column-container"><div class="">
<p>Obviously, the codes here are only for illustration and should not be used in practice.</p>
</div></div><p>We will use a synthetic data generated from the following generating process:</p>
<p><span class="math display">\[
\begin{aligned}
y = \theta(X)\cdot T + g(X) + \mu
\end{aligned}
\]</span></p>
<p>, where</p>
<p><span class="math display">\[
\begin{aligned}
\theta(X) &amp; = 2 + 3 \times x_1^2 - \sqrt{3 \times  x_2^3 + 1}\\
g(X) &amp; = 10 \cdot [log(x_2+1) + 2\cdot x_3\times x_2]
\end{aligned}
\]</span></p>
<p>We have 10 feature variables <span class="math inline">\(x_1, \dots, x_{10}\)</span> and only <span class="math inline">\(x_1\)</span> through <span class="math inline">\(x_3\)</span> plays a role.</p>
<ul>
<li><span class="math inline">\(x_1, x_2 \sim U[0, 3]^2\)</span></li>
<li><span class="math inline">\(x_3 \sim N(1, 1)\)</span></li>
<li><span class="math inline">\(x_4, \dots, x_{10} \sim U[0, 1]^{10}\)</span></li>
</ul>
<p>The error term and the treatment variable is defined as follows.</p>
<ul>
<li><span class="math inline">\(\mu \sim N(0, 1)\)</span></li>
<li><span class="math inline">\(T \sim Ber(0.5)\)</span></li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">73843</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">1000</span> <span class="co"># number of observations</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>x4_x10 <span class="ot">&lt;-</span> </span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">matrix</span>(<span class="fu">runif</span>(N <span class="sc">*</span> <span class="dv">7</span>), <span class="at">nrow =</span> N) <span class="sc">%&gt;%</span> </span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data.table</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">setnames</span>(<span class="fu">names</span>(.), <span class="fu">paste0</span>(<span class="st">"x"</span>, <span class="dv">4</span><span class="sc">:</span><span class="dv">10</span>))</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> </span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data.table</span>(</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">x1 =</span> <span class="dv">3</span> <span class="sc">*</span> <span class="fu">runif</span>(N),</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">x2 =</span> <span class="dv">3</span> <span class="sc">*</span> <span class="fu">runif</span>(N),</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    <span class="at">x3 =</span> <span class="fu">rnorm</span>(N, <span class="at">mean =</span> <span class="dv">1</span>, <span class="at">sd =</span> <span class="dv">1</span>),</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    <span class="at">mu =</span> <span class="fu">rnorm</span>(N),</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>    <span class="at">T =</span> <span class="fu">runif</span>(N) <span class="sc">&gt;</span> <span class="fl">0.5</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>  <span class="co">#=== theta(X) ===#</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>  .[, <span class="at">theta_x :=</span> <span class="dv">2</span> <span class="sc">+</span> <span class="dv">3</span> <span class="sc">*</span> x1<span class="sc">^</span><span class="dv">2</span> <span class="sc">-</span> <span class="fu">sqrt</span>(<span class="dv">3</span><span class="sc">*</span> x2<span class="sc">^</span><span class="dv">3</span> <span class="sc">+</span> <span class="dv">2</span>)] <span class="sc">%&gt;%</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>  <span class="co">#=== g(X) ===#</span></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>  .[, <span class="at">g_x :=</span> <span class="dv">10</span> <span class="sc">*</span> (<span class="fu">log</span>(x2 <span class="sc">+</span> <span class="dv">1</span>) <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> x3 <span class="sc">*</span> x2)] <span class="sc">%&gt;%</span> </span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>  .[, <span class="at">y :=</span> theta_x <span class="sc">*</span> T <span class="sc">+</span> g_x <span class="sc">+</span> mu] <span class="sc">%&gt;%</span> </span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>  .[, .(y, T, x1, x2, x3)] <span class="sc">%&gt;%</span> </span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cbind</span>(., x4_x10)</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>               y     T        x1        x2          x3        x4        x5
   1:  31.239440 FALSE 2.6951250 1.1965331  0.96678638 0.7210003 0.6207226
   2:  10.144462 FALSE 0.7919764 0.2722983  1.34510173 0.6267043 0.8869069
   3:  48.939026  TRUE 2.7143095 0.9040185  1.19934789 0.5620508 0.5455276
   4:  36.405483  TRUE 1.0628966 1.0659064  1.20667390 0.3638505 0.3201202
   5:  11.893620 FALSE 2.8824810 1.8198348  0.03356061 0.2570614 0.1447951
  ---                                                                     
 996:  74.029985  TRUE 1.6485987 2.8588233  1.04216169 0.6835214 0.7266654
 997:  21.176748  TRUE 2.0208231 0.4248353  0.49803263 0.2851495 0.8076055
 998:   3.388729 FALSE 1.9203044 0.5051511 -0.02137629 0.9961128 0.8446825
 999:   9.459730 FALSE 1.7274390 0.3468632  0.85507892 0.9669933 0.4943592
1000: 109.282950 FALSE 1.7030476 2.1927368  2.21194227 0.1442201 0.6783044
              x6         x7         x8         x9       x10
   1: 0.78698773 0.88919853 0.25543604 0.05640156 0.3560356
   2: 0.44413365 0.27657781 0.08810472 0.20397022 0.7982468
   3: 0.45238337 0.41956478 0.69912161 0.05602666 0.5905471
   4: 0.71060284 0.11400965 0.43547480 0.57807247 0.5707400
   5: 0.46912302 0.31966840 0.90711440 0.15150039 0.9426457
  ---                                                      
 996: 0.05036928 0.18803515 0.40981100 0.04716956 0.6443493
 997: 0.14171969 0.08375323 0.78676094 0.27132948 0.6570423
 998: 0.36225572 0.47555166 0.40332714 0.28135063 0.9657502
 999: 0.43818445 0.59426511 0.53555370 0.91860942 0.3339725
1000: 0.23674030 0.25005706 0.05512990 0.24352242 0.7227397</code></pre>
</div>
</div>
<section id="orthogonalization-of-y-and-t" class="level3" data-number="17.5.1">
<h3 data-number="17.5.1" class="anchored" data-anchor-id="orthogonalization-of-y-and-t"><span class="header-section-number">17.5.1</span> Orthogonalization of <span class="math inline">\(Y\)</span> and <span class="math inline">\(T\)</span></h3>
<p>Causal forest (and instrumental forest) first orthogonalizes <span class="math inline">\(Y\)</span> and <span class="math inline">\(T\)</span>. This is not the case for the other GRF methods. By default, <code>causal_forest()</code> uses random forest to estimate <span class="math inline">\(E[Y|X]\)</span> and <span class="math inline">\(E[T|X]\)</span>, and then use out-of-bag predictions. Since the treatment assignment is randomized, we could use <span class="math inline">\(0.5\)</span> for as the estimate for <span class="math inline">\(E[T|X]\)</span> for all the observations. But, we will estimate both in this example.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co">#--------------------------</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="co"># E[Y|X]</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co">#--------------------------</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co">#=== train an RF on y ===#</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>y_rf_trained <span class="ot">&lt;-</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">regression_forest</span>(</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">X =</span> data[, .(x1, x2, x3)],</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">Y =</span> data[, y]</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="co">#=== OOB estimates of E[Y|X] ===#</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>ey_x_hat <span class="ot">&lt;-</span> y_rf_trained<span class="sc">$</span>predictions</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a><span class="co">#--------------------------</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a><span class="co"># E[T|X]</span></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a><span class="co">#--------------------------</span></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a><span class="co">#=== train an RF on T ===#</span></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>t_rf_trained <span class="ot">&lt;-</span></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">probability_forest</span>(</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>    <span class="at">X =</span> data[, .(x1, x2, x3)],</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>    <span class="at">Y =</span> data[, <span class="fu">factor</span>(T)]</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a><span class="co">#=== OOB estimates of E[T|X] ===#</span></span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>et_x_hat <span class="ot">&lt;-</span> t_rf_trained<span class="sc">$</span>predictions[, <span class="dv">2</span>] <span class="co"># get the probability of T = 1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s now orthogonalize <span class="math inline">\(Y\)</span> and <span class="math inline">\(T\)</span>,</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>data[, <span class="st">`</span><span class="at">:=</span><span class="st">`</span>(<span class="at">y_tilde =</span> y <span class="sc">-</span> ey_x_hat, <span class="at">t_tilde =</span> T <span class="sc">-</span> et_x_hat)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here is what the data looks like:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          y     T        x1        x2         x3        x4        x5        x6
1: 31.23944 FALSE 2.6951250 1.1965331 0.96678638 0.7210003 0.6207226 0.7869877
2: 10.14446 FALSE 0.7919764 0.2722983 1.34510173 0.6267043 0.8869069 0.4441337
3: 48.93903  TRUE 2.7143095 0.9040185 1.19934789 0.5620508 0.5455276 0.4523834
4: 36.40548  TRUE 1.0628966 1.0659064 1.20667390 0.3638505 0.3201202 0.7106028
5: 11.89362 FALSE 2.8824810 1.8198348 0.03356061 0.2570614 0.1447951 0.4691230
6: 78.17796  TRUE 1.3823141 1.6858092 1.91198906 0.6044051 0.8637295 0.8509355
          x7         x8         x9       x10    y_tilde    t_tilde
1: 0.8891985 0.25543604 0.05640156 0.3560356 -5.6636071 -0.5296217
2: 0.2765778 0.08810472 0.20397022 0.7982468 -4.3210587 -0.4788832
3: 0.4195648 0.69912161 0.05602666 0.5905471 11.9768866  0.4703248
4: 0.1140096 0.43547480 0.57807247 0.5707400  0.2253922  0.5534958
5: 0.3196684 0.90711440 0.15150039 0.9426457 -8.4757974 -0.5011004
6: 0.7302472 0.91788817 0.76401947 0.9238519 -0.6532169  0.5113809</code></pre>
</div>
</div>
</section>
<section id="sec-build-trees" class="level3" data-number="17.5.2">
<h3 data-number="17.5.2" class="anchored" data-anchor-id="sec-build-trees"><span class="header-section-number">17.5.2</span> Building trees</h3>
<hr>
<p><span style="color:blue"> Preparing Data</span>:</p>
<p>When building a tree, GRF use sampling without replacement instead of sampling with replacement as a default for RF implemented by <code>ranger()</code>. <code>sample.fraction</code> parameter determines the fraction of the entire sample drawn for each tree. Let’s use the default value, which is <span class="math inline">\(0.5\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>data_for_the_first_tree <span class="ot">&lt;-</span> data[<span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>N, <span class="fl">0.5</span> <span class="sc">*</span> N, <span class="at">replace =</span> <span class="cn">FALSE</span>), ] </span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              y     T         x1        x2         x3         x4         x5
  1: 24.9025560  TRUE 2.33286846 0.2477951  0.8980258 0.50396757 0.08485642
  2: 34.4617113 FALSE 0.36784858 0.9011077  1.5373583 0.83508147 0.10087835
  3:  4.8131199 FALSE 0.35464284 0.1992028  0.9367359 0.84134117 0.24929184
  4:  9.7958485 FALSE 2.54230941 0.7135930  0.3197887 0.05861231 0.34530906
  5: -0.2399451 FALSE 0.88094406 0.1036436 -0.4209629 0.63817225 0.24831807
 ---                                                                       
496: 81.0907489 FALSE 0.03481615 2.9859855  1.1341612 0.93868895 0.10845090
497: 45.7373362  TRUE 1.11687411 2.1342024  0.8219353 0.18140031 0.52911682
498: 81.7731653  TRUE 0.50482529 1.3572611  2.7416683 0.18128877 0.12207408
499: -9.1361410 FALSE 2.68976646 2.2431148 -0.4614975 0.27532629 0.87256993
500: 21.7882929  TRUE 0.41004493 2.6174698  0.3060092 0.43608964 0.80218867
            x6        x7         x8         x9       x10    y_tilde    t_tilde
  1: 0.5330777 0.0207677 0.75254547 0.28037254 0.5548497  10.348401  0.5165125
  2: 0.6637159 0.8061779 0.34381605 0.74622506 0.3308037  -3.174286 -0.4424109
  3: 0.8667869 0.2351434 0.26094758 0.48312612 0.3273708  -4.899194 -0.4703968
  4: 0.7823649 0.8942680 0.73156533 0.55466979 0.6760430 -13.150179 -0.5710382
  5: 0.8369564 0.9980671 0.02740824 0.36841730 0.6603217  -2.098577 -0.5943349
 ---                                                                          
496: 0.6255471 0.4409065 0.73793156 0.16597397 0.2782975   6.647750 -0.4746825
497: 0.7703280 0.8028466 0.79829377 0.72936803 0.3953351  -1.998624  0.4751614
498: 0.1248504 0.7362142 0.28138516 0.45800679 0.1410164  -3.481113  0.5902025
499: 0.9890033 0.6309494 0.11897912 0.05518469 0.2093569  -7.215089 -0.5353184
500: 0.7045508 0.3854207 0.34308977 0.88430430 0.5601304  -5.074021  0.4222428</code></pre>
</div>
</div>
<p>By default, honest splitting is implemented (<code>honesty = TRUE</code>). The <code>honesty.fraction</code> parameter determines the fraction of the randomly sampled data (<code>data_for_the_first_tree</code>), which will be used for determining splitting decisions. We will use the default value of <code>honesty.fraction</code>, which is <span class="math inline">\(0.5\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co">#=== number of observations ===#</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>N_d <span class="ot">&lt;-</span> <span class="fu">nrow</span>(data_for_the_first_tree)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="co">#=== indices ===#</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>J1_index <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>N_d, <span class="fl">0.5</span> <span class="sc">*</span> N_d, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="co">#=== data for determining splitting rules ===#</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>data_J1 <span class="ot">&lt;-</span> data_for_the_first_tree[J1_index, ]</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>             y     T        x1        x2          x3        x4         x5
  1: 32.168156  TRUE 1.3202547 1.1420641  0.91158029 0.1686512 0.79225674
  2: 63.700906 FALSE 0.5488392 1.5795742  1.65239047 0.3851233 0.64512684
  3: 41.849736  TRUE 2.7792941 0.8210709  0.79316984 0.9278186 0.49904726
  4: 30.063201 FALSE 0.9203382 1.2250174  0.87263820 0.6032529 0.54814557
  5: 74.092869  TRUE 1.3308917 1.7572783  1.75875073 0.1445248 0.19666780
 ---                                                                     
246: 59.044593 FALSE 2.9685008 1.6885064  1.49969895 0.5313985 0.15660748
247: 79.666353  TRUE 2.3577697 1.1953114  2.31067728 0.4455486 0.01321071
248: 37.203768 FALSE 2.1998783 1.6630233  0.78424732 0.1420179 0.95015816
249: 14.141558  TRUE 1.1786313 1.1804383  0.05553601 0.8050410 0.10626896
250:  5.156938 FALSE 0.5296012 1.1397422 -0.09301202 0.5284173 0.92249024
             x6        x7        x8          x9       x10     y_tilde
  1: 0.70056216 0.3013820 0.7349177 0.434629694 0.3931467   1.1037999
  2: 0.94440025 0.7271771 0.4302308 0.009344572 0.2357119  -3.6279591
  3: 0.02594909 0.2632990 0.7471883 0.913540913 0.4693372  12.4304959
  4: 0.79733568 0.1367518 0.9528761 0.445233749 0.8557955  -2.0274153
  5: 0.50274839 0.7840356 0.2542098 0.023763730 0.9715826  -3.1022041
 ---                                                                 
246: 0.52848998 0.7282229 0.5323203 0.261717018 0.2347180 -13.0882951
247: 0.64246528 0.4421114 0.8446926 0.637843209 0.5676528   4.2071069
248: 0.20126400 0.1563281 0.2404751 0.415536469 0.7778129  -3.7512625
249: 0.16361912 0.7145285 0.7101117 0.600831013 0.5476061  -0.2123989
250: 0.61208680 0.1649016 0.9978272 0.497071444 0.5043290  -1.0637976
        t_tilde
  1:  0.4440064
  2: -0.4874906
  3:  0.4410276
  4: -0.5482126
  5:  0.5162645
 ---           
246: -0.5135525
247:  0.5212281
248: -0.3979431
249:  0.4469103
250: -0.5387802</code></pre>
</div>
</div>
<p>As you can see, we are just using a quarter of the original dataset (<code>sample.fraction</code> <span class="math inline">\(\times\)</span> <code>honesty.fraction</code> = <span class="math inline">\(0.5 \times 0.5\)</span>). Here is the data for repopulating the tree once the tree is built.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co">#=== data for repopulate the tree ===#</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>data_J2 <span class="ot">&lt;-</span> data_for_the_first_tree[<span class="sc">-</span>J1_index, ]</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>               y     T        x1        x2          x3         x4         x5
  1:  34.4617113 FALSE 0.3678486 0.9011077  1.53735835 0.83508147 0.10087835
  2:   9.7958485 FALSE 2.5423094 0.7135930  0.31978875 0.05861231 0.34530906
  3:  -0.2399451 FALSE 0.8809441 0.1036436 -0.42096291 0.63817225 0.24831807
  4:  33.8904668  TRUE 2.7083542 0.7223860  0.28685672 0.47576373 0.60371551
  5:  42.0187752 FALSE 1.6180228 1.3468851  1.20854200 0.98942551 0.32931981
 ---                                                                        
246: 112.6993157 FALSE 1.8652859 2.4059719  2.08180730 0.34569836 0.76473310
247:  -1.1378434  TRUE 0.4855630 1.9038798 -0.28249533 0.67252894 0.21756645
248:  48.9883794  TRUE 2.2455929 1.4029347  1.00599231 0.30995650 0.47503141
249:  21.5579803  TRUE 1.9840075 2.8772854  0.05766394 0.95584669 0.02832178
250:  45.7373362  TRUE 1.1168741 2.1342024  0.82193525 0.18140031 0.52911682
            x6        x7         x8        x9       x10    y_tilde    t_tilde
  1: 0.6637159 0.8061779 0.34381605 0.7462251 0.3308037  -3.174286 -0.4424109
  2: 0.7823649 0.8942680 0.73156533 0.5546698 0.6760430 -13.150179 -0.5710382
  3: 0.8369564 0.9980671 0.02740824 0.3684173 0.6603217  -2.098577 -0.5943349
  4: 0.4801798 0.4673316 0.63830962 0.5097753 0.5330969  10.822273  0.4342292
  5: 0.5914005 0.7734454 0.18005650 0.1522792 0.8920292  -5.334219 -0.5456630
 ---                                                                         
246: 0.3229140 0.7352488 0.41670762 0.7729425 0.9237995  -3.439394 -0.4548755
247: 0.8127880 0.2702545 0.58958613 0.2344333 0.7674877  -2.027607  0.4568964
248: 0.3251172 0.1759780 0.42293907 0.3705066 0.1681080   9.109288  0.5642331
249: 0.5679956 0.6444142 0.53908269 0.2941420 0.9354850   1.957145  0.5125393
250: 0.7703280 0.8028466 0.79829377 0.7293680 0.3953351  -1.998624  0.4751614</code></pre>
</div>
</div>
<hr>
<p><span style="color:blue"> Determining splitting rules</span>:</p>
<p>Now let’s find the splitting rules using <code>data_J1</code> (Remember, <code>data_J2</code> is not used).</p>
<p>We are at the root node to which all the observations belong. Let’s first calculate the pseudo outcome. Finding pseudo outcomes starts from solving the unweighted version of <a href="#eq-opt">Equation&nbsp;<span>17.2</span></a> using the samples in the parent node (root node here) in general. For CF, it is</p>
<p><span id="eq-theta-p"><span class="math display">\[
\begin{aligned}
\sum_{i=1}^n (\tilde{Y}_i - \theta\tilde{T}_i)\cdot \tilde{T}_i = 0
\end{aligned}
\tag{17.8}\]</span></span></p>
<p>because the score function for CF is <span class="math inline">\((\tilde{Y}_i - \theta\tilde{T}_i)\cdot \tilde{T}_i\)</span>. So,</p>
<p><span class="math display">\[
\begin{aligned}
\theta_P = \frac{\sum_{i=1}^n \tilde{Y}_i\cdot \tilde{T}_i}{\sum_{i=1}^n\tilde{T}_i\cdot \tilde{T}_i}
\end{aligned}
\]</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>theta_p <span class="ot">&lt;-</span> data_J1[, <span class="fu">sum</span>(y_tilde <span class="sc">*</span> t_tilde)<span class="sc">/</span><span class="fu">sum</span>(t_tilde <span class="sc">*</span> t_tilde)]</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 7.212476</code></pre>
</div>
</div>
<p>The pseudo outcome for CF is <span class="math inline">\((\tilde{Y}_i - \theta_P\tilde{T}_i)\cdot \tilde{T}_i\)</span>. Let’s define that in <code>data_J1</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>data_J1[, rho <span class="sc">:</span><span class="er">=</span> (y_tilde <span class="sc">-</span> theta_p <span class="sc">*</span> t_tilde) <span class="sc">*</span> t_tilde]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, the number of variables to use for splitting is determined by the <code>mtry</code> parameter. Its default is <code>min(ceiling(sqrt(ncol(X))+20), ncol(X))</code>, where <code>X</code> is the feature matrix. Here <code>ncol(X)</code> is 10. So, <code>mtry</code> is set to 10. Following this, we will use all three variables for splitting here. Let <span class="math inline">\(C_j\)</span> (<span class="math inline">\(j = 1, 2\)</span>) and <span class="math inline">\(N_j\)</span> denote the child node <span class="math inline">\(j\)</span> and the number of observations in child node <span class="math inline">\(j\)</span>, respectively. We will write a function that works on a single feature variable to find all the threshold values that would result in unique splits, and then return the following information:</p>
<ul>
<li><span class="math inline">\((\sum_{i\in C_j} \rho_i)^2\)</span></li>
<li><span class="math inline">\(N_j\)</span></li>
<li><span class="math inline">\(\sum_{i=1}^{N_j} (T_i - \bar{T})^2\)</span> (we will call this <code>info_size</code>)</li>
<li>the number of treated and control units</li>
</ul>
<p>The output allows us to calculate the heterogeneity score of the split defined as</p>
<p><span class="math display">\[
\begin{aligned}
\sum_{j=1}^2 \frac{(\sum_{i\in C_j} \rho_i)^2}{N_j}.
\end{aligned}
\]</span></p>
<p>It also allows us to eliminate some of the splits based on <code>mtry</code>, <code>alpha</code>, and <code>imbalance.penalty</code> parameters.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>get_ss_by_var <span class="ot">&lt;-</span> <span class="cf">function</span>(feature_var, outcome_var, parent_data)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>  temp_data <span class="ot">&lt;-</span> </span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">copy</span>(parent_data) <span class="sc">%&gt;%</span> </span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">setnames</span>(feature_var, <span class="st">"temp_var"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">setnames</span>(outcome_var, <span class="st">"outcome"</span>)  </span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>  <span class="co">#=== define a sequence of values of hruns ===#</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>  thr_seq <span class="ot">&lt;-</span></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>    temp_data[<span class="fu">order</span>(temp_var), <span class="fu">unique</span>(temp_var)] <span class="sc">%&gt;%</span></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>    <span class="co">#=== get the rolling mean ===#</span></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>    <span class="fu">frollmean</span>(<span class="dv">2</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>    .[<span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>  <span class="co">#=== get RSS ===#</span></span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>  ss_value <span class="ot">&lt;-</span></span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lapply</span>(</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>      thr_seq,</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>      <span class="cf">function</span>(x){</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>        return_data <span class="ot">&lt;-</span> </span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>          temp_data[, .(</span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>            <span class="at">het_score =</span> <span class="fu">sum</span>(outcome)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>.N, </span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a>            <span class="at">nobs =</span> .N,</span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a>            <span class="at">info_size =</span> <span class="fu">sum</span>((T <span class="sc">-</span> <span class="fu">mean</span>(T))<span class="sc">^</span><span class="dv">2</span>),</span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a>            <span class="at">num_treated =</span> <span class="fu">sum</span>(T),</span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a>            <span class="at">num_ctrl =</span> <span class="fu">sum</span>(<span class="sc">!</span>T)</span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a>          ), by <span class="ot">=</span> temp_var <span class="sc">&lt;</span> x] <span class="sc">%&gt;%</span> </span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a>          <span class="fu">setnames</span>(<span class="st">"temp_var"</span>, <span class="st">"child"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a>          .[, child <span class="sc">:</span><span class="er">=</span> <span class="fu">ifelse</span>(child <span class="sc">==</span> <span class="cn">TRUE</span>, <span class="st">"Child 1"</span>, <span class="st">"Child 2"</span>)] <span class="sc">%&gt;%</span> </span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a>          .[, thr <span class="sc">:</span><span class="er">=</span> x]</span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a>        <span class="fu">return</span>(return_data)</span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a>      } </span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a>    ) <span class="sc">%&gt;%</span> </span>
<span id="cb25-36"><a href="#cb25-36" aria-hidden="true" tabindex="-1"></a>    <span class="fu">rbindlist</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb25-37"><a href="#cb25-37" aria-hidden="true" tabindex="-1"></a>    .[, var <span class="sc">:</span><span class="er">=</span> feature_var] <span class="sc">%&gt;%</span> </span>
<span id="cb25-38"><a href="#cb25-38" aria-hidden="true" tabindex="-1"></a>    <span class="fu">relocate</span>(var, thr, child)</span>
<span id="cb25-39"><a href="#cb25-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-40"><a href="#cb25-40" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(ss_value)</span>
<span id="cb25-41"><a href="#cb25-41" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For example, here is the output for <code>x1</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="fu">get_ss_by_var</span>(<span class="st">"x1"</span>, <span class="st">"rho"</span>, data_J1)[]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     var        thr   child   het_score nobs  info_size num_treated num_ctrl
  1:  x1 0.00332332 Child 2  0.04640943  249 62.1285141         130      119
  2:  x1 0.00332332 Child 1 11.55594717    1  0.0000000           1        0
  3:  x1 0.02031884 Child 2  0.03481370  248 61.8548387         130      118
  4:  x1 0.02031884 Child 1  4.31689906    2  0.5000000           1        1
  5:  x1 0.03951186 Child 2  0.24122929  247 61.5789474         130      117
 ---                                                                        
494:  x1 2.92655569 Child 2 76.66320951    3  0.6666667           1        2
495:  x1 2.95306339 Child 1  0.30401476  248 61.8548387         130      118
496:  x1 2.95306339 Child 2 37.69783018    2  0.5000000           1        1
497:  x1 2.96828632 Child 1  0.09327709  249 62.0803213         131      118
498:  x1 2.96828632 Child 2 23.22599563    1  0.0000000           0        1</code></pre>
</div>
</div>
<p>Repeating this for all the feature variables,</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>thr_score_data <span class="ot">&lt;-</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lapply</span>(</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">paste0</span>(<span class="st">"x"</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>),</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">function</span>(x) <span class="fu">get_ss_by_var</span>(x, <span class="st">"rho"</span>, data_J1)</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rbindlist</span>()</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      var        thr   child    het_score nobs  info_size num_treated num_ctrl
   1:  x1 0.00332332 Child 2 4.640943e-02  249 62.1285141         130      119
   2:  x1 0.00332332 Child 1 1.155595e+01    1  0.0000000           1        0
   3:  x1 0.02031884 Child 2 3.481370e-02  248 61.8548387         130      118
   4:  x1 0.02031884 Child 1 4.316899e+00    2  0.5000000           1        1
   5:  x1 0.03951186 Child 2 2.412293e-01  247 61.5789474         130      117
  ---                                                                         
4976: x10 0.98350685 Child 2 4.706587e-01    3  0.6666667           2        1
4977: x10 0.98856960 Child 1 7.289755e-04  248 61.8991935         129      119
4978: x10 0.98856960 Child 2 9.039296e-02    2  0.0000000           2        0
4979: x10 0.99656474 Child 1 3.553642e-02  249 62.1285141         130      119
4980: x10 0.99656474 Child 2 8.848568e+00    1  0.0000000           1        0</code></pre>
</div>
</div>
<p>Now, we ensure that we have at least as many <code>min.node.size</code> numbers of treated and control units in both child nodes. This provides a safeguard against having a very inaccurate treatment effect estimation. The default value of <code>min.node.size</code> is 5.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>min.node.size <span class="ot">&lt;-</span> <span class="dv">5</span> <span class="co"># referred to as mns</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="co">#=== check if both child nodes have at least mns control and treatment units  ===#</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>thr_score_data[, mns_met_grup <span class="sc">:</span><span class="er">=</span> <span class="fu">all</span>(num_ctrl <span class="sc">&gt;=</span> min.node.size <span class="sc">&amp;</span> num_treated <span class="sc">&gt;=</span> min.node.size), by <span class="ot">=</span> .(var, thr)]</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>msn_met_data <span class="ot">&lt;-</span> thr_score_data[mns_met_grup <span class="sc">==</span> <span class="cn">TRUE</span>, ]</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      var       thr   child   het_score nobs info_size num_treated num_ctrl
   1:  x1 0.2647367 Child 2   4.5435708  238 59.294118         126      112
   2:  x1 0.2647367 Child 1  90.1141537   12  2.916667           5        7
   3:  x1 0.2957597 Child 2   5.5810225  237 59.071730         125      112
   4:  x1 0.2957597 Child 1 101.7463325   13  3.230769           6        7
   5:  x1 0.3148280 Child 2   5.0451340  236 58.792373         125      111
  ---                                                                      
4510: x10 0.9262760 Child 2   0.4888869   18  3.611111          13        5
4511: x10 0.9331452 Child 1   0.1739244  233 58.223176         119      114
4512: x10 0.9331452 Child 2   2.3837878   17  3.529412          12        5
4513: x10 0.9403202 Child 1   0.1370827  234 58.461538         120      114
4514: x10 0.9403202 Child 2   2.0048350   16  3.437500          11        5
      mns_met_grup
   1:         TRUE
   2:         TRUE
   3:         TRUE
   4:         TRUE
   5:         TRUE
  ---             
4510:         TRUE
4511:         TRUE
4512:         TRUE
4513:         TRUE
4514:         TRUE</code></pre>
</div>
</div>
<p>Now, we consider how <code>alpha</code> and <code>imbalance.penalty</code> affect the potential pool of feature-threshold combinations. Each child node needs to have at least as large <code>info_size</code> as <code>alpha</code> <span class="math inline">\(\times\)</span> <code>info_size</code> of the parent node. The <code>info_size</code> of the parent node is</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>info_size_p <span class="ot">&lt;-</span> data_J1[, <span class="fu">sum</span>((T <span class="sc">-</span> <span class="fu">mean</span>(T))<span class="sc">^</span><span class="dv">2</span>)]</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 62.356</code></pre>
</div>
</div>
<p>By default, <code>alpha</code> is set to 0.05. We use this number.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>msn_met_data[, info_size_met <span class="sc">:</span><span class="er">=</span> <span class="fu">all</span>(info_size <span class="sc">&gt;</span> (alpha <span class="sc">*</span> info_size_p)), by <span class="ot">=</span> .(var, thr)]</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>msn_ifs_met <span class="ot">&lt;-</span> msn_met_data[info_size_met <span class="sc">==</span> <span class="cn">TRUE</span>, ]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><code>imbalance.penalty</code> is used to further punish splits that introduce imbalance. For example, for feature <code>x1</code> and <code>thr</code> of 0.2957597,</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>   var       thr   child het_score nobs info_size num_treated num_ctrl
1:  x1 0.2647367 Child 2  4.543571  238 59.294118         126      112
2:  x1 0.2647367 Child 1 90.114154   12  2.916667           5        7
   mns_met_grup info_size_met
1:         TRUE         FALSE
2:         TRUE         FALSE</code></pre>
</div>
</div>
<p>the unpunished heterogeneity score is 94.6577245. With a non-zero value of <code>imbalance.penalty</code>, the following penalty will be subtracted from the unpunished heterogeneity score: <code>imbalance.penalty * (1/info_size_1 + 1/info_size_2)</code>. By default, <code>imbalance.penalty</code> is 0. But, let’s use <code>imbalance.penalty</code> <span class="math inline">\(= 1\)</span> here.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>imbalance.penalty <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>msn_ifs_met[, imb_penalty <span class="sc">:</span><span class="er">=</span> imbalance.penalty <span class="sc">*</span> (<span class="dv">1</span><span class="sc">/</span>info_size)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, we calculate the heterogeneity score for each feature-threshold with imbalance penalty.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>het_score_data <span class="ot">&lt;-</span> msn_ifs_met[, .(<span class="at">het_with_imbp =</span> <span class="fu">sum</span>(het_score <span class="sc">-</span> imb_penalty)), <span class="at">by =</span> .(var, thr)]</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      var       thr het_with_imbp
   1:  x1 0.2957597   107.0009025
   2:  x1 0.3148280    89.7830030
   3:  x1 0.3405788    90.7423400
   4:  x1 0.3557459    82.0711971
   5:  x1 0.3599312    88.7968930
  ---                            
2226: x10 0.9199764     0.9424508
2227: x10 0.9231653     0.1361964
2228: x10 0.9262760     0.2326482
2229: x10 0.9331452     2.2572036
2230: x10 0.9403202     1.8339034</code></pre>
</div>
</div>
<p>We now find the feature-threshold combination that maximizes the score,</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>best_split <span class="ot">&lt;-</span> het_score_data[<span class="fu">which.max</span>(het_with_imbp), ]</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   var      thr het_with_imbp
1:  x1 1.930929      953.3717</code></pre>
</div>
</div>
<p>So, we are splitting the root node using <code>x1</code> with threshold of 1.9309. Here is a visualization of the split,</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(data_J1) <span class="sc">+</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">y =</span> rho, <span class="at">x =</span> x1, <span class="at">color =</span> (x1 <span class="sc">&lt;</span> best_split[, thr]))) <span class="sc">+</span></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_discrete</span>(<span class="st">"Less than the threshold"</span>) <span class="sc">+</span></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="E02-grf_files/figure-html/viz-first-split-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Now, let’s look at the pseudo outcome and elaborate more on what we are seeing here.</p>
<p><span class="math display">\[
\begin{aligned}
\rho_i = (\tilde{Y}_i- \hat{\theta}_P\tilde{T}_i)\tilde{T}_i
\end{aligned}
\]</span></p>
<p>Note that <span class="math inline">\(\hat{\theta}_P\)</span> is just a constant and represents the average treatment effect for the parent node. So, <span class="math inline">\(\rho_i\)</span> tends to be higher for the observations whose CATE is higher (above-average treatment effect). So, splitting on <span class="math inline">\(\rho\)</span> (instead of <span class="math inline">\(Y\)</span>) made it possible to pick the right feature variable <code>x1</code>. This is because <code>x1</code> is influential in determining CATE and higher values of <code>x1</code> lead to more positive CATE.</p>
<p>Note that the checks implemented with <code>mtry</code>, <code>alpha</code>, and <code>imbalance.penalty</code> is more important further down a tree. But, the examples above should have illustrated how they are used.</p>
<p>Okay, now let’s split the root node into two child nodes according to the best split on <span class="math inline">\(\rho\)</span> we found earlier.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>depth_2_node_1 <span class="ot">&lt;-</span> data_J1[x1 <span class="sc">&lt;</span> best_split[, thr], ]</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>depth_2_node_2 <span class="ot">&lt;-</span> data_J1[x1 <span class="sc">&gt;=</span> best_split[, thr], ]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s consider splitting the second node, <code>depth_2_node_1</code>. We basically follow exactly the same process the we just did. First, find <span class="math inline">\(\hat{\theta}_P\)</span> for this (parent) node using <a href="#eq-theta-p">Equation&nbsp;<span>17.8</span></a> and define the pseudo outcomes.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>theta_p <span class="ot">&lt;-</span> depth_2_node_1[, <span class="fu">sum</span>(y_tilde <span class="sc">*</span> t_tilde)<span class="sc">/</span><span class="fu">sum</span>(t_tilde <span class="sc">*</span> t_tilde)]</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.8311055</code></pre>
</div>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>depth_2_node_1[, rho <span class="sc">:</span><span class="er">=</span> (y_tilde <span class="sc">-</span> theta_p <span class="sc">*</span> t_tilde) <span class="sc">*</span> t_tilde]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, we find the best split (here we ignore various checks),</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>best_split_d2_n1 <span class="ot">&lt;-</span></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lapply</span>(</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">paste0</span>(<span class="st">"x"</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>),</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">function</span>(x) <span class="fu">get_ss_by_var</span>(x, <span class="st">"rho"</span>, depth_2_node_1)</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rbindlist</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>  .[, .(<span class="at">het_score =</span> <span class="fu">sum</span>(het_score)), <span class="at">by =</span> .(var, thr)] <span class="sc">%&gt;%</span> </span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>  .[<span class="fu">which.max</span>(het_score), ]</span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   var      thr het_score
1:  x3 3.827172  610.0837</code></pre>
</div>
</div>
<p>We keep splitting nodes until no splits is possible any more based on the value of <code>min.node.size</code>, <code>alpha</code>, and <code>imbalance.penalty</code>.</p>
<hr>
<p>To see the role of <code>honesty.prune.leaves</code>, let’s suppose we stopped splitting after the first split based on <code>x</code> with threshold of 1.9309. So, we just two terminal nodes. As stated in <a href="#sec-grf-honest"><span>Section&nbsp;17.4</span></a>, when we do prediction, <code>data_J1</code> (which is used for determining the splitting rule) is not used. Rather, <code>data_J2</code> (the data that was set aside) is used. When <code>honesty.prune.leaves = TRUE</code> (default), the tree is pruned so that no leaves are empty. Yes, we made sure that we have certain number of observations in each node with <code>min.node.size</code>, however, that is only for <code>data_J1</code>, not <code>data_J2</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>data_J2[, .N, by <span class="ot">=</span> x1 <span class="sc">&lt;</span> best_split[, thr]]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      x1   N
1:  TRUE 172
2: FALSE  78</code></pre>
</div>
</div>
<p>Okay, so, each child has at least one observation. So, we are not going to prune the tree. If either of the child nodes was empty, then we would have removed the leaves and had the root node as the tree.</p>
<p>As you can see, while <code>data_J1</code> plays a central role in the tree building process, the other half, <code>data_J2</code>, plays a very small role. However, <code>data_J2</code> plays the central role in prediction, which will be seen in the next section.</p>
</section>
<section id="prediction" class="level3 page-columns page-full" data-number="17.5.3">
<h3 data-number="17.5.3" class="anchored" data-anchor-id="prediction"><span class="header-section-number">17.5.3</span> Prediction</h3>
<p>Suppose you have built <span class="math inline">\(T\)</span> trees already. In general for GRF, statistics of interest, <span class="math inline">\(\theta(X)\)</span>, at <span class="math inline">\(X = X_0\)</span> is found by solving the following equation:</p>
<p><span class="math display">\[
\begin{aligned}
\sum_{i=1}^N \alpha_i(X_i, X_0)\psi_{\theta,\nu}(O_i) = 0
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\alpha_i(X_i, X_0)\)</span> is the weight given to each <span class="math inline">\(i\)</span> calculated based on the trees that have been built. Let <span class="math inline">\(\eta_{i,t}(X_i, X_0)\)</span> be 1 if observation <span class="math inline">\(i\)</span> belongs to the same leaf as <span class="math inline">\(X_0\)</span> in tree <span class="math inline">\(t\)</span>. Then,</p>
<p><span id="eq-weight-cf"><span class="math display">\[
\begin{aligned}
\alpha_i(X_i, X_0) = \frac{1}{T}\sum_{t=1}^T\frac{\eta_{i,t}(X_i, X_0)}{\sum_{i=1}^N\eta_{i,t}(X_i, X_0)}
\end{aligned}
\tag{17.9}\]</span></span></p>
<p>So, the weight given to observation <span class="math inline">\(i\)</span> is higher if observation <span class="math inline">\(i\)</span> belongs to the same leaf as the evaluation point <span class="math inline">\(X_0\)</span> in more trees.</p>
<p>For CF, since the score function is defined as <span class="math inline">\(\psi_{\theta,\nu}(O_i) = \tilde{Y}_i - \theta\tilde{T}_i)\cdot \tilde{T}_i\)</span>, the equation can be written as</p>
<p><span class="math display">\[
\begin{aligned}
\sum_{i=1}^N \alpha_i(X_i, X_0)[\tilde{Y_i} - \theta\cdot \tilde{T_i}]\hat{\tilde{T_i}} = 0
\end{aligned}
\]</span></p>
<p>So,</p>
<p><span class="math display">\[
\begin{aligned}
\theta_P = \frac{\sum_{i=1}^n \alpha_i(X_i, X_0)\tilde{Y}_i\cdot \tilde{T}_i}{\sum_{i=1}^n\alpha_i(X_i, X_0)\tilde{T}_i\cdot \tilde{T}_i}
\end{aligned}
\]</span></p>
<hr>
<p>We now illustrate how predictions are done once trees have been built. Let’s first build trees using <code>causal_forest()</code>. <code>min.node.size</code> is set deliberately high so we can work with very simple trees.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>You do not need to know how <code>causal_forest()</code> works. You only need to know that <code>causal_forest()</code> build trees for CATE estimation.</p>
</div></div><div class="cell">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">443784</span>)</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>cf_trained <span class="ot">&lt;-</span></span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">causal_forest</span>(</span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">X =</span> <span class="fu">select</span>(data, <span class="fu">starts_with</span>(<span class="st">"x"</span>)),</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">Y =</span> data[, y],</span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">W =</span> data[, T],</span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">min.node.size =</span> <span class="dv">50</span></span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>After training a causal forest model, we have trees like the one shown in <a href="#fig-cf-tree">Figure&nbsp;<span>17.3</span></a>, which is the first of the <span class="math inline">\(2000\)</span> trees.</p>
<div class="cell">
<div id="fig-cf-tree" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<div id="htmlwidget-91939835f7582e3bf1b5" style="width:100%;height:371px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-91939835f7582e3bf1b5">{"x":{"diagram":"digraph nodes { \n node [shape=box] ;\n0 [label=\" x1 <= 1.44 \"] ;\n0 -> 1 [labeldistance=2.5, labelangle=45, headlabel=\"True\", arrowhead=normal];\n0 -> 2 [labeldistance=2.5, labelangle=-45, headlabel=\"False\", arrowhead=normal];\n1  [shape=box,style=filled,color=\".7 .3 1.0\" , label=\"size =  121 \n avg_Y = 38.6\navg_W = 0.55 \"];\n2  [shape=box,style=filled,color=\".7 .3 1.0\" , label=\"size =  129 \n avg_Y = 42.1\navg_W = 0.5 \"];\n}","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
<p></p><figcaption aria-hidden="true" class="figure-caption">Figure 17.3: Example trees built in causal forest estimation</figcaption><p></p>
</figure>
</div>
</div>
<p>You probably noticed that the total number of samples in the leaves is only <span class="math inline">\(250\)</span> instead of <span class="math inline">\(1000\)</span>, which is the total number of observations in <code>data</code>. When causal forest was trained on this dataset, only half of the entire sample are randomly selected for building each tree (due to the default setting of <code>sample.fraction = 0.5</code>). The halved sample is further split into two groups, each containing <span class="math inline">\(250\)</span> observations (due to the default setting of <code>honesty = TRUE</code> and <code>honest.fraction = 0.5</code>). Let’s call them <span class="math inline">\(J_1\)</span> and <span class="math inline">\(J_2\)</span>. Then, <span class="math inline">\(J_1\)</span> is used to train a tree to find the splitting rules (e.g., <span class="math inline">\(x_1 \leq 1.44\)</span> for the first tree). See <a href="#sec-build-trees"><span>Section&nbsp;17.5.2</span></a> for how only a subset of the drawn samples was used to determine the splitting rules. Once the splitting rules are determined (tree building process is complete), then <span class="math inline">\(J_1\)</span> is “vacated” (or thrown out) from the tree. This will become clearer when we talk about finding individual weights.</p>
<p>Let’s take a look at a tree to see what happened. We can use <code>get_tree()</code> to access individual trees from <code>cf_trained</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co">#=== get the first tree ===#</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>a_tree <span class="ot">&lt;-</span> <span class="fu">get_tree</span>(cf_trained, <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><code>drawn_samples</code> attribute of the tree contains row indices that are selected randomly for this tree.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(a_tree<span class="sc">$</span>drawn_samples)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 788 968 693  47 673 584</code></pre>
</div>
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(a_tree<span class="sc">$</span>drawn_samples)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 500</code></pre>
</div>
</div>
<p>As you can see, there are 500 samples (due to <code>sample.fraction = 0.5</code>). The rest of the observations were not used for this tree at all. Accessing <code>nodes</code> attribute will give you the splitting rules for the tree built and which samples are in what node.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>nodes <span class="ot">&lt;-</span> a_tree<span class="sc">$</span>nodes</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[1]]
[[1]]$is_leaf
[1] FALSE

[[1]]$split_variable
[1] 1

[[1]]$split_value
[1] 1.438102

[[1]]$send_missing_left
[1] TRUE

[[1]]$left_child
[1] 2

[[1]]$right_child
[1] 3


[[2]]
[[2]]$is_leaf
[1] TRUE

[[2]]$samples
  [1] 539  75 927 812 489 264 344 479 567 116 750 744 295 363 580 754 210 601
 [19] 246  68 237 528 751 782 422 842 801 517 916 319 308   7 164 449 485  37
 [37] 576 235 431 154  22  14 867 354 940 911 373 123 104 798 133 587 506 219
 [55] 531 577 562 693 298 654 895 991 974 566 176 475  21 184 380 706  59 522
 [73] 288 936 792 117 141  89 783 252 663 513 417 756 605 427 126 401 242 818
 [91] 708 983 271 512 737 817 753 811 658 494 142 932 969  69  70 370 837 124
[109] 265 128 470 860 550 611 200 950 721 584 293  81 262

[[2]]$leaf_stats
avg_Y avg_W 
38.60  0.55 


[[3]]
[[3]]$is_leaf
[1] TRUE

[[3]]$samples
  [1] 411 478  38 307  39 997 869 268 276 480 425 633   8 645 999 544 984 579
 [19] 855 690 592 673 776 253   3 909 939 423 490 409 607 337 103 274 734 174
 [37] 342 339 526 878 634 503 407 429 161 788 486 699 937 330 361 406 435 809
 [55] 948 163 747 724 416 665 329  52 714 864 483 957 946 970 204 458   5 543
 [73] 624 333 698 358  34 903 791 378 581  41 146 107 735 109 741 858 666 495
 [91] 955 438 106 125 606 840  44 151  28 334 891  49 498 583 879 518 291 908
[109] 275 460 310 767 968 959 722  10 873 487 619 178 393 474 352 364 914 136
[127] 770 757 976

[[3]]$leaf_stats
avg_Y avg_W 
 42.1   0.5 </code></pre>
</div>
</div>
<p><code>nodes</code> is a list of three elements (one root node and two terminal nodes here). The <code>samples</code> attribute gives you row indices of the samples that belong to the terminal node.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>nodes[[<span class="dv">2</span>]]<span class="sc">$</span>samples</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  [1] 539  75 927 812 489 264 344 479 567 116 750 744 295 363 580 754 210 601
 [19] 246  68 237 528 751 782 422 842 801 517 916 319 308   7 164 449 485  37
 [37] 576 235 431 154  22  14 867 354 940 911 373 123 104 798 133 587 506 219
 [55] 531 577 562 693 298 654 895 991 974 566 176 475  21 184 380 706  59 522
 [73] 288 936 792 117 141  89 783 252 663 513 417 756 605 427 126 401 242 818
 [91] 708 983 271 512 737 817 753 811 658 494 142 932 969  69  70 370 837 124
[109] 265 128 470 860 550 611 200 950 721 584 293  81 262</code></pre>
</div>
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>nodes[[<span class="dv">3</span>]]<span class="sc">$</span>samples</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  [1] 411 478  38 307  39 997 869 268 276 480 425 633   8 645 999 544 984 579
 [19] 855 690 592 673 776 253   3 909 939 423 490 409 607 337 103 274 734 174
 [37] 342 339 526 878 634 503 407 429 161 788 486 699 937 330 361 406 435 809
 [55] 948 163 747 724 416 665 329  52 714 864 483 957 946 970 204 458   5 543
 [73] 624 333 698 358  34 903 791 378 581  41 146 107 735 109 741 858 666 495
 [91] 955 438 106 125 606 840  44 151  28 334 891  49 498 583 879 518 291 908
[109] 275 460 310 767 968 959 722  10 873 487 619 178 393 474 352 364 914 136
[127] 770 757 976</code></pre>
</div>
</div>
<p>It is important to keep in mind that these observations with these row indices belong to <span class="math inline">\(J_2\)</span>. These observations were <span style="color:red">NOT</span> used in determining the splitting rule of <span class="math inline">\(x_1 \leq 1.44\)</span>. They were populating the terminal nodes by simply following the splitting rule, which is determined using the data in <span class="math inline">\(J_1\)</span> following the process described in <a href="#sec-build-trees"><span>Section&nbsp;17.5.2</span></a>. The difference in <code>a_tree$drawn_samples</code> and the combination of <code>nodes[[2]]$samples</code> and <code>nodes[[3]]$samples</code> is <span class="math inline">\(J_1\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>J2_rows <span class="ot">&lt;-</span> <span class="fu">c</span>(nodes[[<span class="dv">2</span>]]<span class="sc">$</span>samples, nodes[[<span class="dv">3</span>]]<span class="sc">$</span>samples)</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>J1_J2_rows <span class="ot">&lt;-</span> a_tree<span class="sc">$</span>drawn_samples</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>J1_rows <span class="ot">&lt;-</span> J1_J2_rows[J1_J2_rows <span class="sc">%in%</span> J2_rows]</span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  [1] 788 968 693 673 584  10 706 378 879 506 576 344 809 690 867 818 393 480
 [19] 665 107 908 486   7 352 513 109 339 744 298   8 747 983 767 174 291  22
 [37]  49 754 125 478 539 531 724 310 204 801 253 409 624 550 361 423 106  70
 [55] 406 543 526 293 237 490 663 528 991 136 869 911 518 811 946 722 916  39
 [73] 487 370 333 425 658 184 579  21 337 698 937 219 708 566 782 936 737 178
 [91]  89 334 721  28 592 128 970 435 798 271 753 151  59 116 460 103 948  38
[109] 154 431 817 751 133 909 489 633 878 308 276 999 950 997 619 939  14 873
[127] 562 252 605 458   3 163 714 246 858 164 319  68  81 984 959 123 581 438
[145] 666 416 645 757 380 756 634 407   5 587 770 265 483 611 927 401 274 268
[163] 974  34 567 842 429  75 577 475 812 734 914 517 449 494 141 606 161 932
[181] 474 699 330 792 601 275 583 895 860 295 783 544 503 117 235 855 654 891
[199] 126 864 969 411 176 485 840 142 735 976 512 373 262 417 364 498 422 903
[217] 342 607 522 580 307 358 791  52 288 210 354 146 200 124 427  44 837 104
[235]  37 495 940 329 470 479 955 264 242 776 741 363 957  69 750  41</code></pre>
</div>
</div>
<p>As you can see, there are 250 samples in <span class="math inline">\(J_1\)</span> as well.</p>
<hr>
<p>Suppose you are interested in predicting <span class="math inline">\(\hat{\theta}\)</span> at <span class="math inline">\(X_0 = \{x_1 = 2, x_2,\dots, x_{10} = 1\}\)</span>. For a given tree, we give 1 to the observations in <span style="color:red"><span class="math inline">\(J_1\)</span></span> that belong to the same leaf as <span class="math inline">\(X_0\)</span>. For example, for the first tree, <span class="math inline">\(X_0\)</span> belongs to the right leaf because <span class="math inline">\(x1 = 2 &gt; 1.44\)</span> for <span class="math inline">\(X_0\)</span>. We can tell which node <span class="math inline">\(X_0\)</span> belongs to by supplying <span class="math inline">\(X_0\)</span> to <code>get_leaf_node()</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>X_0 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">2</span>, <span class="fu">rep</span>(<span class="dv">1</span>, <span class="dv">9</span>)), <span class="at">nrow =</span> <span class="dv">1</span>)</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>which_tree_is_X0_in <span class="ot">&lt;-</span> <span class="fu">get_leaf_node</span>(a_tree, X_0)</span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 3</code></pre>
</div>
</div>
<p>So, we give <span class="math inline">\(1/N_t(X_0)\)</span> to all those in the right leaf (the third node in <code>nodes</code>) and 0 to those in the left leaf, where <span class="math inline">\(N_t(X_0)\)</span> is the number of observations that belong to the same leaf as <span class="math inline">\(X_0\)</span>. All the other observations are assigned 0.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a><span class="co">#=== which row numbers in the same leaf as X_0? ===#</span></span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>rows_1 <span class="ot">&lt;-</span> nodes[[which_tree_is_X0_in]]<span class="sc">$</span>samples</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a><span class="co">#=== define eta for tree 1  ===#</span></span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a>data[, eta_t1 <span class="sc">:</span><span class="er">=</span> <span class="dv">0</span>] <span class="co"># first set eta to 0 for all the observations</span></span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a>data[rows_1, eta_t1 <span class="sc">:</span><span class="er">=</span> <span class="dv">1</span> <span class="sc">/</span> <span class="fu">length</span>(rows_1)] <span class="co"># replace eta with 1/N_t(X_0) if in the right node</span></span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-8"><a href="#cb66-8" aria-hidden="true" tabindex="-1"></a><span class="co">#=== see the data ===#</span></span>
<span id="cb66-9"><a href="#cb66-9" aria-hidden="true" tabindex="-1"></a>data</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>               y     T        x1        x2          x3        x4        x5
   1:  31.239440 FALSE 2.6951250 1.1965331  0.96678638 0.7210003 0.6207226
   2:  10.144462 FALSE 0.7919764 0.2722983  1.34510173 0.6267043 0.8869069
   3:  48.939026  TRUE 2.7143095 0.9040185  1.19934789 0.5620508 0.5455276
   4:  36.405483  TRUE 1.0628966 1.0659064  1.20667390 0.3638505 0.3201202
   5:  11.893620 FALSE 2.8824810 1.8198348  0.03356061 0.2570614 0.1447951
  ---                                                                     
 996:  74.029985  TRUE 1.6485987 2.8588233  1.04216169 0.6835214 0.7266654
 997:  21.176748  TRUE 2.0208231 0.4248353  0.49803263 0.2851495 0.8076055
 998:   3.388729 FALSE 1.9203044 0.5051511 -0.02137629 0.9961128 0.8446825
 999:   9.459730 FALSE 1.7274390 0.3468632  0.85507892 0.9669933 0.4943592
1000: 109.282950 FALSE 1.7030476 2.1927368  2.21194227 0.1442201 0.6783044
              x6         x7         x8         x9       x10    y_tilde
   1: 0.78698773 0.88919853 0.25543604 0.05640156 0.3560356 -5.6636071
   2: 0.44413365 0.27657781 0.08810472 0.20397022 0.7982468 -4.3210587
   3: 0.45238337 0.41956478 0.69912161 0.05602666 0.5905471 11.9768866
   4: 0.71060284 0.11400965 0.43547480 0.57807247 0.5707400  0.2253922
   5: 0.46912302 0.31966840 0.90711440 0.15150039 0.9426457 -8.4757974
  ---                                                                 
 996: 0.05036928 0.18803515 0.40981100 0.04716956 0.6443493  6.1784463
 997: 0.14171969 0.08375323 0.78676094 0.27132948 0.6570423  6.4855740
 998: 0.36225572 0.47555166 0.40332714 0.28135063 0.9657502 -8.0385906
 999: 0.43818445 0.59426511 0.53555370 0.91860942 0.3339725 -3.2516710
1000: 0.23674030 0.25005706 0.05512990 0.24352242 0.7227397 -1.5570047
         t_tilde      eta_t1
   1: -0.5296217 0.000000000
   2: -0.4788832 0.000000000
   3:  0.4703248 0.007751938
   4:  0.5534958 0.000000000
   5: -0.5011004 0.007751938
  ---                       
 996:  0.5504383 0.000000000
 997:  0.5684978 0.007751938
 998: -0.5107851 0.000000000
 999: -0.5187684 0.007751938
1000: -0.3917276 0.000000000</code></pre>
</div>
</div>
<p>We repeat this for all the trees and use <a href="#eq-weight-cf">Equation&nbsp;<span>17.9</span></a> to calculate the weights for the individual observations. The following function gets <span class="math inline">\(\eta_{i,t}(X_i, X_0)\)</span> for a given tree for all the observations.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>get_eta <span class="ot">&lt;-</span> <span class="cf">function</span>(t, X_0) {</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>  w_tree <span class="ot">&lt;-</span> <span class="fu">get_tree</span>(cf_trained, t)</span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>  which_tree_is_X0_in <span class="ot">&lt;-</span> <span class="fu">get_leaf_node</span>(w_tree, X_0)</span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a>  rows <span class="ot">&lt;-</span> w_tree<span class="sc">$</span>nodes[[which_tree_is_X0_in]]<span class="sc">$</span>samples</span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a>  eta_data <span class="ot">&lt;-</span> </span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">data.table</span>(</span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true" tabindex="-1"></a>      <span class="at">row_id =</span> <span class="fu">seq_len</span>(<span class="fu">nrow</span>(data)),</span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true" tabindex="-1"></a>      <span class="at">eta =</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="fu">nrow</span>(data))</span>
<span id="cb68-10"><a href="#cb68-10" aria-hidden="true" tabindex="-1"></a>    ) <span class="sc">%&gt;%</span> </span>
<span id="cb68-11"><a href="#cb68-11" aria-hidden="true" tabindex="-1"></a>    .[rows, eta <span class="sc">:</span><span class="er">=</span> <span class="dv">1</span> <span class="sc">/</span> <span class="fu">length</span>(rows)]</span>
<span id="cb68-12"><a href="#cb68-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-13"><a href="#cb68-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(eta_data)</span>
<span id="cb68-14"><a href="#cb68-14" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We apply <code>get_eta()</code> for each of the 2000 trees.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>eta_all <span class="ot">&lt;-</span></span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lapply</span>(</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>    <span class="dv">1</span><span class="sc">:</span><span class="dv">2000</span>,</span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">function</span>(x) <span class="fu">get_eta</span>(x, X_0)</span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rbindlist</span>(<span class="at">idcol =</span> <span class="st">"t"</span>)</span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>            t row_id         eta
      1:    1      1 0.000000000
      2:    1      2 0.000000000
      3:    1      3 0.007751938
      4:    1      4 0.000000000
      5:    1      5 0.007751938
     ---                        
1999996: 2000    996 0.006622517
1999997: 2000    997 0.000000000
1999998: 2000    998 0.000000000
1999999: 2000    999 0.000000000
2000000: 2000   1000 0.000000000</code></pre>
</div>
</div>
<p>Calculate the mean of <span class="math inline">\(\eta_{i,t}\)</span> by <code>row_id</code> (observation) to calculate <span class="math inline">\(\alpha(X_i, X_0)\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>weights <span class="ot">&lt;-</span> </span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>  eta_all <span class="sc">%&gt;%</span> </span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true" tabindex="-1"></a>  .[, .(<span class="at">weight =</span> <span class="fu">mean</span>(eta)), <span class="at">by =</span> row_id]</span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      row_id       weight
   1:      1 0.0020590623
   2:      2 0.0002230001
   3:      3 0.0021290241
   4:      4 0.0002044371
   5:      5 0.0018758049
  ---                    
 996:    996 0.0007930756
 997:    997 0.0020828039
 998:    998 0.0022451720
 999:    999 0.0011955923
1000:   1000 0.0009944451</code></pre>
</div>
</div>
<p>Here is the observations that was given the highest and lowest weights.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>data_with_wights <span class="ot">&lt;-</span> <span class="fu">cbind</span>(data, weights)</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true" tabindex="-1"></a><span class="co">#=== highest (1st) and lowest (2nd) ===#</span></span>
<span id="cb73-4"><a href="#cb73-4" aria-hidden="true" tabindex="-1"></a>data_with_wights[weight <span class="sc">%in%</span> <span class="fu">c</span>(<span class="fu">max</span>(weight), <span class="fu">min</span>(weight)), ]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>            y     T        x1       x2        x3         x4        x5
1:   9.795849 FALSE 2.5423094 0.713593 0.3197887 0.05861231 0.3453091
2: 140.585346  TRUE 0.8604428 2.631900 2.4980926 0.62398495 0.2051564
           x6        x7        x8        x9       x10    y_tilde    t_tilde
1: 0.78236485 0.8942680 0.7315653 0.5546698 0.6760430 -13.150179 -0.5710382
2: 0.09216643 0.4551321 0.2967835 0.3810359 0.3444893   1.000322  0.4291793
   eta_t1 row_id       weight
1:      0    563 2.400879e-03
2:      0    987 1.020478e-05</code></pre>
</div>
</div>
<p>Then, we can use <a href="C03-cf-orf.html#eq-theta-solution">Equation&nbsp;<span>13.3</span></a> to calculate <span class="math inline">\(\hat{\theta}(X_0)\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>theta_X0 <span class="ot">&lt;-</span> <span class="fu">sum</span>(data_with_wights[, weight <span class="sc">*</span> (T<span class="sc">-</span>cf_trained<span class="sc">$</span>W.hat) <span class="sc">*</span> (y<span class="sc">-</span>cf_trained<span class="sc">$</span>Y.hat)]) <span class="sc">/</span> <span class="fu">sum</span>(data_with_wights[, weight <span class="sc">*</span> (T<span class="sc">-</span>cf_trained<span class="sc">$</span>W.hat)<span class="sc">^</span><span class="dv">2</span>])</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 13.44869</code></pre>
</div>
</div>
<p>Note that <span class="math inline">\(Y.hat\)</span> and <span class="math inline">\(W.hat\)</span> attributes of <code>cf_trained</code> are the estimates of <span class="math inline">\(E[Y|X]\)</span> and <span class="math inline">\(E[T|X]\)</span>, respectively. By subtracting them from <span class="math inline">\(Y\)</span> and <span class="math inline">\(T\)</span>, <span class="math inline">\(\tilde{Y}\)</span> and <span class="math inline">\(\tilde{T}\)</span> are calculated in the above code.</p>
<p>To get the weights, we could have just used <code>get_forest_weights()</code> like below:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>weights <span class="ot">&lt;-</span> <span class="fu">get_forest_weights</span>(cf_trained, X_0)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s compare the weights and see if we did it right.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">as.matrix</span>(weights) <span class="sc">-</span> data_with_wights<span class="sc">$</span>weight)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.478887e-18</code></pre>
</div>
</div>
</section>
</section>
<section id="references" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="references">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-athey2019generalized" class="csl-entry" role="doc-biblioentry">
Athey, Susan, Julie Tibshirani, and Stefan Wager. 2019. <span>“Generalized Random Forests.”</span> <em>The Annals of Statistics</em> 47 (2): 1148–78.
</div>
</div>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      let href = ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./E01-spatial-cv.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Spatial Cross-validation</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./PROG-R-01-mlr3.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Machine Learning with <code>mlr3</code></span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb80" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Generalized Random Forest {#sec-grf}</span></span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a>:::{.callout-important}</span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a><span class="fu">## What you will learn</span></span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-8"><a href="#cb80-8" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>What GRF is</span>
<span id="cb80-9"><a href="#cb80-9" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>How GRF was motivated</span>
<span id="cb80-10"><a href="#cb80-10" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>Random forest is a special case of GRF</span>
<span id="cb80-11"><a href="#cb80-11" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>Honesty rule</span>
<span id="cb80-12"><a href="#cb80-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-13"><a href="#cb80-13" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb80-14"><a href="#cb80-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-15"><a href="#cb80-15" aria-hidden="true" tabindex="-1"></a>:::{.callout-tip}</span>
<span id="cb80-16"><a href="#cb80-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-17"><a href="#cb80-17" aria-hidden="true" tabindex="-1"></a><span class="fu">## Background knowledge</span></span>
<span id="cb80-18"><a href="#cb80-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-19"><a href="#cb80-19" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>(necessary) random forest (@sec-rf)</span>
<span id="cb80-20"><a href="#cb80-20" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>(necessary) local regression (@sec-local)</span>
<span id="cb80-21"><a href="#cb80-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-22"><a href="#cb80-22" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb80-23"><a href="#cb80-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-24"><a href="#cb80-24" aria-hidden="true" tabindex="-1"></a>::: {.callout-note}</span>
<span id="cb80-25"><a href="#cb80-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-26"><a href="#cb80-26" aria-hidden="true" tabindex="-1"></a><span class="fu">## Packages to load for replication</span></span>
<span id="cb80-27"><a href="#cb80-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-30"><a href="#cb80-30" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-31"><a href="#cb80-31" aria-hidden="true" tabindex="-1"></a><span class="co">#| include: false</span></span>
<span id="cb80-32"><a href="#cb80-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-33"><a href="#cb80-33" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb80-34"><a href="#cb80-34" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(data.table)</span>
<span id="cb80-35"><a href="#cb80-35" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ranger)</span>
<span id="cb80-36"><a href="#cb80-36" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(grf)</span>
<span id="cb80-37"><a href="#cb80-37" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-38"><a href="#cb80-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-41"><a href="#cb80-41" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-42"><a href="#cb80-42" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb80-43"><a href="#cb80-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-44"><a href="#cb80-44" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb80-45"><a href="#cb80-45" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(data.table)</span>
<span id="cb80-46"><a href="#cb80-46" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ranger)</span>
<span id="cb80-47"><a href="#cb80-47" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(grf)</span>
<span id="cb80-48"><a href="#cb80-48" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-49"><a href="#cb80-49" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb80-50"><a href="#cb80-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-51"><a href="#cb80-51" aria-hidden="true" tabindex="-1"></a><span class="fu">## Random forest as a local constant regression</span></span>
<span id="cb80-52"><a href="#cb80-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-53"><a href="#cb80-53" aria-hidden="true" tabindex="-1"></a>Suppose you are interested in estimating $E<span class="co">[</span><span class="ot">y|X</span><span class="co">]</span>$ using a dataset and you have trained a random forest model with $T$ tress. Now, let $\eta_{i,t}(X)$ takes $1$ if observation $i$ belongs to the same leaf as $X$ in tree $t$, where $X$ is a vector of covariates ($K$ variables). Then, the RF's predicted value of $y$ conditional on a particular value of $X$ (say, $X_0$) can be written as follows:</span>
<span id="cb80-54"><a href="#cb80-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-55"><a href="#cb80-55" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-56"><a href="#cb80-56" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb80-57"><a href="#cb80-57" aria-hidden="true" tabindex="-1"></a>\hat{y}(X_0) = \frac{1}{T} \cdot\sum_{t=1}^T\sum_{i=1}^N \frac{\eta_{i,t}(X_0)}{\sum_{i=1}^N\eta_{i,t}(X_0)}\cdot y_i</span>
<span id="cb80-58"><a href="#cb80-58" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb80-59"><a href="#cb80-59" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-60"><a href="#cb80-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-61"><a href="#cb80-61" aria-hidden="true" tabindex="-1"></a>Note that $\sum_{i=1}^N\eta_{i,t}(X_0)$ represents the number of observations in the same leaf as $X_0$. Therefore, $\sum_{i=1}^N \frac{\eta_{i,t}(X_0)}{\sum_{i=1}^N\eta_{i,t}(X_0)}\cdot y_i$ is the average value of $y$ of the leaf that $X_0$ belongs to. So, while looking slightly complicated, it is the average value of $y$ of the tree $X_0$ belongs to averaged across the trees, which we know is how RF predicts $y$ at $X_0$.</span>
<span id="cb80-62"><a href="#cb80-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-63"><a href="#cb80-63" aria-hidden="true" tabindex="-1"></a>We can switch the summations like this,</span>
<span id="cb80-64"><a href="#cb80-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-65"><a href="#cb80-65" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-66"><a href="#cb80-66" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb80-67"><a href="#cb80-67" aria-hidden="true" tabindex="-1"></a>\hat{y}(X_0) = \sum_{i=1}^N \cdot\frac{1}{T}\sum_{t=1}^T\frac{\eta_{i,t}(X_0)}{\sum_{i=1}^N\eta_{i,t}(X_0)}\cdot y_i</span>
<span id="cb80-68"><a href="#cb80-68" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb80-69"><a href="#cb80-69" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-70"><a href="#cb80-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-71"><a href="#cb80-71" aria-hidden="true" tabindex="-1"></a>Let $\alpha(X_i, X_0)$ denote $\frac{1}{T}\sum_{t=1}^T\frac{\eta_{i,t}(X_0)}{\sum_{i=1}^N\eta_{i,t}(X_0)}$. Then, we can rewrite the above equation as</span>
<span id="cb80-72"><a href="#cb80-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-73"><a href="#cb80-73" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-74"><a href="#cb80-74" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb80-75"><a href="#cb80-75" aria-hidden="true" tabindex="-1"></a>\hat{y}(X_0) = \sum_{i=1}^N \alpha(X_i,X_0) \cdot y_i</span>
<span id="cb80-76"><a href="#cb80-76" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb80-77"><a href="#cb80-77" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-78"><a href="#cb80-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-79"><a href="#cb80-79" aria-hidden="true" tabindex="-1"></a>Now, it is easy to show that $\hat{y}(X_0)$ is a solution to the following minimization problem.</span>
<span id="cb80-80"><a href="#cb80-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-81"><a href="#cb80-81" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-82"><a href="#cb80-82" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb80-83"><a href="#cb80-83" aria-hidden="true" tabindex="-1"></a>Min_{\theta} \sum_{i=1}^N \alpha(X_i,X_0)\cdot<span class="co">[</span><span class="ot">y_i -\theta</span><span class="co">]</span>^2</span>
<span id="cb80-84"><a href="#cb80-84" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb80-85"><a href="#cb80-85" aria-hidden="true" tabindex="-1"></a>$$ {#eq-ll-constant}</span>
<span id="cb80-86"><a href="#cb80-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-87"><a href="#cb80-87" aria-hidden="true" tabindex="-1"></a>In this formulation of the problem, $\alpha(X_i,X_0)$ can be considered the weight given to observation $i$. </span>
<span id="cb80-88"><a href="#cb80-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-89"><a href="#cb80-89" aria-hidden="true" tabindex="-1"></a>By definition,</span>
<span id="cb80-90"><a href="#cb80-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-91"><a href="#cb80-91" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>$0 \leq \alpha(X_i,X_0) \leq 1$</span>
<span id="cb80-92"><a href="#cb80-92" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>$\sum_{i=1}^N \alpha(X_i,X_0) = 1$</span>
<span id="cb80-93"><a href="#cb80-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-94"><a href="#cb80-94" aria-hidden="true" tabindex="-1"></a>You may notice that @eq-ll-constant is actually a special case of <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:blue"</span><span class="kw">&gt;</span>local constant regression<span class="kw">&lt;/span&gt;</span> where the individual weights are $\alpha(X_i, X_0)$. Roughly speaking, $\alpha(X_i, X_0)$ measures how often observation $i$ share the same leaves as the evaluation point ($X_0$) across $T$ trees. So, it measures how similar $X_i$ is to $X_0$ in the RF way, but not based on euclidean distance (which is subject to curse of dimensionality). So, RF is actually a local constant regression with a special way of distributing weights to the individual observations. </span>
<span id="cb80-95"><a href="#cb80-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-96"><a href="#cb80-96" aria-hidden="true" tabindex="-1"></a><span class="fu">## GRF</span></span>
<span id="cb80-97"><a href="#cb80-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-98"><a href="#cb80-98" aria-hidden="true" tabindex="-1"></a>Interpretation of RF as a local regression led @athey2019generalized to conceive GRF, under which various statistics (e.g., conditional average treatment effect, conditional quantile) can be estimated under the unified framework.</span>
<span id="cb80-99"><a href="#cb80-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-100"><a href="#cb80-100" aria-hidden="true" tabindex="-1"></a>You can consider prediction of $y$ at $X = X_0$ using RF as a three-step process.</span>
<span id="cb80-101"><a href="#cb80-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-102"><a href="#cb80-102" aria-hidden="true" tabindex="-1"></a>:::{.callout-note}</span>
<span id="cb80-103"><a href="#cb80-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-104"><a href="#cb80-104" aria-hidden="true" tabindex="-1"></a><span class="fu">## Predicting $y$ at $X=X_0$ using RF</span></span>
<span id="cb80-105"><a href="#cb80-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-106"><a href="#cb80-106" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Grow trees (find splitting rules for each tree)</span>
<span id="cb80-107"><a href="#cb80-107" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>For $X = X_0$, find the weights for individual observations based on the trees grown  </span>
<span id="cb80-108"><a href="#cb80-108" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Solve @eq-ll-constant based on the weights</span>
<span id="cb80-109"><a href="#cb80-109" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb80-110"><a href="#cb80-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-111"><a href="#cb80-111" aria-hidden="true" tabindex="-1"></a>Step 1 is done only once. Every time you make a prediction at a different value of $X$, you go over steps 2 and 3. </span>
<span id="cb80-112"><a href="#cb80-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-113"><a href="#cb80-113" aria-hidden="true" tabindex="-1"></a>GRF follows exactly the same steps except that it <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>adjusts the way trees are grown <span class="kw">&lt;/span&gt;</span>in step 1 and <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>adjusts the way the local minimization problem<span class="kw">&lt;/span&gt;</span> is solved at step 3 (they are actually interrelated) depending on what you would like to estimate.</span>
<span id="cb80-114"><a href="#cb80-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-115"><a href="#cb80-115" aria-hidden="true" tabindex="-1"></a>Here, we follow the notations used in @athey2019generalized as much as possible. Here are the list of notations:</span>
<span id="cb80-116"><a href="#cb80-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-117"><a href="#cb80-117" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>$O_i$: data for observation $i$</span>
<span id="cb80-118"><a href="#cb80-118" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>$\theta$: the statistics of interest (e.g., CATE for causal forest, conditional quantile for for quantile forest)</span>
<span id="cb80-119"><a href="#cb80-119" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>$\nu$: nuisance statistics (you are not interested in estimating this). </span>
<span id="cb80-120"><a href="#cb80-120" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>$\Psi_{\theta, \nu}(O)$: score function</span>
<span id="cb80-121"><a href="#cb80-121" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>$\alpha_i(x)$: weight given to observation $i$ when predicting at $X=x$. </span>
<span id="cb80-122"><a href="#cb80-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-123"><a href="#cb80-123" aria-hidden="true" tabindex="-1"></a>In general, GRF solves the following problem to find the estimate of $\theta$ conditional on $X_i= x$: </span>
<span id="cb80-124"><a href="#cb80-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-125"><a href="#cb80-125" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-126"><a href="#cb80-126" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb80-127"><a href="#cb80-127" aria-hidden="true" tabindex="-1"></a>\sum_{i=1}^n \alpha_i(x)\Psi_{\theta, \nu}(O_i) = 0</span>
<span id="cb80-128"><a href="#cb80-128" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb80-129"><a href="#cb80-129" aria-hidden="true" tabindex="-1"></a>$$ {#eq-opt}</span>
<span id="cb80-130"><a href="#cb80-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-131"><a href="#cb80-131" aria-hidden="true" tabindex="-1"></a>As we saw earlier, GRF is RF when $\Psi_{\theta, \nu}(O_i) = Y_i-\theta$. There is no nuisance statistics, $\nu(x)$, in the RF case. By changing how the score function ($\Psi_{\theta, \nu}(O_i)$) is defined, you can estimate <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:blue"</span><span class="kw">&gt;</span> a wide range of statistics using different approaches <span class="kw">&lt;/span&gt;</span>under the <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:blue"</span><span class="kw">&gt;</span> same<span class="kw">&lt;/span&gt;</span> estimation framework (this is why it is called <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>generalized <span class="kw">&lt;/span&gt;</span>random forest). Here are some of the statistics and estimation approaches that are under the GRF framework.</span>
<span id="cb80-132"><a href="#cb80-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-133"><a href="#cb80-133" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>Conditional expectation ($E<span class="co">[</span><span class="ot">Y|X</span><span class="co">]</span>$)</span>
<span id="cb80-134"><a href="#cb80-134" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>Regression forest (Random forest for regression)</span>
<span id="cb80-135"><a href="#cb80-135" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>Local linear forest</span>
<span id="cb80-136"><a href="#cb80-136" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>Boosted regression forest</span>
<span id="cb80-137"><a href="#cb80-137" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>Conditional average treatment effect (CATE)</span>
<span id="cb80-138"><a href="#cb80-138" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>Causal forest</span>
<span id="cb80-139"><a href="#cb80-139" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>Instrumental forest</span>
<span id="cb80-140"><a href="#cb80-140" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>Conditional quantile</span>
<span id="cb80-141"><a href="#cb80-141" aria-hidden="true" tabindex="-1"></a><span class="ss">  * </span>Quantile forest</span>
<span id="cb80-142"><a href="#cb80-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-143"><a href="#cb80-143" aria-hidden="true" tabindex="-1"></a>:::{.callout-note}</span>
<span id="cb80-144"><a href="#cb80-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-145"><a href="#cb80-145" aria-hidden="true" tabindex="-1"></a><span class="fu">## Score function examples</span></span>
<span id="cb80-146"><a href="#cb80-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-147"><a href="#cb80-147" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>Random forest: $\Psi_{\theta, \nu}(O_i) = Y_i - \theta$</span>
<span id="cb80-148"><a href="#cb80-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-149"><a href="#cb80-149" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>Causal forest: $\Psi_{\theta, \nu}(O_i) = (\tilde{Y}_i- \theta\tilde{T}_i)\cdot\tilde{T}_i$, where $\tilde{v_i} = v_i - E<span class="co">[</span><span class="ot">v_i|X_i</span><span class="co">]</span>$</span>
<span id="cb80-150"><a href="#cb80-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-151"><a href="#cb80-151" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>Quantile forest: $\Psi_{\theta, \nu}(O_i) = qI<span class="sc">\{</span>Y_i &gt; \theta<span class="sc">\}</span> - (1-q)I<span class="sc">\{</span>Y_i \leq \theta<span class="sc">\}</span>$</span>
<span id="cb80-152"><a href="#cb80-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-153"><a href="#cb80-153" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb80-154"><a href="#cb80-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-155"><a href="#cb80-155" aria-hidden="true" tabindex="-1"></a>::: {.column-margin}</span>
<span id="cb80-156"><a href="#cb80-156" aria-hidden="true" tabindex="-1"></a>$I<span class="sc">\{\}</span>$ is an indicator function that takes 1 if the condition inside the curly brackets and 0 otherwise.</span>
<span id="cb80-157"><a href="#cb80-157" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb80-158"><a href="#cb80-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-159"><a href="#cb80-159" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb80-160"><a href="#cb80-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-161"><a href="#cb80-161" aria-hidden="true" tabindex="-1"></a>So far, we have only talked about score functions so far, but not the weights. Do all the approaches listed above use the weights derived from the trees grown by the traditional RF in solving @eq-opt? You could (you are free to use any weights), but that would not be wise. As mentioned earlier, GRF adjusts the way trees are grown (thus weights) as well according to the score function so that weights are optimized for your objective. This makes sense. Right <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:blue"</span><span class="kw">&gt;</span> neighbors <span class="kw">&lt;/span&gt;</span>should be different based on what you are interesting in estimating.</span>
<span id="cb80-162"><a href="#cb80-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-163"><a href="#cb80-163" aria-hidden="true" tabindex="-1"></a>Specifically, GRF uses the random forest <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:blue"</span><span class="kw">&gt;</span>algorithm<span class="kw">&lt;/span&gt;</span> to grow trees based on <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:blue"</span><span class="kw">&gt;</span> pseudo outcome ($\rho_i$) <span class="kw">&lt;/span&gt;</span> derived from the score function that is specific to the type of regression you are running, but not on $Y$. Basically, you are using exactly the same algorithm as the traditional RF we saw in @sec-rf except that $Y$ is swapped with the pseudo outcome.</span>
<span id="cb80-164"><a href="#cb80-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-165"><a href="#cb80-165" aria-hidden="true" tabindex="-1"></a>::: {.column-margin}</span>
<span id="cb80-166"><a href="#cb80-166" aria-hidden="true" tabindex="-1"></a>See @athey2019generalized for how the pseudo outcome is derived from a score function in general. </span>
<span id="cb80-167"><a href="#cb80-167" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb80-168"><a href="#cb80-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-169"><a href="#cb80-169" aria-hidden="true" tabindex="-1"></a>:::{.callout-note}</span>
<span id="cb80-170"><a href="#cb80-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-171"><a href="#cb80-171" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example pseudo outcomes</span></span>
<span id="cb80-172"><a href="#cb80-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-173"><a href="#cb80-173" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:blue"</span><span class="kw">&gt;</span>Random forest<span class="kw">&lt;/span&gt;</span>: </span>
<span id="cb80-174"><a href="#cb80-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-175"><a href="#cb80-175" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-176"><a href="#cb80-176" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb80-177"><a href="#cb80-177" aria-hidden="true" tabindex="-1"></a>\rho_i = Y_i - \hat{\theta}_P</span>
<span id="cb80-178"><a href="#cb80-178" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb80-179"><a href="#cb80-179" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-180"><a href="#cb80-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-181"><a href="#cb80-181" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:blue"</span><span class="kw">&gt;</span> Causal forest<span class="kw">&lt;/span&gt;</span>:</span>
<span id="cb80-182"><a href="#cb80-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-183"><a href="#cb80-183" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-184"><a href="#cb80-184" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb80-185"><a href="#cb80-185" aria-hidden="true" tabindex="-1"></a>\rho_i = (\tilde{Y}_i- \hat{\theta}_P \tilde{T}_i)\cdot\tilde{T}_i</span>
<span id="cb80-186"><a href="#cb80-186" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb80-187"><a href="#cb80-187" aria-hidden="true" tabindex="-1"></a>$$ </span>
<span id="cb80-188"><a href="#cb80-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-189"><a href="#cb80-189" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:blue"</span><span class="kw">&gt;</span> Quantile forest<span class="kw">&lt;/span&gt;</span>:</span>
<span id="cb80-190"><a href="#cb80-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-191"><a href="#cb80-191" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-192"><a href="#cb80-192" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb80-193"><a href="#cb80-193" aria-hidden="true" tabindex="-1"></a>\rho_i = I<span class="sc">\{</span>Y_i &gt; \hat{\theta}_P<span class="sc">\}</span></span>
<span id="cb80-194"><a href="#cb80-194" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb80-195"><a href="#cb80-195" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-196"><a href="#cb80-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-197"><a href="#cb80-197" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb80-198"><a href="#cb80-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-199"><a href="#cb80-199" aria-hidden="true" tabindex="-1"></a>Note that $\hat{\theta}_P$ in the pseudo outcomes presented above is the solution to @eq-opt with their respective score functions using the data in the parent node. For example, $\hat{\theta}_P = \bar{Y}_p$ for RF, which is the average value of $Y$ in the parent node. In quantile forest, $\hat{\theta}_P$ is the $q$th quantile of the parent node if you are estimating the $q$the conditional quantile.</span>
<span id="cb80-200"><a href="#cb80-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-201"><a href="#cb80-201" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb80-202"><a href="#cb80-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-203"><a href="#cb80-203" aria-hidden="true" tabindex="-1"></a>Finally, here are the conceptual steps of GRF:</span>
<span id="cb80-204"><a href="#cb80-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-205"><a href="#cb80-205" aria-hidden="true" tabindex="-1"></a>:::{.callout-note}</span>
<span id="cb80-206"><a href="#cb80-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-207"><a href="#cb80-207" aria-hidden="true" tabindex="-1"></a><span class="fu">## GRF: conceptual steps </span></span>
<span id="cb80-208"><a href="#cb80-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-209"><a href="#cb80-209" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>Training (Forest Growing) Step:</span>
<span id="cb80-210"><a href="#cb80-210" aria-hidden="true" tabindex="-1"></a><span class="ss">  1. </span>Determine what statistics you are interested in ($\theta(X)$, e.g., CATE, conditional quantile)</span>
<span id="cb80-211"><a href="#cb80-211" aria-hidden="true" tabindex="-1"></a><span class="ss">  2. </span>Define the appropriate score function according to the statistics of interest</span>
<span id="cb80-212"><a href="#cb80-212" aria-hidden="true" tabindex="-1"></a><span class="ss">  3. </span>Derive the pseudo outcome based on the score function</span>
<span id="cb80-213"><a href="#cb80-213" aria-hidden="true" tabindex="-1"></a><span class="ss">  4. </span>Grow trees using the RF algorithm based on the pseudo outcome</span>
<span id="cb80-214"><a href="#cb80-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-215"><a href="#cb80-215" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>Prediction Step:</span>
<span id="cb80-216"><a href="#cb80-216" aria-hidden="true" tabindex="-1"></a><span class="ss">  1. </span>Determine what value of $X$ you would like to predict $\theta(X)$ (call it $X_0$)</span>
<span id="cb80-217"><a href="#cb80-217" aria-hidden="true" tabindex="-1"></a><span class="ss">  2. </span>Find the individual weights $\alpha(X_i, X_0)$ according to </span>
<span id="cb80-218"><a href="#cb80-218" aria-hidden="true" tabindex="-1"></a>  $$</span>
<span id="cb80-219"><a href="#cb80-219" aria-hidden="true" tabindex="-1"></a>  \begin{aligned}</span>
<span id="cb80-220"><a href="#cb80-220" aria-hidden="true" tabindex="-1"></a>  \alpha(X_i, X_0) = \frac{1}{T}\sum_{t=1}^T\frac{\eta_{i,t}(X_0)}{\sum_{i=1}^N\eta_{i,t}(X_0)}</span>
<span id="cb80-221"><a href="#cb80-221" aria-hidden="true" tabindex="-1"></a>  \end{aligned}</span>
<span id="cb80-222"><a href="#cb80-222" aria-hidden="true" tabindex="-1"></a>  $$</span>
<span id="cb80-223"><a href="#cb80-223" aria-hidden="true" tabindex="-1"></a>  , which measures how often observation $i$ belongs to the same node as the evaluation point $X_0$.</span>
<span id="cb80-224"><a href="#cb80-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-225"><a href="#cb80-225" aria-hidden="true" tabindex="-1"></a><span class="ss">  3. </span>Solve @eq-opt with the weights obtained above and the score function specified earlier</span>
<span id="cb80-226"><a href="#cb80-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-227"><a href="#cb80-227" aria-hidden="true" tabindex="-1"></a>::: </span>
<span id="cb80-228"><a href="#cb80-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-229"><a href="#cb80-229" aria-hidden="true" tabindex="-1"></a>The training step is done only once (trees are built only once). But, whenever you predict $\theta(X)$ at different values of $X$, you go through the prediction step.</span>
<span id="cb80-230"><a href="#cb80-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-231"><a href="#cb80-231" aria-hidden="true" tabindex="-1"></a>::: {.column-margin}</span>
<span id="cb80-232"><a href="#cb80-232" aria-hidden="true" tabindex="-1"></a>Orthogonal random forest (a forest-based heterogeneous treatment effect estimator like causal forest), on the other hand, build trees every time when predicting treatment effect $\theta(X)$ at particular values of $X$, which is why orthogonal random forest takes a very long time especially when there are many evaluation points. Orthogonal random forest is covered in @sec-cf-orf.</span>
<span id="cb80-233"><a href="#cb80-233" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb80-234"><a href="#cb80-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-235"><a href="#cb80-235" aria-hidden="true" tabindex="-1"></a><span class="fu">## Examples of GRF </span></span>
<span id="cb80-236"><a href="#cb80-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-237"><a href="#cb80-237" aria-hidden="true" tabindex="-1"></a><span class="fu">### Random forest as a GRF</span></span>
<span id="cb80-238"><a href="#cb80-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-239"><a href="#cb80-239" aria-hidden="true" tabindex="-1"></a>Here, we take a look at RF as a GRF as an illustration to understand the general GRF procedure better. When $\Psi_{\theta, \nu}(Y_i)$ is set to $Y_i - \theta$, then GRF is equivalent to the traditional RF. By plugging $Y_i - \theta$ into @eq-opt, the estimate of $E<span class="co">[</span><span class="ot">Y|X=X_0</span><span class="co">]</span>$, $\theta(X_0)$, is identified by solving </span>
<span id="cb80-240"><a href="#cb80-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-241"><a href="#cb80-241" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-242"><a href="#cb80-242" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb80-243"><a href="#cb80-243" aria-hidden="true" tabindex="-1"></a>\sum_{i=1}^n \alpha_i(X_0)(Y_i - \theta) = 0</span>
<span id="cb80-244"><a href="#cb80-244" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb80-245"><a href="#cb80-245" aria-hidden="true" tabindex="-1"></a>$$ {#eq-rf}</span>
<span id="cb80-246"><a href="#cb80-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-247"><a href="#cb80-247" aria-hidden="true" tabindex="-1"></a>The weights $\alpha_i(X_i, X_0)$ are defined as follows:</span>
<span id="cb80-248"><a href="#cb80-248" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-249"><a href="#cb80-249" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb80-250"><a href="#cb80-250" aria-hidden="true" tabindex="-1"></a>\alpha_i(X_i, X_0) = \frac{1}{T}\cdot\sum_{t=1}^T\frac{\eta_{i,t}(X_0)}{\sum_{i=1}^N\eta_{i,t}(X_0)}</span>
<span id="cb80-251"><a href="#cb80-251" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb80-252"><a href="#cb80-252" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-253"><a href="#cb80-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-254"><a href="#cb80-254" aria-hidden="true" tabindex="-1"></a>, where $\eta_{i,t}(X_0)$ is 1 if observation $i$ which has feature values $X_i$ belongs to the same leaf as $X_0$. </span>
<span id="cb80-255"><a href="#cb80-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-256"><a href="#cb80-256" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb80-257"><a href="#cb80-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-258"><a href="#cb80-258" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:blue"</span><span class="kw">&gt;</span> Step 1: Grow trees<span class="kw">&lt;/span&gt;</span>:</span>
<span id="cb80-259"><a href="#cb80-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-260"><a href="#cb80-260" aria-hidden="true" tabindex="-1"></a>Now, let's consider growing trees to find $\alpha_i(X_0)$ in @eq-rf. For a given sample and set of variables randomly selected, GRF starts with solving the unweighted version of @eq-rf. </span>
<span id="cb80-261"><a href="#cb80-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-262"><a href="#cb80-262" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-263"><a href="#cb80-263" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb80-264"><a href="#cb80-264" aria-hidden="true" tabindex="-1"></a>\sum_{i=1}^n Y_i - \theta = 0</span>
<span id="cb80-265"><a href="#cb80-265" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb80-266"><a href="#cb80-266" aria-hidden="true" tabindex="-1"></a>$$ {#eq-rf-initial}</span>
<span id="cb80-267"><a href="#cb80-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-268"><a href="#cb80-268" aria-hidden="true" tabindex="-1"></a>The solution to this problem is simply the mean of $Y$, which will be denoted as $\bar{Y}_P$, where $P$ represents the parent node. Here, the parent node include all the data points as this is the first split.</span>
<span id="cb80-269"><a href="#cb80-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-270"><a href="#cb80-270" aria-hidden="true" tabindex="-1"></a>Then the pseudo outcome ($\rho_i$) that is used in splitting is</span>
<span id="cb80-271"><a href="#cb80-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-272"><a href="#cb80-272" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-273"><a href="#cb80-273" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb80-274"><a href="#cb80-274" aria-hidden="true" tabindex="-1"></a>\rho_i = Y_i - \bar{Y}_P</span>
<span id="cb80-275"><a href="#cb80-275" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb80-276"><a href="#cb80-276" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-277"><a href="#cb80-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-278"><a href="#cb80-278" aria-hidden="true" tabindex="-1"></a>Now, a standard CART regression split is applied on the pseudo outcomes. That is, the variable-threshold combination that maximizes the following criteria is found:</span>
<span id="cb80-279"><a href="#cb80-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-280"><a href="#cb80-280" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-281"><a href="#cb80-281" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb80-282"><a href="#cb80-282" aria-hidden="true" tabindex="-1"></a>\tilde{\Delta}(C_1, C_2) = \frac{(\sum_{i \in C_1} \rho_i)^2}{N_{C_1}} + \frac{(\sum_{i \in C_2} \rho_i)^2}{N_{C_2}}</span>
<span id="cb80-283"><a href="#cb80-283" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb80-284"><a href="#cb80-284" aria-hidden="true" tabindex="-1"></a>$$ {#eq-criteria}</span>
<span id="cb80-285"><a href="#cb80-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-286"><a href="#cb80-286" aria-hidden="true" tabindex="-1"></a>where $C_1$ and $C_2$ represent two child node candidates for a given split. Since $\bar{Y}_P$ is just a constant, it is equivalent to splitting on $Y_i$. So, this is exactly the same as how the traditional RF builds trees (see @sec-split-sim for the rationale behind maximizing the criteria presented in @eq-criteria).</span>
<span id="cb80-287"><a href="#cb80-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-288"><a href="#cb80-288" aria-hidden="true" tabindex="-1"></a>Once the best split is identified, each of the new child nodes is split following the exactly the same procedure. Splitting continues until one of the user-specified condition prevent a further splitting. Many trees from bootstrapped samples are created (just like the regular random forest) and they form a forest. This shows that the GRF with $\Psi_{\theta, \nu}(Y_i, X_i) = Y_i - \theta$ grows trees in the same manner as the traditional RF. RF in GRF is implemented by <span class="in">`regression_forest()`</span>. But, note that running <span class="in">`ranger()`</span> and <span class="in">`regression_forest()`</span> will not result in the same forest because of the randomness involved in resampling and random selection of variables. Only their algorithms are equivalent</span>
<span id="cb80-289"><a href="#cb80-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-290"><a href="#cb80-290" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb80-291"><a href="#cb80-291" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:blue"</span><span class="kw">&gt;</span> Step 2: Predict<span class="kw">&lt;/span&gt;</span>:</span>
<span id="cb80-292"><a href="#cb80-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-293"><a href="#cb80-293" aria-hidden="true" tabindex="-1"></a>To predict $E<span class="co">[</span><span class="ot">Y|X=X_0</span><span class="co">]</span>$, @eq-rf is solved</span>
<span id="cb80-294"><a href="#cb80-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-295"><a href="#cb80-295" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-296"><a href="#cb80-296" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb80-297"><a href="#cb80-297" aria-hidden="true" tabindex="-1"></a>\sum_{i=1}^N \alpha_i(X_i, X_0)(Y_i-\theta) = 0</span>
<span id="cb80-298"><a href="#cb80-298" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb80-299"><a href="#cb80-299" aria-hidden="true" tabindex="-1"></a>$$  </span>
<span id="cb80-300"><a href="#cb80-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-301"><a href="#cb80-301" aria-hidden="true" tabindex="-1"></a>So,</span>
<span id="cb80-302"><a href="#cb80-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-303"><a href="#cb80-303" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-304"><a href="#cb80-304" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb80-305"><a href="#cb80-305" aria-hidden="true" tabindex="-1"></a>\theta(X_0) &amp; = \frac{\sum_{i=1}^N \alpha_i(X_0)Y_i}{\sum_{i=1}^N \alpha_i(X_0)}<span class="sc">\\</span></span>
<span id="cb80-306"><a href="#cb80-306" aria-hidden="true" tabindex="-1"></a>&amp; = \sum_{i=1}^N \alpha_i(X_0)Y_i \;\; \mbox{(since } \sum_{i=1}^N \alpha_i(X_0) = 1\mbox{)} <span class="sc">\\</span></span>
<span id="cb80-307"><a href="#cb80-307" aria-hidden="true" tabindex="-1"></a>&amp; = \sum_{i=1}^N \huge<span class="co">[</span><span class="ot">\normalsize \frac{1}{T}\cdot\sum_{t=1}^T\frac{\eta_{i,t}(X_0)}{\sum_{i=1}^N\eta_{i,t}(X_0)}\cdot y_i\huge</span><span class="co">]</span><span class="sc">\\</span></span>
<span id="cb80-308"><a href="#cb80-308" aria-hidden="true" tabindex="-1"></a>&amp; = \frac{1}{T}  \cdot\sum_{t=1}^T\sum_{i=1}^N \frac{\eta_{i,t}(X_0)}{\sum_{i=1}^N\eta_{i,t}(X_0)}\cdot y_i \;\; \mbox{(changing the order of the summations)} <span class="sc">\\</span></span>
<span id="cb80-309"><a href="#cb80-309" aria-hidden="true" tabindex="-1"></a>&amp; = \frac{1}{T} \cdot\sum_{t=1}^T \bar{Y}_t</span>
<span id="cb80-310"><a href="#cb80-310" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb80-311"><a href="#cb80-311" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-312"><a href="#cb80-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-313"><a href="#cb80-313" aria-hidden="true" tabindex="-1"></a>So, $\theta(X_0)$ from GRF is the average of tree-specific predictions for $X_0$, which is exactly how RF predicts $E<span class="co">[</span><span class="ot">Y|X=X_0</span><span class="co">]</span>$ as well.</span>
<span id="cb80-314"><a href="#cb80-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-315"><a href="#cb80-315" aria-hidden="true" tabindex="-1"></a>So, it has been shown that GRF with $\Psi_{\theta, \nu}(Y_i)=Y_i - \theta$ grows trees in the same manner as the traditional RF and also that GRF predicts $E<span class="co">[</span><span class="ot">Y|X=X_0</span><span class="co">]</span>$ in the same manner as the traditional RF. So, RF is a special case of GRF, where $\Psi_{\theta, \nu}(Y_i)=Y_i - \theta$.</span>
<span id="cb80-316"><a href="#cb80-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-317"><a href="#cb80-317" aria-hidden="true" tabindex="-1"></a><span class="fu">### Causal forest as a GRF</span></span>
<span id="cb80-318"><a href="#cb80-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-319"><a href="#cb80-319" aria-hidden="true" tabindex="-1"></a>Causal forest (with a single treatment variable) as an R-learner is a GRF with</span>
<span id="cb80-320"><a href="#cb80-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-321"><a href="#cb80-321" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-322"><a href="#cb80-322" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb80-323"><a href="#cb80-323" aria-hidden="true" tabindex="-1"></a>\Psi_{\theta, \nu}(O_i) = <span class="co">[</span><span class="ot">(Y_i - E[Y|X_i])- \theta(X_i)(T_i - E[T|X_i])</span><span class="co">](T_i - E[T|X_i])</span></span>
<span id="cb80-324"><a href="#cb80-324" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb80-325"><a href="#cb80-325" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-326"><a href="#cb80-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-327"><a href="#cb80-327" aria-hidden="true" tabindex="-1"></a>In practice $E<span class="co">[</span><span class="ot">Y|X_i</span><span class="co">]</span>$ and $E<span class="co">[</span><span class="ot">T|X_i</span><span class="co">]</span>$ are first estimated using appropriate machine learning methods (e.g., lasso, random forest, gradient boosted forest) in a cross-fitting manner and then the estimation of $Y_i - E<span class="co">[</span><span class="ot">Y|X_i</span><span class="co">]</span>$ and $T_i - E<span class="co">[</span><span class="ot">T|X_i</span><span class="co">]</span>$ are constructed. Let's denote them by $\tilde{Y}_i$ and $\tilde{T}_i$. Then the score function is written as</span>
<span id="cb80-328"><a href="#cb80-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-329"><a href="#cb80-329" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-330"><a href="#cb80-330" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb80-331"><a href="#cb80-331" aria-hidden="true" tabindex="-1"></a>\Psi_{\theta} = (\tilde{Y}_i- \theta\tilde{T}_i)\tilde{T}_i</span>
<span id="cb80-332"><a href="#cb80-332" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb80-333"><a href="#cb80-333" aria-hidden="true" tabindex="-1"></a>$$ {#eq-cf-score}</span>
<span id="cb80-334"><a href="#cb80-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-335"><a href="#cb80-335" aria-hidden="true" tabindex="-1"></a>So, the conditional treatment effect at $X=X_0$, $\theta(X_0)$, is estimated by solving the following problem:</span>
<span id="cb80-336"><a href="#cb80-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-337"><a href="#cb80-337" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-338"><a href="#cb80-338" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb80-339"><a href="#cb80-339" aria-hidden="true" tabindex="-1"></a>\hat{\theta}(X_0) = \sum_{i=1}^N \alpha_i(x)(\tilde{Y_i} - \theta\cdot \tilde{T_i})\tilde{T_i} = 0</span>
<span id="cb80-340"><a href="#cb80-340" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb80-341"><a href="#cb80-341" aria-hidden="true" tabindex="-1"></a>$$ {#eq-cf-solve}</span>
<span id="cb80-342"><a href="#cb80-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-343"><a href="#cb80-343" aria-hidden="true" tabindex="-1"></a>where $\alpha_i(x)$ is the weight obtained from the trees built using random forest on the pseudo outcomes that are derived from the score function (@eq-cf-score).</span>
<span id="cb80-344"><a href="#cb80-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-345"><a href="#cb80-345" aria-hidden="true" tabindex="-1"></a>For a given parent node $p$, the pseudo outcomes are defined as</span>
<span id="cb80-346"><a href="#cb80-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-347"><a href="#cb80-347" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-348"><a href="#cb80-348" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb80-349"><a href="#cb80-349" aria-hidden="true" tabindex="-1"></a>\rho_i = (\tilde{Y}_i- \hat{\theta}_P\tilde{T}_i)\tilde{T}_i</span>
<span id="cb80-350"><a href="#cb80-350" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb80-351"><a href="#cb80-351" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-352"><a href="#cb80-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-353"><a href="#cb80-353" aria-hidden="true" tabindex="-1"></a>where $\hat{\theta}_P$ is the solution of the following problem using the observations in the parent node:</span>
<span id="cb80-354"><a href="#cb80-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-355"><a href="#cb80-355" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-356"><a href="#cb80-356" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb80-357"><a href="#cb80-357" aria-hidden="true" tabindex="-1"></a>\sum_{i=1}^N \tilde{T_i}(\tilde{Y_i}-\theta \tilde{T_i}) = 0</span>
<span id="cb80-358"><a href="#cb80-358" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb80-359"><a href="#cb80-359" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-360"><a href="#cb80-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-361"><a href="#cb80-361" aria-hidden="true" tabindex="-1"></a>, which is the unweighted version of @eq-cf-solve.</span>
<span id="cb80-362"><a href="#cb80-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-363"><a href="#cb80-363" aria-hidden="true" tabindex="-1"></a>The standard CART splitting algorithm is applied on the pseudo outcomes to build trees.</span>
<span id="cb80-364"><a href="#cb80-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-365"><a href="#cb80-365" aria-hidden="true" tabindex="-1"></a><span class="fu">## Honesty {#sec-grf-honest}</span></span>
<span id="cb80-366"><a href="#cb80-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-367"><a href="#cb80-367" aria-hidden="true" tabindex="-1"></a>GRF applies <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:blue"</span><span class="kw">&gt;</span> honesty <span class="kw">&lt;/span&gt;</span>when it trains forests. Specifically, when building a tree, the bootstrapped sample is first split into two groups: subsamples for <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:blue"</span><span class="kw">&gt;</span> splitting<span class="kw">&lt;/span&gt;</span> and <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:blue"</span><span class="kw">&gt;</span>prediction<span class="kw">&lt;/span&gt;</span>. Then, the a tree is trained on the subsample for splitting and then generate the splitting rules. However, when predicting (say $E<span class="co">[</span><span class="ot">Y|X</span><span class="co">]</span>$ at $X=x$), the value of $Y$ from the subsamples for splitting are not used. Rather, only the splitting rules are taken from the trained tree and then they are applied to the subsamples for prediction (@fig-honest illustrates this process). </span>
<span id="cb80-368"><a href="#cb80-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-369"><a href="#cb80-369" aria-hidden="true" tabindex="-1"></a>::: {.column-margin}</span>
<span id="cb80-370"><a href="#cb80-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-371"><a href="#cb80-371" aria-hidden="true" tabindex="-1"></a>**Packages to load for replication**</span>
<span id="cb80-372"><a href="#cb80-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-375"><a href="#cb80-375" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-376"><a href="#cb80-376" aria-hidden="true" tabindex="-1"></a><span class="co">#| include: false</span></span>
<span id="cb80-377"><a href="#cb80-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-378"><a href="#cb80-378" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb80-379"><a href="#cb80-379" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(data.table)</span>
<span id="cb80-380"><a href="#cb80-380" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(grf)</span>
<span id="cb80-381"><a href="#cb80-381" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb80-382"><a href="#cb80-382" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rattle)</span>
<span id="cb80-383"><a href="#cb80-383" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(wooldridge)</span>
<span id="cb80-384"><a href="#cb80-384" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-385"><a href="#cb80-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-388"><a href="#cb80-388" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-389"><a href="#cb80-389" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb80-390"><a href="#cb80-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-391"><a href="#cb80-391" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb80-392"><a href="#cb80-392" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(data.table)</span>
<span id="cb80-393"><a href="#cb80-393" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(grf)</span>
<span id="cb80-394"><a href="#cb80-394" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rpart)</span>
<span id="cb80-395"><a href="#cb80-395" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(rattle)</span>
<span id="cb80-396"><a href="#cb80-396" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(wooldridge)</span>
<span id="cb80-397"><a href="#cb80-397" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-398"><a href="#cb80-398" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb80-399"><a href="#cb80-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-402"><a href="#cb80-402" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-403"><a href="#cb80-403" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb80-404"><a href="#cb80-404" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 2</span></span>
<span id="cb80-405"><a href="#cb80-405" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 4</span></span>
<span id="cb80-406"><a href="#cb80-406" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Illustration of an honest tree</span></span>
<span id="cb80-407"><a href="#cb80-407" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-honest</span></span>
<span id="cb80-408"><a href="#cb80-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-409"><a href="#cb80-409" aria-hidden="true" tabindex="-1"></a>DiagrammeR<span class="sc">::</span><span class="fu">grViz</span>(</span>
<span id="cb80-410"><a href="#cb80-410" aria-hidden="true" tabindex="-1"></a><span class="st">"</span></span>
<span id="cb80-411"><a href="#cb80-411" aria-hidden="true" tabindex="-1"></a><span class="st">digraph {</span></span>
<span id="cb80-412"><a href="#cb80-412" aria-hidden="true" tabindex="-1"></a><span class="st">  graph [ranksep = 0.2, fontsize = 4]</span></span>
<span id="cb80-413"><a href="#cb80-413" aria-hidden="true" tabindex="-1"></a><span class="st">  node [shape = box]</span></span>
<span id="cb80-414"><a href="#cb80-414" aria-hidden="true" tabindex="-1"></a><span class="st">    SS [label = 'Subsamples for splitting']</span></span>
<span id="cb80-415"><a href="#cb80-415" aria-hidden="true" tabindex="-1"></a><span class="st">    SP [label = 'Subsamples for predicting']</span></span>
<span id="cb80-416"><a href="#cb80-416" aria-hidden="true" tabindex="-1"></a><span class="st">    BD [label = 'Bootstrapped Data']</span></span>
<span id="cb80-417"><a href="#cb80-417" aria-hidden="true" tabindex="-1"></a><span class="st">    TT [label = 'Trained tree']</span></span>
<span id="cb80-418"><a href="#cb80-418" aria-hidden="true" tabindex="-1"></a><span class="st">    PV [label = 'Predicted value']</span></span>
<span id="cb80-419"><a href="#cb80-419" aria-hidden="true" tabindex="-1"></a><span class="st">  edge [minlen = 2]</span></span>
<span id="cb80-420"><a href="#cb80-420" aria-hidden="true" tabindex="-1"></a><span class="st">    BD-&gt;SP</span></span>
<span id="cb80-421"><a href="#cb80-421" aria-hidden="true" tabindex="-1"></a><span class="st">    BD-&gt;SS</span></span>
<span id="cb80-422"><a href="#cb80-422" aria-hidden="true" tabindex="-1"></a><span class="st">    SS-&gt;TT</span></span>
<span id="cb80-423"><a href="#cb80-423" aria-hidden="true" tabindex="-1"></a><span class="st">    SP-&gt;PV</span></span>
<span id="cb80-424"><a href="#cb80-424" aria-hidden="true" tabindex="-1"></a><span class="st">    TT-&gt;SP [label='apply the splitting rules']</span></span>
<span id="cb80-425"><a href="#cb80-425" aria-hidden="true" tabindex="-1"></a><span class="st">  { rank = same; SS; SP}</span></span>
<span id="cb80-426"><a href="#cb80-426" aria-hidden="true" tabindex="-1"></a><span class="st">  { rank = same; TT}</span></span>
<span id="cb80-427"><a href="#cb80-427" aria-hidden="true" tabindex="-1"></a><span class="st">  { rank = same; PV}</span></span>
<span id="cb80-428"><a href="#cb80-428" aria-hidden="true" tabindex="-1"></a><span class="st">}</span></span>
<span id="cb80-429"><a href="#cb80-429" aria-hidden="true" tabindex="-1"></a><span class="st">"</span></span>
<span id="cb80-430"><a href="#cb80-430" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb80-431"><a href="#cb80-431" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-432"><a href="#cb80-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-433"><a href="#cb80-433" aria-hidden="true" tabindex="-1"></a>Let's demonstrate this using a very simple regression tree with two terminal nodes using the <span class="in">`mlb`</span> data from the <span class="in">`wooldridge`</span> package.</span>
<span id="cb80-434"><a href="#cb80-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-437"><a href="#cb80-437" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-438"><a href="#cb80-438" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(mlb1)</span>
<span id="cb80-439"><a href="#cb80-439" aria-hidden="true" tabindex="-1"></a>mlb1_dt <span class="ot">&lt;-</span> <span class="fu">data.table</span>(mlb1)</span>
<span id="cb80-440"><a href="#cb80-440" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-441"><a href="#cb80-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-442"><a href="#cb80-442" aria-hidden="true" tabindex="-1"></a>We would like to train a RF using this data where the dependent variable is logged salary (<span class="in">`lsalary`</span>). We will illustrate the honesty rule by working on building a single tree within the process of building a forest.</span>
<span id="cb80-443"><a href="#cb80-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-444"><a href="#cb80-444" aria-hidden="true" tabindex="-1"></a>We first bootstrap data.</span>
<span id="cb80-445"><a href="#cb80-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-448"><a href="#cb80-448" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-449"><a href="#cb80-449" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">358349</span>)</span>
<span id="cb80-450"><a href="#cb80-450" aria-hidden="true" tabindex="-1"></a>num_obs <span class="ot">&lt;-</span> <span class="fu">nrow</span>(mlb1_dt)</span>
<span id="cb80-451"><a href="#cb80-451" aria-hidden="true" tabindex="-1"></a>row_indices <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">seq_len</span>(num_obs), num_obs, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb80-452"><a href="#cb80-452" aria-hidden="true" tabindex="-1"></a>boot_mlb1_dt <span class="ot">&lt;-</span> mlb1_dt[row_indices, ]</span>
<span id="cb80-453"><a href="#cb80-453" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-454"><a href="#cb80-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-455"><a href="#cb80-455" aria-hidden="true" tabindex="-1"></a>We now split the bootstrapped data into two groups: for splitting and prediction. </span>
<span id="cb80-456"><a href="#cb80-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-459"><a href="#cb80-459" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-460"><a href="#cb80-460" aria-hidden="true" tabindex="-1"></a>rows_split <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">seq_len</span>(num_obs), num_obs <span class="sc">/</span> <span class="dv">2</span>, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb80-461"><a href="#cb80-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-462"><a href="#cb80-462" aria-hidden="true" tabindex="-1"></a><span class="co">#=== data for splitting ===#</span></span>
<span id="cb80-463"><a href="#cb80-463" aria-hidden="true" tabindex="-1"></a>split_data <span class="ot">&lt;-</span> boot_mlb1_dt[rows_split, ]</span>
<span id="cb80-464"><a href="#cb80-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-465"><a href="#cb80-465" aria-hidden="true" tabindex="-1"></a><span class="co">#=== data for prediction ===#</span></span>
<span id="cb80-466"><a href="#cb80-466" aria-hidden="true" tabindex="-1"></a>eval_data <span class="ot">&lt;-</span> boot_mlb1_dt[<span class="sc">-</span>rows_split, ]</span>
<span id="cb80-467"><a href="#cb80-467" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-468"><a href="#cb80-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-469"><a href="#cb80-469" aria-hidden="true" tabindex="-1"></a>We then train a tree using the data for splitting (<span class="in">`split_data`</span>):</span>
<span id="cb80-470"><a href="#cb80-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-473"><a href="#cb80-473" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-474"><a href="#cb80-474" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: A simple regression tree using the subsamples for splitting</span></span>
<span id="cb80-475"><a href="#cb80-475" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-tree-sub</span></span>
<span id="cb80-476"><a href="#cb80-476" aria-hidden="true" tabindex="-1"></a><span class="co">#=== build a simple tree ===#</span></span>
<span id="cb80-477"><a href="#cb80-477" aria-hidden="true" tabindex="-1"></a>tree_trained <span class="ot">&lt;-</span></span>
<span id="cb80-478"><a href="#cb80-478" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rpart</span>(</span>
<span id="cb80-479"><a href="#cb80-479" aria-hidden="true" tabindex="-1"></a>    lsalary <span class="sc">~</span> hits <span class="sc">+</span> runsyr, </span>
<span id="cb80-480"><a href="#cb80-480" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> split_data, </span>
<span id="cb80-481"><a href="#cb80-481" aria-hidden="true" tabindex="-1"></a>    <span class="at">control =</span> <span class="fu">rpart.control</span>(<span class="at">minsplit =</span> <span class="dv">120</span>)</span>
<span id="cb80-482"><a href="#cb80-482" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb80-483"><a href="#cb80-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-484"><a href="#cb80-484" aria-hidden="true" tabindex="-1"></a><span class="fu">fancyRpartPlot</span>(tree_trained, <span class="at">digits =</span> <span class="dv">4</span>)</span>
<span id="cb80-485"><a href="#cb80-485" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-486"><a href="#cb80-486" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-487"><a href="#cb80-487" aria-hidden="true" tabindex="-1"></a>So the splitting rule is <span class="in">`hits &lt; 356`</span> as shown in @fig-tree-sub. At the terminal nodes, you see the prediction of <span class="in">`lsalary`</span>: $12.47$ for the left and $14.23$ for the right. These predictions are NOT honest. They are obtained from the observed values of <span class="in">`lsalary`</span> within the node using the splitting data (the data the tree is trained for). Instead of using these prediction values, an honest prediction applied the splitting rules (<span class="in">`hits &lt; 356`</span>) to the data reserved for prediction.</span>
<span id="cb80-488"><a href="#cb80-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-491"><a href="#cb80-491" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-492"><a href="#cb80-492" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb80-493"><a href="#cb80-493" aria-hidden="true" tabindex="-1"></a>honest_pred <span class="ot">&lt;-</span> eval_data[, <span class="fu">mean</span>(lsalary), <span class="at">by =</span> hits <span class="sc">&lt;</span> <span class="dv">356</span>]</span>
<span id="cb80-494"><a href="#cb80-494" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb80-495"><a href="#cb80-495" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-496"><a href="#cb80-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-497"><a href="#cb80-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-498"><a href="#cb80-498" aria-hidden="true" tabindex="-1"></a>So, instead of $12.47$ and $14.23$, the predicted values from the honest tree are $<span class="in">`r round(honest_pred[hits == TRUE, V1], digits = 2)`</span>$ and $<span class="in">`r round(honest_pred[hits == FALSE, V1], digits = 2)`</span>$ for the left and right nodes, respectively. Trees are built in this manner many times to form a forest.</span>
<span id="cb80-499"><a href="#cb80-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-500"><a href="#cb80-500" aria-hidden="true" tabindex="-1"></a>More generally, in GRF, honesty is applied by using the evaluation data to solve @eq-opt based on the weight $\alpha_i(x)$ derived from the trained forest using the splitting data. Honesty is required for the GRF estimator to be consistent and asymptotically normal <span class="co">[</span><span class="ot">@athey2019generalized</span><span class="co">]</span>. However, the application of honesty can do more damage than help when the sample size is small.</span>
<span id="cb80-501"><a href="#cb80-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-502"><a href="#cb80-502" aria-hidden="true" tabindex="-1"></a><span class="fu">## Understanding GRF better by example (#sec-understand-grf-example)</span></span>
<span id="cb80-503"><a href="#cb80-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-504"><a href="#cb80-504" aria-hidden="true" tabindex="-1"></a>This section goes through CF model training and CATE prediction step by step with codes to enhance our understanding of how GRF works. In the process, we also learn the role of many of the hyper-parameters. They include those that are common across all the GRF methods and those that are specific to CF. </span>
<span id="cb80-505"><a href="#cb80-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-506"><a href="#cb80-506" aria-hidden="true" tabindex="-1"></a>While the process explained here is for CF, the vast majority of the process is exactly the same for other models under GRF. Here are some differences:</span>
<span id="cb80-507"><a href="#cb80-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-508"><a href="#cb80-508" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>Orthogonalization of the data at the beginning for CF </span>
<span id="cb80-509"><a href="#cb80-509" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>The definition of the pseudo outcome (but, the way they are used in the algorithm is identical once they are defined) </span>
<span id="cb80-510"><a href="#cb80-510" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>The role of <span class="in">`min.node.size`</span>, <span class="in">`alpha`</span>, and <span class="in">`imbalance.penalty`</span> used for safeguarding against extreme splits</span>
<span id="cb80-511"><a href="#cb80-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-512"><a href="#cb80-512" aria-hidden="true" tabindex="-1"></a>::: {.column-margin}</span>
<span id="cb80-513"><a href="#cb80-513" aria-hidden="true" tabindex="-1"></a>Obviously, the codes here are only for illustration and should not be used in practice.</span>
<span id="cb80-514"><a href="#cb80-514" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb80-515"><a href="#cb80-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-516"><a href="#cb80-516" aria-hidden="true" tabindex="-1"></a>We will use a synthetic data generated from the following generating process:</span>
<span id="cb80-517"><a href="#cb80-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-518"><a href="#cb80-518" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-519"><a href="#cb80-519" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb80-520"><a href="#cb80-520" aria-hidden="true" tabindex="-1"></a>y = \theta(X)\cdot T + g(X) + \mu</span>
<span id="cb80-521"><a href="#cb80-521" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb80-522"><a href="#cb80-522" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-523"><a href="#cb80-523" aria-hidden="true" tabindex="-1"></a>, where</span>
<span id="cb80-524"><a href="#cb80-524" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-525"><a href="#cb80-525" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-526"><a href="#cb80-526" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb80-527"><a href="#cb80-527" aria-hidden="true" tabindex="-1"></a>\theta(X) &amp; = 2 + 3 \times x_1^2 - \sqrt{3 \times  x_2^3 + 1}<span class="sc">\\</span></span>
<span id="cb80-528"><a href="#cb80-528" aria-hidden="true" tabindex="-1"></a>g(X) &amp; = 10 \cdot <span class="co">[</span><span class="ot">log(x_2+1) + 2\cdot x_3\times x_2</span><span class="co">]</span></span>
<span id="cb80-529"><a href="#cb80-529" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb80-530"><a href="#cb80-530" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-531"><a href="#cb80-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-532"><a href="#cb80-532" aria-hidden="true" tabindex="-1"></a>We have 10 feature variables $x_1, \dots, x_{10}$ and only $x_1$ through $x_3$ plays a role.  </span>
<span id="cb80-533"><a href="#cb80-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-534"><a href="#cb80-534" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>$x_1, x_2 \sim U<span class="co">[</span><span class="ot">0, 3</span><span class="co">]</span>^2$</span>
<span id="cb80-535"><a href="#cb80-535" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>$x_3 \sim N(1, 1)$</span>
<span id="cb80-536"><a href="#cb80-536" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>$x_4, \dots, x_{10} \sim U<span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span>^{10}$</span>
<span id="cb80-537"><a href="#cb80-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-538"><a href="#cb80-538" aria-hidden="true" tabindex="-1"></a>The error term and the treatment variable is defined as follows. </span>
<span id="cb80-539"><a href="#cb80-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-540"><a href="#cb80-540" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>$\mu \sim N(0, 1)$</span>
<span id="cb80-541"><a href="#cb80-541" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>$T \sim Ber(0.5)$</span>
<span id="cb80-542"><a href="#cb80-542" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-545"><a href="#cb80-545" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-546"><a href="#cb80-546" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">73843</span>)</span>
<span id="cb80-547"><a href="#cb80-547" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">1000</span> <span class="co"># number of observations</span></span>
<span id="cb80-548"><a href="#cb80-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-549"><a href="#cb80-549" aria-hidden="true" tabindex="-1"></a>x4_x10 <span class="ot">&lt;-</span> </span>
<span id="cb80-550"><a href="#cb80-550" aria-hidden="true" tabindex="-1"></a>  <span class="fu">matrix</span>(<span class="fu">runif</span>(N <span class="sc">*</span> <span class="dv">7</span>), <span class="at">nrow =</span> N) <span class="sc">%&gt;%</span> </span>
<span id="cb80-551"><a href="#cb80-551" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data.table</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb80-552"><a href="#cb80-552" aria-hidden="true" tabindex="-1"></a>  <span class="fu">setnames</span>(<span class="fu">names</span>(.), <span class="fu">paste0</span>(<span class="st">"x"</span>, <span class="dv">4</span><span class="sc">:</span><span class="dv">10</span>))</span>
<span id="cb80-553"><a href="#cb80-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-554"><a href="#cb80-554" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb80-555"><a href="#cb80-555" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> </span>
<span id="cb80-556"><a href="#cb80-556" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data.table</span>(</span>
<span id="cb80-557"><a href="#cb80-557" aria-hidden="true" tabindex="-1"></a>    <span class="at">x1 =</span> <span class="dv">3</span> <span class="sc">*</span> <span class="fu">runif</span>(N),</span>
<span id="cb80-558"><a href="#cb80-558" aria-hidden="true" tabindex="-1"></a>    <span class="at">x2 =</span> <span class="dv">3</span> <span class="sc">*</span> <span class="fu">runif</span>(N),</span>
<span id="cb80-559"><a href="#cb80-559" aria-hidden="true" tabindex="-1"></a>    <span class="at">x3 =</span> <span class="fu">rnorm</span>(N, <span class="at">mean =</span> <span class="dv">1</span>, <span class="at">sd =</span> <span class="dv">1</span>),</span>
<span id="cb80-560"><a href="#cb80-560" aria-hidden="true" tabindex="-1"></a>    <span class="at">mu =</span> <span class="fu">rnorm</span>(N),</span>
<span id="cb80-561"><a href="#cb80-561" aria-hidden="true" tabindex="-1"></a>    <span class="at">T =</span> <span class="fu">runif</span>(N) <span class="sc">&gt;</span> <span class="fl">0.5</span></span>
<span id="cb80-562"><a href="#cb80-562" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb80-563"><a href="#cb80-563" aria-hidden="true" tabindex="-1"></a>  <span class="co">#=== theta(X) ===#</span></span>
<span id="cb80-564"><a href="#cb80-564" aria-hidden="true" tabindex="-1"></a>  .[, <span class="at">theta_x :=</span> <span class="dv">2</span> <span class="sc">+</span> <span class="dv">3</span> <span class="sc">*</span> x1<span class="sc">^</span><span class="dv">2</span> <span class="sc">-</span> <span class="fu">sqrt</span>(<span class="dv">3</span><span class="sc">*</span> x2<span class="sc">^</span><span class="dv">3</span> <span class="sc">+</span> <span class="dv">2</span>)] <span class="sc">%&gt;%</span></span>
<span id="cb80-565"><a href="#cb80-565" aria-hidden="true" tabindex="-1"></a>  <span class="co">#=== g(X) ===#</span></span>
<span id="cb80-566"><a href="#cb80-566" aria-hidden="true" tabindex="-1"></a>  .[, <span class="at">g_x :=</span> <span class="dv">10</span> <span class="sc">*</span> (<span class="fu">log</span>(x2 <span class="sc">+</span> <span class="dv">1</span>) <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> x3 <span class="sc">*</span> x2)] <span class="sc">%&gt;%</span> </span>
<span id="cb80-567"><a href="#cb80-567" aria-hidden="true" tabindex="-1"></a>  .[, <span class="at">y :=</span> theta_x <span class="sc">*</span> T <span class="sc">+</span> g_x <span class="sc">+</span> mu] <span class="sc">%&gt;%</span> </span>
<span id="cb80-568"><a href="#cb80-568" aria-hidden="true" tabindex="-1"></a>  .[, .(y, T, x1, x2, x3)] <span class="sc">%&gt;%</span> </span>
<span id="cb80-569"><a href="#cb80-569" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cbind</span>(., x4_x10)</span>
<span id="cb80-570"><a href="#cb80-570" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb80-571"><a href="#cb80-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-572"><a href="#cb80-572" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-573"><a href="#cb80-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-574"><a href="#cb80-574" aria-hidden="true" tabindex="-1"></a><span class="fu">### Orthogonalization of $Y$ and $T$</span></span>
<span id="cb80-575"><a href="#cb80-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-576"><a href="#cb80-576" aria-hidden="true" tabindex="-1"></a>Causal forest (and instrumental forest) first orthogonalizes $Y$ and $T$. This is not the case for the other GRF methods. By default, <span class="in">`causal_forest()`</span> uses random forest to estimate $E<span class="co">[</span><span class="ot">Y|X</span><span class="co">]</span>$ and $E<span class="co">[</span><span class="ot">T|X</span><span class="co">]</span>$, and then use out-of-bag predictions. Since the treatment assignment is randomized, we could use $0.5$ for as the estimate for $E<span class="co">[</span><span class="ot">T|X</span><span class="co">]</span>$ for all the observations. But, we will estimate both in this example.</span>
<span id="cb80-577"><a href="#cb80-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-578"><a href="#cb80-578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-581"><a href="#cb80-581" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-582"><a href="#cb80-582" aria-hidden="true" tabindex="-1"></a><span class="co">#--------------------------</span></span>
<span id="cb80-583"><a href="#cb80-583" aria-hidden="true" tabindex="-1"></a><span class="co"># E[Y|X]</span></span>
<span id="cb80-584"><a href="#cb80-584" aria-hidden="true" tabindex="-1"></a><span class="co">#--------------------------</span></span>
<span id="cb80-585"><a href="#cb80-585" aria-hidden="true" tabindex="-1"></a><span class="co">#=== train an RF on y ===#</span></span>
<span id="cb80-586"><a href="#cb80-586" aria-hidden="true" tabindex="-1"></a>y_rf_trained <span class="ot">&lt;-</span></span>
<span id="cb80-587"><a href="#cb80-587" aria-hidden="true" tabindex="-1"></a>  <span class="fu">regression_forest</span>(</span>
<span id="cb80-588"><a href="#cb80-588" aria-hidden="true" tabindex="-1"></a>    <span class="at">X =</span> data[, .(x1, x2, x3)],</span>
<span id="cb80-589"><a href="#cb80-589" aria-hidden="true" tabindex="-1"></a>    <span class="at">Y =</span> data[, y]</span>
<span id="cb80-590"><a href="#cb80-590" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb80-591"><a href="#cb80-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-592"><a href="#cb80-592" aria-hidden="true" tabindex="-1"></a><span class="co">#=== OOB estimates of E[Y|X] ===#</span></span>
<span id="cb80-593"><a href="#cb80-593" aria-hidden="true" tabindex="-1"></a>ey_x_hat <span class="ot">&lt;-</span> y_rf_trained<span class="sc">$</span>predictions</span>
<span id="cb80-594"><a href="#cb80-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-595"><a href="#cb80-595" aria-hidden="true" tabindex="-1"></a><span class="co">#--------------------------</span></span>
<span id="cb80-596"><a href="#cb80-596" aria-hidden="true" tabindex="-1"></a><span class="co"># E[T|X]</span></span>
<span id="cb80-597"><a href="#cb80-597" aria-hidden="true" tabindex="-1"></a><span class="co">#--------------------------</span></span>
<span id="cb80-598"><a href="#cb80-598" aria-hidden="true" tabindex="-1"></a><span class="co">#=== train an RF on T ===#</span></span>
<span id="cb80-599"><a href="#cb80-599" aria-hidden="true" tabindex="-1"></a>t_rf_trained <span class="ot">&lt;-</span></span>
<span id="cb80-600"><a href="#cb80-600" aria-hidden="true" tabindex="-1"></a>  <span class="fu">probability_forest</span>(</span>
<span id="cb80-601"><a href="#cb80-601" aria-hidden="true" tabindex="-1"></a>    <span class="at">X =</span> data[, .(x1, x2, x3)],</span>
<span id="cb80-602"><a href="#cb80-602" aria-hidden="true" tabindex="-1"></a>    <span class="at">Y =</span> data[, <span class="fu">factor</span>(T)]</span>
<span id="cb80-603"><a href="#cb80-603" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb80-604"><a href="#cb80-604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-605"><a href="#cb80-605" aria-hidden="true" tabindex="-1"></a><span class="co">#=== OOB estimates of E[T|X] ===#</span></span>
<span id="cb80-606"><a href="#cb80-606" aria-hidden="true" tabindex="-1"></a>et_x_hat <span class="ot">&lt;-</span> t_rf_trained<span class="sc">$</span>predictions[, <span class="dv">2</span>] <span class="co"># get the probability of T = 1</span></span>
<span id="cb80-607"><a href="#cb80-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-608"><a href="#cb80-608" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-609"><a href="#cb80-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-610"><a href="#cb80-610" aria-hidden="true" tabindex="-1"></a>Let's now orthogonalize $Y$ and $T$,</span>
<span id="cb80-611"><a href="#cb80-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-614"><a href="#cb80-614" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-615"><a href="#cb80-615" aria-hidden="true" tabindex="-1"></a>data[, <span class="st">`</span><span class="at">:=</span><span class="st">`</span>(<span class="at">y_tilde =</span> y <span class="sc">-</span> ey_x_hat, <span class="at">t_tilde =</span> T <span class="sc">-</span> et_x_hat)]</span>
<span id="cb80-616"><a href="#cb80-616" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-617"><a href="#cb80-617" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-618"><a href="#cb80-618" aria-hidden="true" tabindex="-1"></a>Here is what the data looks like:</span>
<span id="cb80-619"><a href="#cb80-619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-622"><a href="#cb80-622" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-623"><a href="#cb80-623" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(data)</span>
<span id="cb80-624"><a href="#cb80-624" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-625"><a href="#cb80-625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-626"><a href="#cb80-626" aria-hidden="true" tabindex="-1"></a><span class="fu">### Building trees #{#sec-build-trees}</span></span>
<span id="cb80-627"><a href="#cb80-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-628"><a href="#cb80-628" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb80-629"><a href="#cb80-629" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-630"><a href="#cb80-630" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:blue"</span><span class="kw">&gt;</span> Preparing Data<span class="kw">&lt;/span&gt;</span>:</span>
<span id="cb80-631"><a href="#cb80-631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-632"><a href="#cb80-632" aria-hidden="true" tabindex="-1"></a>When building a tree, GRF use sampling without replacement instead of sampling with replacement as a default for RF implemented by <span class="in">`ranger()`</span>. <span class="in">`sample.fraction`</span> parameter determines the fraction of the entire sample drawn for each tree. Let's use the default value, which is $0.5$. </span>
<span id="cb80-633"><a href="#cb80-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-636"><a href="#cb80-636" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-637"><a href="#cb80-637" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb80-638"><a href="#cb80-638" aria-hidden="true" tabindex="-1"></a>data_for_the_first_tree <span class="ot">&lt;-</span> data[<span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>N, <span class="fl">0.5</span> <span class="sc">*</span> N, <span class="at">replace =</span> <span class="cn">FALSE</span>), ] </span>
<span id="cb80-639"><a href="#cb80-639" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb80-640"><a href="#cb80-640" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-641"><a href="#cb80-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-642"><a href="#cb80-642" aria-hidden="true" tabindex="-1"></a>By default, honest splitting is implemented (<span class="in">`honesty = TRUE`</span>). The <span class="in">`honesty.fraction`</span> parameter determines the fraction of the randomly sampled data (<span class="in">`data_for_the_first_tree`</span>), which will be used for determining splitting decisions. We will use the default value of <span class="in">`honesty.fraction`</span>, which is $0.5$.</span>
<span id="cb80-643"><a href="#cb80-643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-646"><a href="#cb80-646" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-647"><a href="#cb80-647" aria-hidden="true" tabindex="-1"></a><span class="co">#=== number of observations ===#</span></span>
<span id="cb80-648"><a href="#cb80-648" aria-hidden="true" tabindex="-1"></a>N_d <span class="ot">&lt;-</span> <span class="fu">nrow</span>(data_for_the_first_tree)</span>
<span id="cb80-649"><a href="#cb80-649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-650"><a href="#cb80-650" aria-hidden="true" tabindex="-1"></a><span class="co">#=== indices ===#</span></span>
<span id="cb80-651"><a href="#cb80-651" aria-hidden="true" tabindex="-1"></a>J1_index <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>N_d, <span class="fl">0.5</span> <span class="sc">*</span> N_d, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb80-652"><a href="#cb80-652" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-653"><a href="#cb80-653" aria-hidden="true" tabindex="-1"></a><span class="co">#=== data for determining splitting rules ===#</span></span>
<span id="cb80-654"><a href="#cb80-654" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb80-655"><a href="#cb80-655" aria-hidden="true" tabindex="-1"></a>data_J1 <span class="ot">&lt;-</span> data_for_the_first_tree[J1_index, ]</span>
<span id="cb80-656"><a href="#cb80-656" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb80-657"><a href="#cb80-657" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-658"><a href="#cb80-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-659"><a href="#cb80-659" aria-hidden="true" tabindex="-1"></a>As you can see, we are just using a quarter of the original dataset (<span class="in">`sample.fraction`</span> $\times$ <span class="in">`honesty.fraction`</span> = $0.5 \times 0.5$). Here is the data for repopulating the tree once the tree is built.</span>
<span id="cb80-660"><a href="#cb80-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-663"><a href="#cb80-663" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-664"><a href="#cb80-664" aria-hidden="true" tabindex="-1"></a><span class="co">#=== data for repopulate the tree ===#</span></span>
<span id="cb80-665"><a href="#cb80-665" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb80-666"><a href="#cb80-666" aria-hidden="true" tabindex="-1"></a>data_J2 <span class="ot">&lt;-</span> data_for_the_first_tree[<span class="sc">-</span>J1_index, ]</span>
<span id="cb80-667"><a href="#cb80-667" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb80-668"><a href="#cb80-668" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-669"><a href="#cb80-669" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-670"><a href="#cb80-670" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb80-671"><a href="#cb80-671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-672"><a href="#cb80-672" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:blue"</span><span class="kw">&gt;</span> Determining splitting rules<span class="kw">&lt;/span&gt;</span>:</span>
<span id="cb80-673"><a href="#cb80-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-674"><a href="#cb80-674" aria-hidden="true" tabindex="-1"></a>Now let's find the splitting rules using <span class="in">`data_J1`</span> (Remember, <span class="in">`data_J2`</span> is not used).</span>
<span id="cb80-675"><a href="#cb80-675" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-676"><a href="#cb80-676" aria-hidden="true" tabindex="-1"></a>We are at the root node to which all the observations belong. Let's first calculate the pseudo outcome. Finding pseudo outcomes starts from solving the unweighted version of @eq-opt using the samples in the parent node (root node here) in general. For CF, it is</span>
<span id="cb80-677"><a href="#cb80-677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-678"><a href="#cb80-678" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-679"><a href="#cb80-679" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb80-680"><a href="#cb80-680" aria-hidden="true" tabindex="-1"></a>\sum_{i=1}^n (\tilde{Y}_i - \theta\tilde{T}_i)\cdot \tilde{T}_i = 0</span>
<span id="cb80-681"><a href="#cb80-681" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb80-682"><a href="#cb80-682" aria-hidden="true" tabindex="-1"></a>$$ {#eq-theta-p}</span>
<span id="cb80-683"><a href="#cb80-683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-684"><a href="#cb80-684" aria-hidden="true" tabindex="-1"></a>because the score function for CF is $(\tilde{Y}_i - \theta\tilde{T}_i)\cdot \tilde{T}_i$. So, </span>
<span id="cb80-685"><a href="#cb80-685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-686"><a href="#cb80-686" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-687"><a href="#cb80-687" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb80-688"><a href="#cb80-688" aria-hidden="true" tabindex="-1"></a>\theta_P = \frac{\sum_{i=1}^n \tilde{Y}_i\cdot \tilde{T}_i}{\sum_{i=1}^n\tilde{T}_i\cdot \tilde{T}_i}</span>
<span id="cb80-689"><a href="#cb80-689" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb80-690"><a href="#cb80-690" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-691"><a href="#cb80-691" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-694"><a href="#cb80-694" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-695"><a href="#cb80-695" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb80-696"><a href="#cb80-696" aria-hidden="true" tabindex="-1"></a>theta_p <span class="ot">&lt;-</span> data_J1[, <span class="fu">sum</span>(y_tilde <span class="sc">*</span> t_tilde)<span class="sc">/</span><span class="fu">sum</span>(t_tilde <span class="sc">*</span> t_tilde)]</span>
<span id="cb80-697"><a href="#cb80-697" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb80-698"><a href="#cb80-698" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-699"><a href="#cb80-699" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-700"><a href="#cb80-700" aria-hidden="true" tabindex="-1"></a>The pseudo outcome for CF is $(\tilde{Y}_i - \theta_P\tilde{T}_i)\cdot \tilde{T}_i$. Let's define that in <span class="in">`data_J1`</span>.</span>
<span id="cb80-701"><a href="#cb80-701" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-704"><a href="#cb80-704" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-705"><a href="#cb80-705" aria-hidden="true" tabindex="-1"></a>data_J1[, rho <span class="sc">:</span><span class="er">=</span> (y_tilde <span class="sc">-</span> theta_p <span class="sc">*</span> t_tilde) <span class="sc">*</span> t_tilde]</span>
<span id="cb80-706"><a href="#cb80-706" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-707"><a href="#cb80-707" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-708"><a href="#cb80-708" aria-hidden="true" tabindex="-1"></a>Now, the number of variables to use for splitting is determined by the <span class="in">`mtry`</span> parameter. Its default is <span class="in">`min(ceiling(sqrt(ncol(X))+20), ncol(X))`</span>, where <span class="in">`X`</span> is the feature matrix. Here <span class="in">`ncol(X)`</span> is 10. So, <span class="in">`mtry`</span> is set to <span class="in">`r min(ceiling(sqrt(10)+20), 10)`</span>. Following this, we will use all three variables for splitting here. Let $C_j$ ($j = 1, 2$) and $N_j$ denote the child node $j$ and the number of observations in child node $j$, respectively. We will write a function that works on a single feature variable to find all the threshold values that would result in unique splits, and then return the following information:</span>
<span id="cb80-709"><a href="#cb80-709" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-710"><a href="#cb80-710" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>$(\sum_{i\in C_j} \rho_i)^2$</span>
<span id="cb80-711"><a href="#cb80-711" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>$N_j$</span>
<span id="cb80-712"><a href="#cb80-712" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>$\sum_{i=1}^{N_j} (T_i - \bar{T})^2$ (we will call this <span class="in">`info_size`</span>)</span>
<span id="cb80-713"><a href="#cb80-713" aria-hidden="true" tabindex="-1"></a><span class="ss">+ </span>the number of treated and control units</span>
<span id="cb80-714"><a href="#cb80-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-715"><a href="#cb80-715" aria-hidden="true" tabindex="-1"></a>The output allows us to calculate the heterogeneity score of the split defined as</span>
<span id="cb80-716"><a href="#cb80-716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-717"><a href="#cb80-717" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-718"><a href="#cb80-718" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb80-719"><a href="#cb80-719" aria-hidden="true" tabindex="-1"></a>\sum_{j=1}^2 \frac{(\sum_{i\in C_j} \rho_i)^2}{N_j}.</span>
<span id="cb80-720"><a href="#cb80-720" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb80-721"><a href="#cb80-721" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-722"><a href="#cb80-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-723"><a href="#cb80-723" aria-hidden="true" tabindex="-1"></a>It also allows us to eliminate some of the splits based on <span class="in">`mtry`</span>, <span class="in">`alpha`</span>, and <span class="in">`imbalance.penalty`</span> parameters. </span>
<span id="cb80-724"><a href="#cb80-724" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-727"><a href="#cb80-727" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-728"><a href="#cb80-728" aria-hidden="true" tabindex="-1"></a>get_ss_by_var <span class="ot">&lt;-</span> <span class="cf">function</span>(feature_var, outcome_var, parent_data)</span>
<span id="cb80-729"><a href="#cb80-729" aria-hidden="true" tabindex="-1"></a>{</span>
<span id="cb80-730"><a href="#cb80-730" aria-hidden="true" tabindex="-1"></a>  temp_data <span class="ot">&lt;-</span> </span>
<span id="cb80-731"><a href="#cb80-731" aria-hidden="true" tabindex="-1"></a>    <span class="fu">copy</span>(parent_data) <span class="sc">%&gt;%</span> </span>
<span id="cb80-732"><a href="#cb80-732" aria-hidden="true" tabindex="-1"></a>    <span class="fu">setnames</span>(feature_var, <span class="st">"temp_var"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb80-733"><a href="#cb80-733" aria-hidden="true" tabindex="-1"></a>    <span class="fu">setnames</span>(outcome_var, <span class="st">"outcome"</span>)  </span>
<span id="cb80-734"><a href="#cb80-734" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-735"><a href="#cb80-735" aria-hidden="true" tabindex="-1"></a>  <span class="co">#=== define a sequence of values of hruns ===#</span></span>
<span id="cb80-736"><a href="#cb80-736" aria-hidden="true" tabindex="-1"></a>  thr_seq <span class="ot">&lt;-</span></span>
<span id="cb80-737"><a href="#cb80-737" aria-hidden="true" tabindex="-1"></a>    temp_data[<span class="fu">order</span>(temp_var), <span class="fu">unique</span>(temp_var)] <span class="sc">%&gt;%</span></span>
<span id="cb80-738"><a href="#cb80-738" aria-hidden="true" tabindex="-1"></a>    <span class="co">#=== get the rolling mean ===#</span></span>
<span id="cb80-739"><a href="#cb80-739" aria-hidden="true" tabindex="-1"></a>    <span class="fu">frollmean</span>(<span class="dv">2</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb80-740"><a href="#cb80-740" aria-hidden="true" tabindex="-1"></a>    .[<span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb80-741"><a href="#cb80-741" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-742"><a href="#cb80-742" aria-hidden="true" tabindex="-1"></a>  <span class="co">#=== get RSS ===#</span></span>
<span id="cb80-743"><a href="#cb80-743" aria-hidden="true" tabindex="-1"></a>  ss_value <span class="ot">&lt;-</span></span>
<span id="cb80-744"><a href="#cb80-744" aria-hidden="true" tabindex="-1"></a>    <span class="fu">lapply</span>(</span>
<span id="cb80-745"><a href="#cb80-745" aria-hidden="true" tabindex="-1"></a>      thr_seq,</span>
<span id="cb80-746"><a href="#cb80-746" aria-hidden="true" tabindex="-1"></a>      <span class="cf">function</span>(x){</span>
<span id="cb80-747"><a href="#cb80-747" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-748"><a href="#cb80-748" aria-hidden="true" tabindex="-1"></a>        return_data <span class="ot">&lt;-</span> </span>
<span id="cb80-749"><a href="#cb80-749" aria-hidden="true" tabindex="-1"></a>          temp_data[, .(</span>
<span id="cb80-750"><a href="#cb80-750" aria-hidden="true" tabindex="-1"></a>            <span class="at">het_score =</span> <span class="fu">sum</span>(outcome)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>.N, </span>
<span id="cb80-751"><a href="#cb80-751" aria-hidden="true" tabindex="-1"></a>            <span class="at">nobs =</span> .N,</span>
<span id="cb80-752"><a href="#cb80-752" aria-hidden="true" tabindex="-1"></a>            <span class="at">info_size =</span> <span class="fu">sum</span>((T <span class="sc">-</span> <span class="fu">mean</span>(T))<span class="sc">^</span><span class="dv">2</span>),</span>
<span id="cb80-753"><a href="#cb80-753" aria-hidden="true" tabindex="-1"></a>            <span class="at">num_treated =</span> <span class="fu">sum</span>(T),</span>
<span id="cb80-754"><a href="#cb80-754" aria-hidden="true" tabindex="-1"></a>            <span class="at">num_ctrl =</span> <span class="fu">sum</span>(<span class="sc">!</span>T)</span>
<span id="cb80-755"><a href="#cb80-755" aria-hidden="true" tabindex="-1"></a>          ), by <span class="ot">=</span> temp_var <span class="sc">&lt;</span> x] <span class="sc">%&gt;%</span> </span>
<span id="cb80-756"><a href="#cb80-756" aria-hidden="true" tabindex="-1"></a>          <span class="fu">setnames</span>(<span class="st">"temp_var"</span>, <span class="st">"child"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb80-757"><a href="#cb80-757" aria-hidden="true" tabindex="-1"></a>          .[, child <span class="sc">:</span><span class="er">=</span> <span class="fu">ifelse</span>(child <span class="sc">==</span> <span class="cn">TRUE</span>, <span class="st">"Child 1"</span>, <span class="st">"Child 2"</span>)] <span class="sc">%&gt;%</span> </span>
<span id="cb80-758"><a href="#cb80-758" aria-hidden="true" tabindex="-1"></a>          .[, thr <span class="sc">:</span><span class="er">=</span> x]</span>
<span id="cb80-759"><a href="#cb80-759" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb80-760"><a href="#cb80-760" aria-hidden="true" tabindex="-1"></a>        <span class="fu">return</span>(return_data)</span>
<span id="cb80-761"><a href="#cb80-761" aria-hidden="true" tabindex="-1"></a>      } </span>
<span id="cb80-762"><a href="#cb80-762" aria-hidden="true" tabindex="-1"></a>    ) <span class="sc">%&gt;%</span> </span>
<span id="cb80-763"><a href="#cb80-763" aria-hidden="true" tabindex="-1"></a>    <span class="fu">rbindlist</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb80-764"><a href="#cb80-764" aria-hidden="true" tabindex="-1"></a>    .[, var <span class="sc">:</span><span class="er">=</span> feature_var] <span class="sc">%&gt;%</span> </span>
<span id="cb80-765"><a href="#cb80-765" aria-hidden="true" tabindex="-1"></a>    <span class="fu">relocate</span>(var, thr, child)</span>
<span id="cb80-766"><a href="#cb80-766" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-767"><a href="#cb80-767" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(ss_value)</span>
<span id="cb80-768"><a href="#cb80-768" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb80-769"><a href="#cb80-769" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-770"><a href="#cb80-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-771"><a href="#cb80-771" aria-hidden="true" tabindex="-1"></a>For example, here is the output for <span class="in">`x1`</span>.</span>
<span id="cb80-772"><a href="#cb80-772" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-775"><a href="#cb80-775" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-776"><a href="#cb80-776" aria-hidden="true" tabindex="-1"></a><span class="fu">get_ss_by_var</span>(<span class="st">"x1"</span>, <span class="st">"rho"</span>, data_J1)[]</span>
<span id="cb80-777"><a href="#cb80-777" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-778"><a href="#cb80-778" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-779"><a href="#cb80-779" aria-hidden="true" tabindex="-1"></a>Repeating this for all the feature variables,</span>
<span id="cb80-780"><a href="#cb80-780" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-783"><a href="#cb80-783" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-784"><a href="#cb80-784" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb80-785"><a href="#cb80-785" aria-hidden="true" tabindex="-1"></a>thr_score_data <span class="ot">&lt;-</span></span>
<span id="cb80-786"><a href="#cb80-786" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lapply</span>(</span>
<span id="cb80-787"><a href="#cb80-787" aria-hidden="true" tabindex="-1"></a>    <span class="fu">paste0</span>(<span class="st">"x"</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>),</span>
<span id="cb80-788"><a href="#cb80-788" aria-hidden="true" tabindex="-1"></a>    <span class="cf">function</span>(x) <span class="fu">get_ss_by_var</span>(x, <span class="st">"rho"</span>, data_J1)</span>
<span id="cb80-789"><a href="#cb80-789" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb80-790"><a href="#cb80-790" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rbindlist</span>()</span>
<span id="cb80-791"><a href="#cb80-791" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb80-792"><a href="#cb80-792" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-793"><a href="#cb80-793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-794"><a href="#cb80-794" aria-hidden="true" tabindex="-1"></a>Now, we ensure that we have at least as many <span class="in">`min.node.size`</span> numbers of treated and control units in both child nodes. This provides a safeguard against having a very inaccurate treatment effect estimation. The default value of <span class="in">`min.node.size`</span> is 5. </span>
<span id="cb80-795"><a href="#cb80-795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-798"><a href="#cb80-798" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-799"><a href="#cb80-799" aria-hidden="true" tabindex="-1"></a>min.node.size <span class="ot">&lt;-</span> <span class="dv">5</span> <span class="co"># referred to as mns</span></span>
<span id="cb80-800"><a href="#cb80-800" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-801"><a href="#cb80-801" aria-hidden="true" tabindex="-1"></a><span class="co">#=== check if both child nodes have at least mns control and treatment units  ===#</span></span>
<span id="cb80-802"><a href="#cb80-802" aria-hidden="true" tabindex="-1"></a>thr_score_data[, mns_met_grup <span class="sc">:</span><span class="er">=</span> <span class="fu">all</span>(num_ctrl <span class="sc">&gt;=</span> min.node.size <span class="sc">&amp;</span> num_treated <span class="sc">&gt;=</span> min.node.size), by <span class="ot">=</span> .(var, thr)]</span>
<span id="cb80-803"><a href="#cb80-803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-804"><a href="#cb80-804" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb80-805"><a href="#cb80-805" aria-hidden="true" tabindex="-1"></a>msn_met_data <span class="ot">&lt;-</span> thr_score_data[mns_met_grup <span class="sc">==</span> <span class="cn">TRUE</span>, ]</span>
<span id="cb80-806"><a href="#cb80-806" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb80-807"><a href="#cb80-807" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-808"><a href="#cb80-808" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-809"><a href="#cb80-809" aria-hidden="true" tabindex="-1"></a>Now, we consider how <span class="in">`alpha`</span> and <span class="in">`imbalance.penalty`</span> affect the potential pool of feature-threshold combinations. Each child node needs to have at least as large <span class="in">`info_size`</span> as <span class="in">`alpha`</span> $\times$ <span class="in">`info_size`</span> of the parent node. The <span class="in">`info_size`</span> of the parent node is </span>
<span id="cb80-810"><a href="#cb80-810" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-813"><a href="#cb80-813" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-814"><a href="#cb80-814" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb80-815"><a href="#cb80-815" aria-hidden="true" tabindex="-1"></a>info_size_p <span class="ot">&lt;-</span> data_J1[, <span class="fu">sum</span>((T <span class="sc">-</span> <span class="fu">mean</span>(T))<span class="sc">^</span><span class="dv">2</span>)]</span>
<span id="cb80-816"><a href="#cb80-816" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb80-817"><a href="#cb80-817" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-818"><a href="#cb80-818" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-819"><a href="#cb80-819" aria-hidden="true" tabindex="-1"></a>By default, <span class="in">`alpha`</span> is set to 0.05. We use this number.</span>
<span id="cb80-820"><a href="#cb80-820" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-821"><a href="#cb80-821" aria-hidden="true" tabindex="-1"></a><span class="in">```{r alpha}</span></span>
<span id="cb80-822"><a href="#cb80-822" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.05</span></span>
<span id="cb80-823"><a href="#cb80-823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-824"><a href="#cb80-824" aria-hidden="true" tabindex="-1"></a>msn_met_data[, info_size_met <span class="sc">:</span><span class="er">=</span> <span class="fu">all</span>(info_size <span class="sc">&gt;</span> (alpha <span class="sc">*</span> info_size_p)), by <span class="ot">=</span> .(var, thr)]</span>
<span id="cb80-825"><a href="#cb80-825" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-826"><a href="#cb80-826" aria-hidden="true" tabindex="-1"></a>msn_ifs_met <span class="ot">&lt;-</span> msn_met_data[info_size_met <span class="sc">==</span> <span class="cn">TRUE</span>, ]</span>
<span id="cb80-827"><a href="#cb80-827" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-828"><a href="#cb80-828" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-829"><a href="#cb80-829" aria-hidden="true" tabindex="-1"></a><span class="in">`imbalance.penalty`</span> is used to further punish splits that introduce imbalance. For example, for feature <span class="in">`x1`</span> and <span class="in">`thr`</span> of <span class="in">`r msn_ifs_met[1, thr]`</span>, </span>
<span id="cb80-830"><a href="#cb80-830" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-833"><a href="#cb80-833" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-834"><a href="#cb80-834" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb80-835"><a href="#cb80-835" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb80-836"><a href="#cb80-836" aria-hidden="true" tabindex="-1"></a>ex_score <span class="ot">&lt;-</span> </span>
<span id="cb80-837"><a href="#cb80-837" aria-hidden="true" tabindex="-1"></a>  msn_met_data[var <span class="sc">==</span> <span class="st">"x1"</span>, ] <span class="sc">%&gt;%</span> </span>
<span id="cb80-838"><a href="#cb80-838" aria-hidden="true" tabindex="-1"></a>  .[<span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>,]</span>
<span id="cb80-839"><a href="#cb80-839" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb80-840"><a href="#cb80-840" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-841"><a href="#cb80-841" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-842"><a href="#cb80-842" aria-hidden="true" tabindex="-1"></a>the unpunished heterogeneity score is <span class="in">`r ex_score[, sum(het_score)]`</span>. With a non-zero value of <span class="in">`imbalance.penalty`</span>, the following penalty will be subtracted from the unpunished heterogeneity score: <span class="in">`imbalance.penalty * (1/info_size_1 + 1/info_size_2)`</span>. By default, <span class="in">`imbalance.penalty`</span> is 0. But, let's use <span class="in">`imbalance.penalty`</span> $= 1$ here.</span>
<span id="cb80-843"><a href="#cb80-843" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-846"><a href="#cb80-846" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-847"><a href="#cb80-847" aria-hidden="true" tabindex="-1"></a>imbalance.penalty <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb80-848"><a href="#cb80-848" aria-hidden="true" tabindex="-1"></a>msn_ifs_met[, imb_penalty <span class="sc">:</span><span class="er">=</span> imbalance.penalty <span class="sc">*</span> (<span class="dv">1</span><span class="sc">/</span>info_size)]</span>
<span id="cb80-849"><a href="#cb80-849" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-850"><a href="#cb80-850" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-851"><a href="#cb80-851" aria-hidden="true" tabindex="-1"></a>Now, we calculate the heterogeneity score for each feature-threshold with imbalance penalty.</span>
<span id="cb80-852"><a href="#cb80-852" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-853"><a href="#cb80-853" aria-hidden="true" tabindex="-1"></a><span class="in">```{r imbp-score}</span></span>
<span id="cb80-854"><a href="#cb80-854" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb80-855"><a href="#cb80-855" aria-hidden="true" tabindex="-1"></a>het_score_data <span class="ot">&lt;-</span> msn_ifs_met[, .(<span class="at">het_with_imbp =</span> <span class="fu">sum</span>(het_score <span class="sc">-</span> imb_penalty)), <span class="at">by =</span> .(var, thr)]</span>
<span id="cb80-856"><a href="#cb80-856" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb80-857"><a href="#cb80-857" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-858"><a href="#cb80-858" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-859"><a href="#cb80-859" aria-hidden="true" tabindex="-1"></a>We now find the feature-threshold combination that maximizes the score,</span>
<span id="cb80-860"><a href="#cb80-860" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-863"><a href="#cb80-863" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-864"><a href="#cb80-864" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb80-865"><a href="#cb80-865" aria-hidden="true" tabindex="-1"></a>best_split <span class="ot">&lt;-</span> het_score_data[<span class="fu">which.max</span>(het_with_imbp), ]</span>
<span id="cb80-866"><a href="#cb80-866" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb80-867"><a href="#cb80-867" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-868"><a href="#cb80-868" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-869"><a href="#cb80-869" aria-hidden="true" tabindex="-1"></a>So, we are splitting the root node using <span class="in">`x1`</span> with threshold of <span class="in">`r best_split[, round(thr, digits = 4)]`</span>. Here is a visualization of the split,</span>
<span id="cb80-870"><a href="#cb80-870" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-871"><a href="#cb80-871" aria-hidden="true" tabindex="-1"></a><span class="in">```{r viz-first-split}</span></span>
<span id="cb80-872"><a href="#cb80-872" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: true</span></span>
<span id="cb80-873"><a href="#cb80-873" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(data_J1) <span class="sc">+</span></span>
<span id="cb80-874"><a href="#cb80-874" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">y =</span> rho, <span class="at">x =</span> x1, <span class="at">color =</span> (x1 <span class="sc">&lt;</span> best_split[, thr]))) <span class="sc">+</span></span>
<span id="cb80-875"><a href="#cb80-875" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_discrete</span>(<span class="st">"Less than the threshold"</span>) <span class="sc">+</span></span>
<span id="cb80-876"><a href="#cb80-876" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb80-877"><a href="#cb80-877" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"bottom"</span>)</span>
<span id="cb80-878"><a href="#cb80-878" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-879"><a href="#cb80-879" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-880"><a href="#cb80-880" aria-hidden="true" tabindex="-1"></a>Now, let's look at the pseudo outcome and elaborate more on what we are seeing here.</span>
<span id="cb80-881"><a href="#cb80-881" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-882"><a href="#cb80-882" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-883"><a href="#cb80-883" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb80-884"><a href="#cb80-884" aria-hidden="true" tabindex="-1"></a>\rho_i = (\tilde{Y}_i- \hat{\theta}_P\tilde{T}_i)\tilde{T}_i</span>
<span id="cb80-885"><a href="#cb80-885" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb80-886"><a href="#cb80-886" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-887"><a href="#cb80-887" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-888"><a href="#cb80-888" aria-hidden="true" tabindex="-1"></a>Note that $\hat{\theta}_P$ is just a constant and represents the average treatment effect for the parent node. So, $\rho_i$ tends to be higher for the observations whose CATE is higher (above-average treatment effect). So, splitting on $\rho$ (instead of $Y$) made it possible to pick the right feature variable <span class="in">`x1`</span>. This is because <span class="in">`x1`</span> is influential in determining CATE and higher values of <span class="in">`x1`</span> lead to more positive CATE.</span>
<span id="cb80-889"><a href="#cb80-889" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-890"><a href="#cb80-890" aria-hidden="true" tabindex="-1"></a>Note that the checks implemented with <span class="in">`mtry`</span>, <span class="in">`alpha`</span>, and  <span class="in">`imbalance.penalty`</span> is more important further down a tree. But, the examples above should have illustrated how they are used. </span>
<span id="cb80-891"><a href="#cb80-891" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-892"><a href="#cb80-892" aria-hidden="true" tabindex="-1"></a>Okay, now let's split the root node into two child nodes according to the best split on $\rho$ we found earlier.</span>
<span id="cb80-893"><a href="#cb80-893" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-896"><a href="#cb80-896" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-897"><a href="#cb80-897" aria-hidden="true" tabindex="-1"></a>depth_2_node_1 <span class="ot">&lt;-</span> data_J1[x1 <span class="sc">&lt;</span> best_split[, thr], ]</span>
<span id="cb80-898"><a href="#cb80-898" aria-hidden="true" tabindex="-1"></a>depth_2_node_2 <span class="ot">&lt;-</span> data_J1[x1 <span class="sc">&gt;=</span> best_split[, thr], ]</span>
<span id="cb80-899"><a href="#cb80-899" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-900"><a href="#cb80-900" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-901"><a href="#cb80-901" aria-hidden="true" tabindex="-1"></a>Let's consider splitting the second node, <span class="in">`depth_2_node_1`</span>. We basically follow exactly the same process the we just did. First, find $\hat{\theta}_P$ for this (parent) node using @eq-theta-p and define the pseudo outcomes.</span>
<span id="cb80-902"><a href="#cb80-902" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-905"><a href="#cb80-905" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-906"><a href="#cb80-906" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb80-907"><a href="#cb80-907" aria-hidden="true" tabindex="-1"></a>theta_p <span class="ot">&lt;-</span> depth_2_node_1[, <span class="fu">sum</span>(y_tilde <span class="sc">*</span> t_tilde)<span class="sc">/</span><span class="fu">sum</span>(t_tilde <span class="sc">*</span> t_tilde)]</span>
<span id="cb80-908"><a href="#cb80-908" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb80-909"><a href="#cb80-909" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-910"><a href="#cb80-910" aria-hidden="true" tabindex="-1"></a>depth_2_node_1[, rho <span class="sc">:</span><span class="er">=</span> (y_tilde <span class="sc">-</span> theta_p <span class="sc">*</span> t_tilde) <span class="sc">*</span> t_tilde]</span>
<span id="cb80-911"><a href="#cb80-911" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-912"><a href="#cb80-912" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-913"><a href="#cb80-913" aria-hidden="true" tabindex="-1"></a>Now, we find the best split (here we ignore various checks),</span>
<span id="cb80-914"><a href="#cb80-914" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-917"><a href="#cb80-917" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-918"><a href="#cb80-918" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb80-919"><a href="#cb80-919" aria-hidden="true" tabindex="-1"></a>best_split_d2_n1 <span class="ot">&lt;-</span></span>
<span id="cb80-920"><a href="#cb80-920" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lapply</span>(</span>
<span id="cb80-921"><a href="#cb80-921" aria-hidden="true" tabindex="-1"></a>    <span class="fu">paste0</span>(<span class="st">"x"</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>),</span>
<span id="cb80-922"><a href="#cb80-922" aria-hidden="true" tabindex="-1"></a>    <span class="cf">function</span>(x) <span class="fu">get_ss_by_var</span>(x, <span class="st">"rho"</span>, depth_2_node_1)</span>
<span id="cb80-923"><a href="#cb80-923" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb80-924"><a href="#cb80-924" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rbindlist</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb80-925"><a href="#cb80-925" aria-hidden="true" tabindex="-1"></a>  .[, .(<span class="at">het_score =</span> <span class="fu">sum</span>(het_score)), <span class="at">by =</span> .(var, thr)] <span class="sc">%&gt;%</span> </span>
<span id="cb80-926"><a href="#cb80-926" aria-hidden="true" tabindex="-1"></a>  .[<span class="fu">which.max</span>(het_score), ]</span>
<span id="cb80-927"><a href="#cb80-927" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb80-928"><a href="#cb80-928" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-929"><a href="#cb80-929" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-930"><a href="#cb80-930" aria-hidden="true" tabindex="-1"></a>We keep splitting nodes until no splits is possible any more based on the value of <span class="in">`min.node.size`</span>, <span class="in">`alpha`</span>, and <span class="in">`imbalance.penalty`</span>. </span>
<span id="cb80-931"><a href="#cb80-931" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-932"><a href="#cb80-932" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb80-933"><a href="#cb80-933" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-934"><a href="#cb80-934" aria-hidden="true" tabindex="-1"></a>To see the role of <span class="in">`honesty.prune.leaves`</span>, let's suppose we stopped splitting after the first split based on <span class="in">`x`</span> with threshold of <span class="in">`r best_split[, round(thr, digits = 4)]`</span>. So, we just two terminal nodes. As stated in @sec-grf-honest, when we do prediction, <span class="in">`data_J1`</span> (which is used for determining the splitting rule) is not used. Rather, <span class="in">`data_J2`</span> (the data that was set aside) is used. When <span class="in">`honesty.prune.leaves = TRUE`</span> (default), the tree is pruned so that no leaves are empty. Yes, we made sure that we have certain number of observations in each node with <span class="in">`min.node.size`</span>, however, that is only for <span class="in">`data_J1`</span>, not <span class="in">`data_J2`</span>. </span>
<span id="cb80-935"><a href="#cb80-935" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-938"><a href="#cb80-938" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-939"><a href="#cb80-939" aria-hidden="true" tabindex="-1"></a>data_J2[, .N, by <span class="ot">=</span> x1 <span class="sc">&lt;</span> best_split[, thr]]</span>
<span id="cb80-940"><a href="#cb80-940" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-941"><a href="#cb80-941" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-942"><a href="#cb80-942" aria-hidden="true" tabindex="-1"></a>Okay, so, each child has at least one observation. So, we are not going to prune the tree. If either of the child nodes was empty, then we would have removed the leaves and had the root node as the tree. </span>
<span id="cb80-943"><a href="#cb80-943" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-944"><a href="#cb80-944" aria-hidden="true" tabindex="-1"></a>As you can see, while <span class="in">`data_J1`</span> plays a central role in the tree building process, the other half, <span class="in">`data_J2`</span>, plays a very small role. However, <span class="in">`data_J2`</span> plays the central role in prediction, which will be seen in the next section.</span>
<span id="cb80-945"><a href="#cb80-945" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-946"><a href="#cb80-946" aria-hidden="true" tabindex="-1"></a><span class="fu">### Prediction</span></span>
<span id="cb80-947"><a href="#cb80-947" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-948"><a href="#cb80-948" aria-hidden="true" tabindex="-1"></a>Suppose you have built $T$ trees already. In general for GRF, statistics of interest, $\theta(X)$, at $X = X_0$ is found by solving the following equation:</span>
<span id="cb80-949"><a href="#cb80-949" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-950"><a href="#cb80-950" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-951"><a href="#cb80-951" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb80-952"><a href="#cb80-952" aria-hidden="true" tabindex="-1"></a>\sum_{i=1}^N \alpha_i(X_i, X_0)\psi_{\theta,\nu}(O_i) = 0 </span>
<span id="cb80-953"><a href="#cb80-953" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb80-954"><a href="#cb80-954" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-955"><a href="#cb80-955" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-956"><a href="#cb80-956" aria-hidden="true" tabindex="-1"></a>where $\alpha_i(X_i, X_0)$ is the weight given to each $i$ calculated based on the trees that have been built. Let $\eta_{i,t}(X_i, X_0)$ be 1 if observation $i$ belongs to the same leaf as $X_0$ in tree $t$. Then, </span>
<span id="cb80-957"><a href="#cb80-957" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-958"><a href="#cb80-958" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-959"><a href="#cb80-959" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb80-960"><a href="#cb80-960" aria-hidden="true" tabindex="-1"></a>\alpha_i(X_i, X_0) = \frac{1}{T}\sum_{t=1}^T\frac{\eta_{i,t}(X_i, X_0)}{\sum_{i=1}^N\eta_{i,t}(X_i, X_0)}</span>
<span id="cb80-961"><a href="#cb80-961" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb80-962"><a href="#cb80-962" aria-hidden="true" tabindex="-1"></a>$$ {#eq-weight-cf}</span>
<span id="cb80-963"><a href="#cb80-963" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-964"><a href="#cb80-964" aria-hidden="true" tabindex="-1"></a>So, the weight given to observation $i$ is higher if observation $i$ belongs to the same leaf as the evaluation point $X_0$ in more trees. </span>
<span id="cb80-965"><a href="#cb80-965" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-966"><a href="#cb80-966" aria-hidden="true" tabindex="-1"></a>For CF, since the score function is defined as $\psi_{\theta,\nu}(O_i) = \tilde{Y}_i - \theta\tilde{T}_i)\cdot \tilde{T}_i$, the equation can be written as</span>
<span id="cb80-967"><a href="#cb80-967" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-968"><a href="#cb80-968" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-969"><a href="#cb80-969" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb80-970"><a href="#cb80-970" aria-hidden="true" tabindex="-1"></a>\sum_{i=1}^N \alpha_i(X_i, X_0)<span class="co">[</span><span class="ot">\tilde{Y_i} - \theta\cdot \tilde{T_i}</span><span class="co">]</span>\hat{\tilde{T_i}} = 0 </span>
<span id="cb80-971"><a href="#cb80-971" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb80-972"><a href="#cb80-972" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-973"><a href="#cb80-973" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-974"><a href="#cb80-974" aria-hidden="true" tabindex="-1"></a>So,</span>
<span id="cb80-975"><a href="#cb80-975" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-976"><a href="#cb80-976" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-977"><a href="#cb80-977" aria-hidden="true" tabindex="-1"></a>\begin{aligned}</span>
<span id="cb80-978"><a href="#cb80-978" aria-hidden="true" tabindex="-1"></a>\theta_P = \frac{\sum_{i=1}^n \alpha_i(X_i, X_0)\tilde{Y}_i\cdot \tilde{T}_i}{\sum_{i=1}^n\alpha_i(X_i, X_0)\tilde{T}_i\cdot \tilde{T}_i}</span>
<span id="cb80-979"><a href="#cb80-979" aria-hidden="true" tabindex="-1"></a>\end{aligned}</span>
<span id="cb80-980"><a href="#cb80-980" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb80-981"><a href="#cb80-981" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-982"><a href="#cb80-982" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-983"><a href="#cb80-983" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb80-984"><a href="#cb80-984" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-985"><a href="#cb80-985" aria-hidden="true" tabindex="-1"></a>We now illustrate how predictions are done once trees have been built. Let's first build trees using <span class="in">`causal_forest()`</span>. <span class="in">`min.node.size`</span> is set deliberately high so we can work with very simple trees. </span>
<span id="cb80-986"><a href="#cb80-986" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-987"><a href="#cb80-987" aria-hidden="true" tabindex="-1"></a>::: {.column-margin}</span>
<span id="cb80-988"><a href="#cb80-988" aria-hidden="true" tabindex="-1"></a>You do not need to know how <span class="in">`causal_forest()`</span> works. You only need to know that <span class="in">`causal_forest()`</span> build trees for CATE estimation.</span>
<span id="cb80-989"><a href="#cb80-989" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb80-990"><a href="#cb80-990" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-993"><a href="#cb80-993" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-994"><a href="#cb80-994" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">443784</span>)</span>
<span id="cb80-995"><a href="#cb80-995" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-996"><a href="#cb80-996" aria-hidden="true" tabindex="-1"></a>cf_trained <span class="ot">&lt;-</span></span>
<span id="cb80-997"><a href="#cb80-997" aria-hidden="true" tabindex="-1"></a>  <span class="fu">causal_forest</span>(</span>
<span id="cb80-998"><a href="#cb80-998" aria-hidden="true" tabindex="-1"></a>    <span class="at">X =</span> <span class="fu">select</span>(data, <span class="fu">starts_with</span>(<span class="st">"x"</span>)),</span>
<span id="cb80-999"><a href="#cb80-999" aria-hidden="true" tabindex="-1"></a>    <span class="at">Y =</span> data[, y],</span>
<span id="cb80-1000"><a href="#cb80-1000" aria-hidden="true" tabindex="-1"></a>    <span class="at">W =</span> data[, T],</span>
<span id="cb80-1001"><a href="#cb80-1001" aria-hidden="true" tabindex="-1"></a>    <span class="at">min.node.size =</span> <span class="dv">50</span></span>
<span id="cb80-1002"><a href="#cb80-1002" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb80-1003"><a href="#cb80-1003" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-1004"><a href="#cb80-1004" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1005"><a href="#cb80-1005" aria-hidden="true" tabindex="-1"></a>After training a causal forest model, we have trees like the one shown in @fig-cf-tree, which is the first of the $2000$ trees.</span>
<span id="cb80-1006"><a href="#cb80-1006" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1009"><a href="#cb80-1009" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-1010"><a href="#cb80-1010" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Example trees built in causal forest estimation</span></span>
<span id="cb80-1011"><a href="#cb80-1011" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-cf-tree </span></span>
<span id="cb80-1012"><a href="#cb80-1012" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false </span></span>
<span id="cb80-1013"><a href="#cb80-1013" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 4</span></span>
<span id="cb80-1014"><a href="#cb80-1014" aria-hidden="true" tabindex="-1"></a><span class="fu">get_tree</span>(cf_trained, <span class="dv">1</span>) <span class="sc">%&gt;%</span> <span class="fu">plot</span>()</span>
<span id="cb80-1015"><a href="#cb80-1015" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-1016"><a href="#cb80-1016" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1017"><a href="#cb80-1017" aria-hidden="true" tabindex="-1"></a>You probably noticed that the total number of samples in the leaves is only $250$ instead of $1000$, which is the total number of observations in <span class="in">`data`</span>. When causal forest was trained on this dataset, only half of the entire sample are randomly selected for building each tree (due to the default setting of <span class="in">`sample.fraction = 0.5`</span>). The halved sample is further split into two groups, each containing $250$ observations (due to the default setting of <span class="in">`honesty = TRUE`</span> and <span class="in">`honest.fraction = 0.5`</span>). Let's call them $J_1$ and $J_2$. Then, $J_1$ is used to train a tree to find the splitting rules (e.g., $x_1 \leq 1.44$ for the first tree). See @sec-build-trees for how only a subset of the drawn samples was used to determine the splitting rules. Once the splitting rules are determined (tree building process is complete), then $J_1$ is "vacated" (or thrown out) from the tree. This will become clearer when we talk about finding individual weights.</span>
<span id="cb80-1018"><a href="#cb80-1018" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1019"><a href="#cb80-1019" aria-hidden="true" tabindex="-1"></a>Let's take a look at a tree to see what happened. We can use <span class="in">`get_tree()`</span> to access individual trees from <span class="in">`cf_trained`</span>. </span>
<span id="cb80-1020"><a href="#cb80-1020" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1023"><a href="#cb80-1023" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-1024"><a href="#cb80-1024" aria-hidden="true" tabindex="-1"></a><span class="co">#=== get the first tree ===#</span></span>
<span id="cb80-1025"><a href="#cb80-1025" aria-hidden="true" tabindex="-1"></a>a_tree <span class="ot">&lt;-</span> <span class="fu">get_tree</span>(cf_trained, <span class="dv">1</span>)</span>
<span id="cb80-1026"><a href="#cb80-1026" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-1027"><a href="#cb80-1027" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1028"><a href="#cb80-1028" aria-hidden="true" tabindex="-1"></a><span class="in">`drawn_samples`</span> attribute of the tree contains row indices that are selected randomly for this tree.</span>
<span id="cb80-1029"><a href="#cb80-1029" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1032"><a href="#cb80-1032" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-1033"><a href="#cb80-1033" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(a_tree<span class="sc">$</span>drawn_samples)</span>
<span id="cb80-1034"><a href="#cb80-1034" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1035"><a href="#cb80-1035" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(a_tree<span class="sc">$</span>drawn_samples)</span>
<span id="cb80-1036"><a href="#cb80-1036" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-1037"><a href="#cb80-1037" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1038"><a href="#cb80-1038" aria-hidden="true" tabindex="-1"></a>As you can see, there are 500 samples (due to <span class="in">`sample.fraction = 0.5`</span>). The rest of the observations were not used for this tree at all. Accessing <span class="in">`nodes`</span> attribute will give you the splitting rules for the tree built and which samples are in what node.</span>
<span id="cb80-1039"><a href="#cb80-1039" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1042"><a href="#cb80-1042" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-1043"><a href="#cb80-1043" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb80-1044"><a href="#cb80-1044" aria-hidden="true" tabindex="-1"></a>nodes <span class="ot">&lt;-</span> a_tree<span class="sc">$</span>nodes</span>
<span id="cb80-1045"><a href="#cb80-1045" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb80-1046"><a href="#cb80-1046" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-1047"><a href="#cb80-1047" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1048"><a href="#cb80-1048" aria-hidden="true" tabindex="-1"></a><span class="in">`nodes`</span> is a list of three elements (one root node and two terminal nodes here). The <span class="in">`samples`</span> attribute gives you row indices of the samples that belong to the terminal node. </span>
<span id="cb80-1049"><a href="#cb80-1049" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1052"><a href="#cb80-1052" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-1053"><a href="#cb80-1053" aria-hidden="true" tabindex="-1"></a>nodes[[<span class="dv">2</span>]]<span class="sc">$</span>samples</span>
<span id="cb80-1054"><a href="#cb80-1054" aria-hidden="true" tabindex="-1"></a>nodes[[<span class="dv">3</span>]]<span class="sc">$</span>samples</span>
<span id="cb80-1055"><a href="#cb80-1055" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-1056"><a href="#cb80-1056" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1057"><a href="#cb80-1057" aria-hidden="true" tabindex="-1"></a>It is important to keep in mind that these observations with these row indices belong to $J_2$. These observations were <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>NOT<span class="kw">&lt;/span&gt;</span> used in determining the splitting rule of $x_1 \leq 1.44$. They were populating the terminal nodes by simply following the splitting rule, which is determined using the data in $J_1$ following the process described in @sec-build-trees. The difference in <span class="in">`a_tree$drawn_samples`</span> and the combination of <span class="in">`nodes[[2]]$samples`</span> and <span class="in">`nodes[[3]]$samples`</span> is $J_1$.</span>
<span id="cb80-1058"><a href="#cb80-1058" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1061"><a href="#cb80-1061" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-1062"><a href="#cb80-1062" aria-hidden="true" tabindex="-1"></a>J2_rows <span class="ot">&lt;-</span> <span class="fu">c</span>(nodes[[<span class="dv">2</span>]]<span class="sc">$</span>samples, nodes[[<span class="dv">3</span>]]<span class="sc">$</span>samples)</span>
<span id="cb80-1063"><a href="#cb80-1063" aria-hidden="true" tabindex="-1"></a>J1_J2_rows <span class="ot">&lt;-</span> a_tree<span class="sc">$</span>drawn_samples</span>
<span id="cb80-1064"><a href="#cb80-1064" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1065"><a href="#cb80-1065" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb80-1066"><a href="#cb80-1066" aria-hidden="true" tabindex="-1"></a>J1_rows <span class="ot">&lt;-</span> J1_J2_rows[J1_J2_rows <span class="sc">%in%</span> J2_rows]</span>
<span id="cb80-1067"><a href="#cb80-1067" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb80-1068"><a href="#cb80-1068" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-1069"><a href="#cb80-1069" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1070"><a href="#cb80-1070" aria-hidden="true" tabindex="-1"></a>As you can see, there are 250 samples in $J_1$ as well.</span>
<span id="cb80-1071"><a href="#cb80-1071" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1072"><a href="#cb80-1072" aria-hidden="true" tabindex="-1"></a>---</span>
<span id="cb80-1073"><a href="#cb80-1073" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1074"><a href="#cb80-1074" aria-hidden="true" tabindex="-1"></a>Suppose you are interested in predicting $\hat{\theta}$ at $X_0 = <span class="sc">\{</span>x_1 = 2, x_2,\dots, x_{10} = 1<span class="sc">\}</span>$. For a given tree, we give 1 to the observations in <span class="kw">&lt;span</span> <span class="er">style</span><span class="ot">=</span><span class="st">"color:red"</span><span class="kw">&gt;</span>$J_1$<span class="kw">&lt;/span&gt;</span> that belong to the same leaf as $X_0$. For example, for the first tree, $X_0$ belongs to the right leaf because $x1 = 2 &gt; 1.44$ for $X_0$. We can tell which node $X_0$ belongs to by supplying $X_0$ to <span class="in">`get_leaf_node()`</span>.</span>
<span id="cb80-1075"><a href="#cb80-1075" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1078"><a href="#cb80-1078" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-1079"><a href="#cb80-1079" aria-hidden="true" tabindex="-1"></a>X_0 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">2</span>, <span class="fu">rep</span>(<span class="dv">1</span>, <span class="dv">9</span>)), <span class="at">nrow =</span> <span class="dv">1</span>)</span>
<span id="cb80-1080"><a href="#cb80-1080" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1081"><a href="#cb80-1081" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb80-1082"><a href="#cb80-1082" aria-hidden="true" tabindex="-1"></a>which_tree_is_X0_in <span class="ot">&lt;-</span> <span class="fu">get_leaf_node</span>(a_tree, X_0)</span>
<span id="cb80-1083"><a href="#cb80-1083" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb80-1084"><a href="#cb80-1084" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-1085"><a href="#cb80-1085" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1086"><a href="#cb80-1086" aria-hidden="true" tabindex="-1"></a>So, we give $1/N_t(X_0)$ to all those in the right leaf (the third node in <span class="in">`nodes`</span>) and 0 to those in the left leaf, where $N_t(X_0)$ is the number of observations that belong to the same leaf as $X_0$. All the other observations are assigned 0. </span>
<span id="cb80-1087"><a href="#cb80-1087" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1090"><a href="#cb80-1090" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-1091"><a href="#cb80-1091" aria-hidden="true" tabindex="-1"></a><span class="co">#=== which row numbers in the same leaf as X_0? ===#</span></span>
<span id="cb80-1092"><a href="#cb80-1092" aria-hidden="true" tabindex="-1"></a>rows_1 <span class="ot">&lt;-</span> nodes[[which_tree_is_X0_in]]<span class="sc">$</span>samples</span>
<span id="cb80-1093"><a href="#cb80-1093" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1094"><a href="#cb80-1094" aria-hidden="true" tabindex="-1"></a><span class="co">#=== define eta for tree 1  ===#</span></span>
<span id="cb80-1095"><a href="#cb80-1095" aria-hidden="true" tabindex="-1"></a>data[, eta_t1 <span class="sc">:</span><span class="er">=</span> <span class="dv">0</span>] <span class="co"># first set eta to 0 for all the observations</span></span>
<span id="cb80-1096"><a href="#cb80-1096" aria-hidden="true" tabindex="-1"></a>data[rows_1, eta_t1 <span class="sc">:</span><span class="er">=</span> <span class="dv">1</span> <span class="sc">/</span> <span class="fu">length</span>(rows_1)] <span class="co"># replace eta with 1/N_t(X_0) if in the right node</span></span>
<span id="cb80-1097"><a href="#cb80-1097" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1098"><a href="#cb80-1098" aria-hidden="true" tabindex="-1"></a><span class="co">#=== see the data ===#</span></span>
<span id="cb80-1099"><a href="#cb80-1099" aria-hidden="true" tabindex="-1"></a>data</span>
<span id="cb80-1100"><a href="#cb80-1100" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-1101"><a href="#cb80-1101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1102"><a href="#cb80-1102" aria-hidden="true" tabindex="-1"></a>We repeat this for all the trees and use @eq-weight-cf to calculate the weights for the individual observations. The following function gets $\eta_{i,t}(X_i, X_0)$ for a given tree for all the observations.</span>
<span id="cb80-1103"><a href="#cb80-1103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1106"><a href="#cb80-1106" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-1107"><a href="#cb80-1107" aria-hidden="true" tabindex="-1"></a>get_eta <span class="ot">&lt;-</span> <span class="cf">function</span>(t, X_0) {</span>
<span id="cb80-1108"><a href="#cb80-1108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1109"><a href="#cb80-1109" aria-hidden="true" tabindex="-1"></a>  w_tree <span class="ot">&lt;-</span> <span class="fu">get_tree</span>(cf_trained, t)</span>
<span id="cb80-1110"><a href="#cb80-1110" aria-hidden="true" tabindex="-1"></a>  which_tree_is_X0_in <span class="ot">&lt;-</span> <span class="fu">get_leaf_node</span>(w_tree, X_0)</span>
<span id="cb80-1111"><a href="#cb80-1111" aria-hidden="true" tabindex="-1"></a>  rows <span class="ot">&lt;-</span> w_tree<span class="sc">$</span>nodes[[which_tree_is_X0_in]]<span class="sc">$</span>samples</span>
<span id="cb80-1112"><a href="#cb80-1112" aria-hidden="true" tabindex="-1"></a>  eta_data <span class="ot">&lt;-</span> </span>
<span id="cb80-1113"><a href="#cb80-1113" aria-hidden="true" tabindex="-1"></a>    <span class="fu">data.table</span>(</span>
<span id="cb80-1114"><a href="#cb80-1114" aria-hidden="true" tabindex="-1"></a>      <span class="at">row_id =</span> <span class="fu">seq_len</span>(<span class="fu">nrow</span>(data)),</span>
<span id="cb80-1115"><a href="#cb80-1115" aria-hidden="true" tabindex="-1"></a>      <span class="at">eta =</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="fu">nrow</span>(data))</span>
<span id="cb80-1116"><a href="#cb80-1116" aria-hidden="true" tabindex="-1"></a>    ) <span class="sc">%&gt;%</span> </span>
<span id="cb80-1117"><a href="#cb80-1117" aria-hidden="true" tabindex="-1"></a>    .[rows, eta <span class="sc">:</span><span class="er">=</span> <span class="dv">1</span> <span class="sc">/</span> <span class="fu">length</span>(rows)]</span>
<span id="cb80-1118"><a href="#cb80-1118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1119"><a href="#cb80-1119" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(eta_data)</span>
<span id="cb80-1120"><a href="#cb80-1120" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb80-1121"><a href="#cb80-1121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1122"><a href="#cb80-1122" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-1123"><a href="#cb80-1123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1124"><a href="#cb80-1124" aria-hidden="true" tabindex="-1"></a>We apply <span class="in">`get_eta()`</span> for each of the 2000 trees.</span>
<span id="cb80-1125"><a href="#cb80-1125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1128"><a href="#cb80-1128" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-1129"><a href="#cb80-1129" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb80-1130"><a href="#cb80-1130" aria-hidden="true" tabindex="-1"></a>eta_all <span class="ot">&lt;-</span></span>
<span id="cb80-1131"><a href="#cb80-1131" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lapply</span>(</span>
<span id="cb80-1132"><a href="#cb80-1132" aria-hidden="true" tabindex="-1"></a>    <span class="dv">1</span><span class="sc">:</span><span class="dv">2000</span>,</span>
<span id="cb80-1133"><a href="#cb80-1133" aria-hidden="true" tabindex="-1"></a>    <span class="cf">function</span>(x) <span class="fu">get_eta</span>(x, X_0)</span>
<span id="cb80-1134"><a href="#cb80-1134" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">%&gt;%</span> </span>
<span id="cb80-1135"><a href="#cb80-1135" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rbindlist</span>(<span class="at">idcol =</span> <span class="st">"t"</span>)</span>
<span id="cb80-1136"><a href="#cb80-1136" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb80-1137"><a href="#cb80-1137" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-1138"><a href="#cb80-1138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1139"><a href="#cb80-1139" aria-hidden="true" tabindex="-1"></a>Calculate the mean of $\eta_{i,t}$ by <span class="in">`row_id`</span> (observation) to calculate $\alpha(X_i, X_0)$.</span>
<span id="cb80-1140"><a href="#cb80-1140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1143"><a href="#cb80-1143" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-1144"><a href="#cb80-1144" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb80-1145"><a href="#cb80-1145" aria-hidden="true" tabindex="-1"></a>weights <span class="ot">&lt;-</span> </span>
<span id="cb80-1146"><a href="#cb80-1146" aria-hidden="true" tabindex="-1"></a>  eta_all <span class="sc">%&gt;%</span> </span>
<span id="cb80-1147"><a href="#cb80-1147" aria-hidden="true" tabindex="-1"></a>  .[, .(<span class="at">weight =</span> <span class="fu">mean</span>(eta)), <span class="at">by =</span> row_id]</span>
<span id="cb80-1148"><a href="#cb80-1148" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb80-1149"><a href="#cb80-1149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1150"><a href="#cb80-1150" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-1151"><a href="#cb80-1151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1152"><a href="#cb80-1152" aria-hidden="true" tabindex="-1"></a>Here is the observations that was given the highest and lowest weights.</span>
<span id="cb80-1153"><a href="#cb80-1153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1156"><a href="#cb80-1156" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-1157"><a href="#cb80-1157" aria-hidden="true" tabindex="-1"></a>data_with_wights <span class="ot">&lt;-</span> <span class="fu">cbind</span>(data, weights)</span>
<span id="cb80-1158"><a href="#cb80-1158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1159"><a href="#cb80-1159" aria-hidden="true" tabindex="-1"></a><span class="co">#=== highest (1st) and lowest (2nd) ===#</span></span>
<span id="cb80-1160"><a href="#cb80-1160" aria-hidden="true" tabindex="-1"></a>data_with_wights[weight <span class="sc">%in%</span> <span class="fu">c</span>(<span class="fu">max</span>(weight), <span class="fu">min</span>(weight)), ]</span>
<span id="cb80-1161"><a href="#cb80-1161" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-1162"><a href="#cb80-1162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1163"><a href="#cb80-1163" aria-hidden="true" tabindex="-1"></a>Then, we can use @eq-theta-solution to calculate $\hat{\theta}(X_0)$.</span>
<span id="cb80-1164"><a href="#cb80-1164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1167"><a href="#cb80-1167" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-1168"><a href="#cb80-1168" aria-hidden="true" tabindex="-1"></a>(</span>
<span id="cb80-1169"><a href="#cb80-1169" aria-hidden="true" tabindex="-1"></a>theta_X0 <span class="ot">&lt;-</span> <span class="fu">sum</span>(data_with_wights[, weight <span class="sc">*</span> (T<span class="sc">-</span>cf_trained<span class="sc">$</span>W.hat) <span class="sc">*</span> (y<span class="sc">-</span>cf_trained<span class="sc">$</span>Y.hat)]) <span class="sc">/</span> <span class="fu">sum</span>(data_with_wights[, weight <span class="sc">*</span> (T<span class="sc">-</span>cf_trained<span class="sc">$</span>W.hat)<span class="sc">^</span><span class="dv">2</span>])</span>
<span id="cb80-1170"><a href="#cb80-1170" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb80-1171"><a href="#cb80-1171" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-1172"><a href="#cb80-1172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1173"><a href="#cb80-1173" aria-hidden="true" tabindex="-1"></a>Note that $Y.hat$ and $W.hat$ attributes of <span class="in">`cf_trained`</span> are the estimates of $E<span class="co">[</span><span class="ot">Y|X</span><span class="co">]</span>$ and $E<span class="co">[</span><span class="ot">T|X</span><span class="co">]</span>$, respectively. By subtracting them from $Y$ and $T$, $\tilde{Y}$ and $\tilde{T}$ are calculated in the above code.</span>
<span id="cb80-1174"><a href="#cb80-1174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1175"><a href="#cb80-1175" aria-hidden="true" tabindex="-1"></a>To get the weights, we could have just used <span class="in">`get_forest_weights()`</span> like below:</span>
<span id="cb80-1176"><a href="#cb80-1176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1179"><a href="#cb80-1179" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-1180"><a href="#cb80-1180" aria-hidden="true" tabindex="-1"></a>weights <span class="ot">&lt;-</span> <span class="fu">get_forest_weights</span>(cf_trained, X_0)</span>
<span id="cb80-1181"><a href="#cb80-1181" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-1182"><a href="#cb80-1182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1183"><a href="#cb80-1183" aria-hidden="true" tabindex="-1"></a>Let's compare the weights and see if we did it right.</span>
<span id="cb80-1184"><a href="#cb80-1184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1187"><a href="#cb80-1187" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb80-1188"><a href="#cb80-1188" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">as.matrix</span>(weights) <span class="sc">-</span> data_with_wights<span class="sc">$</span>weight)</span>
<span id="cb80-1189"><a href="#cb80-1189" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb80-1190"><a href="#cb80-1190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1191"><a href="#cb80-1191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1192"><a href="#cb80-1192" aria-hidden="true" tabindex="-1"></a><span class="fu">## References {.unnumbered}</span></span>
<span id="cb80-1193"><a href="#cb80-1193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1194"><a href="#cb80-1194" aria-hidden="true" tabindex="-1"></a><span class="kw">&lt;div</span> <span class="er">id</span><span class="ot">=</span><span class="st">"refs"</span><span class="kw">&gt;&lt;/div&gt;</span></span>
<span id="cb80-1195"><a href="#cb80-1195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1196"><a href="#cb80-1196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1197"><a href="#cb80-1197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-1198"><a href="#cb80-1198" aria-hidden="true" tabindex="-1"></a></span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>