# Bootstrap

## What is it for?

Bootstrap can be used to quantify the uncertainty associated with an estimator. For example, you can use it to estimate the standard error (SE) of a coefficient of a linear model. Since there are closed-form solutions for that, bootstrap is not really bringing any benefits to this case. However, the power of bootstrap comes in handy when you do NOT have a closed form solution. We will first demonstrate how bootstrap works using a linear model, and then apply it to a case where no-closed form solution is available.

## How it works

::: {.column-margin}
**Packages to load for replication**

```{r}
#| include: false
library(data.table)
library(tidyverse)
library(mgcv)
library(caret)
library(parallel)
library(fixest)
library(tidymodels)
library(ranger)
```

```{r}
#| eval: false
library(data.table)
library(tidyverse)
library(mgcv)
library(caret)
library(parallel)
library(fixest)
library(tidymodels)
library(ranger)
```
:::


Here are the general steps of a bootstrap:

+ Step 1: Sample the data with replacement (You can sample the same observations more than one times. You draw a ball and then you put it back in the box.)
+ Step 2: Run a statistical analysis to estimate whatever quantity you are interested in estimating
+ Repeat Steps 1 and 2 many times and store the estimates
+ Derive uncertainty measures from the collection of estimates obtained above

Let's demonstrate this using a very simple linear regression example.

Here is the data generating process:

```{r}
set.seed(89343)
N <- 100
x <- rnorm(N)
y <- 2 * x + 2 * rnorm(N)
```

We would like to estimate the coefficient on $x$ by applying OLS to the following model:

$$
y = \alpha + \beta_x + \mu
$$

We know from the econometric theory class that the SE of $\hat{\beta}_{OLS}$ is $\frac{\sigma}{\sqrt{SST_x}}$, where $\sigma^2$ is the variance of the error term ($\mu$) and $SST_x = \sum_{i=1}^N (x_i - \bar{x})^2$ ($\bar{x}$ is the mean of $x$).  

```{r}
mean_x <- mean(x)
sst_x <- ((x-mean(x))^2) %>% sum()
(
se_bhat <- sqrt(4 / sst_x)
)
```

So, we know that the true SE of $\hat{\beta}_{OLS}$ is `r se_bhat`. There is not really any point in using bootstrap in this case, but this is a good example to see if bootstrap works or not.

Let's implement a single iteration of the entire bootstrap steps (Steps 1 and 2).

```{r}
#=== set up a dataset ===#
data <-
  data.table(
    y = y,
    x = x
  )
```

Now, draw observations with replacement so the resulting dataset has the same number of observations as the original dataset.

```{r}
num_obs <- nrow(data)

#=== draw row numbers ===#
(
row_indices <- sample(seq_len(num_obs), num_obs, replace = TRUE)
)
```

Use the sampled indices to create a bootstrapped dataset:

::: {.column-margin}
You could also use `bootstraps()` from the `rsample` package. 
:::

```{r}
temp_data <- data[row_indices, ]
```

Now, apply OLS to get a coefficient estimate on $x$ using the bootstrapped dataset.

```{r}
lm(y ~ x, data = temp_data)$coefficient["x"]
```

This is the end of Steps 1 and 2. Now, let's repeat this step 1000 times. First, we define a function that implements Steps 1 and 2.

```{r}
get_beta <- function(i, data)
{
  num_obs <- nrow(data)

  #=== sample row numbers ===#
  row_indices <- sample(seq_len(num_obs), num_obs, replace = TRUE)

  #=== bootstrapped data ===#
  temp_data <- data[row_indices, ]

  #=== get coefficient ===#
  beta_hat <- lm(y ~ x, data = temp_data)$coefficient["x"]

  return(beta_hat)
}
```

Now repeat `get_beta()` many times:

```{r}
beta_store <-
  lapply(
    1:1000,
    function(x) get_beta(x, data)
  ) %>% 
  unlist()
```

Calculate standard deviation of $\hat{\beta}_{OLS}$,

```{r}
sd(beta_store)
```

Not, bad. What if we make the number of observations to 1000 instead of 100?

```{r}
set.seed(67343)

#=== generate data ===#
N <- 1000
x <- rnorm(N)
y <- 2 * x + 2 * rnorm(N)

#=== set up a dataset ===#
data <-
  data.table(
    y = y,
    x = x
  )

#=== true SE ===#
mean_x <- mean(x)
sst_x <- sum(((x-mean(x))^2))
(
se_bhat <- sqrt(4 / sst_x)
)
```

```{r}
#=== bootstrap-estimated SE ===#
beta_store <-
  lapply(
    1:1000,
    function(x) get_beta(x, data)
  ) %>% 
  unlist()

sd(beta_store)
```

This is just a single simulation. So, we cannot say bootstrap works better when the number of sample size is larger only from these experiments. But, it is generally true that bootstrap indeed works better when the number of sample size is larger.

## A more complicated example {#sec-boot-comp}

Consider a simple production function (e.g., yield response functions for agronomists):

$$
y = \beta_1 x + \beta_2 x^2 + \mu
$$

-   $y$: output
-   $x$: input
-   $\mu$: error

The price of $y$ is 5 and the cost of $x$ is 2. Your objective is to identify the amount of input that maximizes profit. You do not know $\beta_1$ and $\beta_2$, and will be estimating them using the data you have collected. Letting $\hat{\beta_1}$ and $\hat{\beta_2}$ denote the estimates of $\beta_1$ and $\beta_2$, respectively, the mathematical expression of the optimization problem is:

$$
Max_x 5(\hat{\beta}_1 x + \hat{\beta}_2 x^2) - 2 x
$$

The F.O.C is

$$
5\hat{\beta}_1 + 10 \hat{\beta}_2 x - 2 = 0
$$
So, the estimated profit-maximizing input level is $\hat{x}^* = \frac{2-5\hat{\beta}_1}{10\hat{\beta}_2}$. What we are interested in knowing is the SE of $x^*$. As you can see, it is a non-linear function of the coefficients, which makes it slightly harder than simply getting the SE of $\hat{\beta_1}$ or $\hat{\beta_2}$. However, bootstrap can easily get us an estimate of the SE of $\hat{x}^*$^[Alternatively, you could use the delta method to get an estimate of the SE of a statistics that is a non-linear function of estimated parameters.]. The bootstrap process will be very much the same as the first bootstrap example except that we will estimate $x^*$ in each iteration instead of stopping at estimating just coefficients. Let's work on a single iteration first.

Here is the data generating process:

```{r}
set.seed(894334)

N <-  1000
x <-  runif(N) * 3
ey <- 6 * x - 2 * x^2
mu <- 2 * rnorm(N)
y <- ey + mu

data <- 
  data.table(
    x = x,
    y = y,
    ey = ey
  )
```

Under the data generating process, here is what the production function looks like:

```{r}
ggplot(data = data) +
  geom_line(aes(y = ey, x = x))
```


```{r}
num_obs <- nrow(data)
row_indices <- sample(seq_len(num_obs), num_obs, replace = TRUE)
boot_data <- data[row_indices, ]
reg <- lm(y ~ x + I(x^2), data = boot_data)

```

Now that we have estimated $\beta_1$ and $\beta_2$, we can easily estimate $x^*$ using its analytical formula.

```{r}
(
x_star <- (2 - 5 * reg$coef["x"])/ (10 * reg$coef["I(x^2)"])
)
```

We can repeat this many times to get a collection of $x^*$ estimates and calculate the standard deviation.

```{r}
get_x_star <- function(i)
{
  row_indices <- sample(seq_len(num_obs), num_obs, replace = TRUE)
  boot_data <- data[row_indices, ]
  reg <- lm(y ~ x + I(x^2), data = boot_data)
  x_star <- (2 - 5 * reg$coef["x"])/ (10 * reg$coef["I(x^2)"])
}
```

```{r}
x_stars <- 
  lapply(
    1:1000,
    get_x_star
  ) %>%
  unlist()
```

Here is the histogram:

```{r}
hist(x_stars, breaks = 30)
```

So, it seems to follow a normal distribution. You can get standard deviation of `x_stars` as an estimate of the SE of $\hat{x}^*$.

```{r}
sd(x_stars)
```
You can get the 95% confidence interval (CI) like below:

```{r}
quantile(x_stars, prob = c(0.025, 0.975))
```


## One more example with a non-parametric model

We now demonstrate how we can use bootstrap to get an estimate of the SE of $\hat{x}^*$ when we use random forest (RF) as our regression method instead of OLS. When RF is used, we do not have any coefficients like the OLS case above. Even then, bootstrap allows us to estimate the SE of $\hat{x}^*$.

The procedure is exactly the same except that we use RF to estimate the production function and also that we need to conduct numerical optimization as no analytical formula is available unlike the case above. 

We first implement a single iteration. 

```{r}
#=== get bootstrapped data ===#
row_indices <- sample(seq_len(num_obs), num_obs, replace = TRUE)
boot_data <- data[row_indices, ]

#=== train RF ===#
reg_rf <- ranger(y ~ x, data = boot_data)
```

Once you train RF, we can predict yield at a range of values of $x$, calculate profit, and then pick the value of $x$ that maximizes the estimated profit. Here is what the estimated production function looks like:

```{r}
#=== create series of x values at which yield will be predicted ===#
eval_data <- data.table(x = seq(0, 3, length = 1000))

#=== predict yield based on the trained RF ===#
eval_data[, y_hat := predict(reg_rf, eval_data)$predictions]

#=== plot ===#
ggplot(data = eval_data) +
  geom_line(aes(y = y_hat, x = x))
```

Well, it is very spiky (we need to tune hyper-parameters using KCV. But, more on this later. The quality of RF estimation has nothing to do with the goal of this section). 

We can now predict profit at each value of $x$.

```{r}
#=== calculate profit ===#
eval_data[, profit_hat := 5 * y_hat - 2 * x]

head(eval_data)
```
The only thing left for us to do is to find the $x$ value that maximizes profit. 


```{r}
eval_data[which.max(profit_hat), ]
```
Okay, so `r eval_data[which.max(profit_hat), x]` is the $\hat{x}^*$ from this iteration.

As you might have guessed already, we can just repeat this step to get an estimate of the SE of $\hat{x}^*_{RF}$.

```{r}
get_x_star_rf <- function(i)
{
  print(i) # progress tracker
  
  #=== get bootstrapped data ===#
  row_indices <- sample(seq_len(num_obs), num_obs, replace = TRUE)
  boot_data <- data[row_indices, ]

  #=== train RF ===#
  reg_rf <- ranger(y ~ x, data = boot_data)

  #=== create series of x values at which yield will be predicted ===#
  eval_data <- data.table(x = seq(0, 3, length = 1000))

  #=== predict yield based on the trained RF ===#
  eval_data[, y_hat := predict(reg_rf, eval_data)$predictions]
  
  #=== calculate profit ===#
  eval_data[, profit_hat := 5 * y_hat - 2 * x]
  
  #=== find x_star_hat ===#
  x_star_hat <- eval_data[which.max(profit_hat), x]
  
  return(x_star_hat)
}

```


```{r}
#| cache: true

x_stars_rf <- 
  mclapply(
    1:1000,
    get_x_star_rf,
    mc.cores = 12
  ) %>%
  unlist()

#=== Windows user ===#
# library(future.apply)
# plan("multisession", workers = detectCores() - 2)
# x_stars_rf <- 
#   future_lapply(
#     1:1000,
#     get_x_star_rf
#   ) %>%
#   unlist()
```

Here are the estimate of the SE of $\hat{x}^*_{RF}$ and 95% CI.

```{r}
sd(x_stars_rf)
quantile(x_stars_rf, prob = c(0.025, 0.975))
```

As you can see, the estimation of $x^*$ is much more inaccurate than the previous OLS approach. This is likely due to the fact that we are not doing a good job of tuning the hyper-parameters of RF (but, again, more on this later).

This conclude the illustration of the power of using bootstrap to estimate the uncertainty of the statistics of interest ($x^*$ here) when the analytical formula of the statistics is non-linear or not even known. 


## Bag of Little Bootstraps

Bag of little bootstraps (BLB) is a variant of bootstrap that has similar desirable statistical properties as bootstrap, but is more (computer) memory friendly [@kleiner_scalable_2014].

When the sample size is large (say 1GB) bootstrap is computationally costly both in terms of time and memory because the data of the same size is bootstrapped for many times (say, 1000 times). BLB overcomes this problem.

Suppose you are interested in $\eta$ (e.g., standard error, confidence interval) of the estimator $\hat{\theta}$. Further, let $N$ denoted the sample size of the training data.

BLB works as follows: 

1. Split the dataset into $S$ sets of subsamples of size $B$ so that $s \times B = N$ <span style="color:blue"> without replacement</span>.
2. For each of the subsample, bootstrap $N$ (<span style="color:red"> not $B$ </span>) samples <span style="color:blue"> with replacement </span> $M$ times, and find $\eta$ (call it $\eta_s$) for each of the subsample sets.
3. Average $\hat{\eta}_s$ to get $\hat{eta}$.


### Demonstrations 

Let's go back to the example discussed in @sec-boot-comp, where the goal is to identify the confidence interval ($\eta$) of the economically optimal input rate ($\theta$).

**<span style="color:blue"> Step 1: </span>**

::: {.column-margin}
Step 1: Split the dataset into $S$ sets of subsamples of size $B$ so that $s \times B = N$ <span style="color:blue"> without replacement</span>
:::

Here, we set $S$ at 5, so $B = `r nrow(data)/5`$. 

```{r}
N <- nrow(data)
S <- 5

(
subsamples <-
  data[sample(seq_len(N), N), ] %>%
  .[, subgroup := rep(1:S, each = N/S)] %>% 
  split(.$subgroup)
)
```

::: {.column-margin}
You could alternatively do this:

```{r}
fold_data <- vfold_cv(data, v = 5)

subsamples <-
  lapply(
    1:5,
    function(x) fold_data[x, ]$splits[[1]] %>% assessment()
  )
```
:::

**<span style="color:blue"> Steps 2: </span>**

::: {.column-margin}
+ Step 2: For each of the subsample, bootstrap $N$ (<span style="color:red"> not $B$ </span>) samples <span style="color:blue"> with replacement </span> $M$ times, and find $\eta$ (call it $\eta_s$) for each of the subsample sets.
:::

Instead of creating bootstrapped samples $M$ times, only one bootstrapped sample from the first subsample is created for now, and then the estimate of $x^*$ is obtained. 

```{r}
(
boot_data <-
  sample(
    seq_len(nrow(subsamples[[1]])),
    N,
    replace = TRUE
  ) %>% 
  subsamples[[1]][., ]
)
```

We can now train a linear model using the bootstrapped data and calculate $\hat{x}^*$ based on it.

```{r}
reg <- lm(y ~ x + I(x^2), data = boot_data)

(
x_star <- (2 - 5 * reg$coef["x"])/ (10 * reg$coef["I(x^2)"])
)
```

We can loop this process over all the $100$ ($M = 100$) bootstrapped data from the fist subsample set. Before that, let's define a function that conducts a single implementation of bootstrapping and estimating $x^*$ from a single subsample set.


```{r}
get_xstar_boot <- function(subsample)
{
  boot_data <-
    sample(
      seq_len(nrow(subsample)),
      N,
      replace = TRUE
    ) %>% 
    subsample[., ]
  reg <- lm(y ~ x + I(x^2), data = boot_data)

  x_star <- (2 - 5 * reg$coef["x"])/ (10 * reg$coef["I(x^2)"])

  return(x_star)
}
```

This is a single iteration.

```{r}
get_xstar_boot(subsamples[[1]])
```

Now, let's repeat this 100 ($M = 100$) times.

```{r}
M <- 100

xstar_hats <-
  lapply(
    1:M,
    function(x) get_xstar_boot(subsamples[[1]])
  ) %>% 
  unlist()
```

We can now get the 95% confidence interval of $\hat{x}^*$.

```{r}
cf_lower <- quantile(xstar_hats, prob = 0.025) 
cf_upper <- quantile(xstar_hats, prob = 0.975) 
```


So, [`r paste0(cf_lower, ",", cf_upper)`] is the 95% confidence interval estimate from the first subsample set. We repeat this for the other subsample sets and average the 95% CI from them (`get_CI()` defined on the side).

::: {.column-margin}
This function find the 95% confidence interval ($\eta$) from a single subsample set.

```{r}
get_CI <- function(subsample){
  xstar_hats <-
    lapply(
      1:M,
      function(x) get_xstar_boot(subsamples[[1]])
    ) %>% 
    unlist() 

  cf_lower <- quantile(xstar_hats, prob = 0.025) 
  cf_upper <- quantile(xstar_hats, prob = 0.975) 

  return_data <-
    data.table(
      cf_lower = cf_lower,
      cf_upper = cf_upper
    )

  return(return_data)
}
```
:::


```{r}
(
ci_data <-
  lapply(
    subsamples,
    function(x) get_CI(x)
  ) %>% 
  rbindlist()
)
```

**<span style="color:blue"> Steps 3: </span>**

By averaging the lower and upper bounds, we get our estimate of the lower and upper bound of the 95% CI of $\hat{x}^*$.

```{r}
ci_data[, .(mean(cf_lower), mean(cf_upper))]
```

:::{.callout-tip}

+ Bag of little bootstraps (BLB) is a variant of bootstrap that has similar desirable statistical properties as bootstrap, but is more computation and memory friendly.

+ For example, the `grf` package uses BLB to estimate the standard error of the treatment effect in Causal Forest estimation.

:::



## References {.unnumbered}

<div id="refs"></div>













