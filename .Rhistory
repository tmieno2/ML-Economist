y_estimates
View(y_estimates)
install.packages("gganimate")
#| include: false
library(tidyverse)
library(data.table)
library(rpart)
library(rattle)
library(wooldridge)
library(gganimate)
#| eval: false
library(tidyverse)
library(data.table)
library(rpart)
library(rattle)
library(wooldridge)
library(gganimate)
#| code-fold: true
#=== get mlb1 data (from wooldridge) ===#
data(mlb1)
#=== build a simple tree ===#
simple_tree <-
rpart(
lsalary ~ hits + runsyr,
data = mlb1,
control = rpart.control(minsplit = 200)
)
fancyRpartPlot(simple_tree)
#| code-fold: true
ggplot(mlb1) +
geom_point(aes(y = hits, x = runsyr, color = lsalary)) +
scale_color_viridis_c() +
geom_hline(yintercept = 262) +
geom_line(
data = data.table(x = 44, y = seq(262, max(mlb1$hits), length = 100)),
aes(y = y, x = x)
) +
annotate(
"text",
x = 44, y = 111,
label = "Region 2",
color = "red"
) +
annotate(
"text",
x = 22, y = 1500,
label = "Region 6",
color = "red"
) +
annotate(
"text",
x = 75, y = 1500,
label = "Region 7",
color = "red"
)
#=== get data ===#
library(wooldridge)
data(mlb1)
mlb1_dt <-
mlb1 %>%
data.table() %>% # turn into data.table
.[, salary := NULL] %>% # remove salary (use lsalary instead)
na.omit() # remove observations with NA in any of the variables
value_seq <-
#=== order hruns values and find the unique values ===#
mlb1_dt[order(hruns), unique(hruns)] %>%
#=== get the rolling mean ===#
frollmean(2) %>%
.[-1]
threshold_data <-
data.table(thre = value_seq) %>%
.[, id := 1:.N]
ggplot(mlb1_dt) +
geom_point(aes(y = lsalary, x = hruns)) +
geom_vline(data = threshold_data, aes(xintercept = thre)) +
transition_time(id)
threshold_data <-
data.table(thre = value_seq) %>%
.[, id := 1:.N]
ggplot(mlb1_dt) +
geom_point(aes(y = lsalary, x = hruns)) +
geom_vline(data = threshold_data, aes(xintercept = thre)) +
transition_time(id, range = 30)
threshold_data <-
data.table(thre = value_seq) %>%
.[, id := 1:.N]
ggplot(mlb1_dt) +
geom_point(aes(y = lsalary, x = hruns)) +
geom_vline(data = threshold_data, aes(xintercept = thre)) +
transition_time(id, range = c(20, 30))
threshold_data <-
data.table(thre = value_seq) %>%
.[, id := 1:.N]
ggplot(mlb1_dt) +
geom_point(aes(y = lsalary, x = hruns)) +
geom_vline(data = threshold_data, aes(xintercept = thre)) +
transition_time(id, range = c(20, 30))
threshold_data <-
data.table(thre = value_seq) %>%
.[, id := 1:.N]
ggplot(mlb1_dt) +
geom_point(aes(y = lsalary, x = hruns)) +
geom_vline(data = threshold_data, aes(xintercept = thre)) +
transition_time(id, range = c(20L, 30L))
threshold_data <-
data.table(thre = value_seq) %>%
.[, id := 1:.N]
g_thre <-
ggplot(mlb1_dt) +
geom_point(aes(y = lsalary, x = hruns)) +
geom_vline(data = threshold_data, aes(xintercept = thre)) +
transition_time(id)
animate(g_thre, nframes = 3 * nrow(threshold_data))
g_thre
ggplot(mlb1_dt) +
geom_point(aes(y = lsalary, x = hruns)) +
geom_vline(data = threshold_data, aes(xintercept = thre)) +
transition_reveal(id)
threshold_data <-
data.table(thre = value_seq) %>%
.[, id := 1:.N]
g_thre <-
ggplot(mlb1_dt) +
geom_point(aes(y = lsalary, x = hruns)) +
geom_vline(data = threshold_data, aes(xintercept = thre)) +
transition_reveal(id)
animate(g_thre, nframes = 3 * nrow(threshold_data))
reticulate::repl_python()
#| eval: false
library(reticulate)
use_virtualenv("ml-learning")
#=== econml.dml ===#
em_dml <- import("econml.dml")
#| include: false
library(data.table)
library(magick)
library(fixest)
library(officer)
library(dplyr)
library(ggplot2)
library(reticulate)
library(DoubleML)
library(MASS)
#| eval: false
library(data.table)
library(magick)
library(fixest)
library(officer)
library(dplyr)
library(ggplot2)
library(reticulate)
library(DoubleML)
library(MASS)
#| eval: false
library(reticulate)
use_virtualenv("ml-learning")
reticulate::repl_python()
#=== sklearn ===#
sl <- import("sklearn")
gbr <- sl$ensemble$GradientBoostingRegressor
lassocv <- sl$linear_model$LassoCV
#| eval: false
#=== sample size ===#
N <- 1000
#=== generate data ===#
synth_data <-
gen_data(
te_formula = formula(~ I(exp(x1)*d)),
n_obs = N *2
)
gen_data <- function(
g_formula = formula(~ I(exp(x1)/(1+exp(x1))) + I(x3/4)), # formula that defines g(x)
m_formula = formula(~ x1 + I(exp(x3)/(1+exp(x3))/4)), # formula that defines m(x)
te_formula = formula(~ I(0.5*d)), # formula that defines theta(x) * t
n_obs = 500,
n_vars = 20,
mu_x = 0,
vcov_x = NULL,
sigma = 1 # sd of the error term in the y equation
)
{
if (is.null(vcov_x)) {
vcov_x <- matrix(rep(0, n_vars^2), nrow = n_vars)
for (i in seq_len(n_vars)) {
vcov_x[i, ] <- 0.7^abs(i - seq_len(n_vars))
}
}
#=== draw from multivariate normal ===#
data <-
mvrnorm(n_obs, mu = rep(0, n_vars), Sigma = vcov_x) %>%
data.table() %>%
setnames(names(.), paste0("x", 1:n_vars))
#=== generate d ===#
if (m_formula == "independent") {
data[, d := rnorm(n_obs)]
} else {
data[, d := model.frame(m_formula, data = data) %>% rowSums() + rnorm(n_obs)]
}
#=== generate y ===#
data[, g := model.frame(g_formula, data = data) %>% rowSums()]
#=== generate treatment effect ===#
data[, te := model.frame(te_formula, data = data) %>% rowSums()]
#=== generate y ===#
data[, y := te + g + rnorm(n_obs, sd = sigma)]
return(data[])
}
#| eval: false
#=== sample size ===#
N <- 1000
#=== generate data ===#
synth_data <-
gen_data(
te_formula = formula(~ I(exp(x1)*d)),
n_obs = N *2
)
X <- select(synth_data, starts_with("x"))
X <- dplur::select(synth_data, starts_with("x"))
X <- dplyr::select(synth_data, starts_with("x"))
X <- dplyr::select(synth_data, starts_with("x"))
y <- dplyr::select(synth_data, y)
y
X
X <- dplyr::select(synth_data, starts_with("x")) %>% as.matrix()
X
reticulate::repl_python()
#| eval: false
#=== sample size ===#
N <- 1000
#=== generate data ===#
synth_data <-
gen_data(
te_formula = formula(~ I(exp(x1)*d)),
n_obs = N *2
)
X <- dplyr::select(synth_data, starts_with("x")) %>% as.matrix()
y <- synth_data[, y]
d <- synth_data[, d]
reticulate::repl_python()
#| eval: false
ggplot(test_data) +
geom_point(aes(y = py$te_test, x = X_test[1, ])) +
geom_line(aes(y = exp(x1), x = x1), color = "blue") +
theme_bw()
#| eval: false
ggplot() +
geom_point(aes(y = py$te_test, x = X_test[1, ])) +
geom_line(aes(y = exp(x1), x = x1), color = "blue") +
theme_bw()
#| eval: false
ggplot() +
geom_point(aes(y = py$te_test, x = py$X_test[1, ])) +
geom_line(aes(y = exp(x1), x = x1), color = "blue") +
theme_bw()
py$X_test[, 1]
#| eval: false
ggplot() +
geom_point(aes(y = py$te_test, x = py$X_test[, 1])) +
geom_line(aes(y = exp(x1), x = x1), color = "blue") +
theme_bw()
#| eval: false
plot_data <-
data.table(
x1 = py$X_test[, 1],
te = py$te_test
)
ggplot(plot_data) +
geom_point(aes(y = te, x = x1)) +
geom_line(aes(y = exp(x1), x = x1), color = "blue") +
theme_bw()
#| fig-cap: Estimated and true marginal treatment effects
#| label: fig-est-theta-hat
plot_data <-
data.table(
x1 = py$X_test[, 1],
te = py$te_test
)
g_het_te <-
ggplot(plot_data) +
geom_point(aes(y = te, x = x1)) +
geom_line(aes(y = exp(x1), x = x1), color = "blue") +
theme_bw()
g_het_te <- readRDS("g_hte_te.rds")
g_het_te
library(reticulate)
repl_python()
library(reticulate)
sk_ensemble <- import("sklearn.ensemble")
RFR <- sk_ensemble$RandomForestRegressor
GBR <- sk_ensemble$GradientBoostingRegressor
sk_lm <- import("sklearn.linear_model")
Lasso <- sk_lm$Lasso
sk_ms <- import("sklearn.model_selection")
RepatedKF <- sk_ms$RepeatedKFold
econml_sk_ms <- import("econml.sklearn_extensions.model_selection")
GridSearchCVList <- econml_sk_ms$GridSearchCVList
sk_data <- import("sklearn.datasets")
make_regression <- sk_data$make_regression
np <- import("numpy")
#=== set parameters for data generation ===#
n_samples <- as.integer(2000)
n_features <- as.integer(20)
n_informative <- as.integer(15)
noise <- as.integer(2)
rng <- np$random$RandomState(as.integer(8934))
#=== generate synthetic data ===#
data <-
make_regression(
n_samples,
n_features,
n_informative = n_informative,
noise = noise,
random_state = rng
)
XT <- data[[1]]
T <- XT[, 1]
X <- XT[, -1]
y  <- data[[2]]
est_list <-
list(
Lasso(max_iter=10000),
GBR(),
RFR(min_samples_leaf = 5)
)
par_grid_list <- list(
alpha = c(0.001, 0.01, 0.1, 1, 10),
list(max_depth = c(3, 5, NA), n_estimators = c(50, 100, 200)),
list(max_depth = c(3, 5, 10), max_features = c(3, 5, 10, 20))
)
reticulate::repl_python()
est_list <-
list(
Lasso(max_iter=10000),
GBR(),
RFR(min_samples_leaf = 5)
)
par_grid_list <- list(
list(alpha = c(0.001, 0.01, 0.1, 1, 10)),
list(max_depth = c(3, 5, NA), n_estimators = c(50, 100, 200)),
list(max_depth = c(3, 5, 10), max_features = c(3, 5, 10, 20))
)
reticulate::repl_python()
#| echo: false
max_depth <- c(3, 5, 10)
max_features <- c(3, 5, 10, 20)
expand.grid(max_depth = max_depth, max_features = max_features)
reticulate::repl_python()
est_list <-
c(
sk_lm$Lasso(max_iter=10000),
sk_ensemble$GradientBoostingRegressor(),
sk_ensemble$RandomForestRegressor(min_samples_leaf = 5)
)
par_grid_list <-
list(
list(alpha = c(0.001, 0.01, 0.1, 1, 10)),
list(max_depth = c(3, 5, NA), n_estimators = c(50, 100, 200)),
list(max_depth = c(3, 5, 10), max_features = c(3, 5, 10, 20))
)
rk_cv <-
sk_ms$RepeatedKFold(
n_splits = as.integer(4),
n_repeats = as.integer(2),
random_state = as.integer(123)
)
reticulate::repl_python()
est_list <-
c(
sk_lm$Lasso(max_iter=as.integer(10000)),
sk_ensemble$GradientBoostingRegressor(),
sk_ensemble$RandomForestRegressor(min_samples_leaf = as.integer(5))
)
par_grid_list <-
list(
list(alpha = c(0.001, 0.01, 0.1, 1, 10)),
list(max_depth = c(3, 5, NA), n_estimators = c(50, 100, 200)),
list(max_depth = c(3, 5, 10), max_features = c(3, 5, 10, 20))
)
rk_cv <-
sk_ms$RepeatedKFold(
n_splits = as.integer(4),
n_repeats = as.integer(2),
random_state = as.integer(123)
)
reticulate::repl_python()
est_list <-
c(
sk_lm$Lasso(max_iter=as.integer(10000)),
sk_ensemble$GradientBoostingRegressor(),
sk_ensemble$RandomForestRegressor(min_samples_leaf = as.integer(5))
)
par_grid_list <-
list(
list(alpha = c(0.001, 0.01, 0.1, 1, 10)),
list(max_depth = c(3, 5, NULL), n_estimators = c(50, 100, 200)),
list(max_depth = c(3, 5, 10), max_features = c(3, 5, 10, 20))
)
rk_cv <-
sk_ms$RepeatedKFold(
n_splits = as.integer(4),
n_repeats = as.integer(2),
random_state = as.integer(123)
)
reticulate::repl_python()
est_list <-
c(
sk_lm$Lasso(max_iter=as.integer(10000)),
sk_ensemble$GradientBoostingRegressor(),
sk_ensemble$RandomForestRegressor(min_samples_leaf = as.integer(5))
)
par_grid_list <-
list(
list(alpha = c(0.001, 0.01, 0.1, 1, 10)),
list(max_depth = as.integer(c(3, 5, 10)), n_estimators = c(50, 100, 200)),
list(max_depth = c(3, 5, 10), max_features = c(3, 5, 10, 20))
)
rk_cv <-
sk_ms$RepeatedKFold(
n_splits = as.integer(4),
n_repeats = as.integer(2),
random_state = as.integer(123)
)
reticulate::repl_python()
est_list <-
c(
sk_lm$Lasso(max_iter=as.integer(10000)),
sk_ensemble$GradientBoostingRegressor(),
sk_ensemble$RandomForestRegressor(min_samples_leaf = as.integer(5))
)
par_grid_list <-
list(
list(alpha = c(0.001, 0.01, 0.1, 1, 10)),
list(max_depth = as.integer(c(3, 5, 10)), n_estimators = as.integer(c(50, 100, 200))),
list(max_depth = as.integer(c(3, 5, 10)), max_features = as.integer(c(3, 5, 10, 20)))
)
rk_cv <-
sk_ms$RepeatedKFold(
n_splits = as.integer(4),
n_repeats = as.integer(2),
random_state = as.integer(123)
)
reticulate::repl_python()
first_stage <-
econml_sk_ms$GridSearchCVList(
estimator_list = est_list,
param_grid_list= par_grid_list,
cv = rk_cv
)
first_stage$fit(X, y)
first_stage.best_estimator_
first_stage$best_estimator_
model_y <- first_stage$best_estimator_
model_t <- first_stage$fit(X, T)$best_estimator_
est_list <-
c(
sk_lm$Lasso(max_iter=as.integer(1000)),
sk_ensemble$GradientBoostingRegressor(),
sk_ensemble$RandomForestRegressor(min_samples_leaf = as.integer(5))
)
par_grid_list <-
list(
list(alpha = c(0.001, 0.01, 0.1, 1, 10)),
list(
max_depth = as.integer(c(3, 5, 10)),
n_estimators = as.integer(c(50, 100, 200))
),
list(
max_depth = as.integer(c(3, 5, 10)),
max_features = as.integer(c(3, 5, 10, 20))
)
)
rk_cv <-
sk_ms$RepeatedKFold(
n_splits = as.integer(4),
n_repeats = as.integer(2),
random_state = as.integer(123)
)
first_stage <-
econml_sk_ms$GridSearchCVList(
estimator_list = est_list,
param_grid_list= par_grid_list,
cv = rk_cv
)
model_y <- first_stage$fit(X, y)$best_estimator_
model_t <- first_stage$fit(X, T)$best_estimator_
#=== set up a linear DML ===#
est = LinearDML(
model_y=model_y,
model_t=model_t
)
sk_ensemble <- import("sklearn.ensemble")
sk_lm <- import("sklearn.linear_model")
sk_ms <- import("sklearn.model_selection")
econml_sk_ms <- import("econml.sklearn_extensions.model_selection")
econml_dml <- import("econml.dml")
est = econml_dml$LinearDML(
model_y=model_y,
model_t=model_t
)
#=== train ===#
est.fit(Y, T, X=X, W=X)
#=== train ===#
est$fit(Y, T, X=X, W=X)
#=== train ===#
est$fit(y, T, X=X, W=X)
reticulate::repl_python()
install.packages("mlr3")
reticulate::repl_python()
#| echo: false
max_depth <- c(3, 5, 10)
max_features <- c(3, 5, 10, 20)
expand.grid(max_depth = max_depth, max_features = max_features)
reticulate::repl_python()
