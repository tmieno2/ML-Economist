n_features,
n_informative = n_informative,
noise = noise,
random_state = rng
)
T = XT[:, 0] # first column as the treatment variable
X = XT[:, 1:] # the rest as X
est_list = [
Lasso(max_iter=10000),
GradientBoostingRegressor(),
RandomForestRegressor(min_samples_leaf = 5)
]
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.linear_model import Lasso
from sklearn.model_selection import RepeatedKFold
from econml.sklearn_extensions.model_selection import GridSearchCVList
est_list = [
Lasso(max_iter=10000),
GradientBoostingRegressor(),
RandomForestRegressor(min_samples_leaf = 5)
]
est_list
par_grid_list = [
{"alpha": [0.001, 0.01, 0.1, 1, 10]},
{"max_depth": [3, 5, None], "n_estimators": [50, 100, 200]},
{"max_depth": [3, 5, 10], "max_features": [3, 5, 10, 20]},
]
par_grid_list
rk_cv = RepeatedKFold(n_splits = 4, n_repeats = 2, random_state = 123)
#| results: hide
#| cache: true
first_stage.fit(X, y)
r.est_list
est_list
par_grid_list
r.est_list
r.par_grid_list
r.par_grid_list
r.par_grid_list
r.par_grid_list
#| cache: true
model_y = first_stage.fit(X, y).best_estimator_
model_y
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.linear_model import Lasso
from sklearn.model_selection import RepeatedKFold
from econml.sklearn_extensions.model_selection import GridSearchCVList
from sklearn.datasets import make_regression
import numpy as np
#=== set parameters for data generation ===#
n_samples, n_features, n_informative, noise = 2000, 20, 15, 2
rng = np.random.RandomState(8934)
#=== generate synthetic data ===#
XT, y = make_regression(
n_samples,
n_features,
n_informative = n_informative,
noise = noise,
random_state = rng
)
T = XT[:, 0] # first column as the treatment variable
X = XT[:, 1:] # the rest as X
est_list = [
Lasso(max_iter=1000),
GradientBoostingRegressor(),
RandomForestRegressor(min_samples_leaf = 5)
]
par_grid_list = [
{"alpha": [0.001, 0.01, 0.1, 1, 10]},
{"max_depth": [3, 5, None], "n_estimators": [50, 100, 200]},
{"max_depth": [3, 5, 10], "max_features": [3, 5, 10, 20]},
]
rk_cv = RepeatedKFold(n_splits = 4, n_repeats = 2, random_state = 123)
temp = rk_cv.split(X)
View(temp)
rk_cv = RepeatedKFold(n_splits = 4, n_repeats = 2, random_state = 123)
rk_cv.split(X)
View(rk_cv)
rk_cv = RepeatedKFold(n_splits = 4, n_repeats = 2, random_state = 123)
rk_cv.split(X)
print(rk_cv)
for train_index, test_index in rk_cv(X):
print(“TRAIN:”, train_index, “TEST:”, test_index)
rk_cv = RepeatedKFold(n_splits = 4, n_repeats = 2, random_state = 123)
for train_index, test_index in rk_cv(X):
print(“TRAIN:”, train_index, “TEST:”, test_index)
rk_cv = RepeatedKFold(n_splits = 4, n_repeats = 2, random_state = 123)
for train_index, test_index in rk_cv.split(X):
print(“TRAIN:”, train_index, “TEST:”, test_index)
rk_cv = RepeatedKFold(n_splits = 4, n_repeats = 2, random_state = 123)
for train_index, test_index in rk_cv.split(X):
print(“TRAIN:”, train_index, “TEST:”, test_index)
rk_cv = RepeatedKFold(n_splits = 4, n_repeats = 2, random_state = 123)
for train_index, test_index in rk_cv.split(X):
print(“TRAIN:”, train_index, “TEST:”, test_index)
rk_cv.split(X)
splits = rk_cv.split(X)
splits
rk_cv = KFold(n_splits = 4, random_state = 123)
from sklearn.model_selection import KFold
rk_cv = KFold(n_splits = 4, random_state = 123)
rk_cv = KFold(n_splits = 4)
splits = rk_cv.split(X)
splits
View(splits)
X
splits
View(splits)
View(splits)
View(`T`)
View(temp)
View(X)
View(X)
View(splits)
X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]]) # create an array
y = np.array([1, 2, 3, 4]) # Create another array
kf = KFold(n_splits=2) # Define the split - into 2 folds
kf.get_n_splits(X) # returns the number of splitting iterations in the cross-validator
for train_index, test_index in kf.split(X):
print(“TRAIN:”, train_index, “TEST:”, test_index)
for train_index, test_index in kf.split(X):
print(“TRAIN:”, train_index, “TEST:”, test_index)
kf.split(X)
for train_index, test_index in kf.split(X):
print(“TRAIN:”, train_index, “TEST:”, test_index)
for train_index, test_index in kf.split(X):
print(“TRAIN:”, train_index, “TEST:”, test_index)
rk_cv = RepeatedKFold(n_splits = 4, n_repeats = 2, random_state = 123)
from sklearn.model_selection import KFold
rk_cv = KFold(n_splits = 4)
splits = rk_cv.split(X)
for train_index, test_index in rk_cv.split(X):
print(“TRAIN:”, train_index, “TEST:”, test_index)
X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]]) # create an array
y = np.array([1, 2, 3, 4]) # Create another array
kf = KFold(n_splits=2) # Define the split - into 2 folds
kf.get_n_splits(X) # returns the number of splitting iterations in the cross-validator
for train_index, test_index in kf.split(X):
print(“TRAIN:”, train_index, “TEST:”, test_index)
for train_index, test_index in kf.split(X):
X_train, X_test = X[train_index], X[test_index]
y_train, y_test = y[train_index], y[test_index]
rk_cv = RepeatedKFold(n_splits = 4, n_repeats = 2, random_state = 123)
from sklearn.model_selection import KFold
rk_cv = KFold(n_splits = 4)
splits = rk_cv.split(X)
for train_index, test_index in rk_cv.split(X):
print(“TRAIN:”, train_index, “TEST:”, test_index)
X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]]) # create an array
y = np.array([1, 2, 3, 4]) # Create another array
kf = KFold(n_splits=2) # Define the split - into 2 folds
kf.get_n_splits(X) # returns the number of splitting iterations in the cross-validator
for train_index, test_index in kf.split(X):
X_train, X_test = X[train_index], X[test_index]
y_train, y_test = y[train_index], y[test_index]
X_train
X_test
y_train
y_test
for train_index, test_index in kf.split(X):
X_train, X_test = X[train_index], X[test_index]
X_train
X_test
from sklearn.datasets import make_regression
import numpy as np
#=== set parameters for data generation ===#
n_samples, n_features, n_informative, noise = 2000, 20, 15, 2
rng = np.random.RandomState(8934)
#=== generate synthetic data ===#
XT, y = make_regression(
n_samples,
n_features,
n_informative = n_informative,
noise = noise,
random_state = rng
)
T = XT[:, 0] # first column as the treatment variable
X = XT[:, 1:] # the rest as X
rk_cv = RepeatedKFold(n_splits = 4, n_repeats = 2, random_state = 123)
from sklearn.model_selection import KFold
rk_cv = KFold(n_splits = 4)
splits = rk_cv.split(X)
for train_index, test_index in rk_cv.split(X):
X_train, X_test = X[train_index], X[test_index]
View(X_test)
View(X_test)
X_train.shape
for train_index, test_index in rk_cv.split(X):
X_train, X_test = X[train_index], X[test_index]
X_train
splits = rk_cv.split(X)
next(splits)
next(splits)
next(splits)
next(splits)
next(splits)
splits = rk_cv.split(X)
a = next(splits)
View(a)
for train, test in rk_cv.split(X):
X_train, X_test = X[train], X[test]
splits.__next()__
splits.__next__()
a = splits.__next__()
View(a)
a[1]
a[2]
a[0]
a[1]
a = list(splits)
View(a)
a.shape
a.shape)_
a.shape()
a.len
View(a)
a[0]
a[0][0]
rk_cv.n_splits
rk_cv = RepeatedKFold(n_splits = 4, n_repeats = 3, random_state = 123)
rk_cv.n_splits
rk_cv = RepeatedKFold(n_splits = 4, n_repeats = 3, random_state = 123)
rk_cv.n_splits
rk_cv = RepeatedKFold(n_splits = 4, n_repeats = 3, random_state = 123)
rk_cv.get_n_splits
rk_cv.get_n_splits.n_splits
rk_cv.get_n_splits(.n_splits))
rk_cv.get_n_splits()
#| include: false
library(data.table)
library(DoubleML)
library(mlr3verse)
library(parallel)
mlr_learners
library(mlr3verse)
#| include: false
library(data.table)
library(DoubleML)
library(mlr3verse)
library(parallel)
#| eval: false
library(data.table)
library(DoubleML)
library(mlr3verse)
library(parallel)
#=== load mtcars ===#
data("mtcars", package = "datasets")
#=== see the first 6 rows ===#
head(mtcars)
(
reg_task <-
TaskRegr$new(
id = "example",
backend = mtcars,
target = "mpg"
)
)
reg_task$col_roles
#=== row ids ===#
reg_task$row_ids
#=== data ===#
reg_task$data()
reg_task$data(rows = 1:10, cols = c("mpg", 'wt'))
(
data_extracted <- as.data.table(reg_task)
)
#=== first select few variables ===#
reg_task$select(c("am", "carb", "cyl"))
#=== see the backend now ===#
reg_task$data()
#=== create a dataset ===#
data_temp <- reg_task$data()
#=== select mpg, carb ===#
dplyr::select(data_temp, mpg, carb)
#=== look at data_temp ===#
data_temp
#=== create a clone ===#
reg_task_independent <- reg_task$clone()
#=== filter ===#
reg_task$filter(1:10)
#=== see the backend ===#
reg_task$data()
reg_task_independent$data()
reg_task$rbind(
data.table(mpg = 20, am = 1, carb = 3, cyl = 99)
)
#=== see the change ===#
reg_task$data()
mlr_learners
learner <- lrn("regr.ranger")
learner$param_set
learner$param_set$values
#=== set max.depth to 5 ===#
learner$param_set$values$max.depth <- 5
#=== see the values ===#
learner$param_set$values
#=== create a named vector ===#
parameter_values <- list("min.node.size" = 10, "mtry" = 5, "num.trees" = 500)
#=== assign them ===#
learner$param_set$values <- parameter_values
#=== see the values ===#
learner$param_set$values
#=== define a task ===#
reg_task <-
TaskRegr$new(
id = "example",
backend = mtcars,
target = "mpg"
)
#=== set up a learner ===#
learner <- lrn("regr.ranger")
learner$param_set$values <-
list(
"min.node.size" = 10,
"mtry" = 5,
"num.trees" = 500
)
learner$model
learner$train(reg_task)
learner$model
#=== extract row ids ===#
row_ids <- reg_task$row_ids
#=== train ===#
train_ids <- row_ids[1:(length(row_ids) / 2)]
#=== test ===#
test_ids <- row_ids[!(row_ids %in% train_ids)]
#=== train ===#
learner$train(reg_task, row_ids = train_ids)
#=== seed the trained model ===#
learner$model
prediction <- learner$predict(reg_task, row_ids = test_ids)
as.data.table(prediction)
prediction <- learner$predict_newdata(mtcars)
mlr_measures
#=== get a measure ===#
measure <- msr("regr.mse")
#=== check the class ===#
class(measure)
prediction$score(measure)
prediction <- learner$predict(reg_task, row_ids = test_ids)
as.data.table(prediction)
prediction <- learner$predict_newdata(mtcars)
mlr_measures
#=== get a measure ===#
measure <- msr("regr.mse")
#=== check the class ===#
class(measure)
prediction$score(measure)
as.data.table(mlr_resamplings)
(
resampling <- rsmp("repeated_cv", repeats = 2, folds = 3)
)
#=== update ===#
resampling$param_set$values = list(repeats = 3, folds = 4)
#=== see the updates ===#
resampling
resampling$instantiate(reg_task)
View(parameter_values)
View(parameter_values)
install.packages("Ecdat")
install.packages("mltools")
#| include: false
library(data.table)
library(tidyverse)
library(grf)
#| eval: false
library(data.table)
library(tidyverse)
library(grf)
set.seed(293)
N <- 1000
(
data <-
data.table(
x1 = rnorm(N),
x2 = runif(N) + 1,
e = rnorm(N), # error term
T = runif(N) > 0.5 # treatment that is independent
) %>%
.[, y := (x1 + 1/x2)*T + e]
)
#| fig-cap: Example trees built in causal forest estimation
#| label: fig-cf-trees
#| code-fold: true
#| fig-subcap:
#|   - "Tree 1"
#|   - "Tree 2"
cf_trained <-
causal_forest(
X = data[, .(x1, x2)],
Y = data[, y],
W = data[, T],
min.node.size = 30
)
get_tree(cf_trained, 1) %>% plot()
get_tree(cf_trained, 2000) %>% plot()
get_tree(cf_trained, 1) %>% plot()
get_tree(cf_trained, 2000) %>% plot()
get_tree(cf_trained, 1) %>% plot()
get_tree(cf_trained, 2000) %>% plot()
library(devtools)
install_github("xnie/rlearner")
library(mlr3verse)
install.packages("measurements")
library(reticulate)
#| eval: false
use_virtualenv("ml-learning")
#| eval: false
repl_python()
#| eval: false
use_virtualenv("ml-learning")
library(reticulate)
#| eval: false
use_virtualenv("ml-learning")
#| eval: false
use_virtualenv(here::here("ml-learning"))
reticulate::repl_python()
#| include: false
library(reticulate)
use_virtualenv(here::here("ml-learning"))
reticulate::repl_python()
#| include: false
library(reticulate)
use_virtualenv(here::here("ml-learning"))
#| include: false
library(reticulate)
use_virtualenv(here::here("ml-learning"))
use_python("Users/tmieno2/Library/r-miniconda-arm64/bin/python")
use_python("/Users/tmieno2/Library/r-miniconda-arm64/bin/python")
use_python("/Users/tmieno2/Library/r-miniconda-arm64/envs/r-reticulate/bin/python")
use_python("/Users/tmieno2/Library/r-miniconda-arm64//envs/r-reticulate/bin/python")
#| include: false
library(reticulate)
use_python("/Users/tmieno2/Library/r-miniconda-arm64/envs/r-reticulate/bin/python")
use_virtualenv(here::here("ml-learning"))
use_python("/Users/tmieno2/Library/r-miniconda-arm64/envs/r-reticulate/bin/python")
use_virtualenv(here::here("ml-learning"))
reticulate::repl_python()
#| include: false
library(reticulate)
use_virtualenv(here::here("ml-learning"))
#| include: false
library(reticulate)
use_virtualenv(here::here("ml-learning"))
py_install("llvmlite")
py_install("econml")
use_condaenv()
use_condaenv("/opt/anaconda3/bin/python")
#| include: false
library(reticulate)
use_condaenv("/opt/anaconda3/bin/python")
library(reticulate)
use_condaenv("/opt/anaconda3/bin/python")
#| include: false
library(reticulate)
# use_virtualenv(here::here("ml-learning"))
use_condaenv("/opt/anaconda3/bin/python")
conda_install("wheel")
conda_install(envname = "/opt/anaconda3/bin/python", c("wheel", "numpy", "scikitlearn", "econml"))
conda_create(envname = "ml-learning-conda", packages = c("wheel", "numpy", "scikitlearn", "econml"))
conda_create(envname = "ml-learning-conda", packages = c("wheel", "numpy", "scikitlearn", "econml"))
conda_create(envname = "ml-learning-conda", packages = c("wheel", "numpy", "scikit-learn", "econml"))
# use_virtualenv(here::here("ml-learning"))
use_condaenv("/opt/anaconda3/bin/python")
use_condaenv("ml-learning-conda")
#| include: false
library(reticulate)
use_condaenv("ml-learning-conda")
reticulate::repl_python()
conda_create(envname = "Dropbox/TeachingUNL/MachineLearning/LectureNotes/ml-learning-conda", packages = c("wheel", "numpy", "scikit-learn", "econml"))
conda_create(envname = "ml-learning-conda", packages = c("wheel", "numpy", "scikit-learn", "econml"))
conda_create(envname = "/Users/tmieno2/Dropbox/TeachingUNL/MachineLearning/LectureNotes/ml-learning-conda", packages = c("wheel", "numpy", "scikit-learn", "econml"))
use_condaenv("ml-learning-conda")
#| include: false
library(reticulate)
use_condaenv("ml-learning-conda")
use_condaenv("ml-learning-conda")
conda_install("plotnine")
conda_install("ml-learning-conda", "plotnine")
use_condaenv("ml-learning-conda")
conda_install("ml-learning-conda", "plotnine")
#| include: false
library(reticulate)
use_condaenv(here::here("ml-learning-conda"))
conda_install("ml-learning-conda", "plotnine")
here::here("ml-learning-conda")
use_condaenv(here::here("ml-learning-conda"))
conda_install(here::here("ml-learning-conda"), "plotnine")
#| include: false
library(reticulate)
use_condaenv(here::here("ml-learning-conda"))
reticulate::repl_python()
#| include: false
library(reticulate)
use_condaenv(here::here("ml-learning-conda"))
conda_install(here::here("ml-learning-conda"), "skgrf")
#| include: false
library(reticulate)
virtualenv_create("/Users/tmieno2/Dropbox/TeachingUNL/MachineLearning/LectureNotes/ml-learning", packages = c("numpy", "scikit-learn", "skgrf", "econml", "plotnine", "pandas"))
virtualenv_create("/Users/tmieno2/Dropbox/TeachingUNL/MachineLearning/LectureNotes/ml-learning", packages = c("wheel", "numpy", "scikit-learn", "skgrf", "econml", "plotnine", "pandas"))
virtualenv_create("/Users/tmieno2/Dropbox/TeachingUNL/MachineLearning/LectureNotes/ml-learning", packages = c("wheel", "numpy", "scikit-learn", "skgrf", "econml", "plotnine", "pandas"))
virtualenv_create("/Users/tmieno2/Dropbox/TeachingUNL/MachineLearning/LectureNotes/ml-learning", packages = c("wheel", "numpy", "scikit-learn"))
virtualenv_install("/Users/tmieno2/Dropbox/TeachingUNL/MachineLearning/LectureNotes/ml-learning", packages = c("econml"))
virtualenv_install("/Users/tmieno2/Dropbox/TeachingUNL/MachineLearning/LectureNotes/ml-learning", packages = c("skgrf"))
# use_condaenv(here::here("ml-learning-conda"))
use_virtualenv(here::here("ml-learning"))
#| include: false
library(reticulate)
# use_condaenv(here::here("ml-learning-conda"))
use_virtualenv(here::here("ml-learning"))
