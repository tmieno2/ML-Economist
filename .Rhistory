import econml
R.par_grid_list
r.par_grid_list
r.par_grid_list
from sklearn.datasets import make_regression
import numpy as np
#=== set parameters for data generation ===#
n_samples, n_features, n_informative, noise = 2000, 20, 15, 2
rng = np.random.RandomState(8934)
#=== generate synthetic data ===#
XT, y = make_regression(
n_samples,
n_features,
n_informative = n_informative,
noise = noise,
random_state = rng
)
T = XT[:, 0] # first column as the treatment variable
X = XT[:, 1:] # the rest as X
est_list = [
Lasso(max_iter=10000),
GradientBoostingRegressor(),
RandomForestRegressor(min_samples_leaf = 5)
]
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.linear_model import Lasso
from sklearn.model_selection import RepeatedKFold
from econml.sklearn_extensions.model_selection import GridSearchCVList
est_list = [
Lasso(max_iter=10000),
GradientBoostingRegressor(),
RandomForestRegressor(min_samples_leaf = 5)
]
est_list
par_grid_list = [
{"alpha": [0.001, 0.01, 0.1, 1, 10]},
{"max_depth": [3, 5, None], "n_estimators": [50, 100, 200]},
{"max_depth": [3, 5, 10], "max_features": [3, 5, 10, 20]},
]
par_grid_list
rk_cv = RepeatedKFold(n_splits = 4, n_repeats = 2, random_state = 123)
#| results: hide
#| cache: true
first_stage.fit(X, y)
r.est_list
est_list
par_grid_list
r.est_list
r.par_grid_list
r.par_grid_list
r.par_grid_list
r.par_grid_list
#| cache: true
model_y = first_stage.fit(X, y).best_estimator_
model_y
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.linear_model import Lasso
from sklearn.model_selection import RepeatedKFold
from econml.sklearn_extensions.model_selection import GridSearchCVList
from sklearn.datasets import make_regression
import numpy as np
#=== set parameters for data generation ===#
n_samples, n_features, n_informative, noise = 2000, 20, 15, 2
rng = np.random.RandomState(8934)
#=== generate synthetic data ===#
XT, y = make_regression(
n_samples,
n_features,
n_informative = n_informative,
noise = noise,
random_state = rng
)
T = XT[:, 0] # first column as the treatment variable
X = XT[:, 1:] # the rest as X
est_list = [
Lasso(max_iter=1000),
GradientBoostingRegressor(),
RandomForestRegressor(min_samples_leaf = 5)
]
par_grid_list = [
{"alpha": [0.001, 0.01, 0.1, 1, 10]},
{"max_depth": [3, 5, None], "n_estimators": [50, 100, 200]},
{"max_depth": [3, 5, 10], "max_features": [3, 5, 10, 20]},
]
rk_cv = RepeatedKFold(n_splits = 4, n_repeats = 2, random_state = 123)
temp = rk_cv.split(X)
View(temp)
rk_cv = RepeatedKFold(n_splits = 4, n_repeats = 2, random_state = 123)
rk_cv.split(X)
View(rk_cv)
rk_cv = RepeatedKFold(n_splits = 4, n_repeats = 2, random_state = 123)
rk_cv.split(X)
print(rk_cv)
for train_index, test_index in rk_cv(X):
print(“TRAIN:”, train_index, “TEST:”, test_index)
rk_cv = RepeatedKFold(n_splits = 4, n_repeats = 2, random_state = 123)
for train_index, test_index in rk_cv(X):
print(“TRAIN:”, train_index, “TEST:”, test_index)
rk_cv = RepeatedKFold(n_splits = 4, n_repeats = 2, random_state = 123)
for train_index, test_index in rk_cv.split(X):
print(“TRAIN:”, train_index, “TEST:”, test_index)
rk_cv = RepeatedKFold(n_splits = 4, n_repeats = 2, random_state = 123)
for train_index, test_index in rk_cv.split(X):
print(“TRAIN:”, train_index, “TEST:”, test_index)
rk_cv = RepeatedKFold(n_splits = 4, n_repeats = 2, random_state = 123)
for train_index, test_index in rk_cv.split(X):
print(“TRAIN:”, train_index, “TEST:”, test_index)
rk_cv.split(X)
splits = rk_cv.split(X)
splits
rk_cv = KFold(n_splits = 4, random_state = 123)
from sklearn.model_selection import KFold
rk_cv = KFold(n_splits = 4, random_state = 123)
rk_cv = KFold(n_splits = 4)
splits = rk_cv.split(X)
splits
View(splits)
X
splits
View(splits)
View(splits)
View(`T`)
View(temp)
View(X)
View(X)
View(splits)
X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]]) # create an array
y = np.array([1, 2, 3, 4]) # Create another array
kf = KFold(n_splits=2) # Define the split - into 2 folds
kf.get_n_splits(X) # returns the number of splitting iterations in the cross-validator
for train_index, test_index in kf.split(X):
print(“TRAIN:”, train_index, “TEST:”, test_index)
for train_index, test_index in kf.split(X):
print(“TRAIN:”, train_index, “TEST:”, test_index)
kf.split(X)
for train_index, test_index in kf.split(X):
print(“TRAIN:”, train_index, “TEST:”, test_index)
for train_index, test_index in kf.split(X):
print(“TRAIN:”, train_index, “TEST:”, test_index)
rk_cv = RepeatedKFold(n_splits = 4, n_repeats = 2, random_state = 123)
from sklearn.model_selection import KFold
rk_cv = KFold(n_splits = 4)
splits = rk_cv.split(X)
for train_index, test_index in rk_cv.split(X):
print(“TRAIN:”, train_index, “TEST:”, test_index)
X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]]) # create an array
y = np.array([1, 2, 3, 4]) # Create another array
kf = KFold(n_splits=2) # Define the split - into 2 folds
kf.get_n_splits(X) # returns the number of splitting iterations in the cross-validator
for train_index, test_index in kf.split(X):
print(“TRAIN:”, train_index, “TEST:”, test_index)
for train_index, test_index in kf.split(X):
X_train, X_test = X[train_index], X[test_index]
y_train, y_test = y[train_index], y[test_index]
rk_cv = RepeatedKFold(n_splits = 4, n_repeats = 2, random_state = 123)
from sklearn.model_selection import KFold
rk_cv = KFold(n_splits = 4)
splits = rk_cv.split(X)
for train_index, test_index in rk_cv.split(X):
print(“TRAIN:”, train_index, “TEST:”, test_index)
X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]]) # create an array
y = np.array([1, 2, 3, 4]) # Create another array
kf = KFold(n_splits=2) # Define the split - into 2 folds
kf.get_n_splits(X) # returns the number of splitting iterations in the cross-validator
for train_index, test_index in kf.split(X):
X_train, X_test = X[train_index], X[test_index]
y_train, y_test = y[train_index], y[test_index]
X_train
X_test
y_train
y_test
for train_index, test_index in kf.split(X):
X_train, X_test = X[train_index], X[test_index]
X_train
X_test
from sklearn.datasets import make_regression
import numpy as np
#=== set parameters for data generation ===#
n_samples, n_features, n_informative, noise = 2000, 20, 15, 2
rng = np.random.RandomState(8934)
#=== generate synthetic data ===#
XT, y = make_regression(
n_samples,
n_features,
n_informative = n_informative,
noise = noise,
random_state = rng
)
T = XT[:, 0] # first column as the treatment variable
X = XT[:, 1:] # the rest as X
rk_cv = RepeatedKFold(n_splits = 4, n_repeats = 2, random_state = 123)
from sklearn.model_selection import KFold
rk_cv = KFold(n_splits = 4)
splits = rk_cv.split(X)
for train_index, test_index in rk_cv.split(X):
X_train, X_test = X[train_index], X[test_index]
View(X_test)
View(X_test)
X_train.shape
for train_index, test_index in rk_cv.split(X):
X_train, X_test = X[train_index], X[test_index]
X_train
splits = rk_cv.split(X)
next(splits)
next(splits)
next(splits)
next(splits)
next(splits)
splits = rk_cv.split(X)
a = next(splits)
View(a)
for train, test in rk_cv.split(X):
X_train, X_test = X[train], X[test]
splits.__next()__
splits.__next__()
a = splits.__next__()
View(a)
a[1]
a[2]
a[0]
a[1]
a = list(splits)
View(a)
a.shape
a.shape)_
a.shape()
a.len
View(a)
a[0]
a[0][0]
rk_cv.n_splits
rk_cv = RepeatedKFold(n_splits = 4, n_repeats = 3, random_state = 123)
rk_cv.n_splits
rk_cv = RepeatedKFold(n_splits = 4, n_repeats = 3, random_state = 123)
rk_cv.n_splits
rk_cv = RepeatedKFold(n_splits = 4, n_repeats = 3, random_state = 123)
rk_cv.get_n_splits
rk_cv.get_n_splits.n_splits
rk_cv.get_n_splits(.n_splits))
rk_cv.get_n_splits()
