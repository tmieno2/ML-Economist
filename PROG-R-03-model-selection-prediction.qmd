# Model Selection (Prediction)

When you use a DML approach, the first stage estimations are prediction tasks. In this section, R and python codes (and the `reticulate` version of the python codes) to select a model for the first stage estimations are presented.

## Python implementation

Suppose you are considering multiple classes of estimators with various sets of hyper-parameter values for each model class. We can use `GridSearchCVList()` from the `econml` pacakge to conduct cross-validation to see which model works the best in predicting $E[Y|X, W]$ and $E[T|X, W]$ in terms of CV MSE. `GridSearchCVList()` is an extension of `GridSearchCV()` from the `sklearn` package. While `GridSearchCV()` conducts CV for a <span style="color:blue"> single</span> model class with various sets of hyper-parameter values, `GridSearchCVList()` conducts CV for <span style="color:blue"> multiple</span> classes of models with various sets of hyper-parameter values.

Let's go through an example use of `GridSearchCVList()`. First, we import all the functions we will be using. 

```{python}
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.linear_model import Lasso
from sklearn.model_selection import RepeatedKFold
from econml.sklearn_extensions.model_selection import GridSearchCVList
```

Let's also generate synthetic dataset using `make_regression()`.

```{python}
from sklearn.datasets import make_regression
import numpy as np

#=== set parameters for data generation ===#
n_samples, n_features, n_informative, noise = 2000, 20, 15, 2
rng = np.random.RandomState(8934)

#=== generate synthetic data ===#
XT, y = make_regression(
  n_samples, 
  n_features, 
  n_informative = n_informative, 
  noise = noise, 
  random_state = rng
)

T = XT[:, 0] # first column as the treatment variable
X = XT[:, 1:] # the rest as X
```

Some of the key parameters for `GridSearchCVList()` are:

+ `estimator_list`: List of estimators. Each estimator needs to implement the scikit-learn estimator interface. 
+ `param_grid`: List of the name of parameters to tune with their values for each model. 
+ `cv`: Specification of how CV is conducted

Let's define them one by one.

> `estimator_list`

```{python}
est_list = [
    Lasso(max_iter=1000), 
    GradientBoostingRegressor(),
    RandomForestRegressor(min_samples_leaf = 5)
]
```

So, the model classes we consider are lasso, random forest, and boosted forest. Note that you can fix the value of parameters that you do not vary in CV. For example, `RandomForestRegressor(min_samples_leaf = 5)` sets `min_samples_leaf` at 5. 

> `param_grid`

```{python}
par_grid_list = [
    {"alpha": [0.001, 0.01, 0.1, 1, 10]},
    {"max_depth": [3, 5, None], "n_estimators": [50, 100, 200]},
    {"max_depth": [3, 5, 10], "max_features": [3, 5, 10, 20]},
]
```

The $n$th entry of `param_grid` is for the $n$th entry of `estimator_list`. For example, two hyper-parameters will be tried for `RandomForestRegressor()`: `max_depth` and `max_features`. The complete combinations of the values from the parameters will be evaluated. For example, here are all the set of parameter values tried for `RandomForestRegressor()`.

```{r}
#| echo: false 
max_depth <- c(3, 5, 10)
max_features <- c(3, 5, 10, 20)
expand.grid(max_depth = max_depth, max_features = max_features)
```

> `cv`

```{python}
rk_cv = RepeatedKFold(n_splits = 4, n_repeats = 3, random_state = 123)

#=== check the number of splits ===#  
rk_cv.get_n_splits()
```

Cross-validation specification can be done using the `sklearn.model_selection` module. Here, we are using `RepeatedKFold`, and it is a 4-fold CV repeated 2 times.

Let's now specify `GridSearchCVList()` using the above parameters.

```{python}
first_stage = GridSearchCVList(
    estimator_list = est_list,
    param_grid_list= par_grid_list,
    cv = rk_cv,
    scoring = "neg_mean_squared_error"
)
```

::: {.column-margin}
[scoring](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)
:::
  
We now conduct CV with the `fit()` method and then access the best estimator by referring to `best_estimator_` attribute.

```{python}
#| cache: true 
model_y = first_stage.fit(X, y).best_estimator_
model_y
```

We do the same for $T$.

```{python}
#| cache: true 
model_t = first_stage.fit(X, T).best_estimator_
model_t
```

We can now run DML with the best first stage models like below.

```{python}
#| eval: false 

#=== set up a linear DML ===#
est = LinearDML(
    model_y=model_y, 
    model_t=model_t
)

#=== train ===#
est.fit(Y, T, X=X, W=X)
```

## `reticulate` implementation in R

::: {.column-margin}
**Packages to load for replication**

```{r}
#| include: false

library(data.table)
library(reticulate)
library(mlr3verse)
library(tidyverse)
```

```{r}
#| eval: false
library(data.table)
library(reticulate)
library(mlr3verse)
library(tidyverse)
```
:::

Here is the R codes that does the same thing as above using the `reticulate` package without needing to interact with Python console directly.

```{r}
#| eval: false 

#--------------------------
# Import modules  
#--------------------------
library(reticulate)
sk_ensemble <- import("sklearn.ensemble") 
sk_lm <- import("sklearn.linear_model")
sk_ms <- import("sklearn.model_selection")
econml_sk_ms <- import("econml.sklearn_extensions.model_selection")
econml_dml <- import("econml.dml")

sk_data <- import("sklearn.datasets")
make_regression <- sk_data$make_regression
np <- import("numpy")

#--------------------------
# Generate synthetic data
#--------------------------
#=== set parameters for data generation ===#
n_samples <- 2000L
n_features <- 20L
n_informative <- 15L
noise <- 2L
rng <- np$random$RandomState(8934L)

#=== generate synthetic data ===#
data <-
  make_regression(
    n_samples, 
    n_features, 
    n_informative = n_informative, 
    noise = noise, 
    random_state = rng
  )

#=== Create data matrices ===#
XT <- data[[1]]
T <- XT[, 1]
X <- XT[, -1]
Y  <- data[[2]]

#--------------------------
# Set up GridSearchCVList
#--------------------------
#=== list of estimators ===#
est_list <- 
  c(
    sk_lm$Lasso(max_iter=1000L), 
    sk_ensemble$GradientBoostingRegressor(),
    sk_ensemble$RandomForestRegressor(min_samples_leaf = 5L)
  ) 

#=== list of parameter values ===#
par_grid_list <- 
  list(
    list(alpha = c(0.001, 0.01, 0.1, 1, 10)),
    list(
      max_depth = as.integer(c(3, 5, 10)), 
      n_estimators = as.integer(c(50, 100, 200))
    ),
    list(
      max_depth = as.integer(c(3, 5, 10)), 
      max_features = as.integer(c(3, 5, 10, 20))
    )
  )

#=== CV specification ===#
rk_cv <- 
  sk_ms$RepeatedKFold(
    n_splits = 4L, 
    n_repeats = 2L, 
    random_state = 123L
  )

#=== set up a GridSearchCVList ===#
first_stage <-
  econml_sk_ms$GridSearchCVList(
    estimator_list = est_list,
    param_grid_list = par_grid_list,
    cv = rk_cv
  )

#--------------------------
# Run CV
#--------------------------
#=== Y ===#
model_y <- first_stage$fit(X, Y)$best_estimator_

#=== T ===#
model_t <- first_stage$fit(X, T)$best_estimator_

#--------------------------
# DML
#--------------------------
#=== set up a linear DML ===#
est = econml_dml$LinearDML(
    model_y = model_y, 
    model_t = model_t
)

#=== train ===#
est$fit(Y, T, X=X, W=X)
```

## R implementation using `mlr3`

You can use `benchmark_grid()` and `benchmark()` to imitate `GridSearchCVList()` from the Python `econml` package with a bit of extra coding. 

Let's use `mtcars` data for demonstration.

```{r}
#=== load the mtcars data ===#
data("mtcars", package = "datasets")

reg_task <-
  TaskRegr$new(
    id = "regression",
    backend = mtcars,
    target = "mpg"
  )
```

We can create a list of learners that vary in the value of hyper-parameters we would like to tune. Suppose you are interested in using random forest and extreme gradient boosting. We can create such a list for each of them and then combine. Let's do that for random forest first. It is much like what we did for `GridSearchCVList()` above.

First create a sequence of values for parameters to be tuned. 

```{r}
#=== sequence of values ===#
mtry_seq <- c(1, 3, 5)
min_node_size_seq <- c(5, 10, 20)
```

We can create a grid of values ourselves. 

```{r}
(
search_space_ranger <- 
  data.table::CJ(
    mtry = mtry_seq,
    min_node_size = min_node_size_seq
  )
)
```

We now loop over the row of `search_space_ranger` to create learners with each pair (row) stored in `search_space_ranger`.

```{r}
lrn_list_ranger <-
  lapply(
    seq_len(nrow(search_space_ranger)),
    function(x) {
      lrn(
        "regr.ranger", 
        mtry = search_space_ranger[x, mtry], 
        min.node.size = search_space_ranger[x, min_node_size]
      )
    }
  )
```

We can do the same for xgboost.

```{r}
#=== sequence of values ===#
nrounds_seq <- c(100, 400, by = 100)
eta_seq <- seq(0.01, 1, length = 5)

#=== define the grid ===#
search_space_xgb <- 
  data.table::CJ(
    nrounds = nrounds_seq,
    eta = eta_seq
  )

lrn_list_xgboost <-
  lapply(
    seq_len(nrow(search_space_xgb)),
    function(x) {
      lrn(
        "regr.xgboost",
        nrounds = search_space_xgb[x, nrounds],
        eta = search_space_xgb[x, eta]
      )
    }
  )
```

Now, we combine all the learners we want to evaluate.

```{r}
all_learners <- c(lrn_list_ranger,lrn_list_xgboost)
```

We need all the learners to use the same resampled datasets. So, we need to used an instantiated resampling. 

```{r}
resampling <- rsmp("cv", folds = 3)
resampling$instantiate(reg_task)
```

We can then supply `all_learners` to `benchmark_grid()` along with a task and resampling method, and ten call `benchmark()` on it.

```{r}
#=== benchmark design ===#
benchmark_design <- 
  benchmark_grid(
    task = reg_task, 
    learners = all_learners,
    resampling = resampling
  )

#=== benchmark implementation ===#
bmr_results <- benchmark(benchmark_design)
```

We can calculate MSE for each of the learners using the `aggregate()` method with `msr("regr.mse")`.

```{r}
(
bmr_perf <- bmr_results$aggregate(msr("regr.mse"))
)
```

We then pick the best learner which minimized `regr.mse`.

```{r}
#=== get the index ===#
best_nr <- as.data.table(bmr_perf) %>% 
  .[which.min(regr.mse), nr]

#=== get the best learner by the index ===#
best_learner <- all_learners[[best_nr]] 
``` 








