# Local Linear Forest

Local linear forest is an extension of random forest (RF) and also a generalized random forest (GRF) [@friedberg2020local]. We first start from how RF make predictions and then move on to how local linear forest make predictions.

Suppose $T$ tress have been built after a random forest model is trained on a dataset. Now, let $\eta_{i,t}(X)$ takes $1$ if observation $i$ belongs to the same leaf as $X$ in tree $t$, where $X$ is a vector of covariates ($K$ variables). Then, the RF's predicted value of $y$ conditional on a particular value of $X$ (say, $X_0$) can be written as follows:

$$
\begin{aligned}
\hat{y}(X_0) = \frac{1}{T} \cdot\sum_{t=1}^T\sum_{i=1}^N \frac{\eta_{i,t}(X_0)}{\sum_{i=1}^N\eta_{i,t}(X_0)}\cdot y_i
\end{aligned}
$$

Note that $\sum_{i=1}^N\eta_{i,t}(X_0)$ represents the number of observations in the same leaf as $X_0$. Therefore, $\sum_{i=1}^N \frac{\eta_{i,t}(X_0)}{\sum_{i=1}^N\eta_{i,t}(X_0)}\cdot y_i$ is the average value of $y$ of the leaf that $X_0$ belongs to. So, while looking slightly complicated, it is the average value of $y$ of the tree $X_0$ belongs to averaged across the trees. 

We can switch the summations like this,

$$
\begin{aligned}
\hat{y}(X_0) = \sum_{i=1}^N \cdot\frac{1}{T}\sum_{t=1}^T\frac{\eta_{i,t}(X_0)}{\sum_{i=1}^N\eta_{i,t}(X_0)}\cdot y_i
\end{aligned}
$$

Let $\alpha(X_i, X_0)$ denote $\frac{1}{T}\sum_{t=1}^T\frac{\eta_{i,t}(X_0)}{\sum_{i=1}^N\eta_{i,t}(X_0)}$. Then, we can rewrite the above equation as

$$
\begin{aligned}
\hat{y}(X_0) = \sum_{i=1}^N \alpha(X_i,X_0) \cdot y_i
\end{aligned}
$$

It is easy to show that $\hat{y}(X_0)$ is a solution to the following minimization problem.

$$
\begin{aligned}
Min_{\theta} \sum_{i=1}^N \alpha(X_i,X_0)\cdot[y_i -\theta]^2
\end{aligned}
$$ {#eq-ll-constant}

In this formulation of the problem, $\alpha(X_i,X_0)$ can be considered the weight given to observation $i$. By definition,

+ $0 \leq \alpha(X_i,X_0) \leq 1$
+ $\sum_{i=1}^N \alpha(X_i,X_0) = 1$

You may notice that @eq-ll-constant is actually a special case of local constant regression (see @sec-local-reg) where the individual weights are $\alpha(X_i, X_0)$. Roughly speaking, $\alpha(X_i, X_0)$ measures how often observation $i$ share the same leaves as the evaluation point ($X_0$) across $T$ trees. So, it measures how similar $X_i$ is to $X_0$ in the RF way, but not based on euclidean distance (which is subject to curse of dimensionality). So, RF is actually a local <span style='color:blue'>constant</span> regression with a special way of distributing weights to the individual observations. This interpretation leads to a natural extension. Why don't we solve local <span style='color:blue'>linear</span> regression problem instead, which would be more appropriate if $y$ is a smooth function of $X$? 

Rewriting @eq-ll-constant as a  local linear regression problem.

$$
\begin{aligned}
Min_{\mu, \beta} \sum_{i=1}^N \alpha(X_i,X_0)\cdot[y_i -\mu - (X_i - X_0)\beta]^2 
\end{aligned}
$$ {#eq-ll}

where $\mu$ is a scalar and $\beta$ is a vector of parameters ($K \times 1$). 

This approach was proposed by Bloniarz et al. (2016) and they showed modest improvement over RF. Local linear forest by @friedberg2020local differ from this approach in two important ways.

:::{.callout-important}

## Local linear forest 

1. Modify the splitting process in a way that the resulting splitting rules (and thus weights) are more suitable to the second stage local linear regression   
2. At the local linear regression stage, use ridge regularization
::: 

Let's look at the first modification. In RF, when deciding how to split a node (parent node), we choose a split that solves the following problem:

$$
\begin{aligned}
\sum_{i\in C_1}(Y_i - \bar{Y_1}) + \sum_{i\in C_2}(Y_i - \bar{Y_2})
\end{aligned}
$$

where $C_1$ and $C_2$ are child nodes, and $\bar{Y_1}$ and $\bar{Y_2}$ are the mean value of the outcome for $C_1$ and $C_2$, respectively. Instead, local linear forest by @friedberg2020local first regresses $Y$ on $X$ using ridge regression using the observations in the parent node, finds the residuals, and then uses the residuals in place of $Y$ itself.

Now, let's look at the second modification. Local linear forest implemented by the `grf` package adds the ridge penalty to avoid over-fitting and solve the following problem:

$$
\begin{aligned}
Min_{\mu, \beta} \sum_{i=1}^N \alpha(X_i,X_0)\cdot[y_i -\mu - (X_i - X_0)\beta]^2 + \lambda||\beta||^2_2
\end{aligned}
$$ {#eq-ll-ridge}










