<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-0.9.629">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>c04-grf</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>


<script src="C04-grf_files/libs/clipboard/clipboard.min.js"></script>
<script src="C04-grf_files/libs/quarto-html/quarto.js"></script>
<script src="C04-grf_files/libs/quarto-html/popper.min.js"></script>
<script src="C04-grf_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="C04-grf_files/libs/quarto-html/anchor.min.js"></script>
<link href="C04-grf_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="C04-grf_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="C04-grf_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="C04-grf_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="C04-grf_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
</div>
<main class="content page-columns page-full" id="quarto-document-content">



<section id="generalized-random-forest" class="level1 page-columns page-full">
<h1>Generalized Random Forest</h1>
<p><span class="citation" data-cites="athey2019generalized">@athey2019generalized</span></p>
<section id="grf-in-a-nutshell" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="grf-in-a-nutshell">GRF in a nutshell</h2>
<p>Here, we follow the notations used in <span class="citation" data-cites="athey2019generalized">@athey2019generalized</span> as much as possible. Let, <span class="math inline">\(O_i\)</span> denote the entire data available.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>For random forest, <span class="math inline">\(O_i\)</span> is {<span class="math inline">\(Y_i\)</span>, <span class="math inline">\(X_i\)</span>} where <span class="math inline">\(Y_i\)</span> is the dependent variable and <span class="math inline">\(X_i\)</span> is a collection of independent variables. For causal forest, <span class="math inline">\(O_i\)</span> is {<span class="math inline">\(Y_i\)</span>, <span class="math inline">\(W_i\)</span>, <span class="math inline">\(X_i\)</span>}, where <span class="math inline">\(W_i\)</span> is the treatment variable.</p>
</div></div><p>Let <span class="math inline">\(\theta(X)\)</span> denote the statistics of interest (e.g., CATE for causal forest, conditional quantile for for quantile forest) and <span class="math inline">\(\nu(X)\)</span> denote any nuisance (you are not interested in it) statistics. Generalized random forest (GRF) solves the following problem to find the estimate of <span class="math inline">\(\theta\)</span> conditional on <span class="math inline">\(X_i= x\)</span>:</p>
<p><span id="eq-opt"><span class="math display">\[
\begin{aligned}
\theta(x),\nu(x) = argmin_{\theta,\nu} \sum_{i=1}^n \alpha_i(x)\Psi_{\theta, \nu}(O_i)^2
\end{aligned}
\tag{1}\]</span></span></p>
<p>where <span class="math inline">\(\Psi_{\theta, \nu}(O_i)\)</span> is a score function, and <span class="math inline">\(\alpha_i(x)\)</span> is the weight given to <span class="math inline">\(i\)</span>th observation.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Why is it called <span style="color:red"> generalized </span> random forest?</p>
</div>
</div>
<p>This is because depending on how the score function (<span class="math inline">\(\Psi_{\theta, \nu}(O_i)\)</span>) is defined, you can estimate <span style="color:blue"> a wide range of statistics using different approaches </span>under the <span style="color:blue"> same</span> estimation framework.</p>
<ul>
<li>Conditional expectation (<span class="math inline">\(E[Y|X]\)</span>)
<ul>
<li>Regression Forest (Random forest for regression)</li>
<li>Boosted Regression Fores</li>
</ul></li>
<li>Conditional average treatment effect (CATE)
<ul>
<li>Causal Forest</li>
<li>Instrumental Forest</li>
</ul></li>
<li>Conditional quantile <span class="math inline">\(Q_r(X)\)</span><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>
<ul>
<li>Quantile Forest</li>
</ul></li>
</ul>
<p>How can <a href="#eq-opt">Equation&nbsp;1</a> represents so many (very) different statistical approaches? It all boils down to how <span class="math inline">\(\Psi_{\theta, \nu}(O_i)\)</span> is specified. Here are some examples:</p>
<ul>
<li><span class="math inline">\(\Psi_{\theta, \nu}(Y_i, X_i) = Y_i - \theta(X)\)</span>: GRF is simply random forest</li>
<li><span class="math inline">\(\Psi_{\theta, \nu}(Y_i, X_i, T_i) = (Y_i - E[Y|X])- \theta(X)(T_i - E[T|X])\)</span>: GRF is causal forest</li>
<li><span class="math inline">\(\Psi_{\theta, \nu}(Y_i) = qI\{Y_i &gt; \theta\} - (1-q)I\{Y_i \leq \theta\}\)</span>: GRF is causal forest</li>
</ul>

<div class="no-row-height column-margin column-container"><div class="">
<p><span class="math inline">\(I\{\}\)</span> is an indicator function that takes 1 if the condition inside the curly brackets and 0 otherwise.</p>
</div></div><div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Why is it called generalized <span style="color:red"> random forest</span>?</p>
</div>
</div>
<p>GRF uses random forest to find the weights <span class="math inline">\(\alpha_i(x)\)</span>. Specifically, it trains a random forest in which the dependent variable is <span style="color:blue"> pseudo outcome (<span class="math inline">\(\rho_i\)</span>) </span>derived from the score function that is specific to the type of regression you are running. Based on the trees build, then <span class="math inline">\(\alpha_i(x)\)</span> is calculated as the proportion of the number of times observation <span class="math inline">\(i\)</span> ended up in the same terminal node (leaf) relative to the total number of observations that <span class="math inline">\(X = x\)</span> share leaves with for all the trees. Note that trees are build only once in GRF and it is used repeatedly when predicting <span class="math inline">\(\theta(X)\)</span> at particular values of <span class="math inline">\(X\)</span>. So, the forest it builds is applied <span style="color:blue"> globally</span>, but the weights obtained based on the forest are <span style="color:blue"> local </span>to the evaluation point (<span class="math inline">\(X_i = x\)</span>).</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>Orthogonal random forest (a forest-based heterogeneous treatment effect estimator like causal forest), on the other hand, build trees every time when predicting treatment effect <span class="math inline">\(\theta(X)\)</span> at particular values of <span class="math inline">\(X\)</span>, which is why orthogonal random forest takes a very long time especially when there are many evaluation points.</p>
</div></div><p>You probably have noticed the similarity in idea between GRF and generalized method moments (GMM). Indeed, GRF can also be considered as local GMM (see <strong>?@sec-local-reg</strong> to get a sense of what a local regression is like).</p>
</section>
<section id="random-forest-as-a-grf" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="random-forest-as-a-grf">Random forest as a GRF</h2>
<p>When <span class="math inline">\(\Psi_{\theta, \nu}(Y_i, X_i)\)</span> is set to <span class="math inline">\(Y_i - \theta(X)\)</span>, then GRF is RF.</p>
<p><span id="eq-rf"><span class="math display">\[
\begin{aligned}
\theta(x) = argmin_{\theta} \sum_{i=1}^n \alpha_i(x)[Y_i - \theta(X)]^2
\end{aligned}
\tag{2}\]</span></span></p>

<div class="no-row-height column-margin column-container"><div class="">
<p>No nuisance parameters (<span class="math inline">\(\nu(X)\)</span>) here.</p>
</div></div><p>Now, let’s consider building a forest to find <span class="math inline">\(\alpha_i(x)\)</span> in <a href="#eq-rf">Equation&nbsp;2</a>. For a given bootstrapped sample and set of variables randomly selected, GRF starts with solving the unweighted version of <a href="#eq-rf">Equation&nbsp;2</a>.</p>
<p><span id="eq-rf-initial"><span class="math display">\[
\begin{aligned}
\theta(x) = argmin_{\theta} \sum_{i=1}^n [Y_i - \theta(X)]^2
\end{aligned}
\tag{3}\]</span></span></p>
<p>The solution to this problem is simply the mean of <span class="math inline">\(Y\)</span>, which will be denoted as <span class="math inline">\(\bar{Y}_P\)</span>, where <span class="math inline">\(P\)</span> represents the parent node. Here, the parent node include all the data points as this is the first split.</p>
<p>Then the pseudo outcome (<span class="math inline">\(\rho_i\)</span>) that is used in splitting is</p>
<p><span class="math display">\[
\begin{aligned}
\rho_i = Y_i - \bar{Y}_P
\end{aligned}
\]</span></p>

<div class="no-row-height column-margin column-container"><div class="">
<p>In general,</p>
<p><span class="math display">\[
\begin{aligned}
\rho_i = - \xi^T A_P^{-1}\Psi_{\hat{\theta}_P, \hat{\nu}_P}(O_i)
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(A_P = \frac{1}{N_P} \sum_{i=1}^{N_P} \nabla \Psi_{\hat{\theta}_P, \hat{\nu}_P}(O_i)\)</span></p>
<p>Here,</p>
<ul>
<li><span class="math inline">\(\xi^T = 1\)</span></li>
<li><span class="math inline">\(\Psi_{\hat{\theta}_P, \hat{\nu}_P}(O_i) = Y_i - \theta_P\)</span></li>
</ul>
<p>Therefore,</p>
<p><span class="math display">\[
\begin{aligned}
A_P = \frac{1}{N_P} \sum_{i=1}^{N_P} \times (-1) = -1
\end{aligned}
\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[
\begin{aligned}
\rho_i = -1(-1)(Y_i - \theta_P) = Y_i - \bar{Y}_P
\end{aligned}
\]</span></p>
<p>since <span class="math inline">\(\theta_P = \bar{Y}_P\)</span>.</p>
</div></div><p>Now, a standard CART regression split is applied on the pseudo outcomes. That is, the variable-cutpoint combination that maximizes the following criteria is found in a greedy manner (see <strong>?@sec-rt</strong> for how a CART is build):</p>
<p><span class="math display">\[
\begin{aligned}
\tilde{\Delta}(C_1, C_2) = \frac{(\sum_{i \in C_1} \rho_i)^2}{N_{C_1}} + \frac{(\sum_{i \in C_2} \rho_i)^2}{N_{C_2}}
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span> represent two child node candidates for a given split. This is exactly the same as how the traditional RF builds trees.</p>
<p>Note that the pseudo outcomes are first summed and then squared in <span class="math inline">\((\sum_{i \in C_1} \rho_i)^2\)</span>. This is a similarity score. If the pseudo outcomes are similar to one another, then they do not cancel each other out, which leads to a higher similarity score. Maximizing the weighted sum of similarity scores from the two child node candidates means that you are trying to find a split so that each of the group have similar pseudo outcomes <span style="color:blue"> within </span>the group (which in turn means larger heterogeneity in pseudo outcomes <span style="color:blue"> between </span>the child nodes).</p>
<p>Once the best split is identified, each of the new child nodes is split following the exactly the same procedure. Splitting continues until one of the user-specified condition prevent a further splitting.</p>
<p>Many trees from bootstrapped samples are created (just like the regular random forest) and they form a random forest.</p>
<section id="prediction" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="prediction">Prediction</h3>
<p>While <span class="math inline">\(GRF\)</span> with <span class="math inline">\(\Psi_{\theta, \nu}(Y_i, X_i) = Y_i - \theta(X)\)</span> build trees in exactly the same manner as the traditional RF, its prediction of <span class="math inline">\(E[Y|X=x]\)</span> is a bit different.</p>
<p>Prediction of <span class="math inline">\(\theta\)</span> at <span class="math inline">\(X = x\)</span> follows <a href="#eq-opt">Equation&nbsp;1</a>. Suppose you have built <span class="math inline">\(T\)</span> trees based on the pseudo outcomes.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>In this case, pseudo outcomes coincide with the true outcomes of interest.</p>
</div></div><p>Each of the tree has its own splitting rules. For each of the <span class="math inline">\(T\)</span> trees, you can identify which leaf <span class="math inline">\(X=x\)</span> belongs to. Now, let <span class="math inline">\(\eta_{i,t}(X)\)</span> is 1 if observation <span class="math inline">\(i\)</span> belongs to the same leaf as <span class="math inline">\(X=x\)</span> in tree <span class="math inline">\(t\)</span>. Then the weight given to observation <span class="math inline">\(i\)</span> is</p>
<p><span class="math display">\[
\begin{aligned}
\alpha_i(x) = \frac{\sum_{t=1}^{T}\eta_{i,t}(x)}{\sum_{t=1}^T\sum_{i=1}^N \eta_{i,t}(x)}
\end{aligned}
\]</span></p>
<ul>
<li><span class="math inline">\(\sum_{t=1}^{T}\eta_{i,t}(x)\)</span>: the number of trees in which observation <span class="math inline">\(i\)</span> belongs to the same leaf as <span class="math inline">\(X=x\)</span></li>
<li><span class="math inline">\(\sum_{t=1}^T\sum_{i=1}^N \eta_{i,t}(x)\)</span> is the total number of observations that belong to the same leaf as <span class="math inline">\(X=x\)</span> across the <span class="math inline">\(T\)</span> trees.</li>
</ul>

<div class="no-row-height column-margin column-container"><div class="">
<p>Note that some trees do not even have observation <span class="math inline">\(i\)</span> as bootstrapped samples are used to build trees.</p>
</div></div><p>Solving <a href="#eq-rf">Equation&nbsp;2</a> with the weights,</p>
<p><span class="math display">\[
\begin{aligned}
\theta(x) &amp; = \sum_{i=1}^N \alpha_i(x)Y_i\\
          &amp; = \sum_{i=1}^N\frac{\sum_{t=1}^{T}\eta_{i,t}(x)Y_i}{\sum_{t=1}^T\sum_{i=1}^N \eta_{i,t}(x)}
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\(\sum_{t=1}^T\sum_{i=1}^N \eta_{i,t}(x)\)</span> is just a constant,</p>
<p><span class="math display">\[
\begin{aligned}
\theta(x) &amp; = \frac{\sum_{i=1}^N\sum_{t=1}^{T}\eta_{i,t}(x)Y_i}{\sum_{t=1}^T\sum_{i=1}^N \eta_{i,t}(x)} \\
          &amp; = \frac{N_1 \bar{Y}_1 + N_2 \bar{Y}_2 + \dots + N_T \bar{Y}_T}{N_1 + N_2 + \dots + N_T}
\end{aligned}
\]</span></p>
<p>So, the RF under the GRF framework uses the weighted (by sample size in the terminal nodes) average of the mean of <span class="math inline">\(Y\)</span> of the terminal nodes that <span class="math inline">\(x\)</span> belong to. This is a little bit different from typical RF implementation where the simple average is taken from the trees like below:</p>
<p><span class="math display">\[
\begin{aligned}
\theta(x) = \frac{\bar{Y}_1 + \bar{Y}_2 + \dots + \bar{Y}_T}{T}
\end{aligned}
\]</span></p>
</section>
</section>
<section id="causal-forest-as-a-grf" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="causal-forest-as-a-grf">Causal forest as a GRF</h2>
<p>When <span class="math inline">\(\Psi_{\theta, \nu}(Y_i, X_i, T_i) = (Y_i - E[Y|X])- \theta(X)(T_i - E[T|X])\)</span>, GRF is causal forest. In practice <span class="math inline">\(E[Y|X]\)</span> and <span class="math inline">\(E[T|X]\)</span> are first estimated using appropriate machine learning methods (e.g., lasso, random forest) in a cross-fitting manner and then the estimation of <span class="math inline">\(Y_i - E[Y|X]\)</span> and <span class="math inline">\(T_i - E[T|X]\)</span> are constructed. Let’s denote them by <span class="math inline">\(\hat{\tilde{Y}}_i\)</span> and <span class="math inline">\(\hat{\tilde{T}}_i\)</span>. Then the empirical score function is written as</p>
<p><span id="eq-cf-score"><span class="math display">\[
\begin{aligned}
\Psi_{\theta} = \hat{\tilde{Y}}_i- \theta(X)\hat{\tilde{T}}_i
\end{aligned}
\tag{4}\]</span></span></p>
<p>Then, the heterogeneous treatment effect (<span class="math inline">\(\theta(X)\)</span>) is estimated by solving the following problem:</p>
<p><span id="eq-cf-solve"><span class="math display">\[
\begin{aligned}
\hat{\theta}(X) = argmin_{\theta}\;\;\sum_{i=1}^N \alpha_i(x)[\hat{\tilde{Y_i}} - \theta\cdot \hat{\tilde{T_i}}]^2
\end{aligned}
\tag{5}\]</span></span></p>
<p>where <span class="math inline">\(\alpha_i(x)\)</span> is the weight obtained from the trees built using random forest on the pseudo outcomes that are derived from the score function (<a href="#eq-cf-score">Equation&nbsp;4</a>).</p>
<p>In building a tree, CF sets <span class="math inline">\(\theta_P\)</span> as a solution to the unweighted version of <a href="#eq-cf-solve">Equation&nbsp;5</a>.</p>
<p><span class="math display">\[
\begin{aligned}
\hat{\theta}(X) = \sum_{i=1}^N \hat{\tilde{T_i}}(\hat{\tilde{Y_i}}-\theta \hat{\tilde{T_i}})
\end{aligned}
\]</span></p>
<p>The pseudo outcome for CF is</p>
<p><span class="math display">\[
\begin{aligned}
\rho_i =
\end{aligned}
\]</span></p>

<div class="no-row-height column-margin column-container"><div class="">
<ul>
<li><span class="math inline">\(\xi^T = 1\)</span></li>
<li><span class="math inline">\(\Psi_{\hat{\theta}_P, \hat{\nu}_P}(O_i) = Y_i - \theta_P T\)</span></li>
</ul>
<p>Therefore,</p>
<p><span class="math display">\[
\begin{aligned}
\nabla \Psi_{\hat{\theta}_P} = -T
\end{aligned}
\]</span></p>
<p>, which leads to</p>
<p><span class="math display">\[
\begin{aligned}
A_P = \frac{1}{N_P} \sum_{i=1}^{N_P} \times (-T) = -T
\end{aligned}
\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[
\begin{aligned}
\rho_i = -1\cdot \frac{-1}{T}\cdot(Y_i - \theta_P T) = \frac{Y_i}{T} - \theta_P
\end{aligned}
\]</span></p>
</div></div></section>
<section id="honest-tree" class="level2">
<h2 class="anchored" data-anchor-id="honest-tree">Honest tree</h2>
</section>
</section>


<div id="quarto-appendix" class="default"><section class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1" role="doc-endnote"><p><span class="math inline">\(r \in [0, 1]\)</span> is a quantile<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      let href = ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>