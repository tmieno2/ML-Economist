# Model Selection (Prediction)

When you use a DML approach, the first stage estimations are prediction tasks. In this section, python codes to select the model for the first stage estimations are presented.

Suppose you are considering multiple class of estimators with various sets of hyper-parameter values for each class. We can use `GridSearchCVList()` from the `econml` pacakge to conduct cross-validation to see which model works the best in predicting $E[Y|X, W]$ and $E[T|X, W]$. `GridSearchCVList()` is an extension of `GridSearchCV()` from the `sklearn` package. While `GridSearchCV()` conducts CV for a <span style="color:blue"> single</span> model class with various sets of hyper-parameter values, `GridSearchCVList()` conducts CV for <span style="color:blue"> multiple</span> classes of models with various sets of hyper-parameter values.

Let's go through an example use of `GridSearchCVList()`. First, we import all the functions we will be using. 

```{python}
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.linear_model import Lasso
from sklearn.model_selection import RepeatedKFold
from econml.sklearn_extensions.model_selection import GridSearchCVList
```

Let's also generate synthetic dataset using 

```{python}
from sklearn.datasets import make_regression
import numpy as np

#=== set parameters for data generation ===#
n_samples, n_features, n_informative, noise = 2000, 20, 15, 2
rng = np.random.RandomState(8934)

#=== generate synthetic data ===#
XT, y = make_regression(
  n_samples, 
  n_features, 
  n_informative = n_informative, 
  noise = noise, 
  random_state = rng
)

T = XT[:, 0] # first column as the treatment variable
X = XT[:, 1:] # the rest as X
```

Some of the key parameters for `GridSearchCVList()` are:

+ `estimator_list`: List of estimators. Each estimator needs to implement the scikit-learn estimator interface. 
+ `param_grid`: List of the name of parameters to tune with their values for each model. 
+ `cv`: Specification of how CV is conducted

Let's define them one by one.

+ `estimator_list`

```{python}
est_list = [
    Lasso(max_iter=10000), 
    GradientBoostingRegressor(),
    RandomForestRegressor(min_samples_leaf = 5)
]
```

So, the model classes we consider are lasso, random forest, and boosted forest. Note that you can fix the value of parameters that you do not vary in CV. For example, `RandomForestRegressor(min_samples_leaf = 5)` sets `min_samples_leaf` at 5. 

+ `param_grid`

```{python}
par_grid_list = [
    {"alpha": [0.001, 0.01, 0.1, 1, 10]},
    {"max_depth": [3, 5, None], "n_estimators": [50, 100, 200]},
    {"max_depth": [3, 5, 10], "max_features": [3, 5, 10, 20]},
]
```

The $n$th entry of `param_grid` is for the $n$th entry of `estimator_list`. For example, two hyper-parameters will be tried for `RandomForestRegressor()`: `max_depth` and `max_features`. The complete combinations of the values from the parameters will be evaluated. For example, here are all the set of parameter values tried for `RandomForestRegressor()`.

```{r}
#| echo: false 
max_depth <- c(3, 5, 10)
max_features <- c(3, 5, 10, 20)
expand.grid(max_depth = max_depth, max_features = max_features)
```

+ `cv`

```{python}
rk_cv = RepeatedKFold(n_splits = 4, n_repeats = 2, random_state = 123)
```

Cross-validation specification can be done using the `sklearn.model_selection` module. Here, we are using `RepeatedKFold`, and it is a 4-fold CV repeated 2 times.

Let's now specify `GridSearchCVList()` using the above parameters.

```{python}
first_stage = GridSearchCVList(
    estimator_list = est_list,
    param_grid_list= par_grid_list,
    cv = rk_cv,
)
```

We now conduct CV with the `fit()` method.

```{python}
first_stage.fit(X, y)
```

We can access the best model by accessing the `best_estimator_` attribute. 

```{python}
first_stage.best_estimator_
```

Assign this to `model_y` for later use in DML estimation.

```{python}
model_y = first_stage.best_estimator_
```

We do the same for $T$.

```{python}
first_stage.fit(X, T)
model_t = first_stage.best_estimator_
```

We can now run DML with the best first stage model.

```{python}
#| eval: false 

#=== set up a linear DML ===#
est = LinearDML(
    model_y=model_y, 
    model_t=model_t
)

#=== train ===#
est.fit(Y, T, X=X, W=X)
```


